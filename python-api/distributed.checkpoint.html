
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Distributed Checkpoint - torch.distributed.checkpoint &#8212; PyTorch main documentation</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  <!--
    this give us a css class that will be invisible only if js is disabled
  -->
  <noscript>
    <style>
      .pst-js-only { display: none !important; }

    </style>
  </noscript>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=949a1ff5" />
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/katex.min.css" />
    <link rel="stylesheet" type="text/css" href="../_static/katex-math.css?v=91adb8b6" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="../_static/css/jit.css?v=8de1ea5d" />
    <link rel="stylesheet" type="text/css" href="../_static/css/custom.css?v=3ccf5357" />
    <link rel="stylesheet" type="text/css" href="../_static/css/custom2.css?v=baa440dc" />
  
  <!-- So that users can add custom icons -->
  <script src="../_static/scripts/fontawesome.js?digest=8878045cc6db502f8baf"></script>
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf" />

    <script src="../_static/documentation_options.js?v=a8da1a53"></script>
    <script src="../_static/doctools.js?v=9bcbadda"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=940804e7"></script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'python-api/distributed.checkpoint';</script>
    <script src="../_static/js/star-rating.js?v=8861fcb6"></script>
    <script src="../_static/js/send-feedback.js?v=5646bf45"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />

  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  <meta name="docsearch:version" content="main (2.6.0 )" />
<script src="https://code.jquery.com/jquery-3.7.1.min.js"></script>
<script type="text/javascript" src="../_static/js/send-feedback.js"></script>
<script type="text/javascript" src="../_static/js/star-rating.js"></script>
<script type="text/javascript" src="../_static/js/cookie-banner.js"></script>

<script async src="https://www.googletagmanager.com/gtag/js?id=G-TEST12345"></script>
    <script type="text/javascript">
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-TEST12345');
    </script>

  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
<body data-feedback-url="https://github.com/pytorch/pytorch">
  <div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>

          <li class="main-menu-item">
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Learn
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="">
                  <span class=dropdown-title>Get Started</span>
                  <p>Run PyTorch locally or get started quickly with one of the supported cloud platforms</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Tutorials</span>
                  <p>Whats new in PyTorch tutorials</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Learn the Basics</span>
                  <p>Familiarize yourself with PyTorch concepts and modules</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">PyTorch Recipes</span>
                  <p>Bite-size, ready-to-deploy PyTorch code examples</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Intro to PyTorch - YouTube Series</span>
                  <p>Master PyTorch basics with our engaging YouTube tutorial series</p>
                </a>
              </div>
            </div>
          </li>

          <li>
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Ecosystem
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Tools</span>
                  <p>Learn about the tools and frameworks in the PyTorch Ecosystem</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class=dropdown-title>Community</span>
                  <p>Join the PyTorch developer community to contribute, learn, and get your questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="" target="_blank">
                  <span class=dropdown-title>Forums</span>
                  <p>A place to discuss PyTorch code, issues, install, research</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class=dropdown-title>Developer Resources</span>
                  <p>Find resources and get questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Contributor Awards - 2024</span>
                  <p>Award winners announced at this year's PyTorch Conference</p>
                </a>
              </div>
            </div>
          </li>

          <li>
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Edge
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">About PyTorch Edge</span>
                  <p>Build innovative and privacy-aware AI experiences for edge devices</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">ExecuTorch</span>
                  <p>End-to-end solution for enabling on-device inference capabilities across mobile and edge devices</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">ExecuTorch Docs</span>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Docs
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">PyTorch</span>
                  <p>Explore the documentation for comprehensive guidance on how to use PyTorch</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">PyTorch Domains</span>
                  <p>Read the PyTorch Domains documentation to learn more about domain-specific libraries</p>
                </a>
              </div>
            </div>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Blogs & News
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">PyTorch Blog</span>
                  <p>Catch up on the latest technical news and happenings</p>
                </a>
                 <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Community Blog</span>
                  <p>Stories from the PyTorch ecosystem</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Videos</span>
                  <p>Learn about the latest PyTorch tutorials, new, and more </p>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Community Stories</span>
                  <p>Learn how our community solves real, everyday machine learning problems with PyTorch</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Events</span>
                  <p>Find events, webinars, and podcasts</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Newsletter</span>
                  <p>Stay up-to-date with the latest updates</p>
                </a>
            </div>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                About
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">PyTorch Foundation</span>
                  <p>Learn more about the PyTorch Foundation</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Governing Board</span>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Cloud Credit Program</span>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Technical Advisory Council</span>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Staff</span>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Contact Us</span>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
            <div class="no-dropdown">
              <a href="" data-cta="join">
                Become a Member
              </a>
            </div>
          </li>
          <li>
           <div class="main-menu-item">
             <a href="" class="github-icon">
             </a>
           </div>
          </li>
          <!--- TODO: This block adds the search icon to the nav bar. We will enable it later.
          <li>
            <div class="main-menu-item">
             <a href="" class="search-icon">
             </a>
            </div>
          </li>
          --->
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>
  </div>
</div>

  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <dialog id="pst-search-dialog">
    
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </dialog>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
<div class="bd-header__inner bd-page-width">
  <button class="pst-navbar-icon sidebar-toggle primary-toggle" aria-label="Site navigation">
    <span class="fa-solid fa-bars"></span>
  </button>
  
  
  <div class=" navbar-header-items__start">
    
      <div class="navbar-item">

  
    
  

<a class="navbar-brand logo" href="../index.html">
  
  
  
  
  
  
    <p class="title logo__title">Home</p>
  
</a></div>
    
  </div>
  
  <div class=" navbar-header-items">
    
    <div class="me-auto navbar-header-items__center">
      
        <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item ">
  <a class="nav-link nav-internal" href="index.html">
    Python API
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../notes/index.html">
    Developer Notes
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-external" href="https://pytorch.org/tutorials/">
    Tutorials
  </a>
</li>

  </ul>
</nav></div>
      
    </div>
    
    
    <div class="navbar-header-items__end">
      
        <div class="navbar-item navbar-persistent--container">
          

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button>
        </div>
      
      
        <div class="navbar-item">
<div class="version-switcher__container dropdown pst-js-only">
  <button id="pst-version-switcher-button-2"
    type="button"
    class="version-switcher__button btn btn-sm dropdown-toggle"
    data-bs-toggle="dropdown"
    aria-haspopup="listbox"
    aria-controls="pst-version-switcher-list-2"
    aria-label="Version switcher list"
  >
    Choose version  <!-- this text may get changed later by javascript -->
    <span class="caret"></span>
  </button>
  <div id="pst-version-switcher-list-2"
    class="version-switcher__menu dropdown-menu list-group-flush py-0"
    role="listbox" aria-labelledby="pst-version-switcher-button-2">
    <!-- dropdown will be populated by javascript on page load -->
  </div>
</div></div>
      
        <div class="navbar-item">

<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button></div>
      
        <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://x.com/PyTorch" title="X" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-x-twitter fa-lg" aria-hidden="true"></i>
            <span class="sr-only">X</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/pytorch/pytorch" title="GitHub" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">GitHub</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://pypi.org/project/torch" title="PyPI" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-custom fa-pypi fa-lg" aria-hidden="true"></i>
            <span class="sr-only">PyPI</span></a>
        </li>
</ul></div>
      
    </div>
    
  </div>
  
  
    <div class="navbar-persistent--mobile">

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button>
    </div>
  

  
    <button class="pst-navbar-icon sidebar-toggle secondary-toggle" aria-label="On this page">
      <span class="fa-solid fa-outdent"></span>
    </button>
  
</div>

    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
        
      
      <dialog id="pst-primary-sidebar-modal"></dialog>
      <div id="pst-primary-sidebar" class="bd-sidebar-primary bd-sidebar hide-on-wide">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
      <div class="sidebar-header-items__center">
        
          
          
            <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item ">
  <a class="nav-link nav-internal" href="index.html">
    Python API
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../notes/index.html">
    Developer Notes
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-external" href="https://pytorch.org/tutorials/">
    Tutorials
  </a>
</li>

  </ul>
</nav></div>
          
        
      </div>
    
    
    
      <div class="sidebar-header-items__end">
        
          <div class="navbar-item">
<div class="version-switcher__container dropdown pst-js-only">
  <button id="pst-version-switcher-button-3"
    type="button"
    class="version-switcher__button btn btn-sm dropdown-toggle"
    data-bs-toggle="dropdown"
    aria-haspopup="listbox"
    aria-controls="pst-version-switcher-list-3"
    aria-label="Version switcher list"
  >
    Choose version  <!-- this text may get changed later by javascript -->
    <span class="caret"></span>
  </button>
  <div id="pst-version-switcher-list-3"
    class="version-switcher__menu dropdown-menu list-group-flush py-0"
    role="listbox" aria-labelledby="pst-version-switcher-button-3">
    <!-- dropdown will be populated by javascript on page load -->
  </div>
</div></div>
        
          <div class="navbar-item">

<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button></div>
        
          <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://x.com/PyTorch" title="X" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-x-twitter fa-lg" aria-hidden="true"></i>
            <span class="sr-only">X</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/pytorch/pytorch" title="GitHub" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">GitHub</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://pypi.org/project/torch" title="PyPI" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-custom fa-pypi fa-lg" aria-hidden="true"></i>
            <span class="sr-only">PyPI</span></a>
        </li>
</ul></div>
        
      </div>
    
  </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
      <div class="sidebar-primary-item">
<div id="ethical-ad-placement"
      class="flat"
      data-ea-publisher="readthedocs"
      data-ea-type="readthedocs-sidebar"
      data-ea-manual="true">
</div></div>
  </div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        
          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item">

<nav aria-label="Breadcrumb" class="d-print-none">
  <ul class="bd-breadcrumbs">
    
    <li class="breadcrumb-item breadcrumb-home">
      <a href="../index.html" class="nav-link" aria-label="Home">
        <i class="fa-solid fa-home"></i>
      </a>
    </li>
    <li class="breadcrumb-item active" aria-current="page"><span class="ellipsis">Distributed Checkpoint - torch.distributed.checkpoint</span></li>
  </ul>
</nav>
</div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">
<div class="rating">
    Rate this Page
    <div class="stars">
        
        <span class="star" data-value="1">★</span>
        
        <span class="star" data-value="2">★</span>
        
        <span class="star" data-value="3">★</span>
        
        <span class="star" data-value="4">★</span>
        
        <span class="star" data-value="5">★</span>
        
    </div>
</div>
</div>
      
    </div>
  
</div>
</div>
              
              
              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section id="distributed-checkpoint-torch-distributed-checkpoint">
<h1>Distributed Checkpoint - torch.distributed.checkpoint<a class="headerlink" href="#distributed-checkpoint-torch-distributed-checkpoint" title="Link to this heading">#</a></h1>
<p>Distributed Checkpoint (DCP) support loading and saving models from multiple ranks in parallel.
It handles load-time resharding which enables saving in one cluster topology and loading into another.</p>
<p>DCP is different than <cite>torch.save</cite> and <cite>torch.load</cite> in a few significant ways:</p>
<ul class="simple">
<li><p>It produces multiple files per checkpoint, with at least one per rank.</p></li>
<li><p>It operates in place, meaning that the model should allocate its data first and DCP uses that storage instead.</p></li>
</ul>
<p>The entrypoints to load and save a checkpoint are the following:</p>
<section id="additional-resources">
<h2>Additional resources:<a class="headerlink" href="#additional-resources" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><a class="reference external" href="https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html">Getting Started with Distributed Checkpoint (DCP)</a></p></li>
<li><p><a class="reference external" href="https://pytorch.org/tutorials/recipes/distributed_async_checkpoint_recipe.html">Asynchronous Saving with Distributed Checkpoint (DCP)</a></p></li>
<li><p><a class="reference external" href="https://github.com/pytorch/torchtitan/blob/main/docs/checkpoint.md">TorchTitan Checkpointing Docs</a></p></li>
<li><p><a class="reference external" href="https://github.com/pytorch/torchtitan/blob/main/torchtitan/checkpoint.py">TorchTitan DCP Implementation</a></p></li>
</ul>
<dl class="py function" id="module-torch.distributed.checkpoint">
<dt class="sig sig-object py" id="torch.distributed.checkpoint.state_dict_saver.save">
<span class="sig-prename descclassname"><span class="pre">torch.distributed.checkpoint.state_dict_saver.</span></span><span class="sig-name descname"><span class="pre">save</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">state_dict</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">checkpoint_id</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">storage_writer</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">planner</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">process_group</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torch/distributed/checkpoint/state_dict_saver.html#save"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributed/checkpoint/state_dict_saver.py#L57"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.checkpoint.state_dict_saver.save" title="Link to this definition">#</a></dt>
<dd><p>Save a distributed model in SPMD style.</p>
<p>This function is different from <code class="docutils literal notranslate"><span class="pre">torch.save()</span></code> as it handles
<code class="docutils literal notranslate"><span class="pre">ShardedTensor</span></code> , and <code class="docutils literal notranslate"><span class="pre">DTensor</span></code> by having each rank only save their local shards.</p>
<p>For each <code class="docutils literal notranslate"><span class="pre">Stateful</span></code> object (having both a <code class="docutils literal notranslate"><span class="pre">state_dict</span></code> and a <code class="docutils literal notranslate"><span class="pre">load_state_dict</span></code>),
save will call <code class="docutils literal notranslate"><span class="pre">state_dict</span></code> before serialization.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>There is no guarantees of Backwards Compatibility across PyTorch versions
for saved state_dicts.</p>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>If using the <cite>process_group</cite> argument, make sure that only its ranks
call <cite>save_state_dict</cite> and that all data in state_dict belong to it.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>When saving checkpoint for FSDP’s <cite>ShardingStrategy.HYBRID_SHARD</cite>, only one of
the shard_group should be calling <cite>save_state_dict</cite> and the corresponding process
group needs to be passed in.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<dl class="simple">
<dt>If no process group is available, this function assumes the intention is to save the</dt><dd><p>state_dict in the local process.</p>
</dd>
</dl>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>state_dict</strong> (<em>Dict</em><em>[</em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><em>str</em></a><em>, </em><em>Any</em><em>]</em>) – The state_dict to save.</p></li>
<li><p><strong>checkpoint_id</strong> (<em>Union</em><em>[</em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><em>str</em></a><em>, </em><a class="reference external" href="https://docs.python.org/3/library/os.html#os.PathLike" title="(in Python v3.13)"><em>os.PathLike</em></a><em>, </em><em>None</em><em>]</em>) – The ID of this checkpoint instance. The meaning of the checkpoint_id
depends on the storage. It can be a path to a folder or to a file.
It can also be a key if the storage is a key-value store.
(Default: <code class="docutils literal notranslate"><span class="pre">None</span></code>)</p></li>
<li><p><strong>storage_writer</strong> (<em>Optional</em><em>[</em><a class="reference internal" href="#torch.distributed.checkpoint.StorageWriter" title="torch.distributed.checkpoint.StorageWriter"><em>StorageWriter</em></a><em>]</em>) – Instance of StorageWriter used to perform writes. If this is not
specified, DCP will automatically infer the writer based on the
checkpoint_id. If checkpoint_id is also None, an exception will
be raised. (Default: <code class="docutils literal notranslate"><span class="pre">None</span></code>)</p></li>
<li><p><strong>planner</strong> (<em>Optional</em><em>[</em><a class="reference internal" href="#torch.distributed.checkpoint.SavePlanner" title="torch.distributed.checkpoint.SavePlanner"><em>SavePlanner</em></a><em>]</em>) – Instance of SavePlanner. If this is not specificed, the default
planner will be used. (Default: <code class="docutils literal notranslate"><span class="pre">None</span></code>)</p></li>
<li><p><strong>process_group</strong> (<em>Optional</em><em>[</em><em>ProcessGroup</em><em>]</em>) – ProcessGroup to be used for cross-rank synchronization.
(Default: <code class="docutils literal notranslate"><span class="pre">None</span></code>)</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Metadata object for the saved checkpoint.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>Metadata</p>
</dd>
</dl>
<p class="rubric">Example</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">my_model</span> <span class="o">=</span> <span class="n">MyModule</span><span class="p">()</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">state_dict</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;model&quot;</span><span class="p">:</span> <span class="n">my_model</span><span class="p">}</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">fs_storage_writer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">checkpoint</span><span class="o">.</span><span class="n">FileSystemWriter</span><span class="p">(</span><span class="s2">&quot;/checkpoint/1&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">checkpoint</span><span class="o">.</span><span class="n">save</span><span class="p">(</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">state_dict</span><span class="o">=</span><span class="n">state_dict</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">storage_writer</span><span class="o">=</span><span class="n">fs_storage_writer</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">)</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>save_state_dict uses collectives to coordinate writes across ranks.
For NCCL-based process groups, internal tensor representations of
objects must be moved to the GPU device before communication takes place.
In this case, the device used is given by <code class="docutils literal notranslate"><span class="pre">torch.cuda.current_device()</span></code>
and it is the user’s responsibility to ensure that this is set so that
each rank has an individual GPU, via <code class="docutils literal notranslate"><span class="pre">torch.cuda.set_device()</span></code>.</p>
</div>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="torch.distributed.checkpoint.state_dict_saver.async_save">
<span class="sig-prename descclassname"><span class="pre">torch.distributed.checkpoint.state_dict_saver.</span></span><span class="sig-name descname"><span class="pre">async_save</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">state_dict</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">checkpoint_id</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">storage_writer</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">planner</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">process_group</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torch/distributed/checkpoint/state_dict_saver.html#async_save"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributed/checkpoint/state_dict_saver.py#L161"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.checkpoint.state_dict_saver.async_save" title="Link to this definition">#</a></dt>
<dd><p>Asynchronous version of <code class="docutils literal notranslate"><span class="pre">save</span></code>. This code first de-stages the state_dict on to the
staging storage (defaults to CPU memory), and then calls the <cite>save</cite> in a separate thread.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>This feature is experimental and subject to change.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>state_dict</strong> (<em>Dict</em><em>[</em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><em>str</em></a><em>, </em><em>Any</em><em>]</em>) – The state_dict to save.</p></li>
<li><p><strong>checkpoint_id</strong> (<em>Union</em><em>[</em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><em>str</em></a><em>, </em><a class="reference external" href="https://docs.python.org/3/library/os.html#os.PathLike" title="(in Python v3.13)"><em>os.PathLike</em></a><em>, </em><em>None</em><em>]</em>) – The ID of this checkpoint instance. The meaning of the checkpoint_id
depends on the storage. It can be a path to a folder or to a file.
It can also be a key if the storage is a key-value store.
(Default: <code class="docutils literal notranslate"><span class="pre">None</span></code>)</p></li>
<li><p><strong>storage_writer</strong> (<em>Optional</em><em>[</em><a class="reference internal" href="#torch.distributed.checkpoint.StorageWriter" title="torch.distributed.checkpoint.StorageWriter"><em>StorageWriter</em></a><em>]</em>) – Instance of StorageWriter used to perform ‘stage’ and  ‘save’. If
this is not specified, DCP will automatically infer the writer based on the
checkpoint_id. If checkpoint_id is also None, an exception will
be raised. (Default: <code class="docutils literal notranslate"><span class="pre">None</span></code>)</p></li>
<li><p><strong>planner</strong> (<em>Optional</em><em>[</em><a class="reference internal" href="#torch.distributed.checkpoint.SavePlanner" title="torch.distributed.checkpoint.SavePlanner"><em>SavePlanner</em></a><em>]</em>) – Instance of SavePlanner. If this is not specificed, the default
planner will be used. (Default: <code class="docutils literal notranslate"><span class="pre">None</span></code>)</p></li>
<li><p><strong>process_group</strong> (<em>Optional</em><em>[</em><em>ProcessGroup</em><em>]</em>) – ProcessGroup to be used for cross-rank synchronization.
(Default: <code class="docutils literal notranslate"><span class="pre">None</span></code>)</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A future holding the resultant Metadata object from <cite>save</cite>.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference internal" href="futures.html#torch.futures.Future" title="torch.futures.Future">Future</a></p>
</dd>
</dl>
<p class="rubric">Example</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">my_model</span> <span class="o">=</span> <span class="n">MyModule</span><span class="p">()</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">state_dict</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;model&quot;</span><span class="p">:</span> <span class="n">my_model</span><span class="p">}</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">fs_storage_writer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">checkpoint</span><span class="o">.</span><span class="n">FileSystemWriter</span><span class="p">(</span><span class="s2">&quot;/checkpoint/1&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">checkpoint_future</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">checkpoint</span><span class="o">.</span><span class="n">async_save</span><span class="p">(</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">state_dict</span><span class="o">=</span><span class="n">state_dict</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">storage_writer</span><span class="o">=</span><span class="n">fs_storage_writer</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># ... do some work ...</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">checkpoint_future</span><span class="o">.</span><span class="n">result</span><span class="p">()</span>
</pre></div>
</div>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="torch.distributed.checkpoint.state_dict_saver.save_state_dict">
<span class="sig-prename descclassname"><span class="pre">torch.distributed.checkpoint.state_dict_saver.</span></span><span class="sig-name descname"><span class="pre">save_state_dict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">state_dict</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">storage_writer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">process_group</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">coordinator_rank</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">no_dist</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">planner</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torch/distributed/checkpoint/state_dict_saver.html#save_state_dict"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributed/checkpoint/state_dict_saver.py#L29"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.checkpoint.state_dict_saver.save_state_dict" title="Link to this definition">#</a></dt>
<dd><p>This method is deprecated. Please switch to ‘save’.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><em>Metadata</em></p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="torch.distributed.checkpoint.state_dict_loader.load">
<span class="sig-prename descclassname"><span class="pre">torch.distributed.checkpoint.state_dict_loader.</span></span><span class="sig-name descname"><span class="pre">load</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">state_dict</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">checkpoint_id</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">storage_reader</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">planner</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">process_group</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torch/distributed/checkpoint/state_dict_loader.html#load"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributed/checkpoint/state_dict_loader.py#L51"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.checkpoint.state_dict_loader.load" title="Link to this definition">#</a></dt>
<dd><p>Load a distributed <code class="docutils literal notranslate"><span class="pre">state_dict</span></code> in SPMD style.</p>
<p>Each rank will try to read the least amount of data necessary
to fullfill the requested <cite>state_dict</cite>. When loading <code class="xref py py-class docutils literal notranslate"><span class="pre">ShardedTensor</span></code>
or <code class="xref py py-class docutils literal notranslate"><span class="pre">DTensor</span></code> instances, each rank only reads data for their local shards.</p>
<p>For each <code class="docutils literal notranslate"><span class="pre">Stateful</span></code> object (having both a <code class="docutils literal notranslate"><span class="pre">state_dict</span></code> and a <code class="docutils literal notranslate"><span class="pre">load_state_dict</span></code>),
load will first call <code class="docutils literal notranslate"><span class="pre">state_dict</span></code> before attempting deserialization, followed by
<code class="docutils literal notranslate"><span class="pre">load_state_dict</span></code> once the deserialization is complete.
For each non-<code class="docutils literal notranslate"><span class="pre">Stateful</span></code> object, load will deserailize the object, and then replace
it in the <code class="docutils literal notranslate"><span class="pre">state_dict</span></code> with the deserialized object.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>All tensors in <code class="docutils literal notranslate"><span class="pre">state_dict</span></code> must be allocated on their
destination device <em>prior to</em> calling this function.</p>
<p>All non-tensor data is loaded using <cite>torch.load()</cite> and modified in place
on state_dict.</p>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Users must call <cite>load_state_dict</cite> on the root module to ensure load
pos-processing and non-tensor data properly propagates.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>state_dict</strong> (<em>Dict</em><em>[</em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><em>str</em></a><em>, </em><em>Any</em><em>]</em>) – The state_dict to save.</p></li>
<li><p><strong>checkpoint_id</strong> (<em>Union</em><em>[</em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><em>str</em></a><em>, </em><a class="reference external" href="https://docs.python.org/3/library/os.html#os.PathLike" title="(in Python v3.13)"><em>os.PathLike</em></a><em>, </em><em>None</em><em>]</em>) – The ID of this checkpoint instance. The meaning of the checkpoint_id
depends on the storage. It can be a path to a folder or to a file.
It can also be a key if the storage is a key-value store.
(Default: <code class="docutils literal notranslate"><span class="pre">None</span></code>)</p></li>
<li><p><strong>storage_reader</strong> (<em>Optional</em><em>[</em><a class="reference internal" href="#torch.distributed.checkpoint.StorageReader" title="torch.distributed.checkpoint.StorageReader"><em>StorageReader</em></a><em>]</em>) – Instance of StorageWriter used to perform reads. If this is not
specified, DCP will automatically infer the reader based on the
checkpoint_id. If checkpoint_id is also None, an exception will
be raised. (Default: <code class="docutils literal notranslate"><span class="pre">None</span></code>)</p></li>
<li><p><strong>planner</strong> (<em>Optional</em><em>[</em><a class="reference internal" href="#torch.distributed.checkpoint.LoadPlanner" title="torch.distributed.checkpoint.LoadPlanner"><em>LoadPlanner</em></a><em>]</em>) – Instance of LoadPlanner. If this is not specificed, the default
planner will be used. (Default: <code class="docutils literal notranslate"><span class="pre">None</span></code>)</p></li>
<li><p><strong>process_group</strong> (<em>Optional</em><em>[</em><em>ProcessGroup</em><em>]</em>) – ProcessGroup to be used for cross-rank synchronization.
(Default: <code class="docutils literal notranslate"><span class="pre">None</span></code>)</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>None.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>None</p>
</dd>
</dl>
<dl>
<dt>Examples</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">my_model</span> <span class="o">=</span> <span class="n">MyModule</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">Adagrad</span><span class="p">(</span><span class="n">my_model</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model_state_dict</span> <span class="o">=</span> <span class="n">my_model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">fs_storage_reader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">checkpoint</span><span class="o">.</span><span class="n">FileSystemReader</span><span class="p">(</span><span class="s2">&quot;/checkpoint/1&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">checkpoint</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">state_dict</span><span class="o">=</span><span class="n">model_state_dict</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">storage_reader</span><span class="o">=</span><span class="n">fs_storage_reader</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">)</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># module.load_state_dict() function might have customized steps</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># to flush the state_dict, must call it to</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># ensure correct behavior.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">my_model</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">model_state_dict</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>load_state_dict uses collectives to coordinate reads across ranks.
For NCCL-based process groups, internal tensor representations of
objects must be moved to the GPU device before communication takes place.
In this case, the device used is given by <code class="docutils literal notranslate"><span class="pre">torch.cuda.current_device()</span></code>
and it is the user’s responsibility to ensure that this is set so that each
rank has an individual GPU, via <code class="docutils literal notranslate"><span class="pre">torch.cuda.set_device()</span></code>.</p>
</div>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="torch.distributed.checkpoint.state_dict_loader.load_state_dict">
<span class="sig-prename descclassname"><span class="pre">torch.distributed.checkpoint.state_dict_loader.</span></span><span class="sig-name descname"><span class="pre">load_state_dict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">state_dict</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">storage_reader</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">process_group</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">coordinator_rank</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">no_dist</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">planner</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torch/distributed/checkpoint/state_dict_loader.html#load_state_dict"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributed/checkpoint/state_dict_loader.py#L24"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.checkpoint.state_dict_loader.load_state_dict" title="Link to this definition">#</a></dt>
<dd><p>This method is deprecated. Please switch to ‘load’.</p>
<dl class="field-list simple">
</dl>
</dd></dl>

<p>The following module is also useful for additional customization of the staging mechanisms used for asynchronous checkpointing (<cite>torch.distributed.checkpoint.async_save</cite>):</p>
<dl class="py class" id="module-torch.distributed.checkpoint.staging">
<dt class="sig sig-object py" id="torch.distributed.checkpoint.staging.AsyncStager">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torch.distributed.checkpoint.staging.</span></span><span class="sig-name descname"><span class="pre">AsyncStager</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torch/distributed/checkpoint/staging.html#AsyncStager"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributed/checkpoint/staging.py#L15"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.checkpoint.staging.AsyncStager" title="Link to this definition">#</a></dt>
<dd><p>This protocol is meant to provide customization and extensibility for dcp.async_save, allowing users
to customize how data is staged previous to executing the usual dcp.save path in parallel.
The expected order of operations (concretely defined in <cite>torch.distributed.state_dict_saver.async_save</cite>)
is the following:</p>
<ol class="arabic simple">
<li><dl class="simple">
<dt>AsyncStager.stage_data(state_dict):</dt><dd><p>This call gives the AsyncStager the opportunity to ‘stage’
the state_dict. The expectation and purpose of staging in this context is to create a “training-safe”
representation of the state dict, meaning that any updates to module data after staging is complete
should not be reflected in the state dict returned from this method. For example, in the default
case a copy of the entire state dict is created on CPU RAM and returned here, allowing users
to continue training without risking changes to data which is being serialized.</p>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>dcp.save is called on the state_dict returned from stage in parallel. This call is responsible</dt><dd><p>for serializing the state_dict and writing it to storage.</p>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>If AsyncStager.should_synchronize_after_execute is True, this method will be called immediately after</dt><dd><p>the serialization thread starts and before returning from dcp.async_save. If this is set to False,
the assumption is the user has defined a custom synchronization point for the the purpose of further
optimizing save latency in the training loop (for example, by overlapping staging with the
forward/backward pass), and it is the respondsibility of the user to call <cite>AsyncStager.synchronize_staging</cite>
at the appropriate time.</p>
</dd>
</dl>
</li>
</ol>
<dl class="py property">
<dt class="sig sig-object py" id="torch.distributed.checkpoint.staging.AsyncStager.should_synchronize_after_execute">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">should_synchronize_after_execute</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><span class="pre">bool</span></a></em><a class="headerlink" href="#torch.distributed.checkpoint.staging.AsyncStager.should_synchronize_after_execute" title="Link to this definition">#</a></dt>
<dd><p>Whether to synchronize after executing the stage.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributed.checkpoint.staging.AsyncStager.stage">
<span class="sig-name descname"><span class="pre">stage</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">state_dict</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torch/distributed/checkpoint/staging.html#AsyncStager.stage"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributed/checkpoint/staging.py#L54"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.checkpoint.staging.AsyncStager.stage" title="Link to this definition">#</a></dt>
<dd><p>Returns a “staged” copy of <cite>state_dict</cite>. The expectation of the staged copy is that it is
innoculated from any updates incurred after the stage call is complete.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Dict" title="(in Python v3.13)"><em>Dict</em></a>[<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)">str</a>, <em>StatefulT</em> | <a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.13)"><em>Any</em></a>]</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributed.checkpoint.staging.AsyncStager.synchronize_staging">
<span class="sig-name descname"><span class="pre">synchronize_staging</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torch/distributed/checkpoint/staging.html#AsyncStager.synchronize_staging"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributed/checkpoint/staging.py#L63"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.checkpoint.staging.AsyncStager.synchronize_staging" title="Link to this definition">#</a></dt>
<dd><p>In the case <cite>stage</cite> is async in some way, this method should be called to ensure staging
is complete and it is safe to begin modifying the original <cite>state_dict</cite></p>
<dl class="field-list simple">
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="torch.distributed.checkpoint.staging.BlockingAsyncStager">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torch.distributed.checkpoint.staging.</span></span><span class="sig-name descname"><span class="pre">BlockingAsyncStager</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">cache_staged_state_dict</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">type_check</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torch/distributed/checkpoint/staging.html#BlockingAsyncStager"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributed/checkpoint/staging.py#L70"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.checkpoint.staging.BlockingAsyncStager" title="Link to this definition">#</a></dt>
<dd><p>An implementation of AsyncStager which stages the state_dict on CPU RAM and blocks until the copy is complete.
This implementation also provides an option to optimize stage latency using pinned memory.</p>
<p>N.B. synchronize_staging is a no-op in this case.</p>
<dl class="field-list simple">
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="torch.distributed.checkpoint.staging.BlockingAsyncStager.stage">
<span class="sig-name descname"><span class="pre">stage</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">state_dict</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torch/distributed/checkpoint/staging.html#BlockingAsyncStager.stage"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributed/checkpoint/staging.py#L102"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.checkpoint.staging.BlockingAsyncStager.stage" title="Link to this definition">#</a></dt>
<dd><p>Returns a copy of <cite>state_dict</cite> on the CPU.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Dict" title="(in Python v3.13)"><em>Dict</em></a>[<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)">str</a>, <em>StatefulT</em> | <a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.13)"><em>Any</em></a>]</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributed.checkpoint.staging.BlockingAsyncStager.synchronize_staging">
<span class="sig-name descname"><span class="pre">synchronize_staging</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torch/distributed/checkpoint/staging.html#BlockingAsyncStager.synchronize_staging"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributed/checkpoint/staging.py#L114"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.checkpoint.staging.BlockingAsyncStager.synchronize_staging" title="Link to this definition">#</a></dt>
<dd><p>No-op function, since staging is blocking.</p>
<dl class="field-list simple">
</dl>
</dd></dl>

</dd></dl>

<p>In addition to the above entrypoints, <cite>Stateful</cite> objects, as described below, provide additional customization during saving/loading
.. automodule:: torch.distributed.checkpoint.stateful</p>
<dl class="py class">
<dt class="sig sig-object py" id="torch.distributed.checkpoint.stateful.Stateful">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torch.distributed.checkpoint.stateful.</span></span><span class="sig-name descname"><span class="pre">Stateful</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torch/distributed/checkpoint/stateful.html#Stateful"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributed/checkpoint/stateful.py#L8"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.checkpoint.stateful.Stateful" title="Link to this definition">#</a></dt>
<dd><p>Stateful protocol for objects that can be checkpointed and restored.</p>
<dl class="py method">
<dt class="sig sig-object py" id="torch.distributed.checkpoint.stateful.Stateful.load_state_dict">
<span class="sig-name descname"><span class="pre">load_state_dict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">state_dict</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torch/distributed/checkpoint/stateful.html#Stateful.load_state_dict"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributed/checkpoint/stateful.py#L31"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.checkpoint.stateful.Stateful.load_state_dict" title="Link to this definition">#</a></dt>
<dd><p>Restore the object’s state from the provided state_dict.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>state_dict</strong> (<a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Dict" title="(in Python v3.13)"><em>Dict</em></a><em>[</em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><em>str</em></a><em>, </em><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.13)"><em>Any</em></a><em>]</em>) – The state dict to restore from</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributed.checkpoint.stateful.Stateful.state_dict">
<span class="sig-name descname"><span class="pre">state_dict</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torch/distributed/checkpoint/stateful.html#Stateful.state_dict"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributed/checkpoint/stateful.py#L14"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.checkpoint.stateful.Stateful.state_dict" title="Link to this definition">#</a></dt>
<dd><p>Objects should return their state_dict representation as a dictionary.
The output of this function will be checkpointed, and later restored in
<cite>load_state_dict()</cite>.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Because of the inplace nature of restoring a checkpoint, this function
is also called during <cite>torch.distributed.checkpoint.load</cite>.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>The objects state dict</p>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p>Dict</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<p>This <a class="reference external" href="https://github.com/pytorch/pytorch/blob/main/torch/distributed/checkpoint/examples/fsdp_checkpoint_example.py">example</a> shows how to use Pytorch Distributed Checkpoint to save a FSDP model.</p>
<p>The following types define the IO interface used during checkpoint:</p>
<dl class="py class">
<dt class="sig sig-object py" id="torch.distributed.checkpoint.StorageReader">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torch.distributed.checkpoint.</span></span><span class="sig-name descname"><span class="pre">StorageReader</span></span><a class="reference internal" href="../_modules/torch/distributed/checkpoint/storage.html#StorageReader"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributed/checkpoint/storage.py#L166"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.checkpoint.StorageReader" title="Link to this definition">#</a></dt>
<dd><p>Interface used by <code class="docutils literal notranslate"><span class="pre">load_state_dict</span></code> to read from storage.</p>
<p>One StorageReader instance acts as both the coordinator and the follower
in a distributed checkpoint. As part of initialization, each instance
is told its role.</p>
<p>A subclass should expected the following sequence of calls by <code class="docutils literal notranslate"><span class="pre">load_state_dict</span></code>:</p>
<ol class="arabic simple" start="0">
<li><p>(all ranks) set checkpoint_id if users pass a valid checkpoint_id.</p></li>
<li><p>(all ranks) read_metadata()</p></li>
<li><p>(all ranks) set_up_storage_reader()</p></li>
<li><p>(all ranks) prepare_local_plan()</p></li>
<li><p>(coordinator) prepare_global_plan()</p></li>
<li><p>(all ranks) read_data()</p></li>
</ol>
<dl class="py method">
<dt class="sig sig-object py" id="torch.distributed.checkpoint.StorageReader.prepare_global_plan">
<em class="property"><span class="pre">abstract</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">prepare_global_plan</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">plans</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torch/distributed/checkpoint/storage.html#StorageReader.prepare_global_plan"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributed/checkpoint/storage.py#L238"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.checkpoint.StorageReader.prepare_global_plan" title="Link to this definition">#</a></dt>
<dd><p>Perform centralized planning of storage loading.</p>
<p>This method is only called on the coordinator instance.</p>
<p>While this method can produce a completely different plan, the preferred
way is to store storage specific data in LoadPlan::storage_data.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>plans</strong> (<a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.List" title="(in Python v3.13)"><em>List</em></a><em>[</em><a class="reference internal" href="#torch.distributed.checkpoint.LoadPlan" title="torch.distributed.checkpoint.planner.LoadPlan"><em>LoadPlan</em></a><em>]</em>) – A list of <code class="docutils literal notranslate"><span class="pre">LoadPlan</span></code> instances, one for each rank.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A list of transformed <code class="docutils literal notranslate"><span class="pre">LoadPlan</span></code> after storage global planning</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.List" title="(in Python v3.13)"><em>List</em></a>[<a class="reference internal" href="#torch.distributed.checkpoint.LoadPlan" title="torch.distributed.checkpoint.planner.LoadPlan"><em>LoadPlan</em></a>]</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributed.checkpoint.StorageReader.prepare_local_plan">
<em class="property"><span class="pre">abstract</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">prepare_local_plan</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">plan</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torch/distributed/checkpoint/storage.html#StorageReader.prepare_local_plan"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributed/checkpoint/storage.py#L223"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.checkpoint.StorageReader.prepare_local_plan" title="Link to this definition">#</a></dt>
<dd><p>Perform storage-specific local planning.</p>
<p>While this method can produce a completely different plan, the recommended
way is to store storage specific data in LoadPlan::storage_data.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>plan</strong> (<a class="reference internal" href="#torch.distributed.checkpoint.LoadPlan" title="torch.distributed.checkpoint.LoadPlan"><em>LoadPlan</em></a>) – The local plan from the <code class="docutils literal notranslate"><span class="pre">LoadPlan</span></code> in use.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A transformed <code class="docutils literal notranslate"><span class="pre">LoadPlan</span></code> after storage local planning</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference internal" href="#torch.distributed.checkpoint.LoadPlan" title="torch.distributed.checkpoint.planner.LoadPlan"><em>LoadPlan</em></a></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributed.checkpoint.StorageReader.read_data">
<em class="property"><span class="pre">abstract</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">read_data</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">plan</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">planner</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torch/distributed/checkpoint/storage.html#StorageReader.read_data"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributed/checkpoint/storage.py#L255"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.checkpoint.StorageReader.read_data" title="Link to this definition">#</a></dt>
<dd><p>Read all items from <code class="docutils literal notranslate"><span class="pre">plan</span></code> using <code class="docutils literal notranslate"><span class="pre">planner</span></code> to resolve the data.</p>
<p>A subclass should call <code class="docutils literal notranslate"><span class="pre">LoadPlanner::load_bytes</span></code> to deserialize a BytesIO
object into the right place.</p>
<p>A subclass should call <code class="docutils literal notranslate"><span class="pre">LoadPlanner::resolve_tensor</span></code> to get access to the
tensors that in should load data into.</p>
<p>It’s the StorageLayer responsibility to properly schedule any cross device copies
required.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>plan</strong> (<a class="reference internal" href="#torch.distributed.checkpoint.LoadPlan" title="torch.distributed.checkpoint.LoadPlan"><em>LoadPlan</em></a>) – The local plan to execute on</p></li>
<li><p><strong>planner</strong> (<a class="reference internal" href="#torch.distributed.checkpoint.LoadPlanner" title="torch.distributed.checkpoint.LoadPlanner"><em>LoadPlanner</em></a>) – The planner object to use to resolve items.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A future that completes once all reads are finished.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference internal" href="futures.html#torch.futures.Future" title="torch.jit.Future"><em>Future</em></a>[None]</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributed.checkpoint.StorageReader.read_metadata">
<em class="property"><span class="pre">abstract</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">read_metadata</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torch/distributed/checkpoint/storage.html#StorageReader.read_metadata"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributed/checkpoint/storage.py#L202"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.checkpoint.StorageReader.read_metadata" title="Link to this definition">#</a></dt>
<dd><p>Read the checkpoint metadata.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>The metadata object associated with the checkpoint being loaded.</p>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p><em>Metadata</em></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributed.checkpoint.StorageReader.reset">
<em class="property"><span class="pre">abstract</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">reset</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">checkpoint_id</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torch/distributed/checkpoint/storage.html#StorageReader.reset"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributed/checkpoint/storage.py#L184"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.checkpoint.StorageReader.reset" title="Link to this definition">#</a></dt>
<dd><p>Calls to indicates a brand new checkpoint read is going to happen.
A checkpoint_id may be present if users set the checkpoint_id for
this checkpoint read. The meaning of the checkpiont_id is
storage-dependent. It can be a path to a folder/file or a key for
a key-value storage.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>checkpoint_id</strong> (<em>Union</em><em>[</em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><em>str</em></a><em>, </em><a class="reference external" href="https://docs.python.org/3/library/os.html#os.PathLike" title="(in Python v3.13)"><em>os.PathLike</em></a><em>, </em><em>None</em><em>]</em>) – The ID of this checkpoint instance. The meaning of the checkpoint_id
depends on the storage. It can be a path to a folder or to a file.
It can also be a key if the storage is more like a key-value store.
(Default: <code class="docutils literal notranslate"><span class="pre">None</span></code>)</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributed.checkpoint.StorageReader.set_up_storage_reader">
<em class="property"><span class="pre">abstract</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">set_up_storage_reader</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">metadata</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">is_coordinator</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torch/distributed/checkpoint/storage.html#StorageReader.set_up_storage_reader"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributed/checkpoint/storage.py#L212"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.checkpoint.StorageReader.set_up_storage_reader" title="Link to this definition">#</a></dt>
<dd><p>Initialize this instance.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>metadata</strong> (<em>Metadata</em>) – The metadata schema to use.</p></li>
<li><p><strong>is_coordinator</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><em>bool</em></a>) – Whether this instance is responsible for coordinating
the checkpoint.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributed.checkpoint.StorageReader.validate_checkpoint_id">
<em class="property"><span class="pre">abstract</span><span class="w"> </span><span class="pre">classmethod</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">validate_checkpoint_id</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">checkpoint_id</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torch/distributed/checkpoint/storage.html#StorageReader.validate_checkpoint_id"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributed/checkpoint/storage.py#L277"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.checkpoint.StorageReader.validate_checkpoint_id" title="Link to this definition">#</a></dt>
<dd><p>Check if the given checkpoint_id is supported by the stroage. This allow
us to enable automatic storage selection.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)">bool</a></p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="torch.distributed.checkpoint.StorageWriter">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torch.distributed.checkpoint.</span></span><span class="sig-name descname"><span class="pre">StorageWriter</span></span><a class="reference internal" href="../_modules/torch/distributed/checkpoint/storage.html#StorageWriter"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributed/checkpoint/storage.py#L27"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.checkpoint.StorageWriter" title="Link to this definition">#</a></dt>
<dd><p>Interface used by <code class="docutils literal notranslate"><span class="pre">save_state_dict</span></code> to write to storage.</p>
<p>One StorageWriter instance acts as both the coordinator and the follower
in a distributed checkpoint. As part of initialization, each instance
is told its role.</p>
<p>A subclass should expect the following sequence of calls.</p>
<ol class="arabic simple" start="0">
<li><p>(all ranks) set checkpoint_id if users pass a valid checkpoint_id.</p></li>
<li><p>(all ranks) set_up_storage_writer()</p></li>
<li><p>(all ranks) prepare_local_plan()</p></li>
<li><p>(coordinator) prepare_global_plan()</p></li>
<li><p>(all ranks) write_data()</p></li>
<li><p>(coordinator) finish()</p></li>
</ol>
<dl class="py method">
<dt class="sig sig-object py" id="torch.distributed.checkpoint.StorageWriter.finish">
<em class="property"><span class="pre">abstract</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">finish</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">metadata</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">results</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torch/distributed/checkpoint/storage.html#StorageWriter.finish"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributed/checkpoint/storage.py#L129"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.checkpoint.StorageWriter.finish" title="Link to this definition">#</a></dt>
<dd><p>Write the metadata and marks the current checkpoint as successful.</p>
<p>The actual format/schema used for serializing <cite>metadata</cite> is an
implementation detail. The only requirement is that it’s recoverable
in to the same object graph.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>metadata</strong> (<em>Metadata</em>) – metadata for the new checkpoint</p></li>
<li><p><strong>results</strong> (<a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.List" title="(in Python v3.13)"><em>List</em></a><em>[</em><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.List" title="(in Python v3.13)"><em>List</em></a><em>[</em><em>WriteResult</em><em>]</em><em>]</em>) – A list of WriteResults from all ranks.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>None</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>None</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributed.checkpoint.StorageWriter.prepare_global_plan">
<em class="property"><span class="pre">abstract</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">prepare_global_plan</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">plans</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torch/distributed/checkpoint/storage.html#StorageWriter.prepare_global_plan"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributed/checkpoint/storage.py#L88"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.checkpoint.StorageWriter.prepare_global_plan" title="Link to this definition">#</a></dt>
<dd><p>Perform centralized planning of storage.</p>
<p>This method is only called on the coordinator instance.</p>
<p>While this method can produce a completely different plan, the preferred
way is to store storage specific data in SavePlan::storage_data.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>plans</strong> (<a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.List" title="(in Python v3.13)"><em>List</em></a><em>[</em><a class="reference internal" href="#torch.distributed.checkpoint.SavePlan" title="torch.distributed.checkpoint.planner.SavePlan"><em>SavePlan</em></a><em>]</em>) – A list of <code class="docutils literal notranslate"><span class="pre">SavePlan</span></code> instances, one for each rank.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A list of transformed <code class="docutils literal notranslate"><span class="pre">SavePlan</span></code> after storage global planning</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.List" title="(in Python v3.13)"><em>List</em></a>[<a class="reference internal" href="#torch.distributed.checkpoint.SavePlan" title="torch.distributed.checkpoint.planner.SavePlan"><em>SavePlan</em></a>]</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributed.checkpoint.StorageWriter.prepare_local_plan">
<em class="property"><span class="pre">abstract</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">prepare_local_plan</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">plan</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torch/distributed/checkpoint/storage.html#StorageWriter.prepare_local_plan"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributed/checkpoint/storage.py#L73"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.checkpoint.StorageWriter.prepare_local_plan" title="Link to this definition">#</a></dt>
<dd><p>Perform storage-specific local planning.</p>
<p>While this method can produce a completely different plan, the recommended
way is to store storage specific data in SavePlan::storage_data.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>plan</strong> (<a class="reference internal" href="#torch.distributed.checkpoint.SavePlan" title="torch.distributed.checkpoint.SavePlan"><em>SavePlan</em></a>) – The local plan from the <code class="docutils literal notranslate"><span class="pre">SavePlanner</span></code> in use.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A transformed <code class="docutils literal notranslate"><span class="pre">SavePlan</span></code> after storage local planning</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference internal" href="#torch.distributed.checkpoint.SavePlan" title="torch.distributed.checkpoint.planner.SavePlan"><em>SavePlan</em></a></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributed.checkpoint.StorageWriter.reset">
<em class="property"><span class="pre">abstract</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">reset</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">checkpoint_id</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torch/distributed/checkpoint/storage.html#StorageWriter.reset"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributed/checkpoint/storage.py#L45"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.checkpoint.StorageWriter.reset" title="Link to this definition">#</a></dt>
<dd><p>Calls to indicates a brand new checkpoint write is going to happen.
A checkpoint_id may be present if users set the checkpoint_id for
this checkpoint write. The meaning of the checkpiont_id is
storage-dependent. It can be a path to a folder/file or a key for
a key-value storage.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>checkpoint_id</strong> (<em>Union</em><em>[</em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><em>str</em></a><em>, </em><a class="reference external" href="https://docs.python.org/3/library/os.html#os.PathLike" title="(in Python v3.13)"><em>os.PathLike</em></a><em>, </em><em>None</em><em>]</em>) – The ID of this checkpoint instance. The meaning of the checkpoint_id
depends on the storage. It can be a path to a folder or to a file.
It can also be a key if the storage is a key-value store.
(Default: <code class="docutils literal notranslate"><span class="pre">None</span></code>)</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributed.checkpoint.StorageWriter.set_up_storage_writer">
<em class="property"><span class="pre">abstract</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">set_up_storage_writer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">is_coordinator</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torch/distributed/checkpoint/storage.html#StorageWriter.set_up_storage_writer"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributed/checkpoint/storage.py#L63"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.checkpoint.StorageWriter.set_up_storage_writer" title="Link to this definition">#</a></dt>
<dd><p>Initialize this instance.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>is_coordinator</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><em>bool</em></a>) – Whether this instance is responsible for coordinating
the checkpoint.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributed.checkpoint.StorageWriter.storage_meta">
<span class="sig-name descname"><span class="pre">storage_meta</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torch/distributed/checkpoint/storage.html#StorageWriter.storage_meta"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributed/checkpoint/storage.py#L155"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.checkpoint.StorageWriter.storage_meta" title="Link to this definition">#</a></dt>
<dd><p>Return the storage-specific metadata. This is used to store additional information
in a checkpoint that can be useful for providing request-level observability. StorageMeta
is passed to the <code class="docutils literal notranslate"><span class="pre">SavePlanner</span></code> during save calls. Returns None by default.</p>
<p>TODO: provide an example</p>
<dl class="field-list simple">
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><em>StorageMeta</em> | None</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributed.checkpoint.StorageWriter.validate_checkpoint_id">
<em class="property"><span class="pre">abstract</span><span class="w"> </span><span class="pre">classmethod</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">validate_checkpoint_id</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">checkpoint_id</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torch/distributed/checkpoint/storage.html#StorageWriter.validate_checkpoint_id"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributed/checkpoint/storage.py#L146"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.checkpoint.StorageWriter.validate_checkpoint_id" title="Link to this definition">#</a></dt>
<dd><p>Check if the given checkpoint_id is supported by the stroage. This allow
us to enable automatic storage selection.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)">bool</a></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributed.checkpoint.StorageWriter.write_data">
<em class="property"><span class="pre">abstract</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">write_data</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">plan</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">planner</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torch/distributed/checkpoint/storage.html#StorageWriter.write_data"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributed/checkpoint/storage.py#L105"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.checkpoint.StorageWriter.write_data" title="Link to this definition">#</a></dt>
<dd><p>Write all items from <code class="docutils literal notranslate"><span class="pre">plan</span></code> using <code class="docutils literal notranslate"><span class="pre">planner</span></code> to resolve the data.</p>
<p>A subclass should call <code class="docutils literal notranslate"><span class="pre">SavePlanner::resolve_data</span></code> on each item
from the plan to get access to the underlying object to write.</p>
<p>Subclasses should lazily call <cite>resolve_data</cite> as it can allocate memory.
In case of tensors, make following assumptions:</p>
<ul class="simple">
<li><p>They might be on any device, including not matching the one on <code class="docutils literal notranslate"><span class="pre">WriteItem::tensor_data</span></code></p></li>
<li><p>They might be views or not contiguous. Only the projection needs to be saved.</p></li>
</ul>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>plan</strong> (<a class="reference internal" href="#torch.distributed.checkpoint.SavePlan" title="torch.distributed.checkpoint.SavePlan"><em>SavePlan</em></a>) – The save plan to execute.</p></li>
<li><p><strong>planner</strong> (<a class="reference internal" href="#torch.distributed.checkpoint.SavePlanner" title="torch.distributed.checkpoint.SavePlanner"><em>SavePlanner</em></a>) – Planner object to be used to resolve items to data.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A future that completes to a list of WriteResult</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference internal" href="futures.html#torch.futures.Future" title="torch.jit.Future"><em>Future</em></a>[<a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.List" title="(in Python v3.13)"><em>List</em></a>[<em>WriteResult</em>]]</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<p>The following types define the planner interface used during checkpoint:</p>
<dl class="py class">
<dt class="sig sig-object py" id="torch.distributed.checkpoint.LoadPlanner">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torch.distributed.checkpoint.</span></span><span class="sig-name descname"><span class="pre">LoadPlanner</span></span><a class="reference internal" href="../_modules/torch/distributed/checkpoint/planner.html#LoadPlanner"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributed/checkpoint/planner.py#L271"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.checkpoint.LoadPlanner" title="Link to this definition">#</a></dt>
<dd><p>Abstract class defining the protocol used by load_state_dict to plan the load process.</p>
<p>LoadPlanner are stateful objects that can be used to customize the whole load process.</p>
<p>LoadPlanner acts as an access proxy to the state_dict, so any transformation done to it
will be visible to the whole process.</p>
<p>A planner subclass can expect the following sequence of calls during load_state_dict:</p>
<ol class="arabic simple">
<li><dl class="simple">
<dt>set_up_planner - called on all ranks.</dt><dd><p>Signals the start of loading a checkpoint.</p>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>create_local_plan - called on all ranks.</dt><dd><p>Process the state_dict and produces a <cite>LoadPlan</cite> that will be sent for global planning.</p>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>create_global_plan - called on the coordinator rank only.</dt><dd><p>Takes the LoadPlan from all ranks and make any global decision.</p>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>load_bytes - called multiple times on each rank</dt><dd><p>This is called once per non-tensor value in state_dict.</p>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>resolve_tensor and commit_tensor - called multiple times on each rank</dt><dd><p>They are called in pair for each Tensor value in state_dict.</p>
</dd>
</dl>
</li>
</ol>
<p>Users are recommended to extend DefaultLoadPlanner instead of this interface directly as
most changes can be expressed by changes in a single method.</p>
<p>There are two usual patterns of extension:</p>
<p>Rewriting state_dict. This is the simplest way to extend the load process as it
doesn’t requite understanding the intrincacies of how LoadPlan works. We need
to keep a reference to the original state_dict as load happens in place so
we need to be able to perform it in place</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">class</span><span class="w"> </span><span class="nc">RenamePlanner</span><span class="p">(</span><span class="n">DefaultLoadPlanner</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span><span class="w"> </span><span class="nf">set_up_planner</span><span class="p">(</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="bp">self</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">state_dict</span><span class="p">:</span> <span class="n">STATE_DICT_TYPE</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">metadata</span><span class="p">:</span> <span class="n">Metadata</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">is_coordinator</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="bp">self</span><span class="o">.</span><span class="n">original_state_dict</span> <span class="o">=</span> <span class="n">state_dict</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">state_dict</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;foo_&quot;</span> <span class="o">+</span> <span class="n">k</span><span class="p">:</span> <span class="n">v</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">flatten_sharded_tensors</span><span class="p">:</span>
<span class="gp">&gt;&gt;&gt; </span>            <span class="n">state_dict</span> <span class="o">=</span> <span class="n">_flatten_sharded_tensors</span><span class="p">(</span><span class="n">state_dict</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">flatten_state_dict</span><span class="p">:</span>
<span class="gp">&gt;&gt;&gt; </span>            <span class="n">state_dict</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">mappings</span> <span class="o">=</span> <span class="n">flatten_state_dict</span><span class="p">(</span><span class="n">state_dict</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="bp">self</span><span class="o">.</span><span class="n">state_dict</span> <span class="o">=</span> <span class="n">state_dict</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="bp">self</span><span class="o">.</span><span class="n">metadata</span> <span class="o">=</span> <span class="n">metadata</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="bp">self</span><span class="o">.</span><span class="n">is_coordinator</span> <span class="o">=</span> <span class="n">is_coordinator</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span><span class="w"> </span><span class="nf">load_bytes</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">read_item</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="c1"># Remove the &quot;foo_&quot; prefix</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="bp">self</span><span class="o">.</span><span class="n">original_state_dict</span><span class="p">[</span><span class="n">read_item</span><span class="o">.</span><span class="n">dest_index</span><span class="o">.</span><span class="n">fqn</span><span class="p">[</span><span class="mi">4</span><span class="p">:]]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">weights_only</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
<p>Modifying resolve_tensor and commit_tensor to handle load time transformation.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">class</span><span class="w"> </span><span class="nc">MetaModelMaterialize</span><span class="p">(</span><span class="n">DefaultSavePlanner</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span><span class="w"> </span><span class="nf">resolve_tensor</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">read_item</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">tensor</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">resolve_tensor</span><span class="p">(</span><span class="n">read_item</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty_like</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s2">&quot;cpu&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span><span class="w"> </span><span class="nf">commit_tensor</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">read_item</span><span class="p">,</span> <span class="n">tensor</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="bp">self</span><span class="o">.</span><span class="n">state_dict</span><span class="p">[</span><span class="n">read_item</span><span class="o">.</span><span class="n">dest_index</span><span class="o">.</span><span class="n">fqn</span><span class="p">]</span> <span class="o">=</span> <span class="n">tensor</span>
</pre></div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="torch.distributed.checkpoint.LoadPlanner.commit_tensor">
<em class="property"><span class="pre">abstract</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">commit_tensor</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">read_item</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tensor</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torch/distributed/checkpoint/planner.html#LoadPlanner.commit_tensor"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributed/checkpoint/planner.py#L407"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.checkpoint.LoadPlanner.commit_tensor" title="Link to this definition">#</a></dt>
<dd><p>Call once the StorageReader finished loading data into <code class="docutils literal notranslate"><span class="pre">tensor</span></code>.</p>
<p>The provided tensor is the same one returned by the call to <code class="docutils literal notranslate"><span class="pre">resolve_tensor</span></code>.
This method is only needed if this LoadPlanner needs to post process <code class="docutils literal notranslate"><span class="pre">tensor</span></code> prior to
copying it back to the one in the state_dict.</p>
<p>The contents of tensor will follow its device synchronization model.</p>
<dl class="field-list simple">
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributed.checkpoint.LoadPlanner.create_global_plan">
<em class="property"><span class="pre">abstract</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">create_global_plan</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">global_plan</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torch/distributed/checkpoint/planner.html#LoadPlanner.create_global_plan"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributed/checkpoint/planner.py#L366"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.checkpoint.LoadPlanner.create_global_plan" title="Link to this definition">#</a></dt>
<dd><p>Compute the global load plan and return plans for each rank.</p>
<p>. N.B. This is called on the coordinator rank only</p>
<dl class="field-list simple">
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.List" title="(in Python v3.13)"><em>List</em></a>[<a class="reference internal" href="#torch.distributed.checkpoint.LoadPlan" title="torch.distributed.checkpoint.planner.LoadPlan"><em>LoadPlan</em></a>]</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributed.checkpoint.LoadPlanner.create_local_plan">
<em class="property"><span class="pre">abstract</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">create_local_plan</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torch/distributed/checkpoint/planner.html#LoadPlanner.create_local_plan"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributed/checkpoint/planner.py#L358"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.checkpoint.LoadPlanner.create_local_plan" title="Link to this definition">#</a></dt>
<dd><p>Create a LoadPlan based on state_dict and metadata provided by set_up_planner.</p>
<p>. N.B. This is called on every rank.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference internal" href="#torch.distributed.checkpoint.LoadPlan" title="torch.distributed.checkpoint.planner.LoadPlan"><em>LoadPlan</em></a></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributed.checkpoint.LoadPlanner.finish_plan">
<em class="property"><span class="pre">abstract</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">finish_plan</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">central_plan</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torch/distributed/checkpoint/planner.html#LoadPlanner.finish_plan"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributed/checkpoint/planner.py#L374"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.checkpoint.LoadPlanner.finish_plan" title="Link to this definition">#</a></dt>
<dd><p>Accept the plan from coordinator and return final LoadPlan.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference internal" href="#torch.distributed.checkpoint.LoadPlan" title="torch.distributed.checkpoint.planner.LoadPlan"><em>LoadPlan</em></a></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributed.checkpoint.LoadPlanner.load_bytes">
<em class="property"><span class="pre">abstract</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">load_bytes</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">read_item</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">value</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torch/distributed/checkpoint/planner.html#LoadPlanner.load_bytes"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributed/checkpoint/planner.py#L378"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.checkpoint.LoadPlanner.load_bytes" title="Link to this definition">#</a></dt>
<dd><p>Load the item described by <code class="docutils literal notranslate"><span class="pre">read_item``and</span> <span class="pre">``value</span></code>.</p>
<p>This method is expected to modify in-place the underlying state_dict.</p>
<p>The contents of <code class="docutils literal notranslate"><span class="pre">value</span></code> are defined by the SavePlanner used to produce
the checkpoint being loaded.</p>
<dl class="field-list simple">
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributed.checkpoint.LoadPlanner.resolve_bytes">
<span class="sig-name descname"><span class="pre">resolve_bytes</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">read_item</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torch/distributed/checkpoint/planner.html#LoadPlanner.resolve_bytes"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributed/checkpoint/planner.py#L389"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.checkpoint.LoadPlanner.resolve_bytes" title="Link to this definition">#</a></dt>
<dd><p>Return the BytesIO to be used by the StorageReader to load <cite>read_item</cite>.</p>
<p>The BytesIO should alias with one on the underlying state_dict as StorageReader will replace its contents.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><em>BytesIO</em></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributed.checkpoint.LoadPlanner.resolve_tensor">
<em class="property"><span class="pre">abstract</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">resolve_tensor</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">read_item</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torch/distributed/checkpoint/planner.html#LoadPlanner.resolve_tensor"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributed/checkpoint/planner.py#L397"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.checkpoint.LoadPlanner.resolve_tensor" title="Link to this definition">#</a></dt>
<dd><p>Return the tensor described by <code class="docutils literal notranslate"><span class="pre">read_item</span></code> to be used by the StorageReader to load <cite>read_item</cite>.</p>
<p>The tensor should alias with one on the underlying state_dict as StorageReader will replace its contents.
If, for any reason, that’s not possible, the planner can use the <code class="docutils literal notranslate"><span class="pre">commit_tensor</span></code> method to copy the data
back to the one in state_dict.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributed.checkpoint.LoadPlanner.set_up_planner">
<em class="property"><span class="pre">abstract</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">set_up_planner</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">state_dict</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">metadata</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">is_coordinator</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torch/distributed/checkpoint/planner.html#LoadPlanner.set_up_planner"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributed/checkpoint/planner.py#L345"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.checkpoint.LoadPlanner.set_up_planner" title="Link to this definition">#</a></dt>
<dd><p>Initialize this instance to load data into <code class="docutils literal notranslate"><span class="pre">state_dict</span></code>.</p>
<p>. N.B. This is called on every rank.</p>
<dl class="field-list simple">
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="torch.distributed.checkpoint.LoadPlan">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torch.distributed.checkpoint.</span></span><span class="sig-name descname"><span class="pre">LoadPlan</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">items</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><a class="reference internal" href="#torch.distributed.checkpoint.ReadItem" title="torch.distributed.checkpoint.planner.ReadItem"><span class="pre">torch.distributed.checkpoint.planner.ReadItem</span></a><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">storage_data</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">planner_data</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torch/distributed/checkpoint/planner.html#LoadPlan"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributed/checkpoint/planner.py#L102"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.checkpoint.LoadPlan" title="Link to this definition">#</a></dt>
<dd><dl class="field-list simple">
</dl>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="torch.distributed.checkpoint.ReadItem">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torch.distributed.checkpoint.</span></span><span class="sig-name descname"><span class="pre">ReadItem</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">type</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">torch.distributed.checkpoint.planner.LoadItemType</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dest_index</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">torch.distributed.checkpoint.metadata.MetadataIndex</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dest_offsets</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="size.html#torch.Size" title="torch.Size"><span class="pre">torch.Size</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">storage_index</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">torch.distributed.checkpoint.metadata.MetadataIndex</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">storage_offsets</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="size.html#torch.Size" title="torch.Size"><span class="pre">torch.Size</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">lengths</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="size.html#torch.Size" title="torch.Size"><span class="pre">torch.Size</span></a></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torch/distributed/checkpoint/planner.html#ReadItem"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributed/checkpoint/planner.py#L76"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.checkpoint.ReadItem" title="Link to this definition">#</a></dt>
<dd><dl class="field-list simple">
</dl>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="torch.distributed.checkpoint.SavePlanner">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torch.distributed.checkpoint.</span></span><span class="sig-name descname"><span class="pre">SavePlanner</span></span><a class="reference internal" href="../_modules/torch/distributed/checkpoint/planner.html#SavePlanner"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributed/checkpoint/planner.py#L109"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.checkpoint.SavePlanner" title="Link to this definition">#</a></dt>
<dd><p>Abstract class defining the protocol used by save_state_dict to plan the save process.</p>
<p>SavePlanners are stateful objects that can be used to customize the whole save process.</p>
<p>SavePlanner acts as an access proxy to the state_dict, so any transformation done to it
will be visible to the whole process.</p>
<p>A planner subclass can expect the following sequence of calls during save_state_dict:</p>
<ol class="arabic simple">
<li><dl class="simple">
<dt>set_up_planner - called on all ranks.</dt><dd><p>Signals the start of a checkpoint save.</p>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>create_local_plan - called on all ranks.</dt><dd><p>Process the state_dict and produces a <cite>SavePlan</cite> that will be sent for global planning.</p>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>create_global_plan - called on the coordinator rank only.</dt><dd><p>Takes the SavePlan from all ranks and make any global decision.</p>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>finish_plan - called on all ranks.</dt><dd><p>This gives each rank a chance to adjust to global planning decisions.</p>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>resolve_data - called multiple times on each rank</dt><dd><p>Lookups a value on the <cite>state_dict</cite> for the storage layer to write.</p>
</dd>
</dl>
</li>
</ol>
<p>Users are recommended to extend DefaultSavePlanner instead of this interface directly as
most changes can be expressed by changes in a single method.</p>
<p>There are 3 usual patterns of extension:</p>
<p>Rewriting state_dict. This is the simplest way to extend the save process as it
doesn’t requite understanding the intrincacies of how SavePlan works:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">class</span><span class="w"> </span><span class="nc">RenamePlanner</span><span class="p">(</span><span class="n">DefaultSavePlanner</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span><span class="w"> </span><span class="nf">set_up_planner</span><span class="p">(</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="bp">self</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">state_dict</span><span class="p">:</span> <span class="n">STATE_DICT_TYPE</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">storage_meta</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">StorageMeta</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">is_coordinator</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="c1"># prefix all keys with `foo_``</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">set_up_planner</span><span class="p">({</span><span class="s2">&quot;foo_&quot;</span> <span class="o">+</span> <span class="n">k</span><span class="p">:</span> <span class="n">v</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="o">.</span><span class="n">items</span><span class="p">()},</span> <span class="n">storage_meta</span><span class="p">,</span> <span class="n">is_coordinator</span><span class="p">)</span>
</pre></div>
</div>
<p>Modifying local plan and lookup in tandem. This is useful when fine control of how data is persisted</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">class</span><span class="w"> </span><span class="nc">FP16Planner</span><span class="p">(</span><span class="n">DefaultSavePlanner</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span><span class="w"> </span><span class="nf">create_local_plan</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">plan</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">create_local_plan</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">plan</span><span class="p">:</span>
<span class="gp">&gt;&gt;&gt; </span>            <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">tensor_data</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
<span class="gp">&gt;&gt;&gt; </span>                <span class="n">p</span><span class="o">.</span><span class="n">tensor_data</span><span class="o">.</span><span class="n">properties</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float16</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="n">plan</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span><span class="w"> </span><span class="nf">resolve_data</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">write_item</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">item</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">resolve_data</span><span class="p">(</span><span class="n">write_item</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="n">item</span> <span class="k">if</span> <span class="n">write_item</span><span class="o">.</span><span class="n">type</span> <span class="o">==</span> <span class="n">WriteItemType</span><span class="o">.</span><span class="n">BYTE_IO</span> <span class="k">else</span> <span class="n">item</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
</pre></div>
</div>
<p>Using the global planning step to make central decisions that can’t be made individually by each rank</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span><span class="w"> </span><span class="nn">itertools</span><span class="w"> </span><span class="kn">import</span> <span class="n">zip_longest</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span><span class="w"> </span><span class="nn">dataclasses</span><span class="w"> </span><span class="kn">import</span> <span class="n">replace</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">class</span><span class="w"> </span><span class="nc">DDPLoadBalancingPlanner</span><span class="p">(</span><span class="n">DefaultSavePlanner</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="c1"># This uses the default local plan behavior of having all non-sharded writes in rank 0</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="c1"># This sample doesn&#39;t handle ShardedTensors</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span><span class="w"> </span><span class="nf">create_global_plan</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">all_plans</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">iters</span> <span class="o">=</span> <span class="p">[</span><span class="nb">iter</span><span class="p">(</span><span class="n">all_plans</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">items</span><span class="p">)]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">all_plans</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">items_per_rank</span> <span class="o">=</span> <span class="p">[</span>
<span class="gp">&gt;&gt;&gt; </span>            <span class="p">[</span><span class="n">item</span> <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">items</span> <span class="k">if</span> <span class="n">item</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span>            <span class="k">for</span> <span class="n">items</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="n">zip_longest</span><span class="p">(</span><span class="o">*</span><span class="n">iters</span><span class="p">),</span> <span class="n">strict</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">all_plans</span> <span class="o">=</span> <span class="p">[</span>
<span class="gp">&gt;&gt;&gt; </span>            <span class="n">replace</span><span class="p">(</span><span class="n">plan</span><span class="p">,</span> <span class="n">items</span><span class="o">=</span><span class="n">items</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>            <span class="k">for</span> <span class="n">plan</span><span class="p">,</span> <span class="n">items</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">all_plans</span><span class="p">,</span> <span class="n">items_per_rank</span><span class="p">,</span> <span class="n">strict</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">create_global_plan</span><span class="p">(</span><span class="n">all_plans</span><span class="p">)</span>
</pre></div>
</div>
<p>Finally, some planners need to save additional metadata in the checkpoint, this is
accomplished by having each rank contribute their data items in the local plan and
the global planner aggregate them:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">class</span><span class="w"> </span><span class="nc">SaveExtraDataPlanner</span><span class="p">(</span><span class="n">DefaultSavePlanner</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span><span class="w"> </span><span class="nf">create_local_plan</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">SavePlan</span><span class="p">:</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">plan</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">create_local_plan</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="n">replace</span><span class="p">(</span><span class="n">plan</span><span class="p">,</span> <span class="n">planner_data</span><span class="o">=</span><span class="s2">&quot;per-rank-data&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span><span class="w"> </span><span class="nf">create_global_plan</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">all_plans</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">SavePlan</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">SavePlan</span><span class="p">],</span> <span class="n">Metadata</span><span class="p">]:</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">global_plan</span><span class="p">,</span> <span class="n">metadata</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">create_global_plan</span><span class="p">(</span><span class="n">all_plans</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">merged_data</span> <span class="o">=</span> <span class="p">[</span><span class="n">p</span><span class="o">.</span><span class="n">planner_data</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">global_plan</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">metadata</span> <span class="o">=</span> <span class="n">replace</span><span class="p">(</span><span class="n">metadata</span><span class="p">,</span> <span class="n">planner_data</span><span class="o">=</span><span class="n">merged_data</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="n">global_plan</span><span class="p">,</span> <span class="n">metadata</span>
</pre></div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="torch.distributed.checkpoint.SavePlanner.create_global_plan">
<em class="property"><span class="pre">abstract</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">create_global_plan</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">all_plans</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torch/distributed/checkpoint/planner.html#SavePlanner.create_global_plan"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributed/checkpoint/planner.py#L232"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.checkpoint.SavePlanner.create_global_plan" title="Link to this definition">#</a></dt>
<dd><p>Compute the global checkpoint plan and return the local plan of each rank.</p>
<p>This is called on the coordinator rank only.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Tuple" title="(in Python v3.13)"><em>Tuple</em></a>[<a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.List" title="(in Python v3.13)"><em>List</em></a>[<a class="reference internal" href="#torch.distributed.checkpoint.SavePlan" title="torch.distributed.checkpoint.planner.SavePlan"><em>SavePlan</em></a>], <em>Metadata</em>]</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributed.checkpoint.SavePlanner.create_local_plan">
<em class="property"><span class="pre">abstract</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">create_local_plan</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torch/distributed/checkpoint/planner.html#SavePlanner.create_local_plan"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributed/checkpoint/planner.py#L221"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.checkpoint.SavePlanner.create_local_plan" title="Link to this definition">#</a></dt>
<dd><p>Compute the save plan for the current rank.</p>
<p>This will be aggregated and passed to create_global_plan.
Planner specific data can be passed through SavePlan::planner_data.</p>
<p>This is called on all ranks.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference internal" href="#torch.distributed.checkpoint.SavePlan" title="torch.distributed.checkpoint.planner.SavePlan"><em>SavePlan</em></a></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributed.checkpoint.SavePlanner.finish_plan">
<em class="property"><span class="pre">abstract</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">finish_plan</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">new_plan</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torch/distributed/checkpoint/planner.html#SavePlanner.finish_plan"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributed/checkpoint/planner.py#L242"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.checkpoint.SavePlanner.finish_plan" title="Link to this definition">#</a></dt>
<dd><p>Merge the plan created by <cite>create_local_plan</cite> and the result of <cite>create_global_plan</cite>.</p>
<p>This is called on all ranks.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference internal" href="#torch.distributed.checkpoint.SavePlan" title="torch.distributed.checkpoint.planner.SavePlan"><em>SavePlan</em></a></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributed.checkpoint.SavePlanner.resolve_data">
<em class="property"><span class="pre">abstract</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">resolve_data</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">write_item</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torch/distributed/checkpoint/planner.html#SavePlanner.resolve_data"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributed/checkpoint/planner.py#L250"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.checkpoint.SavePlanner.resolve_data" title="Link to this definition">#</a></dt>
<dd><p>Transform and prepare <code class="docutils literal notranslate"><span class="pre">write_item</span></code> from <code class="docutils literal notranslate"><span class="pre">state_dict</span></code> for storage, ensuring idempotency and thread-safety.</p>
<p>Lookup the object associated with <code class="docutils literal notranslate"><span class="pre">write_item</span></code> in <code class="docutils literal notranslate"><span class="pre">state_dict</span></code> and apply any
transformation (such as serialization) prior to the storage layer consuming it.</p>
<p>Called on each rank multiple times, at least once per WriteItem in the final SavePlan.</p>
<p>This method should be idempotent and thread-save. StorageWriter implementations
are free to call it as frequently as they need.</p>
<p>Any transformation that allocates memory should be lazily done when his method
is called in order to reduce peak memory required by checkpointing.</p>
<p>When returning tensors, they can be on any device or format, they can be views too.
It’s the storage layer responsibility to figure out how to save them.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a> | <em>BytesIO</em></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributed.checkpoint.SavePlanner.set_up_planner">
<em class="property"><span class="pre">abstract</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">set_up_planner</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">state_dict</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">storage_meta</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">is_coordinator</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torch/distributed/checkpoint/planner.html#SavePlanner.set_up_planner"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributed/checkpoint/planner.py#L206"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.checkpoint.SavePlanner.set_up_planner" title="Link to this definition">#</a></dt>
<dd><p>Initialize this planner to save <code class="docutils literal notranslate"><span class="pre">state_dict</span></code>.</p>
<p>Implementations should save those values as they won’t be provided lated in the save process.</p>
<p>This is called on all ranks.</p>
<dl class="field-list simple">
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="torch.distributed.checkpoint.SavePlan">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torch.distributed.checkpoint.</span></span><span class="sig-name descname"><span class="pre">SavePlan</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">items</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><a class="reference internal" href="#torch.distributed.checkpoint.planner.WriteItem" title="torch.distributed.checkpoint.planner.WriteItem"><span class="pre">torch.distributed.checkpoint.planner.WriteItem</span></a><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">storage_data</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">planner_data</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torch/distributed/checkpoint/planner.html#SavePlan"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributed/checkpoint/planner.py#L95"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.checkpoint.SavePlan" title="Link to this definition">#</a></dt>
<dd><dl class="field-list simple">
</dl>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="torch.distributed.checkpoint.planner.WriteItem">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torch.distributed.checkpoint.planner.</span></span><span class="sig-name descname"><span class="pre">WriteItem</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">index</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">type</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tensor_data</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torch/distributed/checkpoint/planner.html#WriteItem"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributed/checkpoint/planner.py#L51"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.checkpoint.planner.WriteItem" title="Link to this definition">#</a></dt>
<dd><p>Dataclass which holds information about what needs to be written to storage.</p>
<dl class="field-list simple">
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="torch.distributed.checkpoint.planner.WriteItem.tensor_storage_size">
<span class="sig-name descname"><span class="pre">tensor_storage_size</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torch/distributed/checkpoint/planner.html#WriteItem.tensor_storage_size"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributed/checkpoint/planner.py#L61"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.checkpoint.planner.WriteItem.tensor_storage_size" title="Link to this definition">#</a></dt>
<dd><p>Calculates the storage size of the underlying tensor, or None if this is not a tensor write.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>Optional[int] storage size, in bytes of underlying tensor if any.</p>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)">int</a> | None</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<p>We provide a filesystem based storage layer:</p>
<dl class="py class">
<dt class="sig sig-object py" id="torch.distributed.checkpoint.FileSystemReader">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torch.distributed.checkpoint.</span></span><span class="sig-name descname"><span class="pre">FileSystemReader</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">path</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torch/distributed/checkpoint/filesystem.html#FileSystemReader"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributed/checkpoint/filesystem.py#L616"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.checkpoint.FileSystemReader" title="Link to this definition">#</a></dt>
<dd><dl class="field-list simple">
</dl>
<dl class="py property">
<dt class="sig sig-object py" id="torch.distributed.checkpoint.FileSystemReader.checkpoint_id">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">checkpoint_id</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/os.html#os.PathLike" title="(in Python v3.13)"><span class="pre">PathLike</span></a></em><a class="headerlink" href="#torch.distributed.checkpoint.FileSystemReader.checkpoint_id" title="Link to this definition">#</a></dt>
<dd><p>return the checkpoint_id that will be used to load the checkpoint.</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="torch.distributed.checkpoint.FileSystemWriter">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torch.distributed.checkpoint.</span></span><span class="sig-name descname"><span class="pre">FileSystemWriter</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">path</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">single_file_per_rank</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sync_files</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">thread_count</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">per_thread_copy_ahead</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">10000000</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cache_staged_state_dict</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">overwrite</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torch/distributed/checkpoint/filesystem.html#FileSystemWriter"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributed/checkpoint/filesystem.py#L710"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.checkpoint.FileSystemWriter" title="Link to this definition">#</a></dt>
<dd><p>Basic implementation of StorageWriter using file IO.</p>
<p>This implementation makes the following assumptions and simplifications:</p>
<ul class="simple">
<li><p>The checkpoint path is an empty or non-existing directory.</p></li>
<li><p>File creation is atomic</p></li>
</ul>
<p>The checkpoint consist of one file per write request plus
a <cite>.metadata</cite> file with the serialized metadata.</p>
<dl class="field-list simple">
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="torch.distributed.checkpoint.FileSystemWriter.stage">
<span class="sig-name descname"><span class="pre">stage</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">state_dict</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torch/distributed/checkpoint/filesystem.html#FileSystemWriter.stage"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributed/checkpoint/filesystem.py#L764"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.checkpoint.FileSystemWriter.stage" title="Link to this definition">#</a></dt>
<dd><p>Override of AsyncStager.stage</p>
<dl class="field-list simple">
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Dict" title="(in Python v3.13)"><em>Dict</em></a>[<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)">str</a>, <em>StatefulT</em> | <a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.13)"><em>Any</em></a>]</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<p>We provide default implementations of <cite>LoadPlanner</cite> and <cite>SavePlanner</cite> that
can handle all of torch.distributed constructs such as FSDP, DDP, ShardedTensor and DistributedTensor.</p>
<dl class="py class">
<dt class="sig sig-object py" id="torch.distributed.checkpoint.DefaultSavePlanner">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torch.distributed.checkpoint.</span></span><span class="sig-name descname"><span class="pre">DefaultSavePlanner</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">flatten_state_dict</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">flatten_sharded_tensors</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dedup_replicated_tensors</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dedup_save_to_lowest_rank</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torch/distributed/checkpoint/default_planner.html#DefaultSavePlanner"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributed/checkpoint/default_planner.py#L66"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.checkpoint.DefaultSavePlanner" title="Link to this definition">#</a></dt>
<dd><dl class="field-list simple">
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="torch.distributed.checkpoint.DefaultSavePlanner.lookup_object">
<span class="sig-name descname"><span class="pre">lookup_object</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">index</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torch/distributed/checkpoint/default_planner.html#DefaultSavePlanner.lookup_object"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributed/checkpoint/default_planner.py#L140"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.checkpoint.DefaultSavePlanner.lookup_object" title="Link to this definition">#</a></dt>
<dd><p>Extension from the planner interface to make it easy to extend the default planner.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.13)"><em>Any</em></a></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributed.checkpoint.DefaultSavePlanner.transform_object">
<span class="sig-name descname"><span class="pre">transform_object</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">write_item</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">object</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torch/distributed/checkpoint/default_planner.html#DefaultSavePlanner.transform_object"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributed/checkpoint/default_planner.py#L144"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.checkpoint.DefaultSavePlanner.transform_object" title="Link to this definition">#</a></dt>
<dd><p>Extension from the planner interface to make it easy to extend the default planner.</p>
<dl class="field-list simple">
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="torch.distributed.checkpoint.DefaultLoadPlanner">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torch.distributed.checkpoint.</span></span><span class="sig-name descname"><span class="pre">DefaultLoadPlanner</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">flatten_state_dict</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">flatten_sharded_tensors</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">allow_partial_load</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torch/distributed/checkpoint/default_planner.html#DefaultLoadPlanner"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributed/checkpoint/default_planner.py#L153"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.checkpoint.DefaultLoadPlanner" title="Link to this definition">#</a></dt>
<dd><p>DefaultLoadPlanner that adds multiple features on top of LoadPlanner.</p>
<p>In particular it adds the following:</p>
<p>flatten_state_dict: Handle state_dict with nested dicts
flatten_sharded_tensors: For FSDP in 2D parallel mode
allow_partial_load: If False, will raise a runtime error if a key is present in state_dict, but not in the checkpoint.</p>
<dl class="field-list simple">
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="torch.distributed.checkpoint.DefaultLoadPlanner.lookup_tensor">
<span class="sig-name descname"><span class="pre">lookup_tensor</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">index</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torch/distributed/checkpoint/default_planner.html#DefaultLoadPlanner.lookup_tensor"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributed/checkpoint/default_planner.py#L262"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.checkpoint.DefaultLoadPlanner.lookup_tensor" title="Link to this definition">#</a></dt>
<dd><p>Extension from the planner interface to make it easy to extend the default planner.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributed.checkpoint.DefaultLoadPlanner.transform_tensor">
<span class="sig-name descname"><span class="pre">transform_tensor</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">read_item</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tensor</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torch/distributed/checkpoint/default_planner.html#DefaultLoadPlanner.transform_tensor"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributed/checkpoint/default_planner.py#L266"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.checkpoint.DefaultLoadPlanner.transform_tensor" title="Link to this definition">#</a></dt>
<dd><p>Extension from the planner interface to make it easy to extend the default planner.</p>
<dl class="field-list simple">
</dl>
</dd></dl>

</dd></dl>

<p>Due to legacy design decisions, the state dictionaries of <cite>FSDP</cite> and <cite>DDP</cite> may have different keys or fully qualified names (e.g., layer1.weight) even when the original unparallelized model is identical. Moreover, <cite>FSDP</cite> offers various types of model state dictionaries, such as full and sharded state dictionaries. Additionally, optimizer state dictionaries employ parameter IDs instead of fully qualified names to identify parameters, potentially causing issues when parallelisms are used (e.g., pipeline parallelism).</p>
<p>To tackle these challenges, we offer a collection of APIs for users to easily manage state_dicts. <cite>get_model_state_dict</cite> returns a model state dictionary with keys consistent with those returned by the unparallelized model state dictionary. Similarly, <cite>get_optimizer_state_dict</cite> provides the optimizer state dictionary with keys uniform across all parallelisms applied. To achieve this consistency, <cite>get_optimizer_state_dict</cite> converts parameter IDs to fully qualified names identical to those found in the unparallelized model state dictionary.</p>
<p>Note that results returned by these APIs can be used directly with the <cite>torch.distributed.checkpoint.save()</cite> and <cite>torch.distributed.checkpoint.load()</cite> methods without requiring any additional conversions.</p>
<p>Note that this feature is experimental, and API signatures might change in the future.</p>
<dl class="py function">
<dt class="sig sig-object py" id="torch.distributed.checkpoint.state_dict.get_state_dict">
<span class="sig-prename descclassname"><span class="pre">torch.distributed.checkpoint.state_dict.</span></span><span class="sig-name descname"><span class="pre">get_state_dict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optimizers</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">submodules</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">options</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torch/distributed/checkpoint/state_dict.html#get_state_dict"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributed/checkpoint/state_dict.py#L1049"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.checkpoint.state_dict.get_state_dict" title="Link to this definition">#</a></dt>
<dd><p>Return the model state_dict and optimizers state_dict.</p>
<p><code class="docutils literal notranslate"><span class="pre">get_state_dict</span></code> can process any module that is parallelized by PyTorch
FSDP/fully_shard, DDP/replicate, tensor_parallel/parallelize_module, and any
combination of these parallelisms. The main functions of <code class="docutils literal notranslate"><span class="pre">get_state_dict</span></code>
are: 1.) returning a model and optimizer state_dict that can be resharded
with a different number of trainers and/or different parallelisms.
2.) hiding the parallelism-specific state_dict APIs. Users don’t have to call
these APIs.
3.) sanity checking the result state_dict.</p>
<p>The keys of the result state dictionary are the canonical FQNs (Fully
Qualified Names).  A canonical FQN refers to the FQN based on a parameter’s
position in an nn.Module hierarchy. More specifically, a canonical FQN to a
parameter is the FQN returned by <code class="docutils literal notranslate"><span class="pre">module.named_parameters()</span></code> or
<code class="docutils literal notranslate"><span class="pre">module.named_buffers()</span></code> when the module is not distributed by any
parallelisms. Since the optimizer internally uses parameter IDs to represent
a parameter, there will be a conversion from the parameter IDs to the
canonical FQNs when calling this API.</p>
<p><code class="docutils literal notranslate"><span class="pre">get_state_dict</span></code> can also process a module that is not parallelized. In
such a case, <code class="docutils literal notranslate"><span class="pre">get_state_dict</span></code> only performs one function – converting the
optimizer parameter IDs to the canonical FQNs.</p>
<p class="rubric">Example</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span><span class="w"> </span><span class="nn">torch.distributed.fsdp</span><span class="w"> </span><span class="kn">import</span> <span class="n">FullyShardedDataParallel</span> <span class="k">as</span> <span class="n">FSDP</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span><span class="w"> </span><span class="nn">torch.nn.parallel</span><span class="w"> </span><span class="kn">import</span> <span class="n">DistributedDataParallel</span> <span class="k">as</span> <span class="n">DDP</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span><span class="w"> </span><span class="nn">torch.distributed.checkpoint.state_dict</span><span class="w"> </span><span class="kn">import</span> <span class="n">get_state_dict</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">fsdp_model</span> <span class="o">=</span> <span class="n">FSDP</span><span class="p">(</span><span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="n">model</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">fsdp_optim</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">ddp_model</span> <span class="o">=</span> <span class="n">DDP</span><span class="p">(</span><span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="n">model</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">ddp_optim</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">)</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">ddp_state_dict</span><span class="p">,</span> <span class="n">ddp_optim_state_dict</span> <span class="o">=</span> <span class="n">get_state_dict</span><span class="p">(</span><span class="n">ddp_model</span><span class="p">,</span> <span class="n">ddp_optim</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">fsdp_state_dict</span><span class="p">,</span> <span class="n">fsdp_optim_state_dict</span> <span class="o">=</span> <span class="n">get_state_dict</span><span class="p">(</span><span class="n">fsdp_model</span><span class="p">,</span> <span class="n">fsdp_optim</span><span class="p">)</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># if we simply call ddp_model.state_dict() and fsdp_model.state_dict(),</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># the asserts will fail.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">assert</span> <span class="n">ddp_state_dict</span> <span class="o">==</span> <span class="n">fsdp_state_dict</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">assert</span> <span class="n">ddp_optim_state</span> <span class="o">==</span> <span class="n">fsdp_optim_state_dict</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model</strong> (<a class="reference internal" href="generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module"><em>nn.Module</em></a>) – the nn.Module to the model.</p></li>
<li><p><strong>optimizers</strong> (<em>Union</em><em>[</em><em>None</em><em>, </em><a class="reference internal" href="optim.html#torch.optim.Optimizer" title="torch.optim.Optimizer"><em>Optimizer</em></a><em>, </em><em>Iterable</em><em>[</em><a class="reference internal" href="optim.html#torch.optim.Optimizer" title="torch.optim.Optimizer"><em>Optimizer</em></a><em>]</em><em>]</em>) – The optimizers that are used to optimize <code class="docutils literal notranslate"><span class="pre">model</span></code>.</p></li>
<li><p><strong>submodules</strong> (<em>deprecated</em>) – Optional[Set[nn.Module]]: only return the model parameters
that belong to the submodules.</p></li>
<li><p><strong>options</strong> (<a class="reference internal" href="#torch.distributed.checkpoint.state_dict.StateDictOptions" title="torch.distributed.checkpoint.state_dict.StateDictOptions"><em>StateDictOptions</em></a>) – the options to control how
model state_dict and optimizer state_dict should be returned. See
<cite>StateDictOptions</cite> for the details.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><code class="docutils literal notranslate"><span class="pre">Tuple</span></code> that contain model state_dict and optimizer state_dict.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Tuple" title="(in Python v3.13)"><em>Tuple</em></a>[<a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Dict" title="(in Python v3.13)"><em>Dict</em></a>[<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)">str</a>, ValueType], OptimizerStateType]</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="torch.distributed.checkpoint.state_dict.get_model_state_dict">
<span class="sig-prename descclassname"><span class="pre">torch.distributed.checkpoint.state_dict.</span></span><span class="sig-name descname"><span class="pre">get_model_state_dict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">submodules</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">options</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torch/distributed/checkpoint/state_dict.html#get_model_state_dict"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributed/checkpoint/state_dict.py#L967"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.checkpoint.state_dict.get_model_state_dict" title="Link to this definition">#</a></dt>
<dd><p>Return the model state_dict of <code class="docutils literal notranslate"><span class="pre">model</span></code>.</p>
<p>See <code class="docutils literal notranslate"><span class="pre">get_state_dict</span></code> for the detail usage.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model</strong> (<a class="reference internal" href="generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module"><em>nn.Module</em></a>) – the nn.Module to the model.</p></li>
<li><p><strong>submodules</strong> (<em>deprecated</em>) – Optional[Set[nn.Module]]: only return the model parameters
that belong to the submodules.</p></li>
<li><p><strong>options</strong> (<a class="reference internal" href="#torch.distributed.checkpoint.state_dict.StateDictOptions" title="torch.distributed.checkpoint.state_dict.StateDictOptions"><em>StateDictOptions</em></a>) – the options to control how
model state_dict and optimizer state_dict should be returned. See
<cite>StateDictOptions</cite> for the details.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The state_dict for <code class="docutils literal notranslate"><span class="pre">model</span></code>.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Dict" title="(in Python v3.13)"><em>Dict</em></a>[<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)">str</a>, ValueType]</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="torch.distributed.checkpoint.state_dict.get_optimizer_state_dict">
<span class="sig-prename descclassname"><span class="pre">torch.distributed.checkpoint.state_dict.</span></span><span class="sig-name descname"><span class="pre">get_optimizer_state_dict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optimizers</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">submodules</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">options</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torch/distributed/checkpoint/state_dict.html#get_optimizer_state_dict"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributed/checkpoint/state_dict.py#L1004"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.checkpoint.state_dict.get_optimizer_state_dict" title="Link to this definition">#</a></dt>
<dd><p>Return the combined state_dict for optimizers.</p>
<p>See <code class="docutils literal notranslate"><span class="pre">get_state_dict</span></code> for the detail usage.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model</strong> (<a class="reference internal" href="generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module"><em>nn.Module</em></a>) – the nn.Module to the model.</p></li>
<li><p><strong>optimizers</strong> (<em>Union</em><em>[</em><em>None</em><em>, </em><a class="reference internal" href="optim.html#torch.optim.Optimizer" title="torch.optim.Optimizer"><em>Optimizer</em></a><em>, </em><em>Iterable</em><em>[</em><a class="reference internal" href="optim.html#torch.optim.Optimizer" title="torch.optim.Optimizer"><em>Optimizer</em></a><em>]</em><em>]</em>) – The optimizers that are used to optimize <code class="docutils literal notranslate"><span class="pre">model</span></code>.</p></li>
<li><p><strong>submodules</strong> (<em>deprecated</em>) – Optional[Set[nn.Module]]: only return the model parameters
that belong to the submodules.</p></li>
<li><p><strong>options</strong> (<a class="reference internal" href="#torch.distributed.checkpoint.state_dict.StateDictOptions" title="torch.distributed.checkpoint.state_dict.StateDictOptions"><em>StateDictOptions</em></a>) – the options to control how
model state_dict and optimizer state_dict should be returned. See
<cite>StateDictOptions</cite> for the details.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The state_dict for <code class="docutils literal notranslate"><span class="pre">optimizers</span></code>.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>OptimizerStateType</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="torch.distributed.checkpoint.state_dict.set_state_dict">
<span class="sig-prename descclassname"><span class="pre">torch.distributed.checkpoint.state_dict.</span></span><span class="sig-name descname"><span class="pre">set_state_dict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optimizers</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">model_state_dict</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optim_state_dict</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">options</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torch/distributed/checkpoint/state_dict.html#set_state_dict"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributed/checkpoint/state_dict.py#L1249"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.checkpoint.state_dict.set_state_dict" title="Link to this definition">#</a></dt>
<dd><p>Load the model state_dict and optimizers state_dict.</p>
<p>The counterpart of <code class="docutils literal notranslate"><span class="pre">get_state_dict</span></code> to set the state_dict to the model and
optimizers.  The given <code class="docutils literal notranslate"><span class="pre">model_state_dict</span></code> and <code class="docutils literal notranslate"><span class="pre">optim_state_dict</span></code> do not
have to be returned by <code class="docutils literal notranslate"><span class="pre">get_state_dict</span></code> but must meet the following
requirements: 1) all FQNs are canonical FQNs as defined in <code class="docutils literal notranslate"><span class="pre">get_state_dict</span></code>,
2) if a tensor is sharded, it must be either a ShardedTensor or DTensor,
3) optimizer state_dict cannot contain the parameter IDs; the keys should be
the canonical FQNs.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model</strong> (<a class="reference internal" href="generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module"><em>nn.Module</em></a>) – the nn.Module to the model.</p></li>
<li><p><strong>optimizers</strong> (<em>Union</em><em>[</em><a class="reference internal" href="optim.html#torch.optim.Optimizer" title="torch.optim.Optimizer"><em>Optimizer</em></a><em>, </em><em>Iterable</em><em>[</em><a class="reference internal" href="optim.html#torch.optim.Optimizer" title="torch.optim.Optimizer"><em>Optimizer</em></a><em>]</em><em>]</em>) – The optimizers that are used to optimize <code class="docutils literal notranslate"><span class="pre">model</span></code>.</p></li>
<li><p><strong>model_state_dict</strong> (<a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Dict" title="(in Python v3.13)"><em>Dict</em></a><em>[</em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><em>str</em></a><em>, </em><em>ValueType</em><em>]</em>) – (Union[Dict[nn.Module, Dict[str, ValueType]], Dict[str, ValueType]]):
the model state_dict to load. If the key of the <code class="docutils literal notranslate"><span class="pre">model_state_dict</span></code>
is nn.Module, the key is a submodule of <code class="docutils literal notranslate"><span class="pre">model</span></code> and the value should
be the state_dict of the submodule. When loading the state_dict,
the prefix of the submodule will be append to the state_dict.</p></li>
<li><p><strong>optim_state_dict</strong> (<em>OptimizerStateType</em>) – OptimizerStateType:
the optimizer state_dict to load.</p></li>
<li><p><strong>options</strong> (<a class="reference internal" href="#torch.distributed.checkpoint.state_dict.StateDictOptions" title="torch.distributed.checkpoint.state_dict.StateDictOptions"><em>StateDictOptions</em></a>) – the options to control how
model state_dict and optimizer state_dict should be loaded. See
<cite>StateDictOptions</cite> for the details.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><ul class="simple">
<li><p><strong>missing_keys</strong> is a list of str containing the missing keys of the model state_dict.</p></li>
<li><p><strong>unexpected_keys</strong> is a list of str containing the unexpected keys of the model state_dict.</p></li>
</ul>
</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><code class="docutils literal notranslate"><span class="pre">NamedTuple</span></code> with <code class="docutils literal notranslate"><span class="pre">missing_keys</span></code> and <code class="docutils literal notranslate"><span class="pre">unexpected_keys</span></code> fields</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="torch.distributed.checkpoint.state_dict.set_model_state_dict">
<span class="sig-prename descclassname"><span class="pre">torch.distributed.checkpoint.state_dict.</span></span><span class="sig-name descname"><span class="pre">set_model_state_dict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">model_state_dict</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">options</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torch/distributed/checkpoint/state_dict.html#set_model_state_dict"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributed/checkpoint/state_dict.py#L1171"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.checkpoint.state_dict.set_model_state_dict" title="Link to this definition">#</a></dt>
<dd><p>Load the model state_dict.</p>
<p>The counterpart of <code class="docutils literal notranslate"><span class="pre">get_model_state_dict</span></code> to set the state_dict to the
model. See <code class="docutils literal notranslate"><span class="pre">set_state_dict</span></code> for the detail usage.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model</strong> (<a class="reference internal" href="generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module"><em>nn.Module</em></a>) – the nn.Module to the model.</p></li>
<li><p><strong>model_state_dict</strong> (<a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Dict" title="(in Python v3.13)"><em>Dict</em></a><em>[</em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><em>str</em></a><em>, </em><em>ValueType</em><em>]</em>) – (Dict[str, ValueType]):
the model state_dict to load. If the key of the <code class="docutils literal notranslate"><span class="pre">model_state_dict</span></code>
is nn.Module, the key is a submodule of <code class="docutils literal notranslate"><span class="pre">model</span></code> and the value should
be the state_dict of the submodule. When loading the state_dict,
the prefix of the submodule will be append to the state_dict.</p></li>
<li><p><strong>options</strong> (<a class="reference internal" href="#torch.distributed.checkpoint.state_dict.StateDictOptions" title="torch.distributed.checkpoint.state_dict.StateDictOptions"><em>StateDictOptions</em></a>) – the options to control how
model state_dict and optimizer state_dict should be loaded. See
<cite>StateDictOptions</cite> for the details.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><ul class="simple">
<li><p><strong>missing_keys</strong> is a list of str containing the missing keys</p></li>
<li><p><strong>unexpected_keys</strong> is a list of str containing the unexpected keys</p></li>
</ul>
</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><code class="docutils literal notranslate"><span class="pre">NamedTuple</span></code> with <code class="docutils literal notranslate"><span class="pre">missing_keys</span></code> and <code class="docutils literal notranslate"><span class="pre">unexpected_keys</span></code> fields</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="torch.distributed.checkpoint.state_dict.set_optimizer_state_dict">
<span class="sig-prename descclassname"><span class="pre">torch.distributed.checkpoint.state_dict.</span></span><span class="sig-name descname"><span class="pre">set_optimizer_state_dict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optimizers</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optim_state_dict</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">options</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torch/distributed/checkpoint/state_dict.html#set_optimizer_state_dict"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributed/checkpoint/state_dict.py#L1210"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.checkpoint.state_dict.set_optimizer_state_dict" title="Link to this definition">#</a></dt>
<dd><p>Load the optimizers state_dict.</p>
<p>The counterpart of <code class="docutils literal notranslate"><span class="pre">get_optimizer_state_dict</span></code> to set the state_dict to the
optimizers. See <code class="docutils literal notranslate"><span class="pre">set_state_dict</span></code> for the detail usage.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model</strong> (<a class="reference internal" href="generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module"><em>nn.Module</em></a>) – the nn.Module to the model.</p></li>
<li><p><strong>optimizers</strong> (<em>Union</em><em>[</em><a class="reference internal" href="optim.html#torch.optim.Optimizer" title="torch.optim.Optimizer"><em>Optimizer</em></a><em>, </em><em>Iterable</em><em>[</em><a class="reference internal" href="optim.html#torch.optim.Optimizer" title="torch.optim.Optimizer"><em>Optimizer</em></a><em>]</em><em>]</em>) – The optimizers that are used to optimize <code class="docutils literal notranslate"><span class="pre">model</span></code>.</p></li>
<li><p><strong>optim_state_dict</strong> (<em>OptimizerStateType</em>) – OptimizerStateType:
the optimizer state_dict to load.</p></li>
<li><p><strong>options</strong> (<a class="reference internal" href="#torch.distributed.checkpoint.state_dict.StateDictOptions" title="torch.distributed.checkpoint.state_dict.StateDictOptions"><em>StateDictOptions</em></a>) – the options to control how
model state_dict and optimizer state_dict should be loaded. See
<cite>StateDictOptions</cite> for the details.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>None</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>None</p>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="torch.distributed.checkpoint.state_dict.StateDictOptions">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torch.distributed.checkpoint.state_dict.</span></span><span class="sig-name descname"><span class="pre">StateDictOptions</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">full_state_dict</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cpu_offload</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ignore_frozen_params</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">keep_submodule_prefixes</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">strict</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">broadcast_from_rank0</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">flatten_optimizer_state_dict</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torch/distributed/checkpoint/state_dict.html#StateDictOptions"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributed/checkpoint/state_dict.py#L104"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.checkpoint.state_dict.StateDictOptions" title="Link to this definition">#</a></dt>
<dd><p>This dataclass specifies how get_state_dict/set_state_dict will work.</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">full_state_dict</span></code>: if this is set to True, all the tensors in the
returned state_dict will be gathered. No ShardedTensor and DTensor
will be in the returned state_dict.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">cpu_offload</span></code>: offload all the tensors to cpu. To prevent CPU OOM, if
<code class="docutils literal notranslate"><span class="pre">full_state_dict</span></code> is also true, then only the rank0 will get the
state_dict and all other ranks will get empty state_dict.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">ignore_frozen_params</span></code>: if the value is True, the returned state_dict
won’t contain any frozen parameters – the <code class="docutils literal notranslate"><span class="pre">requires_grad</span></code> is False.
The default value is False.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">keep_submodule_prefixes</span></code> (deprecated): when <code class="docutils literal notranslate"><span class="pre">submodules</span></code> is not None, this option
indicates whether to keep the submodule prefixes from the state_dict keys.
or example, if the submodule is <code class="docutils literal notranslate"><span class="pre">module.pretrain</span></code> and the full FQN of
the parameter is <code class="docutils literal notranslate"><span class="pre">pretrain.layer1.weight</span></code> of the param. When this option
is True, the parameter’s key in the returned state_dict will be
<code class="docutils literal notranslate"><span class="pre">pretrain.layer1.weight</span></code>. If the options is False, the key will be
<code class="docutils literal notranslate"><span class="pre">layer1.weight</span></code>.
Note that if <code class="docutils literal notranslate"><span class="pre">keep_submodule_prefixes</span></code> is False, there may be conflicted
FQNs, hence there should be only one submodule in <code class="docutils literal notranslate"><span class="pre">submodules</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">strict</span></code>: the <code class="docutils literal notranslate"><span class="pre">strict</span></code> option when <code class="docutils literal notranslate"><span class="pre">set_state_dict</span></code> calls
model.load_state_dict().</p></li>
<li><dl class="simple">
<dt><code class="docutils literal notranslate"><span class="pre">broadcast_from_rank0</span></code>: when the option is True, rank0 should receive a</dt><dd><p>full state_dict and will broadcast the tensors in the state_dict/
optim_state_dict one by one to other ranks. Other ranks will receive
the tensors and shard according to the local shards in the model and
optimizer. <code class="docutils literal notranslate"><span class="pre">full_state_dict</span></code> must be set to True when using this option.
This option currently only supports DTensor, not the legacy ShardedTensor.</p>
</dd>
</dl>
</li>
</ul>
<dl class="field-list simple">
</dl>
</dd></dl>

<p>For users which are used to using and sharing models in the <cite>torch.save</cite> format, the following methods are provided which provide offline utilities for converting betweeing formats.</p>
<dl class="py function" id="module-torch.distributed.checkpoint.format_utils">
<dt class="sig sig-object py" id="torch.distributed.checkpoint.format_utils.dcp_to_torch_save">
<span class="sig-prename descclassname"><span class="pre">torch.distributed.checkpoint.format_utils.</span></span><span class="sig-name descname"><span class="pre">dcp_to_torch_save</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dcp_checkpoint_dir</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">torch_save_path</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torch/distributed/checkpoint/format_utils.html#dcp_to_torch_save"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributed/checkpoint/format_utils.py#L196"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.checkpoint.format_utils.dcp_to_torch_save" title="Link to this definition">#</a></dt>
<dd><p>Given a directory containing a DCP checkpoint, this function will convert it into a
Torch save file.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>dcp_checkpoint_dir</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><em>str</em></a><em> | </em><a class="reference external" href="https://docs.python.org/3/library/os.html#os.PathLike" title="(in Python v3.13)"><em>PathLike</em></a>) – Directory containing the DCP checkpoint.</p></li>
<li><p><strong>torch_save_path</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><em>str</em></a><em> | </em><a class="reference external" href="https://docs.python.org/3/library/os.html#os.PathLike" title="(in Python v3.13)"><em>PathLike</em></a>) – Filename to store the converted Torch save file.</p></li>
</ul>
</dd>
</dl>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>To avoid OOM, it’s recommended to only run this function on a single rank.</p>
</div>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="torch.distributed.checkpoint.format_utils.torch_save_to_dcp">
<span class="sig-prename descclassname"><span class="pre">torch.distributed.checkpoint.format_utils.</span></span><span class="sig-name descname"><span class="pre">torch_save_to_dcp</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">torch_save_path</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dcp_checkpoint_dir</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torch/distributed/checkpoint/format_utils.html#torch_save_to_dcp"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributed/checkpoint/format_utils.py#L221"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.checkpoint.format_utils.torch_save_to_dcp" title="Link to this definition">#</a></dt>
<dd><p>Given the location of a torch save file, converts it into a DCP checkpoint.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>torch_save_path</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><em>str</em></a><em> | </em><a class="reference external" href="https://docs.python.org/3/library/os.html#os.PathLike" title="(in Python v3.13)"><em>PathLike</em></a>) – Filename of the Torch save file.</p></li>
<li><p><strong>dcp_checkpoint_dir</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><em>str</em></a><em> | </em><a class="reference external" href="https://docs.python.org/3/library/os.html#os.PathLike" title="(in Python v3.13)"><em>PathLike</em></a>) – Directory to store the DCP checkpoint.</p></li>
</ul>
</dd>
</dl>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>To avoid OOM, it’s recommended to only run this function on a single rank.</p>
</div>
</dd></dl>

<p>The following classes can also be utilized for online loading and resharding of models from the torch.save format.</p>
<dl class="py class">
<dt class="sig sig-object py" id="torch.distributed.checkpoint.format_utils.BroadcastingTorchSaveReader">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torch.distributed.checkpoint.format_utils.</span></span><span class="sig-name descname"><span class="pre">BroadcastingTorchSaveReader</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">checkpoint_id</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">coordinator_rank</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torch/distributed/checkpoint/format_utils.html#BroadcastingTorchSaveReader"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributed/checkpoint/format_utils.py#L39"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.checkpoint.format_utils.BroadcastingTorchSaveReader" title="Link to this definition">#</a></dt>
<dd><p>StorageReader for reading a Torch Save file. This reader will read the entire checkpoint
on the coordinator rank, and then broadcast and shard each tensor to all ranks.</p>
<p>. N.B. Intended to be used with DynamicMetaLoadPlanner</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Current implementation only supports loading Tensors.</p>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">sd</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;mode&quot;</span><span class="p">:</span> <span class="n">model</span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dcp</span><span class="o">.</span><span class="n">load</span><span class="p">(</span>
<span class="gp">&gt;&gt;&gt; </span>   <span class="n">sd</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span>   <span class="n">storage_reader</span><span class="o">=</span><span class="n">BroadcastingTorchSaveReader</span><span class="p">(),</span>
<span class="gp">&gt;&gt;&gt; </span>   <span class="n">planner</span><span class="o">=</span><span class="n">DynamicMetaLoadPlanner</span><span class="p">(),</span>
<span class="gp">&gt;&gt;&gt; </span>   <span class="n">checkpoint_id</span><span class="o">=</span><span class="s2">&quot;path_to_model.pt&quot;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">)</span>
</pre></div>
</div>
<dl class="field-list simple">
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="torch.distributed.checkpoint.format_utils.BroadcastingTorchSaveReader.prepare_global_plan">
<span class="sig-name descname"><span class="pre">prepare_global_plan</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">global_plan</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torch/distributed/checkpoint/format_utils.html#BroadcastingTorchSaveReader.prepare_global_plan"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributed/checkpoint/format_utils.py#L136"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.checkpoint.format_utils.BroadcastingTorchSaveReader.prepare_global_plan" title="Link to this definition">#</a></dt>
<dd><p>Implementation of the StorageReader method</p>
<dl class="field-list simple">
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.List" title="(in Python v3.13)"><em>List</em></a>[<a class="reference internal" href="#torch.distributed.checkpoint.LoadPlan" title="torch.distributed.checkpoint.planner.LoadPlan"><em>LoadPlan</em></a>]</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributed.checkpoint.format_utils.BroadcastingTorchSaveReader.prepare_local_plan">
<span class="sig-name descname"><span class="pre">prepare_local_plan</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">plan</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torch/distributed/checkpoint/format_utils.html#BroadcastingTorchSaveReader.prepare_local_plan"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributed/checkpoint/format_utils.py#L132"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.checkpoint.format_utils.BroadcastingTorchSaveReader.prepare_local_plan" title="Link to this definition">#</a></dt>
<dd><p>Implementation of the StorageReader method</p>
<dl class="field-list simple">
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference internal" href="#torch.distributed.checkpoint.LoadPlan" title="torch.distributed.checkpoint.planner.LoadPlan"><em>LoadPlan</em></a></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributed.checkpoint.format_utils.BroadcastingTorchSaveReader.read_data">
<span class="sig-name descname"><span class="pre">read_data</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">plan</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">planner</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torch/distributed/checkpoint/format_utils.html#BroadcastingTorchSaveReader.read_data"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributed/checkpoint/format_utils.py#L73"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.checkpoint.format_utils.BroadcastingTorchSaveReader.read_data" title="Link to this definition">#</a></dt>
<dd><p>Reads torch save data on the coordinator rank, and broadcast afterwards
this incurrs a communication cost, but avoids having to load
the entire checkpoint on each rank, hopefully preventing OOM issues</p>
<dl class="field-list simple">
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference internal" href="futures.html#torch.futures.Future" title="torch.jit.Future"><em>Future</em></a>[None]</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributed.checkpoint.format_utils.BroadcastingTorchSaveReader.read_metadata">
<span class="sig-name descname"><span class="pre">read_metadata</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torch/distributed/checkpoint/format_utils.html#BroadcastingTorchSaveReader.read_metadata"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributed/checkpoint/format_utils.py#L67"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.checkpoint.format_utils.BroadcastingTorchSaveReader.read_metadata" title="Link to this definition">#</a></dt>
<dd><p>Extends the default StorageReader to support building the metadata file</p>
<dl class="field-list simple">
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><em>Metadata</em></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributed.checkpoint.format_utils.BroadcastingTorchSaveReader.reset">
<span class="sig-name descname"><span class="pre">reset</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">checkpoint_id</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torch/distributed/checkpoint/format_utils.html#BroadcastingTorchSaveReader.reset"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributed/checkpoint/format_utils.py#L140"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.checkpoint.format_utils.BroadcastingTorchSaveReader.reset" title="Link to this definition">#</a></dt>
<dd><p>Implementation of the StorageReader method</p>
<dl class="field-list simple">
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributed.checkpoint.format_utils.BroadcastingTorchSaveReader.set_up_storage_reader">
<span class="sig-name descname"><span class="pre">set_up_storage_reader</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">metadata</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">is_coordinator</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torch/distributed/checkpoint/format_utils.html#BroadcastingTorchSaveReader.set_up_storage_reader"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributed/checkpoint/format_utils.py#L124"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.checkpoint.format_utils.BroadcastingTorchSaveReader.set_up_storage_reader" title="Link to this definition">#</a></dt>
<dd><p>Implementation of the StorageReader method</p>
<dl class="field-list simple">
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributed.checkpoint.format_utils.BroadcastingTorchSaveReader.validate_checkpoint_id">
<em class="property"><span class="pre">classmethod</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">validate_checkpoint_id</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">checkpoint_id</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torch/distributed/checkpoint/format_utils.html#BroadcastingTorchSaveReader.validate_checkpoint_id"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributed/checkpoint/format_utils.py#L144"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.checkpoint.format_utils.BroadcastingTorchSaveReader.validate_checkpoint_id" title="Link to this definition">#</a></dt>
<dd><p>Implementation of the StorageReader method</p>
<dl class="field-list simple">
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)">bool</a></p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="torch.distributed.checkpoint.format_utils.DynamicMetaLoadPlanner">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torch.distributed.checkpoint.format_utils.</span></span><span class="sig-name descname"><span class="pre">DynamicMetaLoadPlanner</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">flatten_state_dict</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">flatten_sharded_tensors</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">allow_partial_load</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torch/distributed/checkpoint/format_utils.html#DynamicMetaLoadPlanner"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributed/checkpoint/format_utils.py#L150"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.checkpoint.format_utils.DynamicMetaLoadPlanner" title="Link to this definition">#</a></dt>
<dd><p>Extension of DefaultLoadPlanner, which creates a new Metadata object based on the passed in state dict,
avoiding the need to read metadata from disk. This is useful when reading formats which don’t have a
metadata file, like Torch Save files.</p>
<p>. N.B. Intended to be used with BroadcastingTorchSaveReader</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Current implementation only supports loading Tensors.</p>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">sd</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;mode&quot;</span><span class="p">:</span> <span class="n">model</span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dcp</span><span class="o">.</span><span class="n">load</span><span class="p">(</span>
<span class="gp">&gt;&gt;&gt; </span>   <span class="n">sd</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span>   <span class="n">storage_reader</span><span class="o">=</span><span class="n">BroadcastingTorchSaveReader</span><span class="p">(),</span>
<span class="gp">&gt;&gt;&gt; </span>   <span class="n">planner</span><span class="o">=</span><span class="n">DynamicMetaLoadPlanner</span><span class="p">(),</span>
<span class="gp">&gt;&gt;&gt; </span>   <span class="n">checkpoint_id</span><span class="o">=</span><span class="s2">&quot;path_to_model.pt&quot;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">)</span>
</pre></div>
</div>
<dl class="field-list simple">
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="torch.distributed.checkpoint.format_utils.DynamicMetaLoadPlanner.set_up_planner">
<span class="sig-name descname"><span class="pre">set_up_planner</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">state_dict</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">metadata</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">is_coordinator</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torch/distributed/checkpoint/format_utils.html#DynamicMetaLoadPlanner.set_up_planner"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributed/checkpoint/format_utils.py#L171"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.checkpoint.format_utils.DynamicMetaLoadPlanner.set_up_planner" title="Link to this definition">#</a></dt>
<dd><p>Setups of the planner, extnding default behavior by creating the Metadata object from the state dict</p>
<dl class="field-list simple">
</dl>
</dd></dl>

</dd></dl>

<p>The following experimental interfaces are provided for improved observability in production environments:</p>
<span class="target" id="module-torch.distributed.checkpoint.logging_handlers"><span id="module-torch.distributed.checkpoint.logger"></span></span></section>
</section>


                </article>
              
              
              
                <footer class="bd-footer-article">
                  <div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item">
<div class="feedback">
  
<div class="rating">
    Rate this Page
    <div class="stars">
        
        <span class="star" data-value="1">★</span>
        
        <span class="star" data-value="2">★</span>
        
        <span class="star" data-value="3">★</span>
        
        <span class="star" data-value="4">★</span>
        
        <span class="star" data-value="5">★</span>
        
    </div>
</div>

  <div class="feedback-send">
    <button class="feedback-btn" onclick="openGitHubIssue()">Send Feedback</button>
  </div>
</div>
</div>
  
</div>
                </footer>
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
</div>
                </footer>
              
            </div>
            
            
              
                <dialog id="pst-secondary-sidebar-modal"></dialog>
                <div id="pst-secondary-sidebar" class="bd-sidebar-secondary bd-toc">
<div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
<div
    id="pst-page-navigation-heading-2"
    class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> On this page
  </div>
  <nav class="bd-toc-nav page-toc" aria-labelledby="pst-page-navigation-heading-2">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#additional-resources">Additional resources:</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributed.checkpoint.state_dict_saver.save"><code class="docutils literal notranslate"><span class="pre">save()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributed.checkpoint.state_dict_saver.async_save"><code class="docutils literal notranslate"><span class="pre">async_save()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributed.checkpoint.state_dict_saver.save_state_dict"><code class="docutils literal notranslate"><span class="pre">save_state_dict()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributed.checkpoint.state_dict_loader.load"><code class="docutils literal notranslate"><span class="pre">load()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributed.checkpoint.state_dict_loader.load_state_dict"><code class="docutils literal notranslate"><span class="pre">load_state_dict()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributed.checkpoint.staging.AsyncStager"><code class="docutils literal notranslate"><span class="pre">AsyncStager</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributed.checkpoint.staging.AsyncStager.should_synchronize_after_execute"><code class="docutils literal notranslate"><span class="pre">AsyncStager.should_synchronize_after_execute</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributed.checkpoint.staging.AsyncStager.stage"><code class="docutils literal notranslate"><span class="pre">AsyncStager.stage()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributed.checkpoint.staging.AsyncStager.synchronize_staging"><code class="docutils literal notranslate"><span class="pre">AsyncStager.synchronize_staging()</span></code></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributed.checkpoint.staging.BlockingAsyncStager"><code class="docutils literal notranslate"><span class="pre">BlockingAsyncStager</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributed.checkpoint.staging.BlockingAsyncStager.stage"><code class="docutils literal notranslate"><span class="pre">BlockingAsyncStager.stage()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributed.checkpoint.staging.BlockingAsyncStager.synchronize_staging"><code class="docutils literal notranslate"><span class="pre">BlockingAsyncStager.synchronize_staging()</span></code></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributed.checkpoint.stateful.Stateful"><code class="docutils literal notranslate"><span class="pre">Stateful</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributed.checkpoint.stateful.Stateful.load_state_dict"><code class="docutils literal notranslate"><span class="pre">Stateful.load_state_dict()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributed.checkpoint.stateful.Stateful.state_dict"><code class="docutils literal notranslate"><span class="pre">Stateful.state_dict()</span></code></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributed.checkpoint.StorageReader"><code class="docutils literal notranslate"><span class="pre">StorageReader</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributed.checkpoint.StorageReader.prepare_global_plan"><code class="docutils literal notranslate"><span class="pre">StorageReader.prepare_global_plan()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributed.checkpoint.StorageReader.prepare_local_plan"><code class="docutils literal notranslate"><span class="pre">StorageReader.prepare_local_plan()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributed.checkpoint.StorageReader.read_data"><code class="docutils literal notranslate"><span class="pre">StorageReader.read_data()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributed.checkpoint.StorageReader.read_metadata"><code class="docutils literal notranslate"><span class="pre">StorageReader.read_metadata()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributed.checkpoint.StorageReader.reset"><code class="docutils literal notranslate"><span class="pre">StorageReader.reset()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributed.checkpoint.StorageReader.set_up_storage_reader"><code class="docutils literal notranslate"><span class="pre">StorageReader.set_up_storage_reader()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributed.checkpoint.StorageReader.validate_checkpoint_id"><code class="docutils literal notranslate"><span class="pre">StorageReader.validate_checkpoint_id()</span></code></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributed.checkpoint.StorageWriter"><code class="docutils literal notranslate"><span class="pre">StorageWriter</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributed.checkpoint.StorageWriter.finish"><code class="docutils literal notranslate"><span class="pre">StorageWriter.finish()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributed.checkpoint.StorageWriter.prepare_global_plan"><code class="docutils literal notranslate"><span class="pre">StorageWriter.prepare_global_plan()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributed.checkpoint.StorageWriter.prepare_local_plan"><code class="docutils literal notranslate"><span class="pre">StorageWriter.prepare_local_plan()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributed.checkpoint.StorageWriter.reset"><code class="docutils literal notranslate"><span class="pre">StorageWriter.reset()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributed.checkpoint.StorageWriter.set_up_storage_writer"><code class="docutils literal notranslate"><span class="pre">StorageWriter.set_up_storage_writer()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributed.checkpoint.StorageWriter.storage_meta"><code class="docutils literal notranslate"><span class="pre">StorageWriter.storage_meta()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributed.checkpoint.StorageWriter.validate_checkpoint_id"><code class="docutils literal notranslate"><span class="pre">StorageWriter.validate_checkpoint_id()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributed.checkpoint.StorageWriter.write_data"><code class="docutils literal notranslate"><span class="pre">StorageWriter.write_data()</span></code></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributed.checkpoint.LoadPlanner"><code class="docutils literal notranslate"><span class="pre">LoadPlanner</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributed.checkpoint.LoadPlanner.commit_tensor"><code class="docutils literal notranslate"><span class="pre">LoadPlanner.commit_tensor()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributed.checkpoint.LoadPlanner.create_global_plan"><code class="docutils literal notranslate"><span class="pre">LoadPlanner.create_global_plan()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributed.checkpoint.LoadPlanner.create_local_plan"><code class="docutils literal notranslate"><span class="pre">LoadPlanner.create_local_plan()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributed.checkpoint.LoadPlanner.finish_plan"><code class="docutils literal notranslate"><span class="pre">LoadPlanner.finish_plan()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributed.checkpoint.LoadPlanner.load_bytes"><code class="docutils literal notranslate"><span class="pre">LoadPlanner.load_bytes()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributed.checkpoint.LoadPlanner.resolve_bytes"><code class="docutils literal notranslate"><span class="pre">LoadPlanner.resolve_bytes()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributed.checkpoint.LoadPlanner.resolve_tensor"><code class="docutils literal notranslate"><span class="pre">LoadPlanner.resolve_tensor()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributed.checkpoint.LoadPlanner.set_up_planner"><code class="docutils literal notranslate"><span class="pre">LoadPlanner.set_up_planner()</span></code></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributed.checkpoint.LoadPlan"><code class="docutils literal notranslate"><span class="pre">LoadPlan</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributed.checkpoint.ReadItem"><code class="docutils literal notranslate"><span class="pre">ReadItem</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributed.checkpoint.SavePlanner"><code class="docutils literal notranslate"><span class="pre">SavePlanner</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributed.checkpoint.SavePlanner.create_global_plan"><code class="docutils literal notranslate"><span class="pre">SavePlanner.create_global_plan()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributed.checkpoint.SavePlanner.create_local_plan"><code class="docutils literal notranslate"><span class="pre">SavePlanner.create_local_plan()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributed.checkpoint.SavePlanner.finish_plan"><code class="docutils literal notranslate"><span class="pre">SavePlanner.finish_plan()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributed.checkpoint.SavePlanner.resolve_data"><code class="docutils literal notranslate"><span class="pre">SavePlanner.resolve_data()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributed.checkpoint.SavePlanner.set_up_planner"><code class="docutils literal notranslate"><span class="pre">SavePlanner.set_up_planner()</span></code></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributed.checkpoint.SavePlan"><code class="docutils literal notranslate"><span class="pre">SavePlan</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributed.checkpoint.planner.WriteItem"><code class="docutils literal notranslate"><span class="pre">WriteItem</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributed.checkpoint.planner.WriteItem.tensor_storage_size"><code class="docutils literal notranslate"><span class="pre">WriteItem.tensor_storage_size()</span></code></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributed.checkpoint.FileSystemReader"><code class="docutils literal notranslate"><span class="pre">FileSystemReader</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributed.checkpoint.FileSystemReader.checkpoint_id"><code class="docutils literal notranslate"><span class="pre">FileSystemReader.checkpoint_id</span></code></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributed.checkpoint.FileSystemWriter"><code class="docutils literal notranslate"><span class="pre">FileSystemWriter</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributed.checkpoint.FileSystemWriter.stage"><code class="docutils literal notranslate"><span class="pre">FileSystemWriter.stage()</span></code></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributed.checkpoint.DefaultSavePlanner"><code class="docutils literal notranslate"><span class="pre">DefaultSavePlanner</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributed.checkpoint.DefaultSavePlanner.lookup_object"><code class="docutils literal notranslate"><span class="pre">DefaultSavePlanner.lookup_object()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributed.checkpoint.DefaultSavePlanner.transform_object"><code class="docutils literal notranslate"><span class="pre">DefaultSavePlanner.transform_object()</span></code></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributed.checkpoint.DefaultLoadPlanner"><code class="docutils literal notranslate"><span class="pre">DefaultLoadPlanner</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributed.checkpoint.DefaultLoadPlanner.lookup_tensor"><code class="docutils literal notranslate"><span class="pre">DefaultLoadPlanner.lookup_tensor()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributed.checkpoint.DefaultLoadPlanner.transform_tensor"><code class="docutils literal notranslate"><span class="pre">DefaultLoadPlanner.transform_tensor()</span></code></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributed.checkpoint.state_dict.get_state_dict"><code class="docutils literal notranslate"><span class="pre">get_state_dict()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributed.checkpoint.state_dict.get_model_state_dict"><code class="docutils literal notranslate"><span class="pre">get_model_state_dict()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributed.checkpoint.state_dict.get_optimizer_state_dict"><code class="docutils literal notranslate"><span class="pre">get_optimizer_state_dict()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributed.checkpoint.state_dict.set_state_dict"><code class="docutils literal notranslate"><span class="pre">set_state_dict()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributed.checkpoint.state_dict.set_model_state_dict"><code class="docutils literal notranslate"><span class="pre">set_model_state_dict()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributed.checkpoint.state_dict.set_optimizer_state_dict"><code class="docutils literal notranslate"><span class="pre">set_optimizer_state_dict()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributed.checkpoint.state_dict.StateDictOptions"><code class="docutils literal notranslate"><span class="pre">StateDictOptions</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributed.checkpoint.format_utils.dcp_to_torch_save"><code class="docutils literal notranslate"><span class="pre">dcp_to_torch_save()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributed.checkpoint.format_utils.torch_save_to_dcp"><code class="docutils literal notranslate"><span class="pre">torch_save_to_dcp()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributed.checkpoint.format_utils.BroadcastingTorchSaveReader"><code class="docutils literal notranslate"><span class="pre">BroadcastingTorchSaveReader</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributed.checkpoint.format_utils.BroadcastingTorchSaveReader.prepare_global_plan"><code class="docutils literal notranslate"><span class="pre">BroadcastingTorchSaveReader.prepare_global_plan()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributed.checkpoint.format_utils.BroadcastingTorchSaveReader.prepare_local_plan"><code class="docutils literal notranslate"><span class="pre">BroadcastingTorchSaveReader.prepare_local_plan()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributed.checkpoint.format_utils.BroadcastingTorchSaveReader.read_data"><code class="docutils literal notranslate"><span class="pre">BroadcastingTorchSaveReader.read_data()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributed.checkpoint.format_utils.BroadcastingTorchSaveReader.read_metadata"><code class="docutils literal notranslate"><span class="pre">BroadcastingTorchSaveReader.read_metadata()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributed.checkpoint.format_utils.BroadcastingTorchSaveReader.reset"><code class="docutils literal notranslate"><span class="pre">BroadcastingTorchSaveReader.reset()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributed.checkpoint.format_utils.BroadcastingTorchSaveReader.set_up_storage_reader"><code class="docutils literal notranslate"><span class="pre">BroadcastingTorchSaveReader.set_up_storage_reader()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributed.checkpoint.format_utils.BroadcastingTorchSaveReader.validate_checkpoint_id"><code class="docutils literal notranslate"><span class="pre">BroadcastingTorchSaveReader.validate_checkpoint_id()</span></code></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributed.checkpoint.format_utils.DynamicMetaLoadPlanner"><code class="docutils literal notranslate"><span class="pre">DynamicMetaLoadPlanner</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributed.checkpoint.format_utils.DynamicMetaLoadPlanner.set_up_planner"><code class="docutils literal notranslate"><span class="pre">DynamicMetaLoadPlanner.set_up_planner()</span></code></a></li>
</ul>
</li>
</ul>
</li>
</ul>
  </nav></div>

  <div class="sidebar-secondary-item">

  
  <div class="tocsection editthispage">
    <a href="https://github.com/pytorch/pytorch/edit/main/docs/source/python-api/distributed.checkpoint.rst">
      <i class="fa-solid fa-pencil"></i>
      
      
        
          Edit on GitHub
        
      
    </a>
  </div>
</div>

  <div class="sidebar-secondary-item">
    <div class="tocsection sourcelink">
      <a href="../_sources/python-api/distributed.checkpoint.rst.txt">
        <i class="fa-solid fa-file-lines"></i> Show Source
      </a>
    </div>
</div>

</div>
<div class="sidebar-secondary-item">
  <div class="sidebar-heading">PyTorch Community</div>
  <ul style="list-style-type: none; padding: 0;">
  
   <li><a class="nav-link nav-external" href="community/index.html" style="color: var(--pst-color-text-muted)">PyTorch Governance</a></li>
  
   <li><a class="nav-link nav-external" href="https://pytorch.org/docs/stable/community/design.html" style="color: var(--pst-color-text-muted)">PyTorch Design Philosophy</a></li>
  
   <li><a class="nav-link nav-external" href="https://github.com/pytorch/pytorch/wiki/The-Ultimate-Guide-to-PyTorch-Contributions" style="color: var(--pst-color-text-muted)">The Ultimate Guide to PyTorch Contributions</a></li>
  
  </ul>
</div>
<div class="sidebar-secondary-item">
 <div class="sidebar-heading">Language Bindings</div>
 <ul style="list-style-type: none; padding: 0;">
 
  <li><a class="nav-link nav-external" href="https://pytorch.org/docs/stable/cpp_index.html" style="color: var(--pst-color-text-muted)">C++</a></li>
 
  <li><a class="nav-link nav-external" href="https://pytorch.org/javadoc/" style="color: var(--pst-color-text-muted)">Javadoc</a></li>
 
  <li><a class="nav-link nav-external" href="https://github.com/pytorch/multipy" style="color: var(--pst-color-text-muted)">torch.multiply</a></li>
 
 </ul>
</div>
<div class="sidebar-secondary-item">
  <div class="sidebar-heading">PyTorch Libraries</div>
  <ul style="list-style-type: none; padding: 0;">
  
   <li><a class="nav-link nav-external" href="https://pytorch.org/audio/stable/" style="color: var(--pst-color-text-muted)">torchaudio</a></li>
  
   <li><a class="nav-link nav-external" href="https://pytorch.org/ao" style="color: var(--pst-color-text-muted)">torchao</a></li>
  
   <li><a class="nav-link nav-external" href="https://pytorch.org/executorch" style="color: var(--pst-color-text-muted)">ExecuTorch</a></li>
  
   <li><a class="nav-link nav-external" href="https://pytorch.org/torchrec" style="color: var(--pst-color-text-muted)">torchrec</a></li>
  
   <li><a class="nav-link nav-external" href="https://pytorch.org/serve/" style="color: var(--pst-color-text-muted)">torchserve</a></li>
  
   <li><a class="nav-link nav-external" href="https://pytorch.org/data" style="color: var(--pst-color-text-muted)">torchdata</a></li>
  
   <li><a class="nav-link nav-external" href="https://pytorch.org/data" style="color: var(--pst-color-text-muted)">torchvision</a></li>
  
   <li><a class="nav-link nav-external" href="https://pytorch.org/xla" style="color: var(--pst-color-text-muted)">PyTorch on XLA Devices</a></li>
  
  </ul>
</div>
</div>
</div>
              
            
          </div>
          <footer class="bd-footer-content">
            
          </footer>
        
      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script defer src="../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf"></script>
<script defer src="../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf"></script>


  <footer class="bd-footer">
    
    <div class="bd-footer__inner bd-page-width">
      
        <div class="footer-items__start">
          
            <div class="footer-item">

  <p class="copyright">
    
      © Copyright PyTorch Contributors.
      <br/>
    
  </p>
</div>
          
            <div class="footer-item">

  <p class="sphinx-version">
    Created using <a href="https://www.sphinx-doc.org/">Sphinx</a> 8.1.0.
    <br/>
  </p>
</div>
          
        </div>
      
    
    
      <div class="footer-items__end">
        
          <div class="footer-item">
<p class="theme-version">
  <!-- # L10n: Setting the PST URL as an argument as this does not need to be localized -->
  Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.16.1.
</p></div>
        
      </div>
    
   </div>
   

   <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
      <div class="container">
        <div class="row">
          <div class="col-md-4 text-center">
            <h2>Docs</h2>
            <p>Access comprehensive developer documentation for PyTorch</p>
            <a class="with-right-arrow" href="">View Docs</a>
          </div>

          <div class="col-md-4 text-center">
            <h2>Tutorials</h2>
            <p>Get in-depth tutorials for beginners and advanced developers</p>
            <a class="with-right-arrow" href="">View Tutorials</a>
          </div>

          <div class="col-md-4 text-center">
            <h2>Resources</h2>
            <p>Find development resources and get your questions answered</p>
            <a class="with-right-arrow" href="">View Resources</a>
          </div>
        </div>
      </div>
  </div>

  <footer class="site-footer">
      <div class="container footer-container">
        <div class="footer-logo-wrapper">
          <a href="" class="footer-logo"></a>
        </div>

        <div class="footer-links-wrapper">
          <div class="footer-links-col">
            <ul>
              <li class="list-title"><a href="">PyTorch</a></li>
              <li><a href="">Get Started</a></li>
              <li><a href="">Features</a></li>
              <li><a href="">Ecosystem</a></li>
              <li><a href="">Blog</a></li>
              <li><a href="">Contributing</a></li>
            </ul>
          </div>

          <div class="footer-links-col">
            <ul>
              <li class="list-title"><a href="">Resources</a></li>
              <li><a href="">Tutorials</a></li>
              <li><a href="">Docs</a></li>
              <li><a href="" target="_blank">Discuss</a></li>
              <li><a href="" target="_blank">Github Issues</a></li>
              <li><a href="" target="_blank">Brand Guidelines</a></li>
            </ul>
          </div>

          <div class="footer-links-col">
            <ul>
              <li class="list-title">Stay up to date</li>
              <li><a href="" target="_blank">Facebook</a></li>
              <li><a href="" target="_blank">Twitter</a></li>
              <li><a href="" target="_blank">YouTube</a></li>
              <li><a href="" target="_blank">LinkedIn</a></li>
            </ul>
            </div>

          <div class="footer-links-col">
            <ul>
              <li class="list-title">PyTorch Podcasts</li>
              <li><a href="" target="_blank">Spotify</a></li>
              <li><a href="" target="_blank">Apple</a></li>
              <li><a href="" target="_blank">Google</a></li>
              <li><a href="" target="_blank">Amazon</a></li>
            </ul>
           </div>
          </div>

          <div class="privacy-policy">
            <ul>
              <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/terms/" target="_blank">Terms</a></li>
              <li class="privacy-policy-links">|</li>
              <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/privacy-policy/" target="_blank">Privacy</a></li>
            </ul>
          </div>
          <div class="copyright">
          <p>© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation.
            For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see
            <a href="https://www.linuxfoundation.org/policies/">www.linuxfoundation.org/policies/</a>. The PyTorch Foundation supports the PyTorch open source
            project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC,
            please see <a href="https://www.lfprojects.org/policies/">www.lfprojects.org/policies/</a>.</p>
        </div>
       </div>

  <div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="../_static/img/pytorch-x.svg">
  </div>
</div>
  </footer>


  </body>
</html>