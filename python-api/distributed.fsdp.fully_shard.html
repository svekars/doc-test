
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>torch.distributed.fsdp.fully_shard &#8212; PyTorch main documentation</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  <!--
    this give us a css class that will be invisible only if js is disabled
  -->
  <noscript>
    <style>
      .pst-js-only { display: none !important; }

    </style>
  </noscript>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=949a1ff5" />
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/katex.min.css" />
    <link rel="stylesheet" type="text/css" href="../_static/katex-math.css?v=91adb8b6" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="../_static/css/jit.css?v=8de1ea5d" />
    <link rel="stylesheet" type="text/css" href="../_static/css/custom.css?v=3ccf5357" />
    <link rel="stylesheet" type="text/css" href="../_static/css/custom2.css?v=baa440dc" />
  
  <!-- So that users can add custom icons -->
  <script src="../_static/scripts/fontawesome.js?digest=8878045cc6db502f8baf"></script>
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf" />

    <script src="../_static/documentation_options.js?v=a8da1a53"></script>
    <script src="../_static/doctools.js?v=9bcbadda"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=940804e7"></script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'python-api/distributed.fsdp.fully_shard';</script>
    <script src="../_static/js/star-rating.js?v=8861fcb6"></script>
    <script src="../_static/js/send-feedback.js?v=5646bf45"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />

  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  <meta name="docsearch:version" content="main (2.6.0 )" />
<script src="https://code.jquery.com/jquery-3.7.1.min.js"></script>
<script type="text/javascript" src="../_static/js/send-feedback.js"></script>
<script type="text/javascript" src="../_static/js/star-rating.js"></script>
<script type="text/javascript" src="../_static/js/cookie-banner.js"></script>

<script async src="https://www.googletagmanager.com/gtag/js?id=G-TEST12345"></script>
    <script type="text/javascript">
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-TEST12345');
    </script>

  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
<body data-feedback-url="https://github.com/pytorch/pytorch">
  <div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>

          <li class="main-menu-item">
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Learn
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="">
                  <span class=dropdown-title>Get Started</span>
                  <p>Run PyTorch locally or get started quickly with one of the supported cloud platforms</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Tutorials</span>
                  <p>Whats new in PyTorch tutorials</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Learn the Basics</span>
                  <p>Familiarize yourself with PyTorch concepts and modules</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">PyTorch Recipes</span>
                  <p>Bite-size, ready-to-deploy PyTorch code examples</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Intro to PyTorch - YouTube Series</span>
                  <p>Master PyTorch basics with our engaging YouTube tutorial series</p>
                </a>
              </div>
            </div>
          </li>

          <li>
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Ecosystem
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Tools</span>
                  <p>Learn about the tools and frameworks in the PyTorch Ecosystem</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class=dropdown-title>Community</span>
                  <p>Join the PyTorch developer community to contribute, learn, and get your questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="" target="_blank">
                  <span class=dropdown-title>Forums</span>
                  <p>A place to discuss PyTorch code, issues, install, research</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class=dropdown-title>Developer Resources</span>
                  <p>Find resources and get questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Contributor Awards - 2024</span>
                  <p>Award winners announced at this year's PyTorch Conference</p>
                </a>
              </div>
            </div>
          </li>

          <li>
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Edge
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">About PyTorch Edge</span>
                  <p>Build innovative and privacy-aware AI experiences for edge devices</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">ExecuTorch</span>
                  <p>End-to-end solution for enabling on-device inference capabilities across mobile and edge devices</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">ExecuTorch Docs</span>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Docs
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">PyTorch</span>
                  <p>Explore the documentation for comprehensive guidance on how to use PyTorch</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">PyTorch Domains</span>
                  <p>Read the PyTorch Domains documentation to learn more about domain-specific libraries</p>
                </a>
              </div>
            </div>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Blogs & News
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">PyTorch Blog</span>
                  <p>Catch up on the latest technical news and happenings</p>
                </a>
                 <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Community Blog</span>
                  <p>Stories from the PyTorch ecosystem</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Videos</span>
                  <p>Learn about the latest PyTorch tutorials, new, and more </p>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Community Stories</span>
                  <p>Learn how our community solves real, everyday machine learning problems with PyTorch</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Events</span>
                  <p>Find events, webinars, and podcasts</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Newsletter</span>
                  <p>Stay up-to-date with the latest updates</p>
                </a>
            </div>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                About
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">PyTorch Foundation</span>
                  <p>Learn more about the PyTorch Foundation</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Governing Board</span>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Cloud Credit Program</span>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Technical Advisory Council</span>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Staff</span>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Contact Us</span>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
            <div class="no-dropdown">
              <a href="" data-cta="join">
                Become a Member
              </a>
            </div>
          </li>
          <li>
           <div class="main-menu-item">
             <a href="" class="github-icon">
             </a>
           </div>
          </li>
          <!--- TODO: This block adds the search icon to the nav bar. We will enable it later.
          <li>
            <div class="main-menu-item">
             <a href="" class="search-icon">
             </a>
            </div>
          </li>
          --->
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>
  </div>
</div>

  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <dialog id="pst-search-dialog">
    
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </dialog>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
<div class="bd-header__inner bd-page-width">
  <button class="pst-navbar-icon sidebar-toggle primary-toggle" aria-label="Site navigation">
    <span class="fa-solid fa-bars"></span>
  </button>
  
  
  <div class=" navbar-header-items__start">
    
      <div class="navbar-item">

  
    
  

<a class="navbar-brand logo" href="../index.html">
  
  
  
  
  
  
    <p class="title logo__title">Home</p>
  
</a></div>
    
  </div>
  
  <div class=" navbar-header-items">
    
    <div class="me-auto navbar-header-items__center">
      
        <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item ">
  <a class="nav-link nav-internal" href="index.html">
    Python API
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../notes/index.html">
    Developer Notes
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-external" href="https://pytorch.org/tutorials/">
    Tutorials
  </a>
</li>

  </ul>
</nav></div>
      
    </div>
    
    
    <div class="navbar-header-items__end">
      
        <div class="navbar-item navbar-persistent--container">
          

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button>
        </div>
      
      
        <div class="navbar-item">
<div class="version-switcher__container dropdown pst-js-only">
  <button id="pst-version-switcher-button-2"
    type="button"
    class="version-switcher__button btn btn-sm dropdown-toggle"
    data-bs-toggle="dropdown"
    aria-haspopup="listbox"
    aria-controls="pst-version-switcher-list-2"
    aria-label="Version switcher list"
  >
    Choose version  <!-- this text may get changed later by javascript -->
    <span class="caret"></span>
  </button>
  <div id="pst-version-switcher-list-2"
    class="version-switcher__menu dropdown-menu list-group-flush py-0"
    role="listbox" aria-labelledby="pst-version-switcher-button-2">
    <!-- dropdown will be populated by javascript on page load -->
  </div>
</div></div>
      
        <div class="navbar-item">

<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button></div>
      
        <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://x.com/PyTorch" title="X" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-x-twitter fa-lg" aria-hidden="true"></i>
            <span class="sr-only">X</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/pytorch/pytorch" title="GitHub" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">GitHub</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://pypi.org/project/torch" title="PyPI" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-custom fa-pypi fa-lg" aria-hidden="true"></i>
            <span class="sr-only">PyPI</span></a>
        </li>
</ul></div>
      
    </div>
    
  </div>
  
  
    <div class="navbar-persistent--mobile">

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button>
    </div>
  

  
    <button class="pst-navbar-icon sidebar-toggle secondary-toggle" aria-label="On this page">
      <span class="fa-solid fa-outdent"></span>
    </button>
  
</div>

    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
        
      
      <dialog id="pst-primary-sidebar-modal"></dialog>
      <div id="pst-primary-sidebar" class="bd-sidebar-primary bd-sidebar hide-on-wide">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
      <div class="sidebar-header-items__center">
        
          
          
            <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item ">
  <a class="nav-link nav-internal" href="index.html">
    Python API
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../notes/index.html">
    Developer Notes
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-external" href="https://pytorch.org/tutorials/">
    Tutorials
  </a>
</li>

  </ul>
</nav></div>
          
        
      </div>
    
    
    
      <div class="sidebar-header-items__end">
        
          <div class="navbar-item">
<div class="version-switcher__container dropdown pst-js-only">
  <button id="pst-version-switcher-button-3"
    type="button"
    class="version-switcher__button btn btn-sm dropdown-toggle"
    data-bs-toggle="dropdown"
    aria-haspopup="listbox"
    aria-controls="pst-version-switcher-list-3"
    aria-label="Version switcher list"
  >
    Choose version  <!-- this text may get changed later by javascript -->
    <span class="caret"></span>
  </button>
  <div id="pst-version-switcher-list-3"
    class="version-switcher__menu dropdown-menu list-group-flush py-0"
    role="listbox" aria-labelledby="pst-version-switcher-button-3">
    <!-- dropdown will be populated by javascript on page load -->
  </div>
</div></div>
        
          <div class="navbar-item">

<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button></div>
        
          <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://x.com/PyTorch" title="X" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-x-twitter fa-lg" aria-hidden="true"></i>
            <span class="sr-only">X</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/pytorch/pytorch" title="GitHub" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">GitHub</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://pypi.org/project/torch" title="PyPI" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-custom fa-pypi fa-lg" aria-hidden="true"></i>
            <span class="sr-only">PyPI</span></a>
        </li>
</ul></div>
        
      </div>
    
  </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
      <div class="sidebar-primary-item">
<div id="ethical-ad-placement"
      class="flat"
      data-ea-publisher="readthedocs"
      data-ea-type="readthedocs-sidebar"
      data-ea-manual="true">
</div></div>
  </div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        
          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item">

<nav aria-label="Breadcrumb" class="d-print-none">
  <ul class="bd-breadcrumbs">
    
    <li class="breadcrumb-item breadcrumb-home">
      <a href="../index.html" class="nav-link" aria-label="Home">
        <i class="fa-solid fa-home"></i>
      </a>
    </li>
    <li class="breadcrumb-item active" aria-current="page"><span class="ellipsis">torch.distributed.fsdp.fully_shard</span></li>
  </ul>
</nav>
</div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">
<div class="rating">
    Rate this Page
    <div class="stars">
        
        <span class="star" data-value="1">★</span>
        
        <span class="star" data-value="2">★</span>
        
        <span class="star" data-value="3">★</span>
        
        <span class="star" data-value="4">★</span>
        
        <span class="star" data-value="5">★</span>
        
    </div>
</div>
</div>
      
    </div>
  
</div>
</div>
              
              
              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section id="torch-distributed-fsdp-fully-shard">
<h1>torch.distributed.fsdp.fully_shard<a class="headerlink" href="#torch-distributed-fsdp-fully-shard" title="Link to this heading">#</a></h1>
<section id="pytorch-fsdp2-fully-shard">
<h2>PyTorch FSDP2 (<code class="docutils literal notranslate"><span class="pre">fully_shard</span></code>)<a class="headerlink" href="#pytorch-fsdp2-fully-shard" title="Link to this heading">#</a></h2>
<p>PyTorch FSDP2 provides a fully sharded data parallelism (FSDP) implementation
targeting performant eager-mode while using per-parameter sharding for improved
usability.</p>
<ul class="simple">
<li><p>If you are new to FSDP, we recommend that you start with FSDP2 due to improved
usability.</p></li>
<li><p>If you are currently using FSDP1, consider evaluating the following
differences to see if you should switch to FSDP2:</p></li>
</ul>
<p>Compared to PyTorch FSDP1 (<code class="docutils literal notranslate"><span class="pre">FullyShardedDataParallel</span></code>):</p>
<ul class="simple">
<li><p>FSDP2 uses <code class="docutils literal notranslate"><span class="pre">DTensor</span></code>-based dim-0 per-parameter sharding for a simpler
sharding representation compared to FSDP1’s flat-parameter sharding, while
preserving similar throughput performance. More specifically, FSDP2 chunks
each parameter on dim-0 across the data parallel workers (using
<code class="docutils literal notranslate"><span class="pre">torch.chunk(dim=0)</span></code>), whereas FSDP1 flattens, concatenates, and chunks a
group of tensors together, making reasoning about what data is present on
each worker and resharding to different parallelisms complex. Per-parameter
sharding provides a more intuitive user experience, relaxes constraints
around frozen parameters, and allows for communication-free (sharded) state
dicts, which otherwise require all-gathers in FSDP1.</p></li>
<li><p>FSDP2 implements a different memory management approach to handle the
multi-stream usages that avoids <code class="docutils literal notranslate"><span class="pre">torch.Tensor.record_stream</span></code>. This ensures
deterministic and expected memory usage and does not require blocking the CPU
like in FSDP1’s <code class="docutils literal notranslate"><span class="pre">limit_all_gathers=True</span></code>.</p></li>
<li><p>FSDP2 exposes APIs for manual control over prefetching and collective
scheduling, allowing power users more customization. See the methods on
<code class="docutils literal notranslate"><span class="pre">FSDPModule</span></code> below for details.</p></li>
<li><p>FSDP2 simplifies some of the API surface: e.g. FSDP2 does not directly
support full state dicts. Instead, users can reshard the sharded state dicts
containing <code class="docutils literal notranslate"><span class="pre">DTensor</span></code> s to full state dicts themselves using <code class="docutils literal notranslate"><span class="pre">DTensor</span></code>
APIs like <code class="docutils literal notranslate"><span class="pre">DTensor.full_tensor()</span></code> or by using higher-level APIs like
<a class="reference external" href="https://pytorch.org/docs/stable/distributed.checkpoint.html">PyTorch Distributed Checkpoint</a> ‘s
distributed state dict APIs. Also, some other args have been removed; see
<a class="reference external" href="https://github.com/pytorch/torchtitan/blob/main/docs/fsdp.md">here</a> for
details.</p></li>
</ul>
<p>If you are onboarding FSDP for the first time or if any of the above appeals to
your use case, we recommend that you consider using FSDP2.</p>
<p>See <a class="reference external" href="https://github.com/pytorch/pytorch/issues/114299">this RFC</a> for details
on system design and implementation.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><code class="docutils literal notranslate"><span class="pre">torch.distributed.fsdp.fully_shard</span></code> is currently in prototype state and
under development. The core API will likely not change, but we may make some
API changes if necessary.</p>
</div>
<p>The frontend API is <code class="docutils literal notranslate"><span class="pre">fully_shard</span></code> that can be called on a <code class="docutils literal notranslate"><span class="pre">module</span></code>:</p>
<dl class="py function">
<dt class="sig sig-object py" id="torch.distributed.fsdp.fully_shard">
<span class="sig-prename descclassname"><span class="pre">torch.distributed.fsdp.</span></span><span class="sig-name descname"><span class="pre">fully_shard</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">module</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mesh</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reshard_after_forward</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">shard_placement_fn</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mp_policy</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">MixedPrecisionPolicy(param_dtype=None,</span> <span class="pre">reduce_dtype=None,</span> <span class="pre">output_dtype=None,</span> <span class="pre">cast_forward_inputs=True)</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">offload_policy</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">OffloadPolicy()</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributed/fsdp/_fully_shard/_fully_shard.py#L50"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.fsdp.fully_shard" title="Link to this definition">#</a></dt>
<dd><p>Apply fully sharded data parallelism (FSDP) to <code class="docutils literal notranslate"><span class="pre">module</span></code>, where FSDP
shards module parameters, gradients, and optimizer states across data
parallel workers to save memory at the cost of communication.</p>
<p>At initialization, FSDP shards the module’s parameters across the data
parallel workers given by <code class="docutils literal notranslate"><span class="pre">mesh</span></code>. Before forward, FSDP all-gathers the
sharded parameters across the data-parallel workers to get the unsharded
parameters for forward computation. If <code class="docutils literal notranslate"><span class="pre">reshard_after_forward</span></code> is
<code class="docutils literal notranslate"><span class="pre">True</span></code>, then FSDP frees the unsharded parameters after forward and
re-all-gathers them in backward before gradient computation. After gradient
computation, FSDP frees the unsharded parameters and reduce-scatters the
unsharded gradients across data-parallel workers.</p>
<p>This implementation represents the sharded parameters as <code class="xref py py-class docutils literal notranslate"><span class="pre">DTensor</span></code> s
sharded on dim-0, while the unsharded parameters will be like the original
parameters on <code class="docutils literal notranslate"><span class="pre">module</span></code> (e.g. <a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Tensor</span></code></a> if originally
<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Tensor</span></code></a>). A module
<a class="reference external" href="https://pytorch.org/docs/main/generated/torch.nn.Module.html#torch.nn.Module.register_forward_pre_hook">forward pre-hook</a>
on <code class="docutils literal notranslate"><span class="pre">module</span></code> all-gathers the parameters, and a module
<a class="reference external" href="https://pytorch.org/docs/main/generated/torch.nn.Module.html#torch.nn.Module.register_forward_hook">forward hook</a>
on <code class="docutils literal notranslate"><span class="pre">module</span></code> frees them (if needed). Similar backward hooks all-gather
parameters and later free parameters and reduce-scatter gradients.</p>
<p>Since grouping multiple tensors together for one collective is critical for
communication efficiency, this implementation makes this grouping first
class. Calling <a class="reference internal" href="#torch.distributed.fsdp.fully_shard" title="torch.distributed.fsdp.fully_shard"><code class="xref py py-meth docutils literal notranslate"><span class="pre">fully_shard()</span></code></a> on <code class="docutils literal notranslate"><span class="pre">module</span></code> constructs one group that
includes the parameters in <code class="docutils literal notranslate"><span class="pre">module.parameters()</span></code> except those already
assigned to a group from an earlier call on a submodule. This means that
<a class="reference internal" href="#torch.distributed.fsdp.fully_shard" title="torch.distributed.fsdp.fully_shard"><code class="xref py py-meth docutils literal notranslate"><span class="pre">fully_shard()</span></code></a> should be called bottom-up on your model. Each group’s
parameters are all-gathered in one collective, and its gradients are
reduce-scattered in one collective. Partitioning the model into multiple
groups (“layer by layer”) allows for peak memory savings and communication/computation
overlap. Users generally should <em>not</em> call <a class="reference internal" href="#torch.distributed.fsdp.fully_shard" title="torch.distributed.fsdp.fully_shard"><code class="xref py py-meth docutils literal notranslate"><span class="pre">fully_shard()</span></code></a> only on the
topmost root module.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>module</strong> (<em>Union</em><em>[</em><a class="reference internal" href="generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module"><em>nn.Module</em></a><em>, </em><em>List</em><em>[</em><a class="reference internal" href="generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module"><em>nn.Module</em></a><em>]</em>) – The module or modules to
shard with FSDP and group together for communication.</p></li>
<li><p><strong>mesh</strong> (<em>Optional</em><em>[</em><a class="reference internal" href="distributed.html#torch.distributed.device_mesh.DeviceMesh" title="torch.distributed.device_mesh.DeviceMesh"><em>DeviceMesh</em></a><em>]</em>) – This data parallel mesh defines the
sharding and device. If 1D, then parameters are fully sharded
across the 1D mesh (FSDP) with <code class="docutils literal notranslate"><span class="pre">(Shard(0),)</span></code> placement. If 2D,
then parameters are sharded across the 1st dim and replicated
across the 0th dim (HSDP) with <code class="docutils literal notranslate"><span class="pre">(Replicate(),</span> <span class="pre">Shard(0))</span></code>
placement. The mesh’s device type gives the device type used for
communication; if a CUDA or CUDA-like device type, then we use the
current device.</p></li>
<li><p><strong>reshard_after_forward</strong> (<em>Union</em><em>[</em><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><em>bool</em></a><em>, </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a><em>]</em>) – <p>This controls the parameter
behavior after forward and can trade off memory and communication:</p>
<ul>
<li><p>If <code class="docutils literal notranslate"><span class="pre">True</span></code>, then this reshards parameters after forward and
re-all-gathers in backward.</p></li>
<li><p>If <code class="docutils literal notranslate"><span class="pre">False</span></code>, then this keeps the unsharded parameters in memory
after forward and avoids the all-gather in backward.</p></li>
<li><p>If an <code class="docutils literal notranslate"><span class="pre">int</span></code>, then this represents the world size to reshard to
after forward. It should be a non-trivial divisor of the <code class="docutils literal notranslate"><span class="pre">mesh</span></code>
shard dim size (i.e. excluding 1 and the dim size itself). A
choice may be the intra-node size (e.g. <code class="docutils literal notranslate"><span class="pre">torch.cuda.device_count()</span></code>).
This allows the all-gather in backward to be over a smaller world
size at the cost of higher memory usage than setting to <code class="docutils literal notranslate"><span class="pre">True</span></code>.</p></li>
<li><p>The root FSDP state has its value specially set to <code class="docutils literal notranslate"><span class="pre">False</span></code> as a
heuristic since its parameters would typically be immediately
all-gathered for backward.</p></li>
<li><p>After forward, the parameters registered to the module depend on
to this: The registered parameters are the sharded parameters if
<code class="docutils literal notranslate"><span class="pre">True</span></code>; unsharded parameters if <code class="docutils literal notranslate"><span class="pre">False</span></code>; and the paramters
resharded to the smaller mesh otherwise. To modify the parameters
between forward and backward, the registered parameters must be
the sharded parameters. For <code class="docutils literal notranslate"><span class="pre">False</span></code> or an <code class="docutils literal notranslate"><span class="pre">int</span></code>, this can be
done by manually resharding via <code class="xref py py-meth docutils literal notranslate"><span class="pre">reshard()</span></code>.</p></li>
</ul>
</p></li>
<li><p><strong>shard_placement_fn</strong> (<em>Optional</em><em>[</em><em>Callable</em><em>[</em><em>[</em><em>nn.Parameter</em><em>]</em><em>, </em><em>Optional</em><em>[</em><a class="reference internal" href="distributed.tensor.html#torch.distributed.tensor.placement_types.Shard" title="torch.distributed.tensor.placement_types.Shard"><em>Shard</em></a><em>]</em><em>]</em><em>]</em>) – This callable can be used to override the sharding placement for a
parameter to shard a parameter on a dimension other than dim-0. If
this callable returns a <code class="xref py py-class docutils literal notranslate"><span class="pre">Shard</span></code> placement (not <code class="docutils literal notranslate"><span class="pre">None</span></code>),
then FSDP will shard according to that placement (e.g. <code class="docutils literal notranslate"><span class="pre">Shard(1)</span></code>).
If sharding on a nonzero dim, we currently require even sharding,
i.e. the tensor dim size on that dim must be divisible by the FSDP
shard mesh size.</p></li>
<li><p><strong>mp_policy</strong> (<a class="reference internal" href="#torch.distributed.fsdp.MixedPrecisionPolicy" title="torch.distributed.fsdp.MixedPrecisionPolicy"><em>MixedPrecisionPolicy</em></a>) – This controls the mixed precision
policy, which offers parameter/reduction mixed precision for this
module. See <a class="reference internal" href="#torch.distributed.fsdp.MixedPrecisionPolicy" title="torch.distributed.fsdp.MixedPrecisionPolicy"><code class="xref py py-class docutils literal notranslate"><span class="pre">MixedPrecisionPolicy</span></code></a> for details.</p></li>
<li><p><strong>offload_policy</strong> (<a class="reference internal" href="#torch.distributed.fsdp.OffloadPolicy" title="torch.distributed.fsdp.OffloadPolicy"><em>OffloadPolicy</em></a>) – This controls the offloading policy,
which offers parameter/gradient/optimizer state offloading. See
<a class="reference internal" href="#torch.distributed.fsdp.OffloadPolicy" title="torch.distributed.fsdp.OffloadPolicy"><code class="xref py py-class docutils literal notranslate"><span class="pre">OffloadPolicy</span></code></a> and its subclasses for details.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<p>Calling <code class="docutils literal notranslate"><span class="pre">fully_shard(module)</span></code> dynamically constructs a new class that
subclasses <code class="docutils literal notranslate"><span class="pre">type(module)</span></code> and an FSDP class <code class="docutils literal notranslate"><span class="pre">FSDPModule</span></code>. For example, if
we call <code class="docutils literal notranslate"><span class="pre">fully_shard(linear)</span></code> on a module <code class="docutils literal notranslate"><span class="pre">linear:</span> <span class="pre">nn.Linear</span></code>, then FSDP
constructs a new class <code class="docutils literal notranslate"><span class="pre">FSDPLinear</span></code> and changes <code class="docutils literal notranslate"><span class="pre">linear</span></code> ‘s type to this.
Otherwise, <code class="docutils literal notranslate"><span class="pre">fully_shard</span></code> does not change the module structure and parameter
fully-qualified names. The class <code class="docutils literal notranslate"><span class="pre">FSDPModule</span></code> allows providing some
FSDP-specific methods on the module.</p>
<dl class="py class">
<dt class="sig sig-object py" id="torch.distributed.fsdp.FSDPModule">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torch.distributed.fsdp.</span></span><span class="sig-name descname"><span class="pre">FSDPModule</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributed.fsdp.FSDPModule" title="Link to this definition">#</a></dt>
<dd><dl class="py method">
<dt class="sig sig-object py" id="torch.distributed.fsdp.FSDPModule.reshard">
<span class="sig-name descname"><span class="pre">reshard</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torch/distributed/fsdp/_fully_shard/_fully_shard.html#FSDPModule.reshard"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributed/fsdp/_fully_shard/_fully_shard.py#L223"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.fsdp.FSDPModule.reshard" title="Link to this definition">#</a></dt>
<dd><p>Reshards the module’s parameters, freeing the unsharded parameters if
they are allocated and registering the sharded parameters to the
module. This method is <em>not</em> recursive.</p>
<dl class="field-list simple">
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributed.fsdp.FSDPModule.set_is_last_backward">
<span class="sig-name descname"><span class="pre">set_is_last_backward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">is_last_backward</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torch/distributed/fsdp/_fully_shard/_fully_shard.html#FSDPModule.set_is_last_backward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributed/fsdp/_fully_shard/_fully_shard.py#L262"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.fsdp.FSDPModule.set_is_last_backward" title="Link to this definition">#</a></dt>
<dd><p>Sets whether the next backward is the last one. On the last backward,
FSDP waits on pending gradient reduction and clears internal data
data structures for backward prefetching. This can be useful for
microbatching.</p>
<dl class="field-list simple">
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributed.fsdp.FSDPModule.set_modules_to_backward_prefetch">
<span class="sig-name descname"><span class="pre">set_modules_to_backward_prefetch</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">modules</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torch/distributed/fsdp/_fully_shard/_fully_shard.html#FSDPModule.set_modules_to_backward_prefetch"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributed/fsdp/_fully_shard/_fully_shard.py#L354"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.fsdp.FSDPModule.set_modules_to_backward_prefetch" title="Link to this definition">#</a></dt>
<dd><p>Sets the FSDP modules for which this FSDP module should explicitly
prefetch all-gathers in backward. This overrides the default backward
pretching implementation that prefetches the next FSDP module based on
the reverse post-forward order.</p>
<p>Passing a singleton list containing the previous FSDP module gives the
same all-gather overlap behavior as the default overlap behavior.
Passing a list with at least length two is required for more aggressive
overlap and will use more reserved memory.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>modules</strong> (<em>List</em><em>[</em><a class="reference internal" href="#torch.distributed.fsdp.FSDPModule" title="torch.distributed.fsdp.FSDPModule"><em>FSDPModule</em></a><em>]</em>) – FSDP modules to prefetch.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributed.fsdp.FSDPModule.set_modules_to_forward_prefetch">
<span class="sig-name descname"><span class="pre">set_modules_to_forward_prefetch</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">modules</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torch/distributed/fsdp/_fully_shard/_fully_shard.html#FSDPModule.set_modules_to_forward_prefetch"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributed/fsdp/_fully_shard/_fully_shard.py#L334"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.fsdp.FSDPModule.set_modules_to_forward_prefetch" title="Link to this definition">#</a></dt>
<dd><p>Sets the FSDP modules for which this FSDP module should explicitly
prefetch all-gathers in forward. The prefetching runs after this
module’s all-gather copy-out.</p>
<p>Passing a singleton list containing the next FSDP module gives the same
all-gather overlap behavior as the default overlap behavior, except the
prefetched all-gather is issued earlier from the CPU. Passing a list
with at least length two is required for more aggressive overlap and
will use more reserved memory.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>modules</strong> (<em>List</em><em>[</em><a class="reference internal" href="#torch.distributed.fsdp.FSDPModule" title="torch.distributed.fsdp.FSDPModule"><em>FSDPModule</em></a><em>]</em>) – FSDP modules to prefetch.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributed.fsdp.FSDPModule.set_post_optim_event">
<span class="sig-name descname"><span class="pre">set_post_optim_event</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">event</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torch/distributed/fsdp/_fully_shard/_fully_shard.html#FSDPModule.set_post_optim_event"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributed/fsdp/_fully_shard/_fully_shard.py#L374"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.fsdp.FSDPModule.set_post_optim_event" title="Link to this definition">#</a></dt>
<dd><p>Sets a post-optimizer-step event for the root FSDP module to wait the
all-gather streams on.</p>
<p>By default, the root FSDP module waits the all-gather streams on the
current stream to ensure that the optimizer step has finished before
all-gathering. However, this may introduce false dependencies if
there is unrelated computation after the optimizer step. This API
allows the user to provide their own event to wait on. After the root
waits on the event, the event is discarded, so this API should be
called with a new event each iteration.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>event</strong> (<a class="reference internal" href="generated/torch.mtia.Event.html#torch.mtia.Event" title="torch.Event"><em>torch.Event</em></a>) – Event recorded after the optimizer step
to wait all-gather streams on.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributed.fsdp.FSDPModule.set_reduce_scatter_divide_factor">
<span class="sig-name descname"><span class="pre">set_reduce_scatter_divide_factor</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">factor</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torch/distributed/fsdp/_fully_shard/_fully_shard.html#FSDPModule.set_reduce_scatter_divide_factor"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributed/fsdp/_fully_shard/_fully_shard.py#L393"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.fsdp.FSDPModule.set_reduce_scatter_divide_factor" title="Link to this definition">#</a></dt>
<dd><p>Sets a custom divide factor for the reduce-scatter. This becomes a
custom reduce op using NCCL’s PreMulSum, which allows multiplying by
the factor before reduction.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>factor</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a>) – Custom divide factor.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributed.fsdp.FSDPModule.set_requires_all_reduce">
<span class="sig-name descname"><span class="pre">set_requires_all_reduce</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">requires_all_reduce</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">recurse</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torch/distributed/fsdp/_fully_shard/_fully_shard.html#FSDPModule.set_requires_all_reduce"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributed/fsdp/_fully_shard/_fully_shard.py#L295"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.fsdp.FSDPModule.set_requires_all_reduce" title="Link to this definition">#</a></dt>
<dd><p>Sets if the module should all-reduce gradients. This can be used to
implement gradient accumulation with only reduce-scatter but not
all-reduce for HSDP.</p>
<dl class="field-list simple">
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributed.fsdp.FSDPModule.set_requires_gradient_sync">
<span class="sig-name descname"><span class="pre">set_requires_gradient_sync</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">requires_gradient_sync</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">recurse</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torch/distributed/fsdp/_fully_shard/_fully_shard.html#FSDPModule.set_requires_gradient_sync"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributed/fsdp/_fully_shard/_fully_shard.py#L272"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.fsdp.FSDPModule.set_requires_gradient_sync" title="Link to this definition">#</a></dt>
<dd><p>Sets if the module should sync gradients. This can be used to implement
gradient accumulation <em>without communication</em>. For HSDP, this controls
both reduce-scatter and all-reduce together.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>requires_gradient_sync</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><em>bool</em></a>) – Whether to reduce gradients for the
module’s parameters.</p></li>
<li><p><strong>recurse</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><em>bool</em></a>) – Whether to set for all FSDP submodules or just the
passed-in module.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributed.fsdp.FSDPModule.set_reshard_after_backward">
<span class="sig-name descname"><span class="pre">set_reshard_after_backward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">reshard_after_backward</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">recurse</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torch/distributed/fsdp/_fully_shard/_fully_shard.html#FSDPModule.set_reshard_after_backward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributed/fsdp/_fully_shard/_fully_shard.py#L311"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.fsdp.FSDPModule.set_reshard_after_backward" title="Link to this definition">#</a></dt>
<dd><p>Sets if the module should reshard parameters after backward. This can
be used during gradient accumulation to trade off higher memory for
reduced communication since the unsharded parameters do not need to be
re-all-gathered before the next forward.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>reshard_after_backward</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><em>bool</em></a>) – Whether to reshard parameters after
backward.</p></li>
<li><p><strong>recurse</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><em>bool</em></a>) – Whether to set for all FSDP submodules or just the
passed-in module.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributed.fsdp.FSDPModule.set_unshard_in_backward">
<span class="sig-name descname"><span class="pre">set_unshard_in_backward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">unshard_in_backward</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torch/distributed/fsdp/_fully_shard/_fully_shard.html#FSDPModule.set_unshard_in_backward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributed/fsdp/_fully_shard/_fully_shard.py#L408"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.fsdp.FSDPModule.set_unshard_in_backward" title="Link to this definition">#</a></dt>
<dd><p>Sets whether the FSDP module’s parameters need to be unsharded in
backward. This can be used in expert cases when the user knows that all
parameters in this FSDP module’s parameter group are not needed for
backward computation (e.g. embedding).</p>
<dl class="field-list simple">
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributed.fsdp.FSDPModule.unshard">
<span class="sig-name descname"><span class="pre">unshard</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">async_op</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torch/distributed/fsdp/_fully_shard/_fully_shard.html#FSDPModule.unshard"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributed/fsdp/_fully_shard/_fully_shard.py#L233"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.fsdp.FSDPModule.unshard" title="Link to this definition">#</a></dt>
<dd><p>Unshards the module’s parameters by allocating memory and all-gathering
the parameters. This method is <em>not</em> recursive. The unshard follows the
<a class="reference internal" href="#torch.distributed.fsdp.MixedPrecisionPolicy" title="torch.distributed.fsdp.MixedPrecisionPolicy"><code class="xref py py-class docutils literal notranslate"><span class="pre">MixedPrecisionPolicy</span></code></a>, so it will all-gather following
<code class="docutils literal notranslate"><span class="pre">param_dtype</span></code> if set.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>async_op</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><em>bool</em></a>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, then returns a <a class="reference internal" href="#torch.distributed.fsdp.UnshardHandle" title="torch.distributed.fsdp.UnshardHandle"><code class="xref py py-class docutils literal notranslate"><span class="pre">UnshardHandle</span></code></a>
that has a <code class="xref py py-meth docutils literal notranslate"><span class="pre">wait()</span></code> method to wait on the unshard op. If
<code class="docutils literal notranslate"><span class="pre">False</span></code>, then returns <code class="docutils literal notranslate"><span class="pre">None</span></code> and waits on the handle inside
this function.</p>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p><a class="reference internal" href="#torch.distributed.fsdp.UnshardHandle" title="torch.distributed.fsdp.UnshardHandle"><em>UnshardHandle</em></a> | None</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If <code class="docutils literal notranslate"><span class="pre">async_op=True</span></code>, then FSDP will wait on the pending
unshard in the module’s pre-forward for the user. The user only
needs to call <code class="xref py py-meth docutils literal notranslate"><span class="pre">wait()</span></code> explicitly if the wait should happen
before pre-forward.</p>
</div>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="torch.distributed.fsdp.UnshardHandle">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torch.distributed.fsdp.</span></span><span class="sig-name descname"><span class="pre">UnshardHandle</span></span><a class="headerlink" href="#torch.distributed.fsdp.UnshardHandle" title="Link to this definition">#</a></dt>
<dd><p>A handle to wait on a <a class="reference internal" href="#torch.distributed.fsdp.FSDPModule.unshard" title="torch.distributed.fsdp.FSDPModule.unshard"><code class="xref py py-meth docutils literal notranslate"><span class="pre">FSDPModule.unshard()</span></code></a> op.</p>
<dl class="py method">
<dt class="sig sig-object py" id="torch.distributed.fsdp.UnshardHandle.wait">
<span class="sig-name descname"><span class="pre">wait</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torch/distributed/fsdp/_fully_shard/_fully_shard.html#UnshardHandle.wait"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributed/fsdp/_fully_shard/_fully_shard.py#L463"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.fsdp.UnshardHandle.wait" title="Link to this definition">#</a></dt>
<dd><p>Waits on the unshard op. This ensures that the current stream can use
the unsharded parameters, which are now registered to the module.</p>
<dl class="field-list simple">
</dl>
</dd></dl>

</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="torch.distributed.fsdp.register_fsdp_forward_method">
<span class="sig-prename descclassname"><span class="pre">torch.distributed.fsdp.</span></span><span class="sig-name descname"><span class="pre">register_fsdp_forward_method</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">module</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">method_name</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributed/fsdp/_fully_shard/_fully_shard.py#L482"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.fsdp.register_fsdp_forward_method" title="Link to this definition">#</a></dt>
<dd><p>Registers a method on <code class="docutils literal notranslate"><span class="pre">module</span></code> to be considered a forward method for
FSDP.</p>
<p>FSDP all-gathers parameters pre-forward and optionally frees parameters
post-forward (depending on <code class="docutils literal notranslate"><span class="pre">reshard_after_forward</span></code>). FSDP only knows to
do this for <code class="xref py py-meth docutils literal notranslate"><span class="pre">nn.Module.forward()</span></code> by default. This function patches a
user-specified method to run the pre/post-forward hooks before/after the
method, respectively. If <code class="docutils literal notranslate"><span class="pre">module</span></code> is not an <a class="reference internal" href="#torch.distributed.fsdp.FSDPModule" title="torch.distributed.fsdp.FSDPModule"><code class="xref py py-class docutils literal notranslate"><span class="pre">FSDPModule</span></code></a>, then
this is a no-op.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>module</strong> (<a class="reference internal" href="generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module"><em>nn.Module</em></a>) – Module to register the forward method on.</p></li>
<li><p><strong>method_name</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><em>str</em></a>) – Name of the forward method.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="torch.distributed.fsdp.MixedPrecisionPolicy">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torch.distributed.fsdp.</span></span><span class="sig-name descname"><span class="pre">MixedPrecisionPolicy</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">param_dtype</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reduce_dtype</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_dtype</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cast_forward_inputs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributed.fsdp.MixedPrecisionPolicy" title="Link to this definition">#</a></dt>
<dd><p>This configures FSDP’s mixed precision. Unlike autocast, this applies mixed
precision at the module level, not op level, which means low-precision
activations are saved for backward and high-to-low-precision casts are
incurred only at module boundaries.</p>
<p>FSDP works well with module-level mixed precision since it keeps the
high-precision sharded parameters in memory anyway. In other words, FSDP
does not require any extra memory to keep a high-precision copy of the
parameters for the optimizer step.</p>
<dl class="field-list simple">
<dt class="field-odd">Variables<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>param_dtype</strong> (<em>Optional</em><em>[</em><a class="reference internal" href="tensor_attributes.html#torch.dtype" title="torch.dtype"><em>torch.dtype</em></a><em>]</em>) – This specifies the dtype for
the unsharded parameter and hence the dtype for forward/backward
computation and the parameter all-gather. If this is <code class="docutils literal notranslate"><span class="pre">None</span></code>, then
the unsharded parameter uses the original dtype. The optimizer step
uses the sharded parameter in the original dtype. (Default:
<code class="docutils literal notranslate"><span class="pre">None</span></code>)</p></li>
<li><p><strong>reduce_dtype</strong> (<em>Optional</em><em>[</em><a class="reference internal" href="tensor_attributes.html#torch.dtype" title="torch.dtype"><em>torch.dtype</em></a><em>]</em>) – This specifies the dtype for
gradient reduction (i.e. reduce-scatter or all-reduce). If this is
<code class="docutils literal notranslate"><span class="pre">None</span></code> but <code class="docutils literal notranslate"><span class="pre">param_dtype</span></code> is not <code class="docutils literal notranslate"><span class="pre">None</span></code>, then the reduction
uses the compute dtype. This can be used to run gradient reduction
in full precision while using low precision for compute. If also
gradient reduction is disabled via <code class="xref py py-meth docutils literal notranslate"><span class="pre">set_requires_gradient_sync()</span></code>,
then FSDP will accumulate gradients using <code class="docutils literal notranslate"><span class="pre">reduce_dtype</span></code>.
(Default: <code class="docutils literal notranslate"><span class="pre">None</span></code>)</p></li>
<li><p><strong>output_dtype</strong> (<em>Optional</em><em>[</em><a class="reference internal" href="tensor_attributes.html#torch.dtype" title="torch.dtype"><em>torch.dtype</em></a><em>]</em>) – This specifies the dtype for
casting floating-point forward outputs. This can be used to
help implement cases where different modules have different mixed
precision policies. (Default: <code class="docutils literal notranslate"><span class="pre">None</span></code>)</p></li>
<li><p><strong>cast_forward_inputs</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><em>bool</em></a>) – This specifies whether FSDP should cast the
forward’s floating-point input tensors to <code class="docutils literal notranslate"><span class="pre">param_dtype</span></code> or not.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="torch.distributed.fsdp.OffloadPolicy">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torch.distributed.fsdp.</span></span><span class="sig-name descname"><span class="pre">OffloadPolicy</span></span><a class="headerlink" href="#torch.distributed.fsdp.OffloadPolicy" title="Link to this definition">#</a></dt>
<dd><p>This base class represents the policy of no offloading and is only used as
the default value for the <code class="docutils literal notranslate"><span class="pre">offload_policy</span></code> arg.</p>
<dl class="field-list simple">
</dl>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="torch.distributed.fsdp.CPUOffloadPolicy">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torch.distributed.fsdp.</span></span><span class="sig-name descname"><span class="pre">CPUOffloadPolicy</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">pin_memory</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributed.fsdp.CPUOffloadPolicy" title="Link to this definition">#</a></dt>
<dd><p>This offload policy offloads parameters, gradients, and optimizer states to
CPU. Sharded parameters are copied host-to-device before all-gather. The
all-gathered parameters are freed according to <code class="docutils literal notranslate"><span class="pre">reshard_after_forward</span></code>.
Sharded gradients are copied device-to-host in backward, and the optimizer
step runs on CPU with CPU optimizer states.</p>
<dl class="field-list simple">
<dt class="field-odd">Variables<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>pin_memory</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><em>bool</em></a>) – Whether to pin sharded parameter and gradient
memory. Pinning memory allows both more efficient H2D/D2H copies
and for the copies to overlap with compute. However, the pinned
memory cannot be used by other processes. Set this to <code class="docutils literal notranslate"><span class="pre">False</span></code> if
you have insufficient CPU memory. (Default: <code class="docutils literal notranslate"><span class="pre">True</span></code>)</p>
</dd>
</dl>
</dd></dl>

</section>
</section>


                </article>
              
              
              
                <footer class="bd-footer-article">
                  <div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item">
<div class="feedback">
  
<div class="rating">
    Rate this Page
    <div class="stars">
        
        <span class="star" data-value="1">★</span>
        
        <span class="star" data-value="2">★</span>
        
        <span class="star" data-value="3">★</span>
        
        <span class="star" data-value="4">★</span>
        
        <span class="star" data-value="5">★</span>
        
    </div>
</div>

  <div class="feedback-send">
    <button class="feedback-btn" onclick="openGitHubIssue()">Send Feedback</button>
  </div>
</div>
</div>
  
</div>
                </footer>
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
</div>
                </footer>
              
            </div>
            
            
              
                <dialog id="pst-secondary-sidebar-modal"></dialog>
                <div id="pst-secondary-sidebar" class="bd-sidebar-secondary bd-toc">
<div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
<div
    id="pst-page-navigation-heading-2"
    class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> On this page
  </div>
  <nav class="bd-toc-nav page-toc" aria-labelledby="pst-page-navigation-heading-2">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#pytorch-fsdp2-fully-shard">PyTorch FSDP2 (<code class="docutils literal notranslate"><span class="pre">fully_shard</span></code>)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributed.fsdp.fully_shard"><code class="docutils literal notranslate"><span class="pre">fully_shard()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributed.fsdp.FSDPModule"><code class="docutils literal notranslate"><span class="pre">FSDPModule</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributed.fsdp.FSDPModule.reshard"><code class="docutils literal notranslate"><span class="pre">FSDPModule.reshard()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributed.fsdp.FSDPModule.set_is_last_backward"><code class="docutils literal notranslate"><span class="pre">FSDPModule.set_is_last_backward()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributed.fsdp.FSDPModule.set_modules_to_backward_prefetch"><code class="docutils literal notranslate"><span class="pre">FSDPModule.set_modules_to_backward_prefetch()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributed.fsdp.FSDPModule.set_modules_to_forward_prefetch"><code class="docutils literal notranslate"><span class="pre">FSDPModule.set_modules_to_forward_prefetch()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributed.fsdp.FSDPModule.set_post_optim_event"><code class="docutils literal notranslate"><span class="pre">FSDPModule.set_post_optim_event()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributed.fsdp.FSDPModule.set_reduce_scatter_divide_factor"><code class="docutils literal notranslate"><span class="pre">FSDPModule.set_reduce_scatter_divide_factor()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributed.fsdp.FSDPModule.set_requires_all_reduce"><code class="docutils literal notranslate"><span class="pre">FSDPModule.set_requires_all_reduce()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributed.fsdp.FSDPModule.set_requires_gradient_sync"><code class="docutils literal notranslate"><span class="pre">FSDPModule.set_requires_gradient_sync()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributed.fsdp.FSDPModule.set_reshard_after_backward"><code class="docutils literal notranslate"><span class="pre">FSDPModule.set_reshard_after_backward()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributed.fsdp.FSDPModule.set_unshard_in_backward"><code class="docutils literal notranslate"><span class="pre">FSDPModule.set_unshard_in_backward()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributed.fsdp.FSDPModule.unshard"><code class="docutils literal notranslate"><span class="pre">FSDPModule.unshard()</span></code></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributed.fsdp.UnshardHandle"><code class="docutils literal notranslate"><span class="pre">UnshardHandle</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributed.fsdp.UnshardHandle.wait"><code class="docutils literal notranslate"><span class="pre">UnshardHandle.wait()</span></code></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributed.fsdp.register_fsdp_forward_method"><code class="docutils literal notranslate"><span class="pre">register_fsdp_forward_method()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributed.fsdp.MixedPrecisionPolicy"><code class="docutils literal notranslate"><span class="pre">MixedPrecisionPolicy</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributed.fsdp.OffloadPolicy"><code class="docutils literal notranslate"><span class="pre">OffloadPolicy</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributed.fsdp.CPUOffloadPolicy"><code class="docutils literal notranslate"><span class="pre">CPUOffloadPolicy</span></code></a></li>
</ul>
</li>
</ul>
  </nav></div>

  <div class="sidebar-secondary-item">

  
  <div class="tocsection editthispage">
    <a href="https://github.com/pytorch/pytorch/edit/main/docs/source/python-api/distributed.fsdp.fully_shard.rst">
      <i class="fa-solid fa-pencil"></i>
      
      
        
          Edit on GitHub
        
      
    </a>
  </div>
</div>

  <div class="sidebar-secondary-item">
    <div class="tocsection sourcelink">
      <a href="../_sources/python-api/distributed.fsdp.fully_shard.rst.txt">
        <i class="fa-solid fa-file-lines"></i> Show Source
      </a>
    </div>
</div>

</div>
<div class="sidebar-secondary-item">
  <div class="sidebar-heading">PyTorch Community</div>
  <ul style="list-style-type: none; padding: 0;">
  
   <li><a class="nav-link nav-external" href="community/index.html" style="color: var(--pst-color-text-muted)">PyTorch Governance</a></li>
  
   <li><a class="nav-link nav-external" href="https://pytorch.org/docs/stable/community/design.html" style="color: var(--pst-color-text-muted)">PyTorch Design Philosophy</a></li>
  
   <li><a class="nav-link nav-external" href="https://github.com/pytorch/pytorch/wiki/The-Ultimate-Guide-to-PyTorch-Contributions" style="color: var(--pst-color-text-muted)">The Ultimate Guide to PyTorch Contributions</a></li>
  
  </ul>
</div>
<div class="sidebar-secondary-item">
 <div class="sidebar-heading">Language Bindings</div>
 <ul style="list-style-type: none; padding: 0;">
 
  <li><a class="nav-link nav-external" href="https://pytorch.org/docs/stable/cpp_index.html" style="color: var(--pst-color-text-muted)">C++</a></li>
 
  <li><a class="nav-link nav-external" href="https://pytorch.org/javadoc/" style="color: var(--pst-color-text-muted)">Javadoc</a></li>
 
  <li><a class="nav-link nav-external" href="https://github.com/pytorch/multipy" style="color: var(--pst-color-text-muted)">torch.multiply</a></li>
 
 </ul>
</div>
<div class="sidebar-secondary-item">
  <div class="sidebar-heading">PyTorch Libraries</div>
  <ul style="list-style-type: none; padding: 0;">
  
   <li><a class="nav-link nav-external" href="https://pytorch.org/audio/stable/" style="color: var(--pst-color-text-muted)">torchaudio</a></li>
  
   <li><a class="nav-link nav-external" href="https://pytorch.org/ao" style="color: var(--pst-color-text-muted)">torchao</a></li>
  
   <li><a class="nav-link nav-external" href="https://pytorch.org/executorch" style="color: var(--pst-color-text-muted)">ExecuTorch</a></li>
  
   <li><a class="nav-link nav-external" href="https://pytorch.org/torchrec" style="color: var(--pst-color-text-muted)">torchrec</a></li>
  
   <li><a class="nav-link nav-external" href="https://pytorch.org/serve/" style="color: var(--pst-color-text-muted)">torchserve</a></li>
  
   <li><a class="nav-link nav-external" href="https://pytorch.org/data" style="color: var(--pst-color-text-muted)">torchdata</a></li>
  
   <li><a class="nav-link nav-external" href="https://pytorch.org/data" style="color: var(--pst-color-text-muted)">torchvision</a></li>
  
   <li><a class="nav-link nav-external" href="https://pytorch.org/xla" style="color: var(--pst-color-text-muted)">PyTorch on XLA Devices</a></li>
  
  </ul>
</div>
</div>
</div>
              
            
          </div>
          <footer class="bd-footer-content">
            
          </footer>
        
      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script defer src="../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf"></script>
<script defer src="../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf"></script>


  <footer class="bd-footer">
    
    <div class="bd-footer__inner bd-page-width">
      
        <div class="footer-items__start">
          
            <div class="footer-item">

  <p class="copyright">
    
      © Copyright PyTorch Contributors.
      <br/>
    
  </p>
</div>
          
            <div class="footer-item">

  <p class="sphinx-version">
    Created using <a href="https://www.sphinx-doc.org/">Sphinx</a> 8.1.0.
    <br/>
  </p>
</div>
          
        </div>
      
    
    
      <div class="footer-items__end">
        
          <div class="footer-item">
<p class="theme-version">
  <!-- # L10n: Setting the PST URL as an argument as this does not need to be localized -->
  Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.16.1.
</p></div>
        
      </div>
    
   </div>
   

   <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
      <div class="container">
        <div class="row">
          <div class="col-md-4 text-center">
            <h2>Docs</h2>
            <p>Access comprehensive developer documentation for PyTorch</p>
            <a class="with-right-arrow" href="">View Docs</a>
          </div>

          <div class="col-md-4 text-center">
            <h2>Tutorials</h2>
            <p>Get in-depth tutorials for beginners and advanced developers</p>
            <a class="with-right-arrow" href="">View Tutorials</a>
          </div>

          <div class="col-md-4 text-center">
            <h2>Resources</h2>
            <p>Find development resources and get your questions answered</p>
            <a class="with-right-arrow" href="">View Resources</a>
          </div>
        </div>
      </div>
  </div>

  <footer class="site-footer">
      <div class="container footer-container">
        <div class="footer-logo-wrapper">
          <a href="" class="footer-logo"></a>
        </div>

        <div class="footer-links-wrapper">
          <div class="footer-links-col">
            <ul>
              <li class="list-title"><a href="">PyTorch</a></li>
              <li><a href="">Get Started</a></li>
              <li><a href="">Features</a></li>
              <li><a href="">Ecosystem</a></li>
              <li><a href="">Blog</a></li>
              <li><a href="">Contributing</a></li>
            </ul>
          </div>

          <div class="footer-links-col">
            <ul>
              <li class="list-title"><a href="">Resources</a></li>
              <li><a href="">Tutorials</a></li>
              <li><a href="">Docs</a></li>
              <li><a href="" target="_blank">Discuss</a></li>
              <li><a href="" target="_blank">Github Issues</a></li>
              <li><a href="" target="_blank">Brand Guidelines</a></li>
            </ul>
          </div>

          <div class="footer-links-col">
            <ul>
              <li class="list-title">Stay up to date</li>
              <li><a href="" target="_blank">Facebook</a></li>
              <li><a href="" target="_blank">Twitter</a></li>
              <li><a href="" target="_blank">YouTube</a></li>
              <li><a href="" target="_blank">LinkedIn</a></li>
            </ul>
            </div>

          <div class="footer-links-col">
            <ul>
              <li class="list-title">PyTorch Podcasts</li>
              <li><a href="" target="_blank">Spotify</a></li>
              <li><a href="" target="_blank">Apple</a></li>
              <li><a href="" target="_blank">Google</a></li>
              <li><a href="" target="_blank">Amazon</a></li>
            </ul>
           </div>
          </div>

          <div class="privacy-policy">
            <ul>
              <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/terms/" target="_blank">Terms</a></li>
              <li class="privacy-policy-links">|</li>
              <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/privacy-policy/" target="_blank">Privacy</a></li>
            </ul>
          </div>
          <div class="copyright">
          <p>© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation.
            For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see
            <a href="https://www.linuxfoundation.org/policies/">www.linuxfoundation.org/policies/</a>. The PyTorch Foundation supports the PyTorch open source
            project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC,
            please see <a href="https://www.lfprojects.org/policies/">www.lfprojects.org/policies/</a>.</p>
        </div>
       </div>

  <div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="../_static/img/pytorch-x.svg">
  </div>
</div>
  </footer>


  </body>
</html>