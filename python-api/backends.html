
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>torch.backends &#8212; PyTorch main documentation</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  <!--
    this give us a css class that will be invisible only if js is disabled
  -->
  <noscript>
    <style>
      .pst-js-only { display: none !important; }

    </style>
  </noscript>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=949a1ff5" />
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/katex.min.css" />
    <link rel="stylesheet" type="text/css" href="../_static/katex-math.css?v=91adb8b6" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="../_static/css/jit.css?v=8de1ea5d" />
    <link rel="stylesheet" type="text/css" href="../_static/css/custom.css?v=3ccf5357" />
    <link rel="stylesheet" type="text/css" href="../_static/css/custom2.css?v=189c4a6a" />
  
  <!-- So that users can add custom icons -->
  <script src="../_static/scripts/fontawesome.js?digest=8878045cc6db502f8baf"></script>
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf" />

    <script src="../_static/documentation_options.js?v=53c08c8d"></script>
    <script src="../_static/doctools.js?v=9bcbadda"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=940804e7"></script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'python-api/backends';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  <meta name="docsearch:version" content="main (2.7.0a0+git74cfb4f )" />
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>

          <li class="main-menu-item">
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Learn
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="">
                  <span class=dropdown-title>Get Started</span>
                  <p>Run PyTorch locally or get started quickly with one of the supported cloud platforms</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Tutorials</span>
                  <p>Whats new in PyTorch tutorials</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Learn the Basics</span>
                  <p>Familiarize yourself with PyTorch concepts and modules</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">PyTorch Recipes</span>
                  <p>Bite-size, ready-to-deploy PyTorch code examples</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Intro to PyTorch - YouTube Series</span>
                  <p>Master PyTorch basics with our engaging YouTube tutorial series</p>
                </a>
              </div>
            </div>
          </li>

          <li>
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Ecosystem
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Tools</span>
                  <p>Learn about the tools and frameworks in the PyTorch Ecosystem</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class=dropdown-title>Community</span>
                  <p>Join the PyTorch developer community to contribute, learn, and get your questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="" target="_blank">
                  <span class=dropdown-title>Forums</span>
                  <p>A place to discuss PyTorch code, issues, install, research</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class=dropdown-title>Developer Resources</span>
                  <p>Find resources and get questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Contributor Awards - 2024</span>
                  <p>Award winners announced at this year's PyTorch Conference</p>
                </a>
              </div>
            </div>
          </li>

          <li>
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Edge
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">About PyTorch Edge</span>
                  <p>Build innovative and privacy-aware AI experiences for edge devices</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">ExecuTorch</span>
                  <p>End-to-end solution for enabling on-device inference capabilities across mobile and edge devices</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">ExecuTorch Docs</span>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Docs
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">PyTorch</span>
                  <p>Explore the documentation for comprehensive guidance on how to use PyTorch</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">PyTorch Domains</span>
                  <p>Read the PyTorch Domains documentation to learn more about domain-specific libraries</p>
                </a>
              </div>
            </div>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Blogs & News
              lnhetrlnle</a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">PyTorch Blog</span>
                  <p>Catch up on the latest technical news and happenings</p>
                </a>
                 <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Community Blog</span>
                  <p>Stories from the PyTorch ecosystem</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Videos</span>
                  <p>Learn about the latest PyTorch tutorials, new, and more </p>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Community Stories</span>
                  <p>Learn how our community solves real, everyday machine learning problems with PyTorch</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Events</span>
                  <p>Find events, webinars, and podcasts</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Newsletter</span>
                  <p>Stay up-to-date with the latest updates</p>
                </a>
            </div>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                About
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">PyTorch Foundation</span>
                  <p>Learn more about the PyTorch Foundation</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Governing Board</span>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Cloud Credit Program</span>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Technical Advisory Council</span>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Staff</span>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Contact Us</span>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
            <div class="no-dropdown">
              <a href="" data-cta="join">
                Become a Member
              </a>
            </div>
          </li>
          <li>
           <div class="main-menu-item">
             <a href="" class="github-icon">
             </a>
           </div>
          </li>
          <!--- TODO: This block adds the search icon to the nav bar. We will enable it later.
          <li>
            <div class="main-menu-item">
             <a href="" class="search-icon">
             </a>
            </div>
          </li>
          --->
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>
  </div>
</div>


  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <dialog id="pst-search-dialog">
    
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </dialog>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
<div class="bd-header__inner bd-page-width">
  <button class="pst-navbar-icon sidebar-toggle primary-toggle" aria-label="Site navigation">
    <span class="fa-solid fa-bars"></span>
  </button>
  
  
    <div class="navbar-header-items__start" style="display: flex; align-items: center; justify-content: flex-start;">
    <div class="navbar-item">
      <a class="nav-link nav-internal" href="/index.html">Home</a></div>
    
      <div class="navbar-item">

  
    
  

<a class="navbar-brand logo" href="../index.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/pytorch-logo-dark-unstable.png" class="logo__image only-light" alt="PyTorch main documentation - Home"/>
    <img src="../_static/pytorch-logo-dark-unstable.png" class="logo__image only-dark pst-js-only" alt="PyTorch main documentation - Home"/>
  
  
</a></div>
    
  </div>
  
  <div class="col-lg-9 navbar-header-items">
    
    <div class="me-auto navbar-header-items__center">
      
        <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item ">
  <a class="nav-link nav-internal" href="index.html">
    Python API
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../notes/index.html">
    Developer Notes
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../community/index.html">
    Community
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../lang-bindings/index.html">
    Language Bindings
  </a>
</li>

  </ul>
</nav></div>
      
    </div>
    
    
    <div class="navbar-header-items__end">
      
        <div class="navbar-item navbar-persistent--container">
          

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button>
        </div>
      
      
        <div class="navbar-item">

<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button></div>
      
    </div>
    
  </div>
  
  
    <div class="navbar-persistent--mobile">

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button>
    </div>
  

  
    <button class="pst-navbar-icon sidebar-toggle secondary-toggle" aria-label="On this page">
      <span class="fa-solid fa-outdent"></span>
    </button>
  
</div>

    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
        
      
      <dialog id="pst-primary-sidebar-modal"></dialog>
      <div id="pst-primary-sidebar" class="bd-sidebar-primary bd-sidebar hide-on-wide">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
      <div class="sidebar-header-items__center">
        
          
          
            <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item ">
  <a class="nav-link nav-internal" href="index.html">
    Python API
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../notes/index.html">
    Developer Notes
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../community/index.html">
    Community
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../lang-bindings/index.html">
    Language Bindings
  </a>
</li>

  </ul>
</nav></div>
          
        
      </div>
    
    
    
      <div class="sidebar-header-items__end">
        
          <div class="navbar-item">

<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button></div>
        
      </div>
    
  </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
      <div class="sidebar-primary-item">
<div id="ethical-ad-placement"
      class="flat"
      data-ea-publisher="readthedocs"
      data-ea-type="readthedocs-sidebar"
      data-ea-manual="true">
</div></div>
  </div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        
          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item">

<nav aria-label="Breadcrumb" class="d-print-none">
  <ul class="bd-breadcrumbs">
    
    <li class="breadcrumb-item breadcrumb-home">
      <a href="../index.html" class="nav-link" aria-label="Home">
        <i class="fa-solid fa-home"></i>
      </a>
    </li>
    <li class="breadcrumb-item active" aria-current="page"><span class="ellipsis">torch.backends</span></li>
  </ul>
</nav>
</div>
      
    </div>
  
  
</div>
</div>
              
              
              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section id="module-torch.backends">
<span id="torch-backends"></span><h1>torch.backends<a class="headerlink" href="#module-torch.backends" title="Link to this heading">#</a></h1>
<p><cite>torch.backends</cite> controls the behavior of various backends that PyTorch supports.</p>
<p>These backends include:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">torch.backends.cpu</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">torch.backends.cuda</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">torch.backends.cudnn</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">torch.backends.cusparselt</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">torch.backends.mha</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">torch.backends.mps</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">torch.backends.mkl</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">torch.backends.mkldnn</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">torch.backends.nnpack</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">torch.backends.openmp</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">torch.backends.opt_einsum</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">torch.backends.xeon</span></code></p></li>
</ul>
<section id="module-torch.backends.cpu">
<span id="torch-backends-cpu"></span><h2>torch.backends.cpu<a class="headerlink" href="#module-torch.backends.cpu" title="Link to this heading">#</a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="torch.backends.cpu.get_cpu_capability">
<span class="sig-prename descclassname"><span class="pre">torch.backends.cpu.</span></span><span class="sig-name descname"><span class="pre">get_cpu_capability</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torch/backends/cpu.html#get_cpu_capability"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/74cfb4f3647c4eb38a5c887225fc6adc0a8b16d6/torch/backends/cpu/__init__.py#L9"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.backends.cpu.get_cpu_capability" title="Link to this definition">#</a></dt>
<dd><p>Return cpu capability as a string value.</p>
<p>Possible values:
- “DEFAULT”
- “VSX”
- “Z VECTOR”
- “NO AVX”
- “AVX2”
- “AVX512”
- “SVE256”</p>
<dl class="field-list simple">
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)">str</a></p>
</dd>
</dl>
</dd></dl>

</section>
<section id="module-torch.backends.cuda">
<span id="torch-backends-cuda"></span><h2>torch.backends.cuda<a class="headerlink" href="#module-torch.backends.cuda" title="Link to this heading">#</a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="torch.backends.cuda.is_built">
<span class="sig-prename descclassname"><span class="pre">torch.backends.cuda.</span></span><span class="sig-name descname"><span class="pre">is_built</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torch/backends/cuda.html#is_built"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/74cfb4f3647c4eb38a5c887225fc6adc0a8b16d6/torch/backends/cuda/__init__.py#L39"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.backends.cuda.is_built" title="Link to this definition">#</a></dt>
<dd><p>Return whether PyTorch is built with CUDA support.</p>
<p>Note that this doesn’t necessarily mean CUDA is available; just that if this PyTorch
binary were run on a machine with working CUDA drivers and devices, we would be able to use it.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="torch.backends.cuda.matmul.allow_tf32">
<span class="sig-prename descclassname"><span class="pre">torch.backends.cuda.matmul.</span></span><span class="sig-name descname"><span class="pre">allow_tf32</span></span><a class="headerlink" href="#torch.backends.cuda.matmul.allow_tf32" title="Link to this definition">#</a></dt>
<dd><p>A <a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code></a> that controls whether TensorFloat-32 tensor cores may be used in matrix
multiplications on Ampere or newer GPUs. See <a class="reference internal" href="../notes/cuda.html#tf32-on-ampere"><span class="std std-ref">TensorFloat-32 (TF32) on Ampere (and later) devices</span></a>.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="torch.backends.cuda.matmul.allow_fp16_reduced_precision_reduction">
<span class="sig-prename descclassname"><span class="pre">torch.backends.cuda.matmul.</span></span><span class="sig-name descname"><span class="pre">allow_fp16_reduced_precision_reduction</span></span><a class="headerlink" href="#torch.backends.cuda.matmul.allow_fp16_reduced_precision_reduction" title="Link to this definition">#</a></dt>
<dd><p>A <a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code></a> that controls whether reduced precision reductions (e.g., with fp16 accumulation type) are allowed with fp16 GEMMs.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="torch.backends.cuda.matmul.allow_bf16_reduced_precision_reduction">
<span class="sig-prename descclassname"><span class="pre">torch.backends.cuda.matmul.</span></span><span class="sig-name descname"><span class="pre">allow_bf16_reduced_precision_reduction</span></span><a class="headerlink" href="#torch.backends.cuda.matmul.allow_bf16_reduced_precision_reduction" title="Link to this definition">#</a></dt>
<dd><p>A <a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code></a> that controls whether reduced precision reductions are allowed with bf16 GEMMs.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="torch.backends.cuda.cufft_plan_cache">
<span class="sig-prename descclassname"><span class="pre">torch.backends.cuda.</span></span><span class="sig-name descname"><span class="pre">cufft_plan_cache</span></span><a class="headerlink" href="#torch.backends.cuda.cufft_plan_cache" title="Link to this definition">#</a></dt>
<dd><p><code class="docutils literal notranslate"><span class="pre">cufft_plan_cache</span></code> contains the cuFFT plan caches for each CUDA device.
Query a specific device <cite>i</cite>’s cache via <cite>torch.backends.cuda.cufft_plan_cache[i]</cite>.</p>
<dl class="py attribute">
<dt class="sig sig-object py" id="torch.backends.cuda.cufft_plan_cache.size">
<span class="sig-prename descclassname"><span class="pre">torch.backends.cuda.cufft_plan_cache.</span></span><span class="sig-name descname"><span class="pre">size</span></span><a class="headerlink" href="#torch.backends.cuda.cufft_plan_cache.size" title="Link to this definition">#</a></dt>
<dd><p>A readonly <a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code></a> that shows the number of plans currently in a cuFFT plan cache.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="torch.backends.cuda.cufft_plan_cache.max_size">
<span class="sig-prename descclassname"><span class="pre">torch.backends.cuda.cufft_plan_cache.</span></span><span class="sig-name descname"><span class="pre">max_size</span></span><a class="headerlink" href="#torch.backends.cuda.cufft_plan_cache.max_size" title="Link to this definition">#</a></dt>
<dd><p>A <a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code></a> that controls the capacity of a cuFFT plan cache.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.backends.cuda.cufft_plan_cache.clear">
<span class="sig-prename descclassname"><span class="pre">torch.backends.cuda.cufft_plan_cache.</span></span><span class="sig-name descname"><span class="pre">clear</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.backends.cuda.cufft_plan_cache.clear" title="Link to this definition">#</a></dt>
<dd><p>Clears a cuFFT plan cache.</p>
</dd></dl>

</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="torch.backends.cuda.preferred_blas_library">
<span class="sig-prename descclassname"><span class="pre">torch.backends.cuda.</span></span><span class="sig-name descname"><span class="pre">preferred_blas_library</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">backend</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torch/backends/cuda.html#preferred_blas_library"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/74cfb4f3647c4eb38a5c887225fc6adc0a8b16d6/torch/backends/cuda/__init__.py#L225"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.backends.cuda.preferred_blas_library" title="Link to this definition">#</a></dt>
<dd><p>Override the library PyTorch uses for BLAS operations. Choose between cuBLAS, cuBLASLt, and CK [ROCm-only].</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>This flag is experimental and subject to change.</p>
</div>
<p>When PyTorch runs a CUDA BLAS operation it defaults to cuBLAS even if both cuBLAS and cuBLASLt are available.
For PyTorch built for ROCm, hipBLAS, hipBLASLt, and CK may offer different performance.
This flag (a <a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code></a>) allows overriding which BLAS library to use.</p>
<ul class="simple">
<li><p>If <cite>“cublas”</cite> is set then cuBLAS will be used wherever possible.</p></li>
<li><p>If <cite>“cublaslt”</cite> is set then cuBLASLt will be used wherever possible.</p></li>
<li><p>If <cite>“ck”</cite> is set then CK will be used wherever possible.</p></li>
<li><p>When no input is given, this function returns the currently preferred library.</p></li>
<li><p>User may use the environment variable TORCH_BLAS_PREFER_CUBLASLT=1 to set the preferred library to cuBLASLt
globally.
This flag only sets the initial value of the preferred library and the preferred library
may still be overridden by this function call later in your script.</p></li>
</ul>
<p>Note: When a library is preferred other libraries may still be used if the preferred library
doesn’t implement the operation(s) called.
This flag may achieve better performance if PyTorch’s library selection is incorrect
for your application’s inputs.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><em>_BlasBackend</em></p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="torch.backends.cuda.preferred_rocm_fa_library">
<span class="sig-prename descclassname"><span class="pre">torch.backends.cuda.</span></span><span class="sig-name descname"><span class="pre">preferred_rocm_fa_library</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">backend</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torch/backends/cuda.html#preferred_rocm_fa_library"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/74cfb4f3647c4eb38a5c887225fc6adc0a8b16d6/torch/backends/cuda/__init__.py#L279"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.backends.cuda.preferred_rocm_fa_library" title="Link to this definition">#</a></dt>
<dd><p>[ROCm-only]
Override the backend PyTorch uses in ROCm environments for Flash Attention. Choose between AOTriton and CK</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>This flag is experimeental and subject to change.</p>
</div>
<p>When Flash Attention is enabled and desired, PyTorch defaults to using AOTriton as the backend.
This flag (a <a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code></a>) allows users to override this backend to use composable_kernel</p>
<ul class="simple">
<li><p>If <cite>“default”</cite> is set then the default backend will be used wherever possible. Currently AOTriton.</p></li>
<li><p>If <cite>“aotriton”</cite> is set then AOTriton will be used wherever possible.</p></li>
<li><p>If <cite>“ck”</cite> is set then CK will be used wherever possible.</p></li>
<li><p>When no input is given, this function returns the currently preferred library.</p></li>
<li><p>User may use the environment variable TORCH_ROCM_FA_PREFER_CK=1 to set the preferred library to CK
globally.</p></li>
</ul>
<p>Note: When a library is preferred other libraries may still be used if the preferred library
doesn’t implement the operation(s) called.
This flag may achieve better performance if PyTorch’s library selection is incorrect
for your application’s inputs.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><em>_ROCmFABackend</em></p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="torch.backends.cuda.preferred_linalg_library">
<span class="sig-prename descclassname"><span class="pre">torch.backends.cuda.</span></span><span class="sig-name descname"><span class="pre">preferred_linalg_library</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">backend</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torch/backends/cuda.html#preferred_linalg_library"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/74cfb4f3647c4eb38a5c887225fc6adc0a8b16d6/torch/backends/cuda/__init__.py#L156"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.backends.cuda.preferred_linalg_library" title="Link to this definition">#</a></dt>
<dd><p>Override the heuristic PyTorch uses to choose between cuSOLVER and MAGMA for CUDA linear algebra operations.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>This flag is experimental and subject to change.</p>
</div>
<p>When PyTorch runs a CUDA linear algebra operation it often uses the cuSOLVER or MAGMA libraries,
and if both are available it decides which to use with a heuristic.
This flag (a <a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code></a>) allows overriding those heuristics.</p>
<ul class="simple">
<li><p>If <cite>“cusolver”</cite> is set then cuSOLVER will be used wherever possible.</p></li>
<li><p>If <cite>“magma”</cite> is set then MAGMA will be used wherever possible.</p></li>
<li><p>If <cite>“default”</cite> (the default) is set then heuristics will be used to pick between
cuSOLVER and MAGMA if both are available.</p></li>
<li><p>When no input is given, this function returns the currently preferred library.</p></li>
<li><p>User may use the environment variable TORCH_LINALG_PREFER_CUSOLVER=1 to set the preferred library to cuSOLVER
globally.
This flag only sets the initial value of the preferred library and the preferred library
may still be overridden by this function call later in your script.</p></li>
</ul>
<p>Note: When a library is preferred other libraries may still be used if the preferred library
doesn’t implement the operation(s) called.
This flag may achieve better performance if PyTorch’s heuristic library selection is incorrect
for your application’s inputs.</p>
<p>Currently supported linalg operators:</p>
<ul class="simple">
<li><p><a class="reference internal" href="generated/torch.linalg.inv.html#torch.linalg.inv" title="torch.linalg.inv"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.linalg.inv()</span></code></a></p></li>
<li><p><a class="reference internal" href="generated/torch.linalg.inv_ex.html#torch.linalg.inv_ex" title="torch.linalg.inv_ex"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.linalg.inv_ex()</span></code></a></p></li>
<li><p><a class="reference internal" href="generated/torch.linalg.cholesky.html#torch.linalg.cholesky" title="torch.linalg.cholesky"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.linalg.cholesky()</span></code></a></p></li>
<li><p><a class="reference internal" href="generated/torch.linalg.cholesky_ex.html#torch.linalg.cholesky_ex" title="torch.linalg.cholesky_ex"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.linalg.cholesky_ex()</span></code></a></p></li>
<li><p><a class="reference internal" href="generated/torch.cholesky_solve.html#torch.cholesky_solve" title="torch.cholesky_solve"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.cholesky_solve()</span></code></a></p></li>
<li><p><a class="reference internal" href="generated/torch.cholesky_inverse.html#torch.cholesky_inverse" title="torch.cholesky_inverse"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.cholesky_inverse()</span></code></a></p></li>
<li><p><a class="reference internal" href="generated/torch.linalg.lu_factor.html#torch.linalg.lu_factor" title="torch.linalg.lu_factor"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.linalg.lu_factor()</span></code></a></p></li>
<li><p><a class="reference internal" href="generated/torch.linalg.lu.html#torch.linalg.lu" title="torch.linalg.lu"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.linalg.lu()</span></code></a></p></li>
<li><p><a class="reference internal" href="generated/torch.linalg.lu_solve.html#torch.linalg.lu_solve" title="torch.linalg.lu_solve"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.linalg.lu_solve()</span></code></a></p></li>
<li><p><a class="reference internal" href="generated/torch.linalg.qr.html#torch.linalg.qr" title="torch.linalg.qr"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.linalg.qr()</span></code></a></p></li>
<li><p><a class="reference internal" href="generated/torch.linalg.eigh.html#torch.linalg.eigh" title="torch.linalg.eigh"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.linalg.eigh()</span></code></a></p></li>
<li><p><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.linalg.eighvals()</span></code></p></li>
<li><p><a class="reference internal" href="generated/torch.linalg.svd.html#torch.linalg.svd" title="torch.linalg.svd"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.linalg.svd()</span></code></a></p></li>
<li><p><a class="reference internal" href="generated/torch.linalg.svdvals.html#torch.linalg.svdvals" title="torch.linalg.svdvals"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.linalg.svdvals()</span></code></a></p></li>
</ul>
<dl class="field-list simple">
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><em>_LinalgBackend</em></p>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="torch.backends.cuda.SDPAParams">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torch.backends.cuda.</span></span><span class="sig-name descname"><span class="pre">SDPAParams</span></span><a class="headerlink" href="#torch.backends.cuda.SDPAParams" title="Link to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="torch.backends.cuda.flash_sdp_enabled">
<span class="sig-prename descclassname"><span class="pre">torch.backends.cuda.</span></span><span class="sig-name descname"><span class="pre">flash_sdp_enabled</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torch/backends/cuda.html#flash_sdp_enabled"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/74cfb4f3647c4eb38a5c887225fc6adc0a8b16d6/torch/backends/cuda/__init__.py#L324"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.backends.cuda.flash_sdp_enabled" title="Link to this definition">#</a></dt>
<dd><div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>This flag is beta and subject to change.</p>
</div>
<p>Returns whether flash scaled dot product attention is enabled or not.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="torch.backends.cuda.enable_mem_efficient_sdp">
<span class="sig-prename descclassname"><span class="pre">torch.backends.cuda.</span></span><span class="sig-name descname"><span class="pre">enable_mem_efficient_sdp</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">enabled</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torch/backends/cuda.html#enable_mem_efficient_sdp"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/74cfb4f3647c4eb38a5c887225fc6adc0a8b16d6/torch/backends/cuda/__init__.py#L351"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.backends.cuda.enable_mem_efficient_sdp" title="Link to this definition">#</a></dt>
<dd><div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>This flag is beta and subject to change.</p>
</div>
<p>Enables or disables memory efficient scaled dot product attention.</p>
<dl class="field-list simple">
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="torch.backends.cuda.mem_efficient_sdp_enabled">
<span class="sig-prename descclassname"><span class="pre">torch.backends.cuda.</span></span><span class="sig-name descname"><span class="pre">mem_efficient_sdp_enabled</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torch/backends/cuda.html#mem_efficient_sdp_enabled"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/74cfb4f3647c4eb38a5c887225fc6adc0a8b16d6/torch/backends/cuda/__init__.py#L342"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.backends.cuda.mem_efficient_sdp_enabled" title="Link to this definition">#</a></dt>
<dd><div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>This flag is beta and subject to change.</p>
</div>
<p>Returns whether memory efficient scaled dot product attention is enabled or not.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="torch.backends.cuda.enable_flash_sdp">
<span class="sig-prename descclassname"><span class="pre">torch.backends.cuda.</span></span><span class="sig-name descname"><span class="pre">enable_flash_sdp</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">enabled</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torch/backends/cuda.html#enable_flash_sdp"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/74cfb4f3647c4eb38a5c887225fc6adc0a8b16d6/torch/backends/cuda/__init__.py#L333"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.backends.cuda.enable_flash_sdp" title="Link to this definition">#</a></dt>
<dd><div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>This flag is beta and subject to change.</p>
</div>
<p>Enables or disables flash scaled dot product attention.</p>
<dl class="field-list simple">
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="torch.backends.cuda.math_sdp_enabled">
<span class="sig-prename descclassname"><span class="pre">torch.backends.cuda.</span></span><span class="sig-name descname"><span class="pre">math_sdp_enabled</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torch/backends/cuda.html#math_sdp_enabled"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/74cfb4f3647c4eb38a5c887225fc6adc0a8b16d6/torch/backends/cuda/__init__.py#L360"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.backends.cuda.math_sdp_enabled" title="Link to this definition">#</a></dt>
<dd><div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>This flag is beta and subject to change.</p>
</div>
<p>Returns whether math scaled dot product attention is enabled or not.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="torch.backends.cuda.enable_math_sdp">
<span class="sig-prename descclassname"><span class="pre">torch.backends.cuda.</span></span><span class="sig-name descname"><span class="pre">enable_math_sdp</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">enabled</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torch/backends/cuda.html#enable_math_sdp"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/74cfb4f3647c4eb38a5c887225fc6adc0a8b16d6/torch/backends/cuda/__init__.py#L369"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.backends.cuda.enable_math_sdp" title="Link to this definition">#</a></dt>
<dd><div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>This flag is beta and subject to change.</p>
</div>
<p>Enables or disables math scaled dot product attention.</p>
<dl class="field-list simple">
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="torch.backends.cuda.fp16_bf16_reduction_math_sdp_allowed">
<span class="sig-prename descclassname"><span class="pre">torch.backends.cuda.</span></span><span class="sig-name descname"><span class="pre">fp16_bf16_reduction_math_sdp_allowed</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torch/backends/cuda.html#fp16_bf16_reduction_math_sdp_allowed"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/74cfb4f3647c4eb38a5c887225fc6adc0a8b16d6/torch/backends/cuda/__init__.py#L387"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.backends.cuda.fp16_bf16_reduction_math_sdp_allowed" title="Link to this definition">#</a></dt>
<dd><div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>This flag is beta and subject to change.</p>
</div>
<p>Returns whether fp16/bf16 reduction in math scaled dot product attention is enabled or not.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="torch.backends.cuda.allow_fp16_bf16_reduction_math_sdp">
<span class="sig-prename descclassname"><span class="pre">torch.backends.cuda.</span></span><span class="sig-name descname"><span class="pre">allow_fp16_bf16_reduction_math_sdp</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">enabled</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torch/backends/cuda.html#allow_fp16_bf16_reduction_math_sdp"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/74cfb4f3647c4eb38a5c887225fc6adc0a8b16d6/torch/backends/cuda/__init__.py#L378"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.backends.cuda.allow_fp16_bf16_reduction_math_sdp" title="Link to this definition">#</a></dt>
<dd><div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>This flag is beta and subject to change.</p>
</div>
<p>Enables or disables fp16/bf16 reduction in math scaled dot product attention.</p>
<dl class="field-list simple">
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="torch.backends.cuda.cudnn_sdp_enabled">
<span class="sig-prename descclassname"><span class="pre">torch.backends.cuda.</span></span><span class="sig-name descname"><span class="pre">cudnn_sdp_enabled</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torch/backends/cuda.html#cudnn_sdp_enabled"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/74cfb4f3647c4eb38a5c887225fc6adc0a8b16d6/torch/backends/cuda/__init__.py#L469"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.backends.cuda.cudnn_sdp_enabled" title="Link to this definition">#</a></dt>
<dd><div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>This flag is beta and subject to change.</p>
</div>
<p>Returns whether cuDNN scaled dot product attention is enabled or not.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="torch.backends.cuda.enable_cudnn_sdp">
<span class="sig-prename descclassname"><span class="pre">torch.backends.cuda.</span></span><span class="sig-name descname"><span class="pre">enable_cudnn_sdp</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">enabled</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torch/backends/cuda.html#enable_cudnn_sdp"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/74cfb4f3647c4eb38a5c887225fc6adc0a8b16d6/torch/backends/cuda/__init__.py#L478"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.backends.cuda.enable_cudnn_sdp" title="Link to this definition">#</a></dt>
<dd><div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>This flag is beta and subject to change.</p>
</div>
<p>Enables or disables cuDNN scaled dot product attention.</p>
<dl class="field-list simple">
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="torch.backends.cuda.is_flash_attention_available">
<span class="sig-prename descclassname"><span class="pre">torch.backends.cuda.</span></span><span class="sig-name descname"><span class="pre">is_flash_attention_available</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torch/backends/cuda.html#is_flash_attention_available"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/74cfb4f3647c4eb38a5c887225fc6adc0a8b16d6/torch/backends/cuda/__init__.py#L396"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.backends.cuda.is_flash_attention_available" title="Link to this definition">#</a></dt>
<dd><p>Check if PyTorch was built with FlashAttention for scaled_dot_product_attention.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>True if FlashAttention is built and available; otherwise, False.</p>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)">bool</a></p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This function is dependent on a CUDA-enabled build of PyTorch. It will return False
in non-CUDA environments.</p>
</div>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="torch.backends.cuda.can_use_flash_attention">
<span class="sig-prename descclassname"><span class="pre">torch.backends.cuda.</span></span><span class="sig-name descname"><span class="pre">can_use_flash_attention</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">params</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">debug</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torch/backends/cuda.html#can_use_flash_attention"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/74cfb4f3647c4eb38a5c887225fc6adc0a8b16d6/torch/backends/cuda/__init__.py#L409"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.backends.cuda.can_use_flash_attention" title="Link to this definition">#</a></dt>
<dd><p>Check if FlashAttention can be utilized in scaled_dot_product_attention.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>params</strong> (<a class="reference internal" href="#torch.backends.cuda.SDPAParams" title="torch.backends.cuda._SDPAParams"><em>_SDPAParams</em></a>) – An instance of SDPAParams containing the tensors for query,
key, value, an optional attention mask, dropout rate, and
a flag indicating if the attention is causal.</p></li>
<li><p><strong>debug</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><em>bool</em></a>) – Whether to logging.warn debug information as to why FlashAttention could not be run.
Defaults to False.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>True if FlashAttention can be used with the given parameters; otherwise, False.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)">bool</a></p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This function is dependent on a CUDA-enabled build of PyTorch. It will return False
in non-CUDA environments.</p>
</div>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="torch.backends.cuda.can_use_efficient_attention">
<span class="sig-prename descclassname"><span class="pre">torch.backends.cuda.</span></span><span class="sig-name descname"><span class="pre">can_use_efficient_attention</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">params</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">debug</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torch/backends/cuda.html#can_use_efficient_attention"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/74cfb4f3647c4eb38a5c887225fc6adc0a8b16d6/torch/backends/cuda/__init__.py#L429"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.backends.cuda.can_use_efficient_attention" title="Link to this definition">#</a></dt>
<dd><p>Check if efficient_attention can be utilized in scaled_dot_product_attention.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>params</strong> (<a class="reference internal" href="#torch.backends.cuda.SDPAParams" title="torch.backends.cuda._SDPAParams"><em>_SDPAParams</em></a>) – An instance of SDPAParams containing the tensors for query,
key, value, an optional attention mask, dropout rate, and
a flag indicating if the attention is causal.</p></li>
<li><p><strong>debug</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><em>bool</em></a>) – Whether to logging.warn with information as to why efficient_attention could not be run.
Defaults to False.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>True if efficient_attention can be used with the given parameters; otherwise, False.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)">bool</a></p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This function is dependent on a CUDA-enabled build of PyTorch. It will return False
in non-CUDA environments.</p>
</div>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="torch.backends.cuda.can_use_cudnn_attention">
<span class="sig-prename descclassname"><span class="pre">torch.backends.cuda.</span></span><span class="sig-name descname"><span class="pre">can_use_cudnn_attention</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">params</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">debug</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torch/backends/cuda.html#can_use_cudnn_attention"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/74cfb4f3647c4eb38a5c887225fc6adc0a8b16d6/torch/backends/cuda/__init__.py#L449"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.backends.cuda.can_use_cudnn_attention" title="Link to this definition">#</a></dt>
<dd><p>Check if cudnn_attention can be utilized in scaled_dot_product_attention.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>params</strong> (<a class="reference internal" href="#torch.backends.cuda.SDPAParams" title="torch.backends.cuda._SDPAParams"><em>_SDPAParams</em></a>) – An instance of SDPAParams containing the tensors for query,
key, value, an optional attention mask, dropout rate, and
a flag indicating if the attention is causal.</p></li>
<li><p><strong>debug</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><em>bool</em></a>) – Whether to logging.warn with information as to why cuDNN attention could not be run.
Defaults to False.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>True if cuDNN can be used with the given parameters; otherwise, False.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)">bool</a></p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This function is dependent on a CUDA-enabled build of PyTorch. It will return False
in non-CUDA environments.</p>
</div>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="torch.backends.cuda.sdp_kernel">
<span class="sig-prename descclassname"><span class="pre">torch.backends.cuda.</span></span><span class="sig-name descname"><span class="pre">sdp_kernel</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">enable_flash</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">enable_math</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">enable_mem_efficient</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">enable_cudnn</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torch/backends/cuda.html#sdp_kernel"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/74cfb4f3647c4eb38a5c887225fc6adc0a8b16d6/torch/backends/cuda/__init__.py#L487"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.backends.cuda.sdp_kernel" title="Link to this definition">#</a></dt>
<dd><div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>This flag is beta and subject to change.</p>
</div>
<p>This context manager can be used to temporarily enable or disable any of the three backends for scaled dot product attention.
Upon exiting the context manager, the previous state of the flags will be restored.</p>
<dl class="field-list simple">
</dl>
</dd></dl>

</section>
<section id="module-torch.backends.cudnn">
<span id="torch-backends-cudnn"></span><h2>torch.backends.cudnn<a class="headerlink" href="#module-torch.backends.cudnn" title="Link to this heading">#</a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="torch.backends.cudnn.version">
<span class="sig-prename descclassname"><span class="pre">torch.backends.cudnn.</span></span><span class="sig-name descname"><span class="pre">version</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torch/backends/cudnn.html#version"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/74cfb4f3647c4eb38a5c887225fc6adc0a8b16d6/torch/backends/cudnn/__init__.py#L83"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.backends.cudnn.version" title="Link to this definition">#</a></dt>
<dd><p>Return the version of cuDNN.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="torch.backends.cudnn.is_available">
<span class="sig-prename descclassname"><span class="pre">torch.backends.cudnn.</span></span><span class="sig-name descname"><span class="pre">is_available</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torch/backends/cudnn.html#is_available"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/74cfb4f3647c4eb38a5c887225fc6adc0a8b16d6/torch/backends/cudnn/__init__.py#L97"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.backends.cudnn.is_available" title="Link to this definition">#</a></dt>
<dd><p>Return a bool indicating if CUDNN is currently available.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="torch.backends.cudnn.enabled">
<span class="sig-prename descclassname"><span class="pre">torch.backends.cudnn.</span></span><span class="sig-name descname"><span class="pre">enabled</span></span><a class="headerlink" href="#torch.backends.cudnn.enabled" title="Link to this definition">#</a></dt>
<dd><p>A <a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code></a> that controls whether cuDNN is enabled.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="torch.backends.cudnn.allow_tf32">
<span class="sig-prename descclassname"><span class="pre">torch.backends.cudnn.</span></span><span class="sig-name descname"><span class="pre">allow_tf32</span></span><a class="headerlink" href="#torch.backends.cudnn.allow_tf32" title="Link to this definition">#</a></dt>
<dd><p>A <a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code></a> that controls where TensorFloat-32 tensor cores may be used in cuDNN
convolutions on Ampere or newer GPUs. See <a class="reference internal" href="../notes/cuda.html#tf32-on-ampere"><span class="std std-ref">TensorFloat-32 (TF32) on Ampere (and later) devices</span></a>.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="torch.backends.cudnn.deterministic">
<span class="sig-prename descclassname"><span class="pre">torch.backends.cudnn.</span></span><span class="sig-name descname"><span class="pre">deterministic</span></span><a class="headerlink" href="#torch.backends.cudnn.deterministic" title="Link to this definition">#</a></dt>
<dd><p>A <a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code></a> that, if True, causes cuDNN to only use deterministic convolution algorithms.
See also <a class="reference internal" href="generated/torch.are_deterministic_algorithms_enabled.html#torch.are_deterministic_algorithms_enabled" title="torch.are_deterministic_algorithms_enabled"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.are_deterministic_algorithms_enabled()</span></code></a> and
<a class="reference internal" href="generated/torch.use_deterministic_algorithms.html#torch.use_deterministic_algorithms" title="torch.use_deterministic_algorithms"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.use_deterministic_algorithms()</span></code></a>.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="torch.backends.cudnn.benchmark">
<span class="sig-prename descclassname"><span class="pre">torch.backends.cudnn.</span></span><span class="sig-name descname"><span class="pre">benchmark</span></span><a class="headerlink" href="#torch.backends.cudnn.benchmark" title="Link to this definition">#</a></dt>
<dd><p>A <a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code></a> that, if True, causes cuDNN to benchmark multiple convolution algorithms
and select the fastest.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="torch.backends.cudnn.benchmark_limit">
<span class="sig-prename descclassname"><span class="pre">torch.backends.cudnn.</span></span><span class="sig-name descname"><span class="pre">benchmark_limit</span></span><a class="headerlink" href="#torch.backends.cudnn.benchmark_limit" title="Link to this definition">#</a></dt>
<dd><p>A <a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code></a> that specifies the maximum number of cuDNN convolution algorithms to try when
<cite>torch.backends.cudnn.benchmark</cite> is True. Set <cite>benchmark_limit</cite> to zero to try every
available algorithm. Note that this setting only affects convolutions dispatched via the
cuDNN v8 API.</p>
</dd></dl>

</section>
<section id="module-torch.backends.cusparselt">
<span id="torch-backends-cusparselt"></span><span id="module-torch.backends.cudnn.rnn"></span><h2>torch.backends.cusparselt<a class="headerlink" href="#module-torch.backends.cusparselt" title="Link to this heading">#</a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="torch.backends.cusparselt.version">
<span class="sig-prename descclassname"><span class="pre">torch.backends.cusparselt.</span></span><span class="sig-name descname"><span class="pre">version</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torch/backends/cusparselt.html#version"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/74cfb4f3647c4eb38a5c887225fc6adc0a8b16d6/torch/backends/cusparselt/__init__.py#L42"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.backends.cusparselt.version" title="Link to this definition">#</a></dt>
<dd><p>Return the version of cuSPARSELt</p>
<dl class="field-list simple">
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)">int</a> | None</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="torch.backends.cusparselt.is_available">
<span class="sig-prename descclassname"><span class="pre">torch.backends.cusparselt.</span></span><span class="sig-name descname"><span class="pre">is_available</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torch/backends/cusparselt.html#is_available"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/74cfb4f3647c4eb38a5c887225fc6adc0a8b16d6/torch/backends/cusparselt/__init__.py#L49"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.backends.cusparselt.is_available" title="Link to this definition">#</a></dt>
<dd><p>Return a bool indicating if cuSPARSELt is currently available.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)">bool</a></p>
</dd>
</dl>
</dd></dl>

</section>
<section id="module-torch.backends.mha">
<span id="torch-backends-mha"></span><h2>torch.backends.mha<a class="headerlink" href="#module-torch.backends.mha" title="Link to this heading">#</a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="torch.backends.mha.get_fastpath_enabled">
<span class="sig-prename descclassname"><span class="pre">torch.backends.mha.</span></span><span class="sig-name descname"><span class="pre">get_fastpath_enabled</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torch/backends/mha.html#get_fastpath_enabled"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/74cfb4f3647c4eb38a5c887225fc6adc0a8b16d6/torch/backends/mha/__init__.py#L9"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.backends.mha.get_fastpath_enabled" title="Link to this definition">#</a></dt>
<dd><p>Returns whether fast path for TransformerEncoder and MultiHeadAttention
is enabled, or <code class="docutils literal notranslate"><span class="pre">True</span></code> if jit is scripting.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The fastpath might not be run even if <code class="docutils literal notranslate"><span class="pre">get_fastpath_enabled</span></code> returns
<code class="docutils literal notranslate"><span class="pre">True</span></code> unless all conditions on inputs are met.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)">bool</a></p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="torch.backends.mha.set_fastpath_enabled">
<span class="sig-prename descclassname"><span class="pre">torch.backends.mha.</span></span><span class="sig-name descname"><span class="pre">set_fastpath_enabled</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">value</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torch/backends/mha.html#set_fastpath_enabled"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/74cfb4f3647c4eb38a5c887225fc6adc0a8b16d6/torch/backends/mha/__init__.py#L22"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.backends.mha.set_fastpath_enabled" title="Link to this definition">#</a></dt>
<dd><p>Sets whether fast path is enabled</p>
<dl class="field-list simple">
</dl>
</dd></dl>

</section>
<section id="module-torch.backends.mps">
<span id="torch-backends-mps"></span><h2>torch.backends.mps<a class="headerlink" href="#module-torch.backends.mps" title="Link to this heading">#</a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="torch.backends.mps.is_available">
<span class="sig-prename descclassname"><span class="pre">torch.backends.mps.</span></span><span class="sig-name descname"><span class="pre">is_available</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torch/backends/mps.html#is_available"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/74cfb4f3647c4eb38a5c887225fc6adc0a8b16d6/torch/backends/mps/__init__.py#L22"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.backends.mps.is_available" title="Link to this definition">#</a></dt>
<dd><p>Return a bool indicating if MPS is currently available.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)">bool</a></p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="torch.backends.mps.is_built">
<span class="sig-prename descclassname"><span class="pre">torch.backends.mps.</span></span><span class="sig-name descname"><span class="pre">is_built</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torch/backends/mps.html#is_built"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/74cfb4f3647c4eb38a5c887225fc6adc0a8b16d6/torch/backends/mps/__init__.py#L12"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.backends.mps.is_built" title="Link to this definition">#</a></dt>
<dd><p>Return whether PyTorch is built with MPS support.</p>
<p>Note that this doesn’t necessarily mean MPS is available; just that
if this PyTorch binary were run a machine with working MPS drivers
and devices, we would be able to use it.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)">bool</a></p>
</dd>
</dl>
</dd></dl>

</section>
<section id="module-torch.backends.mkl">
<span id="torch-backends-mkl"></span><h2>torch.backends.mkl<a class="headerlink" href="#module-torch.backends.mkl" title="Link to this heading">#</a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="torch.backends.mkl.is_available">
<span class="sig-prename descclassname"><span class="pre">torch.backends.mkl.</span></span><span class="sig-name descname"><span class="pre">is_available</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torch/backends/mkl.html#is_available"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/74cfb4f3647c4eb38a5c887225fc6adc0a8b16d6/torch/backends/mkl/__init__.py#L5"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.backends.mkl.is_available" title="Link to this definition">#</a></dt>
<dd><p>Return whether PyTorch is built with MKL support.</p>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="torch.backends.mkl.verbose">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torch.backends.mkl.</span></span><span class="sig-name descname"><span class="pre">verbose</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">enable</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torch/backends/mkl.html#verbose"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/74cfb4f3647c4eb38a5c887225fc6adc0a8b16d6/torch/backends/mkl/__init__.py#L14"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.backends.mkl.verbose" title="Link to this definition">#</a></dt>
<dd><p>On-demand oneMKL verbosing functionality.</p>
<p>To make it easier to debug performance issues, oneMKL can dump verbose
messages containing execution information like duration while executing
the kernel. The verbosing functionality can be invoked via an environment
variable named <cite>MKL_VERBOSE</cite>. However, this methodology dumps messages in
all steps. Those are a large amount of verbose messages. Moreover, for
investigating the performance issues, generally taking verbose messages
for one single iteration is enough. This on-demand verbosing functionality
makes it possible to control scope for verbose message dumping. In the
following example, verbose messages will be dumped out for the second
inference only.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="n">model</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">mkl</span><span class="o">.</span><span class="n">verbose</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">mkl</span><span class="o">.</span><span class="n">VERBOSE_ON</span><span class="p">):</span>
    <span class="n">model</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>level</strong> – Verbose level
- <code class="docutils literal notranslate"><span class="pre">VERBOSE_OFF</span></code>: Disable verbosing
- <code class="docutils literal notranslate"><span class="pre">VERBOSE_ON</span></code>:  Enable verbosing</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="module-torch.backends.mkldnn">
<span id="torch-backends-mkldnn"></span><h2>torch.backends.mkldnn<a class="headerlink" href="#module-torch.backends.mkldnn" title="Link to this heading">#</a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="torch.backends.mkldnn.is_available">
<span class="sig-prename descclassname"><span class="pre">torch.backends.mkldnn.</span></span><span class="sig-name descname"><span class="pre">is_available</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torch/backends/mkldnn.html#is_available"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/74cfb4f3647c4eb38a5c887225fc6adc0a8b16d6/torch/backends/mkldnn/__init__.py#L10"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.backends.mkldnn.is_available" title="Link to this definition">#</a></dt>
<dd><p>Return whether PyTorch is built with MKL-DNN support.</p>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="torch.backends.mkldnn.verbose">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torch.backends.mkldnn.</span></span><span class="sig-name descname"><span class="pre">verbose</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">level</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torch/backends/mkldnn.html#verbose"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/74cfb4f3647c4eb38a5c887225fc6adc0a8b16d6/torch/backends/mkldnn/__init__.py#L20"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.backends.mkldnn.verbose" title="Link to this definition">#</a></dt>
<dd><p>On-demand oneDNN (former MKL-DNN) verbosing functionality.</p>
<p>To make it easier to debug performance issues, oneDNN can dump verbose
messages containing information like kernel size, input data size and
execution duration while executing the kernel. The verbosing functionality
can be invoked via an environment variable named <cite>DNNL_VERBOSE</cite>. However,
this methodology dumps messages in all steps. Those are a large amount of
verbose messages. Moreover, for investigating the performance issues,
generally taking verbose messages for one single iteration is enough.
This on-demand verbosing functionality makes it possible to control scope
for verbose message dumping. In the following example, verbose messages
will be dumped out for the second inference only.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="n">model</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">mkldnn</span><span class="o">.</span><span class="n">verbose</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">mkldnn</span><span class="o">.</span><span class="n">VERBOSE_ON</span><span class="p">):</span>
    <span class="n">model</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>level</strong> – Verbose level
- <code class="docutils literal notranslate"><span class="pre">VERBOSE_OFF</span></code>: Disable verbosing
- <code class="docutils literal notranslate"><span class="pre">VERBOSE_ON</span></code>:  Enable verbosing
- <code class="docutils literal notranslate"><span class="pre">VERBOSE_ON_CREATION</span></code>: Enable verbosing, including oneDNN kernel creation</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="module-torch.backends.nnpack">
<span id="torch-backends-nnpack"></span><h2>torch.backends.nnpack<a class="headerlink" href="#module-torch.backends.nnpack" title="Link to this heading">#</a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="torch.backends.nnpack.is_available">
<span class="sig-prename descclassname"><span class="pre">torch.backends.nnpack.</span></span><span class="sig-name descname"><span class="pre">is_available</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torch/backends/nnpack.html#is_available"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/74cfb4f3647c4eb38a5c887225fc6adc0a8b16d6/torch/backends/nnpack/__init__.py#L11"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.backends.nnpack.is_available" title="Link to this definition">#</a></dt>
<dd><p>Return whether PyTorch is built with NNPACK support.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="torch.backends.nnpack.flags">
<span class="sig-prename descclassname"><span class="pre">torch.backends.nnpack.</span></span><span class="sig-name descname"><span class="pre">flags</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">enabled</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torch/backends/nnpack.html#flags"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/74cfb4f3647c4eb38a5c887225fc6adc0a8b16d6/torch/backends/nnpack/__init__.py#L23"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.backends.nnpack.flags" title="Link to this definition">#</a></dt>
<dd><p>Context manager for setting if nnpack is enabled globally</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="torch.backends.nnpack.set_flags">
<span class="sig-prename descclassname"><span class="pre">torch.backends.nnpack.</span></span><span class="sig-name descname"><span class="pre">set_flags</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">_enabled</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torch/backends/nnpack.html#set_flags"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/74cfb4f3647c4eb38a5c887225fc6adc0a8b16d6/torch/backends/nnpack/__init__.py#L16"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.backends.nnpack.set_flags" title="Link to this definition">#</a></dt>
<dd><p>Set if nnpack is enabled globally</p>
</dd></dl>

</section>
<section id="module-torch.backends.openmp">
<span id="torch-backends-openmp"></span><h2>torch.backends.openmp<a class="headerlink" href="#module-torch.backends.openmp" title="Link to this heading">#</a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="torch.backends.openmp.is_available">
<span class="sig-prename descclassname"><span class="pre">torch.backends.openmp.</span></span><span class="sig-name descname"><span class="pre">is_available</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torch/backends/openmp.html#is_available"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/74cfb4f3647c4eb38a5c887225fc6adc0a8b16d6/torch/backends/openmp/__init__.py#L5"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.backends.openmp.is_available" title="Link to this definition">#</a></dt>
<dd><p>Return whether PyTorch is built with OpenMP support.</p>
</dd></dl>

</section>
<section id="module-torch.backends.opt_einsum">
<span id="torch-backends-opt-einsum"></span><span id="module-torch.backends.kleidiai"></span><span id="module-torch.backends.xnnpack"></span><span id="module-torch.backends.quantized"></span><h2>torch.backends.opt_einsum<a class="headerlink" href="#module-torch.backends.opt_einsum" title="Link to this heading">#</a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="torch.backends.opt_einsum.is_available">
<span class="sig-prename descclassname"><span class="pre">torch.backends.opt_einsum.</span></span><span class="sig-name descname"><span class="pre">is_available</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torch/backends/opt_einsum.html#is_available"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/74cfb4f3647c4eb38a5c887225fc6adc0a8b16d6/torch/backends/opt_einsum/__init__.py#L17"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.backends.opt_einsum.is_available" title="Link to this definition">#</a></dt>
<dd><p>Return a bool indicating if opt_einsum is currently available.</p>
<p>You must install opt-einsum in order for torch to automatically optimize einsum. To
make opt-einsum available, you can install it along with torch: <code class="docutils literal notranslate"><span class="pre">pip</span> <span class="pre">install</span> <span class="pre">torch[opt-einsum]</span></code>
or by itself: <code class="docutils literal notranslate"><span class="pre">pip</span> <span class="pre">install</span> <span class="pre">opt-einsum</span></code>. If the package is installed, torch will import
it automatically and use it accordingly. Use this function to check whether opt-einsum
was installed and properly imported by torch.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)">bool</a></p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="torch.backends.opt_einsum.get_opt_einsum">
<span class="sig-prename descclassname"><span class="pre">torch.backends.opt_einsum.</span></span><span class="sig-name descname"><span class="pre">get_opt_einsum</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torch/backends/opt_einsum.html#get_opt_einsum"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/74cfb4f3647c4eb38a5c887225fc6adc0a8b16d6/torch/backends/opt_einsum/__init__.py#L30"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.backends.opt_einsum.get_opt_einsum" title="Link to this definition">#</a></dt>
<dd><p>Return the opt_einsum package if opt_einsum is currently available, else None.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.13)"><em>Any</em></a></p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="torch.backends.opt_einsum.enabled">
<span class="sig-prename descclassname"><span class="pre">torch.backends.opt_einsum.</span></span><span class="sig-name descname"><span class="pre">enabled</span></span><a class="headerlink" href="#torch.backends.opt_einsum.enabled" title="Link to this definition">#</a></dt>
<dd><p>A <a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code></a> that controls whether opt_einsum is enabled (<code class="docutils literal notranslate"><span class="pre">True</span></code> by default). If so,
torch.einsum will use opt_einsum (<a class="reference external" href="https://optimized-einsum.readthedocs.io/en/stable/path_finding.html">https://optimized-einsum.readthedocs.io/en/stable/path_finding.html</a>)
if available to calculate an optimal path of contraction for faster performance.</p>
<p>If opt_einsum is not available, torch.einsum will fall back to the default contraction path
of left to right.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="torch.backends.opt_einsum.strategy">
<span class="sig-prename descclassname"><span class="pre">torch.backends.opt_einsum.</span></span><span class="sig-name descname"><span class="pre">strategy</span></span><a class="headerlink" href="#torch.backends.opt_einsum.strategy" title="Link to this definition">#</a></dt>
<dd><p>A <a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code></a> that specifies which strategies to try when <code class="docutils literal notranslate"><span class="pre">torch.backends.opt_einsum.enabled</span></code>
is <code class="docutils literal notranslate"><span class="pre">True</span></code>. By default, torch.einsum will try the “auto” strategy, but the “greedy” and “optimal”
strategies are also supported. Note that the “optimal” strategy is factorial on the number of
inputs as it tries all possible paths. See more details in opt_einsum’s docs
(<a class="reference external" href="https://optimized-einsum.readthedocs.io/en/stable/path_finding.html">https://optimized-einsum.readthedocs.io/en/stable/path_finding.html</a>).</p>
</dd></dl>

</section>
<section id="module-torch.backends.xeon">
<span id="torch-backends-xeon"></span><h2>torch.backends.xeon<a class="headerlink" href="#module-torch.backends.xeon" title="Link to this heading">#</a></h2>
<span class="target" id="module-torch.backends.xeon.run_cpu"></span></section>
</section>


                </article>
              
              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
</div>
                </footer>
              
            </div>
            
            
              
                <dialog id="pst-secondary-sidebar-modal"></dialog>
                <div id="pst-secondary-sidebar" class="bd-sidebar-secondary bd-toc">
<div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
<div
    id="pst-page-navigation-heading-2"
    class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> On this page
  </div>
  <nav class="bd-toc-nav page-toc" aria-labelledby="pst-page-navigation-heading-2">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#module-torch.backends.cpu">torch.backends.cpu</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.backends.cpu.get_cpu_capability"><code class="docutils literal notranslate"><span class="pre">get_cpu_capability()</span></code></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#module-torch.backends.cuda">torch.backends.cuda</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.backends.cuda.is_built"><code class="docutils literal notranslate"><span class="pre">is_built()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.backends.cuda.matmul.allow_tf32"><code class="docutils literal notranslate"><span class="pre">allow_tf32</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.backends.cuda.matmul.allow_fp16_reduced_precision_reduction"><code class="docutils literal notranslate"><span class="pre">allow_fp16_reduced_precision_reduction</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.backends.cuda.matmul.allow_bf16_reduced_precision_reduction"><code class="docutils literal notranslate"><span class="pre">allow_bf16_reduced_precision_reduction</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.backends.cuda.cufft_plan_cache"><code class="docutils literal notranslate"><span class="pre">cufft_plan_cache</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.backends.cuda.cufft_plan_cache.size"><code class="docutils literal notranslate"><span class="pre">size</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.backends.cuda.cufft_plan_cache.max_size"><code class="docutils literal notranslate"><span class="pre">max_size</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.backends.cuda.cufft_plan_cache.clear"><code class="docutils literal notranslate"><span class="pre">clear()</span></code></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.backends.cuda.preferred_blas_library"><code class="docutils literal notranslate"><span class="pre">preferred_blas_library()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.backends.cuda.preferred_rocm_fa_library"><code class="docutils literal notranslate"><span class="pre">preferred_rocm_fa_library()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.backends.cuda.preferred_linalg_library"><code class="docutils literal notranslate"><span class="pre">preferred_linalg_library()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.backends.cuda.SDPAParams"><code class="docutils literal notranslate"><span class="pre">SDPAParams</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.backends.cuda.flash_sdp_enabled"><code class="docutils literal notranslate"><span class="pre">flash_sdp_enabled()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.backends.cuda.enable_mem_efficient_sdp"><code class="docutils literal notranslate"><span class="pre">enable_mem_efficient_sdp()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.backends.cuda.mem_efficient_sdp_enabled"><code class="docutils literal notranslate"><span class="pre">mem_efficient_sdp_enabled()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.backends.cuda.enable_flash_sdp"><code class="docutils literal notranslate"><span class="pre">enable_flash_sdp()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.backends.cuda.math_sdp_enabled"><code class="docutils literal notranslate"><span class="pre">math_sdp_enabled()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.backends.cuda.enable_math_sdp"><code class="docutils literal notranslate"><span class="pre">enable_math_sdp()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.backends.cuda.fp16_bf16_reduction_math_sdp_allowed"><code class="docutils literal notranslate"><span class="pre">fp16_bf16_reduction_math_sdp_allowed()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.backends.cuda.allow_fp16_bf16_reduction_math_sdp"><code class="docutils literal notranslate"><span class="pre">allow_fp16_bf16_reduction_math_sdp()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.backends.cuda.cudnn_sdp_enabled"><code class="docutils literal notranslate"><span class="pre">cudnn_sdp_enabled()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.backends.cuda.enable_cudnn_sdp"><code class="docutils literal notranslate"><span class="pre">enable_cudnn_sdp()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.backends.cuda.is_flash_attention_available"><code class="docutils literal notranslate"><span class="pre">is_flash_attention_available()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.backends.cuda.can_use_flash_attention"><code class="docutils literal notranslate"><span class="pre">can_use_flash_attention()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.backends.cuda.can_use_efficient_attention"><code class="docutils literal notranslate"><span class="pre">can_use_efficient_attention()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.backends.cuda.can_use_cudnn_attention"><code class="docutils literal notranslate"><span class="pre">can_use_cudnn_attention()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.backends.cuda.sdp_kernel"><code class="docutils literal notranslate"><span class="pre">sdp_kernel()</span></code></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#module-torch.backends.cudnn">torch.backends.cudnn</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.backends.cudnn.version"><code class="docutils literal notranslate"><span class="pre">version()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.backends.cudnn.is_available"><code class="docutils literal notranslate"><span class="pre">is_available()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.backends.cudnn.enabled"><code class="docutils literal notranslate"><span class="pre">enabled</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.backends.cudnn.allow_tf32"><code class="docutils literal notranslate"><span class="pre">allow_tf32</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.backends.cudnn.deterministic"><code class="docutils literal notranslate"><span class="pre">deterministic</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.backends.cudnn.benchmark"><code class="docutils literal notranslate"><span class="pre">benchmark</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.backends.cudnn.benchmark_limit"><code class="docutils literal notranslate"><span class="pre">benchmark_limit</span></code></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#module-torch.backends.cusparselt">torch.backends.cusparselt</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.backends.cusparselt.version"><code class="docutils literal notranslate"><span class="pre">version()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.backends.cusparselt.is_available"><code class="docutils literal notranslate"><span class="pre">is_available()</span></code></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#module-torch.backends.mha">torch.backends.mha</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.backends.mha.get_fastpath_enabled"><code class="docutils literal notranslate"><span class="pre">get_fastpath_enabled()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.backends.mha.set_fastpath_enabled"><code class="docutils literal notranslate"><span class="pre">set_fastpath_enabled()</span></code></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#module-torch.backends.mps">torch.backends.mps</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.backends.mps.is_available"><code class="docutils literal notranslate"><span class="pre">is_available()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.backends.mps.is_built"><code class="docutils literal notranslate"><span class="pre">is_built()</span></code></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#module-torch.backends.mkl">torch.backends.mkl</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.backends.mkl.is_available"><code class="docutils literal notranslate"><span class="pre">is_available()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.backends.mkl.verbose"><code class="docutils literal notranslate"><span class="pre">verbose</span></code></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#module-torch.backends.mkldnn">torch.backends.mkldnn</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.backends.mkldnn.is_available"><code class="docutils literal notranslate"><span class="pre">is_available()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.backends.mkldnn.verbose"><code class="docutils literal notranslate"><span class="pre">verbose</span></code></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#module-torch.backends.nnpack">torch.backends.nnpack</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.backends.nnpack.is_available"><code class="docutils literal notranslate"><span class="pre">is_available()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.backends.nnpack.flags"><code class="docutils literal notranslate"><span class="pre">flags()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.backends.nnpack.set_flags"><code class="docutils literal notranslate"><span class="pre">set_flags()</span></code></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#module-torch.backends.openmp">torch.backends.openmp</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.backends.openmp.is_available"><code class="docutils literal notranslate"><span class="pre">is_available()</span></code></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#module-torch.backends.opt_einsum">torch.backends.opt_einsum</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.backends.opt_einsum.is_available"><code class="docutils literal notranslate"><span class="pre">is_available()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.backends.opt_einsum.get_opt_einsum"><code class="docutils literal notranslate"><span class="pre">get_opt_einsum()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.backends.opt_einsum.enabled"><code class="docutils literal notranslate"><span class="pre">enabled</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.backends.opt_einsum.strategy"><code class="docutils literal notranslate"><span class="pre">strategy</span></code></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#module-torch.backends.xeon">torch.backends.xeon</a></li>
</ul>
  </nav></div>

  <div class="sidebar-secondary-item">

  <div class="tocsection sourcelink">
    <a href="../_sources/python-api/backends.rst">
      <i class="fa-solid fa-file-lines"></i> Show Source
    </a>
  </div>
</div>

</div>
<div class="sidebar-secondary-item"> 
</div>
<div class="sidebar-secondary-item">
 <p>PyTorch Libraries</p>
 <ul>
 
  <li><a href="https://pytorch.org/vision">torchvision</a></li>
 
  <li><a href="https://pytorch.org/executorch">ExecuTorch</a></li>
 
  <li><a href="https://https://pytorch.org/ao">torchao</a></li>
 
  <li><a href="https://https://pytorch.org/audio">torchaudio</a></li>
 
  <li><a href="https://https://pytorch.org/torchrec">torchrec</a></li>
 
  <li><a href="https://https://pytorch.org/serve">torchserve</a></li>
 
  <li><a href="https://https://pytorch.org/data">torchdata</a></li>
 
  <li><a href="https://https://pytorch.org/xla">PyTorch on XLA devices</a></li>
 
 </ul>
</div>
</div>

</div>
              
            
          </div>
          <footer class="bd-footer-content">
            
          </footer>
        
      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script defer src="../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf"></script>
<script defer src="../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf"></script>



  <footer class="bd-footer">
<div class="bd-footer__inner bd-page-width">
  
    <div class="footer-items__start">
      
        <div class="footer-item">

  <p class="copyright">
    
      © Copyright PyTorch Contributors.
      <br/>
    
  </p>
</div>
      
        <div class="footer-item">

  <p class="sphinx-version">
    Created using <a href="https://www.sphinx-doc.org/">Sphinx</a> 8.1.0.
    <br/>
  </p>
</div>
      
    </div>
  
  
  
    <div class="footer-items__end">
      
        <div class="footer-item">
<p class="theme-version">
  <!-- # L10n: Setting the PST URL as an argument as this does not need to be localized -->
  Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.16.1.
</p></div>
      
    </div>
  
</div>

  </footer>
  </body>
</html>