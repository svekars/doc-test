
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>torch.Tensor &#8212; PyTorch main documentation</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  <!--
    this give us a css class that will be invisible only if js is disabled
  -->
  <noscript>
    <style>
      .pst-js-only { display: none !important; }

    </style>
  </noscript>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=949a1ff5" />
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/katex.min.css" />
    <link rel="stylesheet" type="text/css" href="../_static/katex-math.css?v=91adb8b6" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="../_static/css/jit.css?v=8de1ea5d" />
    <link rel="stylesheet" type="text/css" href="../_static/css/custom.css?v=3ccf5357" />
    <link rel="stylesheet" type="text/css" href="../_static/css/custom2.css?v=3539c01c" />
  
  <!-- So that users can add custom icons -->
  <script src="../_static/scripts/fontawesome.js?digest=8878045cc6db502f8baf"></script>
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf" />

    <script src="../_static/documentation_options.js?v=a8da1a53"></script>
    <script src="../_static/doctools.js?v=9bcbadda"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=940804e7"></script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'python-api/tensors';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="torch.Tensor.new_tensor" href="generated/torch.Tensor.new_tensor.html" />
    <link rel="prev" title="torch.compile" href="generated/torch.compile.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  <meta name="docsearch:version" content="main (2.6.0 )" />
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>

          <li class="main-menu-item">
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Learn
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="">
                  <span class=dropdown-title>Get Started</span>
                  <p>Run PyTorch locally or get started quickly with one of the supported cloud platforms</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Tutorials</span>
                  <p>Whats new in PyTorch tutorials</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Learn the Basics</span>
                  <p>Familiarize yourself with PyTorch concepts and modules</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">PyTorch Recipes</span>
                  <p>Bite-size, ready-to-deploy PyTorch code examples</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Intro to PyTorch - YouTube Series</span>
                  <p>Master PyTorch basics with our engaging YouTube tutorial series</p>
                </a>
              </div>
            </div>
          </li>

          <li>
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Ecosystem
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Tools</span>
                  <p>Learn about the tools and frameworks in the PyTorch Ecosystem</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class=dropdown-title>Community</span>
                  <p>Join the PyTorch developer community to contribute, learn, and get your questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="" target="_blank">
                  <span class=dropdown-title>Forums</span>
                  <p>A place to discuss PyTorch code, issues, install, research</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class=dropdown-title>Developer Resources</span>
                  <p>Find resources and get questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Contributor Awards - 2024</span>
                  <p>Award winners announced at this year's PyTorch Conference</p>
                </a>
              </div>
            </div>
          </li>

          <li>
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Edge
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">About PyTorch Edge</span>
                  <p>Build innovative and privacy-aware AI experiences for edge devices</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">ExecuTorch</span>
                  <p>End-to-end solution for enabling on-device inference capabilities across mobile and edge devices</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">ExecuTorch Docs</span>
                </a>
              </div>
            </div>  
          </li>

          <li class="main-menu-item">
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Docs
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">PyTorch</span>
                  <p>Explore the documentation for comprehensive guidance on how to use PyTorch</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">PyTorch Domains</span>
                  <p>Read the PyTorch Domains documentation to learn more about domain-specific libraries</p>
                </a>
              </div>
            </div>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Blogs & News 
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">PyTorch Blog</span>
                  <p>Catch up on the latest technical news and happenings</p>
                </a>
                 <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Community Blog</span>
                  <p>Stories from the PyTorch ecosystem</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Videos</span>
                  <p>Learn about the latest PyTorch tutorials, new, and more </p>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Community Stories</span>
                  <p>Learn how our community solves real, everyday machine learning problems with PyTorch</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Events</span>
                  <p>Find events, webinars, and podcasts</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Newsletter</span>
                  <p>Stay up-to-date with the latest updates</p>
                </a>
            </div>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                About
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">PyTorch Foundation</span>
                  <p>Learn more about the PyTorch Foundation</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Governing Board</span>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Cloud Credit Program</span>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Technical Advisory Council</span>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Staff</span>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Contact Us</span>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
            <div class="no-dropdown">
              <a href="" data-cta="join">
                Become a Member
              </a>
            </div>
          </li>
          <li>
           <div class="main-menu-item">
             <a href="" class="github-icon">
             </a>
           </div>
          </li>
          <!--- TODO: This block adds the search icon to the nav bar. We will enable it later. 
          <li>
            <div class="main-menu-item">
             <a href="" class="search-icon">
             </a>
            </div>
          </li>
          --->
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>
  </div>
</div>

  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <dialog id="pst-search-dialog">
    
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </dialog>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
<div class="bd-header__inner bd-page-width">
  <button class="pst-navbar-icon sidebar-toggle primary-toggle" aria-label="Site navigation">
    <span class="fa-solid fa-bars"></span>
  </button>
  
  
  <div class=" navbar-header-items__start">
    
      <div class="navbar-item">
<div class="version-switcher__container dropdown pst-js-only">
  <button id="pst-version-switcher-button-2"
    type="button"
    class="version-switcher__button btn btn-sm dropdown-toggle"
    data-bs-toggle="dropdown"
    aria-haspopup="listbox"
    aria-controls="pst-version-switcher-list-2"
    aria-label="Version switcher list"
  >
    Choose version  <!-- this text may get changed later by javascript -->
    <span class="caret"></span>
  </button>
  <div id="pst-version-switcher-list-2"
    class="version-switcher__menu dropdown-menu list-group-flush py-0"
    role="listbox" aria-labelledby="pst-version-switcher-button-2">
    <!-- dropdown will be populated by javascript on page load -->
  </div>
</div></div>
    
      <div class="navbar-item">

  
    
  

<a class="navbar-brand logo" href="../index.html">
  
  
  
  
  
  
    <p class="title logo__title">Home</p>
  
</a></div>
    
  </div>
  
  <div class=" navbar-header-items">
    
    <div class="me-auto navbar-header-items__center">
      
        <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item current active">
  <a class="nav-link nav-internal" href="index.html">
    Python API
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../notes/index.html">
    Developer Notes
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../community/index.html">
    Community
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../lang-bindings/index.html">
    Language Bindings
  </a>
</li>

  </ul>
</nav></div>
      
    </div>
    
    
    <div class="navbar-header-items__end">
      
        <div class="navbar-item navbar-persistent--container">
          

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button>
        </div>
      
      
        <div class="navbar-item">

<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button></div>
      
        <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://x.com/PyTorch" title="X" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-x-twitter fa-lg" aria-hidden="true"></i>
            <span class="sr-only">X</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/pytorch/pytorch" title="GitHub" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">GitHub</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://pypi.org/project/torch" title="PyPI" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-custom fa-pypi fa-lg" aria-hidden="true"></i>
            <span class="sr-only">PyPI</span></a>
        </li>
</ul></div>
      
    </div>
    
  </div>
  
  
    <div class="navbar-persistent--mobile">

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button>
    </div>
  

  
    <button class="pst-navbar-icon sidebar-toggle secondary-toggle" aria-label="On this page">
      <span class="fa-solid fa-outdent"></span>
    </button>
  
</div>

    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <dialog id="pst-primary-sidebar-modal"></dialog>
      <div id="pst-primary-sidebar" class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
      <div class="sidebar-header-items__center">
        
          
          
            <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item current active">
  <a class="nav-link nav-internal" href="index.html">
    Python API
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../notes/index.html">
    Developer Notes
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../community/index.html">
    Community
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../lang-bindings/index.html">
    Language Bindings
  </a>
</li>

  </ul>
</nav></div>
          
        
      </div>
    
    
    
      <div class="sidebar-header-items__end">
        
          <div class="navbar-item">

<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button></div>
        
          <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://x.com/PyTorch" title="X" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-x-twitter fa-lg" aria-hidden="true"></i>
            <span class="sr-only">X</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/pytorch/pytorch" title="GitHub" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">GitHub</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://pypi.org/project/torch" title="PyPI" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-custom fa-pypi fa-lg" aria-hidden="true"></i>
            <span class="sr-only">PyPI</span></a>
        </li>
</ul></div>
        
      </div>
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
<nav class="bd-docs-nav bd-links"
     aria-label="Section Navigation">
  <p class="bd-links__title" role="heading" aria-level="1">Section Navigation</p>
  <div class="bd-toc-item navbar-nav"><p aria-level="2" class="caption" role="heading"><span class="caption-text">Core Modules</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="torch.html">torch</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.is_tensor.html">torch.is_tensor</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.is_storage.html">torch.is_storage</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.is_complex.html">torch.is_complex</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.is_conj.html">torch.is_conj</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.is_floating_point.html">torch.is_floating_point</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.is_nonzero.html">torch.is_nonzero</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.set_default_dtype.html">torch.set_default_dtype</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.get_default_dtype.html">torch.get_default_dtype</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.set_default_device.html">torch.set_default_device</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.get_default_device.html">torch.get_default_device</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.set_default_tensor_type.html">torch.set_default_tensor_type</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.numel.html">torch.numel</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.set_printoptions.html">torch.set_printoptions</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.set_flush_denormal.html">torch.set_flush_denormal</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.tensor.html">torch.tensor</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.sparse_coo_tensor.html">torch.sparse_coo_tensor</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.sparse_csr_tensor.html">torch.sparse_csr_tensor</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.sparse_csc_tensor.html">torch.sparse_csc_tensor</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.sparse_bsr_tensor.html">torch.sparse_bsr_tensor</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.sparse_bsc_tensor.html">torch.sparse_bsc_tensor</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.asarray.html">torch.asarray</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.as_tensor.html">torch.as_tensor</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.as_strided.html">torch.as_strided</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.from_file.html">torch.from_file</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.from_numpy.html">torch.from_numpy</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.from_dlpack.html">torch.from_dlpack</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.frombuffer.html">torch.frombuffer</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.zeros.html">torch.zeros</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.zeros_like.html">torch.zeros_like</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.ones.html">torch.ones</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.ones_like.html">torch.ones_like</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.arange.html">torch.arange</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.range.html">torch.range</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.linspace.html">torch.linspace</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.logspace.html">torch.logspace</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.eye.html">torch.eye</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.empty.html">torch.empty</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.empty_like.html">torch.empty_like</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.empty_strided.html">torch.empty_strided</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.full.html">torch.full</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.full_like.html">torch.full_like</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.quantize_per_tensor.html">quantize_per_tensor</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.quantize_per_channel.html">quantize_per_channel</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.dequantize.html">dequantize</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.complex.html">torch.complex</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.polar.html">torch.polar</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.heaviside.html">torch.heaviside</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.adjoint.html">torch.adjoint</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.argwhere.html">torch.argwhere</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cat.html">torch.cat</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.concat.html">torch.concat</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.concatenate.html">torch.concatenate</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.conj.html">torch.conj</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.chunk.html">torch.chunk</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.dsplit.html">torch.dsplit</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.column_stack.html">torch.column_stack</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.dstack.html">torch.dstack</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.gather.html">torch.gather</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.hsplit.html">torch.hsplit</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.hstack.html">torch.hstack</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.index_add.html">torch.index_add</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.index_copy.html">torch.index_copy</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.index_reduce.html">torch.index_reduce</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.index_select.html">torch.index_select</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.masked_select.html">torch.masked_select</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.movedim.html">torch.movedim</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.moveaxis.html">torch.moveaxis</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.narrow.html">torch.narrow</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.narrow_copy.html">torch.narrow_copy</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nonzero.html">torch.nonzero</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.permute.html">torch.permute</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.reshape.html">torch.reshape</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.row_stack.html">torch.row_stack</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.select.html">torch.select</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.scatter.html">torch.scatter</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.diagonal_scatter.html">torch.diagonal_scatter</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.select_scatter.html">torch.select_scatter</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.slice_scatter.html">torch.slice_scatter</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.scatter_add.html">torch.scatter_add</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.scatter_reduce.html">torch.scatter_reduce</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.split.html">torch.split</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.squeeze.html">torch.squeeze</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.stack.html">torch.stack</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.swapaxes.html">torch.swapaxes</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.swapdims.html">torch.swapdims</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.t.html">torch.t</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.take.html">torch.take</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.take_along_dim.html">torch.take_along_dim</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.tensor_split.html">torch.tensor_split</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.tile.html">torch.tile</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.transpose.html">torch.transpose</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.unbind.html">torch.unbind</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.unravel_index.html">torch.unravel_index</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.unsqueeze.html">torch.unsqueeze</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.vsplit.html">torch.vsplit</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.vstack.html">torch.vstack</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.where.html">torch.where</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Stream.html">Stream</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Event.html">Event</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Generator.html">Generator</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.seed.html">torch.seed</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.manual_seed.html">torch.manual_seed</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.initial_seed.html">torch.initial_seed</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.get_rng_state.html">torch.get_rng_state</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.set_rng_state.html">torch.set_rng_state</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.bernoulli.html">torch.bernoulli</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.multinomial.html">torch.multinomial</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.normal.html">torch.normal</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.poisson.html">torch.poisson</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.rand.html">torch.rand</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.rand_like.html">torch.rand_like</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.randint.html">torch.randint</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.randint_like.html">torch.randint_like</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.randn.html">torch.randn</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.randn_like.html">torch.randn_like</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.randperm.html">torch.randperm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.quasirandom.SobolEngine.html">SobolEngine</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.save.html">torch.save</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.load.html">torch.load</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.get_num_threads.html">torch.get_num_threads</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.set_num_threads.html">torch.set_num_threads</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.get_num_interop_threads.html">torch.get_num_interop_threads</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.set_num_interop_threads.html">torch.set_num_interop_threads</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.no_grad.html">no_grad</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.enable_grad.html">enable_grad</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.grad_mode.set_grad_enabled.html">set_grad_enabled</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.is_grad_enabled.html">torch.is_grad_enabled</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.grad_mode.inference_mode.html">inference_mode</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.is_inference_mode_enabled.html">torch.is_inference_mode_enabled</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.abs.html">torch.abs</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.absolute.html">torch.absolute</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.acos.html">torch.acos</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.arccos.html">torch.arccos</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.acosh.html">torch.acosh</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.arccosh.html">torch.arccosh</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.add.html">torch.add</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.addcdiv.html">torch.addcdiv</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.addcmul.html">torch.addcmul</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.angle.html">torch.angle</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.asin.html">torch.asin</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.arcsin.html">torch.arcsin</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.asinh.html">torch.asinh</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.arcsinh.html">torch.arcsinh</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.atan.html">torch.atan</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.arctan.html">torch.arctan</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.atanh.html">torch.atanh</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.arctanh.html">torch.arctanh</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.atan2.html">torch.atan2</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.arctan2.html">torch.arctan2</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.bitwise_not.html">torch.bitwise_not</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.bitwise_and.html">torch.bitwise_and</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.bitwise_or.html">torch.bitwise_or</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.bitwise_xor.html">torch.bitwise_xor</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.bitwise_left_shift.html">torch.bitwise_left_shift</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.bitwise_right_shift.html">torch.bitwise_right_shift</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.ceil.html">torch.ceil</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.clamp.html">torch.clamp</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.clip.html">torch.clip</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.conj_physical.html">torch.conj_physical</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.copysign.html">torch.copysign</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cos.html">torch.cos</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cosh.html">torch.cosh</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.deg2rad.html">torch.deg2rad</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.div.html">torch.div</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.divide.html">torch.divide</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.digamma.html">torch.digamma</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.erf.html">torch.erf</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.erfc.html">torch.erfc</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.erfinv.html">torch.erfinv</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.exp.html">torch.exp</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.exp2.html">torch.exp2</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.expm1.html">torch.expm1</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fake_quantize_per_channel_affine.html">torch.fake_quantize_per_channel_affine</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fake_quantize_per_tensor_affine.html">torch.fake_quantize_per_tensor_affine</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fix.html">torch.fix</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.float_power.html">torch.float_power</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.floor.html">torch.floor</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.floor_divide.html">torch.floor_divide</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fmod.html">torch.fmod</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.frac.html">torch.frac</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.frexp.html">torch.frexp</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.gradient.html">torch.gradient</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.imag.html">torch.imag</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.ldexp.html">torch.ldexp</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.lerp.html">torch.lerp</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.lgamma.html">torch.lgamma</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.log.html">torch.log</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.log10.html">torch.log10</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.log1p.html">torch.log1p</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.log2.html">torch.log2</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.logaddexp.html">torch.logaddexp</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.logaddexp2.html">torch.logaddexp2</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.logical_and.html">torch.logical_and</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.logical_not.html">torch.logical_not</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.logical_or.html">torch.logical_or</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.logical_xor.html">torch.logical_xor</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.logit.html">torch.logit</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.hypot.html">torch.hypot</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.i0.html">torch.i0</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.igamma.html">torch.igamma</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.igammac.html">torch.igammac</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mul.html">torch.mul</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.multiply.html">torch.multiply</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mvlgamma.html">torch.mvlgamma</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nan_to_num.html">torch.nan_to_num</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.neg.html">torch.neg</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.negative.html">torch.negative</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nextafter.html">torch.nextafter</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.polygamma.html">torch.polygamma</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.positive.html">torch.positive</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.pow.html">torch.pow</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.quantized_batch_norm.html">torch.quantized_batch_norm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.quantized_max_pool1d.html">torch.quantized_max_pool1d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.quantized_max_pool2d.html">torch.quantized_max_pool2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.rad2deg.html">torch.rad2deg</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.real.html">torch.real</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.reciprocal.html">torch.reciprocal</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.remainder.html">torch.remainder</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.round.html">torch.round</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.rsqrt.html">torch.rsqrt</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.sigmoid.html">torch.sigmoid</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.sign.html">torch.sign</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.sgn.html">torch.sgn</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.signbit.html">torch.signbit</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.sin.html">torch.sin</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.sinc.html">torch.sinc</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.sinh.html">torch.sinh</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.softmax.html">torch.softmax</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.sqrt.html">torch.sqrt</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.square.html">torch.square</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.sub.html">torch.sub</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.subtract.html">torch.subtract</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.tan.html">torch.tan</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.tanh.html">torch.tanh</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.true_divide.html">torch.true_divide</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.trunc.html">torch.trunc</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xlogy.html">torch.xlogy</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.argmax.html">torch.argmax</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.argmin.html">torch.argmin</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.amax.html">torch.amax</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.amin.html">torch.amin</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.aminmax.html">torch.aminmax</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.all.html">torch.all</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.any.html">torch.any</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.max.html">torch.max</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.min.html">torch.min</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.dist.html">torch.dist</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.logsumexp.html">torch.logsumexp</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mean.html">torch.mean</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nanmean.html">torch.nanmean</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.median.html">torch.median</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nanmedian.html">torch.nanmedian</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mode.html">torch.mode</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.norm.html">torch.norm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nansum.html">torch.nansum</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.prod.html">torch.prod</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.quantile.html">torch.quantile</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nanquantile.html">torch.nanquantile</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.std.html">torch.std</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.std_mean.html">torch.std_mean</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.sum.html">torch.sum</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.unique.html">torch.unique</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.unique_consecutive.html">torch.unique_consecutive</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.var.html">torch.var</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.var_mean.html">torch.var_mean</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.count_nonzero.html">torch.count_nonzero</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.allclose.html">torch.allclose</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.argsort.html">torch.argsort</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.eq.html">torch.eq</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.equal.html">torch.equal</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.ge.html">torch.ge</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.greater_equal.html">torch.greater_equal</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.gt.html">torch.gt</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.greater.html">torch.greater</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.isclose.html">torch.isclose</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.isfinite.html">torch.isfinite</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.isin.html">torch.isin</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.isinf.html">torch.isinf</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.isposinf.html">torch.isposinf</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.isneginf.html">torch.isneginf</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.isnan.html">torch.isnan</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.isreal.html">torch.isreal</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.kthvalue.html">torch.kthvalue</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.le.html">torch.le</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.less_equal.html">torch.less_equal</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.lt.html">torch.lt</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.less.html">torch.less</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.maximum.html">torch.maximum</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.minimum.html">torch.minimum</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fmax.html">torch.fmax</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fmin.html">torch.fmin</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.ne.html">torch.ne</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.not_equal.html">torch.not_equal</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.sort.html">torch.sort</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.topk.html">torch.topk</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.msort.html">torch.msort</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.stft.html">torch.stft</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.istft.html">torch.istft</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.bartlett_window.html">torch.bartlett_window</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.blackman_window.html">torch.blackman_window</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.hamming_window.html">torch.hamming_window</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.hann_window.html">torch.hann_window</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.kaiser_window.html">torch.kaiser_window</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.atleast_1d.html">torch.atleast_1d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.atleast_2d.html">torch.atleast_2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.atleast_3d.html">torch.atleast_3d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.bincount.html">torch.bincount</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.block_diag.html">torch.block_diag</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.broadcast_tensors.html">torch.broadcast_tensors</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.broadcast_to.html">torch.broadcast_to</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.broadcast_shapes.html">torch.broadcast_shapes</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.bucketize.html">torch.bucketize</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cartesian_prod.html">torch.cartesian_prod</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cdist.html">torch.cdist</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.clone.html">torch.clone</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.combinations.html">torch.combinations</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.corrcoef.html">torch.corrcoef</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cov.html">torch.cov</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cross.html">torch.cross</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cummax.html">torch.cummax</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cummin.html">torch.cummin</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cumprod.html">torch.cumprod</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cumsum.html">torch.cumsum</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.diag.html">torch.diag</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.diag_embed.html">torch.diag_embed</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.diagflat.html">torch.diagflat</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.diagonal.html">torch.diagonal</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.diff.html">torch.diff</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.einsum.html">torch.einsum</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.flatten.html">torch.flatten</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.flip.html">torch.flip</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fliplr.html">torch.fliplr</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.flipud.html">torch.flipud</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.kron.html">torch.kron</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.rot90.html">torch.rot90</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.gcd.html">torch.gcd</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.histc.html">torch.histc</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.histogram.html">torch.histogram</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.histogramdd.html">torch.histogramdd</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.meshgrid.html">torch.meshgrid</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.lcm.html">torch.lcm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.logcumsumexp.html">torch.logcumsumexp</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.ravel.html">torch.ravel</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.renorm.html">torch.renorm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.repeat_interleave.html">torch.repeat_interleave</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.roll.html">torch.roll</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.searchsorted.html">torch.searchsorted</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.tensordot.html">torch.tensordot</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.trace.html">torch.trace</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.tril.html">torch.tril</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.tril_indices.html">torch.tril_indices</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.triu.html">torch.triu</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.triu_indices.html">torch.triu_indices</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.unflatten.html">torch.unflatten</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.vander.html">torch.vander</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.view_as_real.html">torch.view_as_real</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.view_as_complex.html">torch.view_as_complex</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.resolve_conj.html">torch.resolve_conj</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.resolve_neg.html">torch.resolve_neg</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.addbmm.html">torch.addbmm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.addmm.html">torch.addmm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.addmv.html">torch.addmv</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.addr.html">torch.addr</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.baddbmm.html">torch.baddbmm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.bmm.html">torch.bmm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.chain_matmul.html">torch.chain_matmul</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cholesky.html">torch.cholesky</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cholesky_inverse.html">torch.cholesky_inverse</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cholesky_solve.html">torch.cholesky_solve</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.dot.html">torch.dot</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.geqrf.html">torch.geqrf</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.ger.html">torch.ger</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.inner.html">torch.inner</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.inverse.html">torch.inverse</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.det.html">torch.det</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.logdet.html">torch.logdet</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.slogdet.html">torch.slogdet</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.lu.html">torch.lu</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.lu_solve.html">torch.lu_solve</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.lu_unpack.html">torch.lu_unpack</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.matmul.html">torch.matmul</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.matrix_power.html">torch.matrix_power</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.matrix_exp.html">torch.matrix_exp</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mm.html">torch.mm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mv.html">torch.mv</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.orgqr.html">torch.orgqr</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.ormqr.html">torch.ormqr</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.outer.html">torch.outer</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.pinverse.html">torch.pinverse</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.qr.html">torch.qr</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.svd.html">torch.svd</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.svd_lowrank.html">torch.svd_lowrank</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.pca_lowrank.html">torch.pca_lowrank</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.lobpcg.html">torch.lobpcg</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.trapz.html">torch.trapz</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.trapezoid.html">torch.trapezoid</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cumulative_trapezoid.html">torch.cumulative_trapezoid</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.triangular_solve.html">torch.triangular_solve</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.vdot.html">torch.vdot</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_abs.html">torch._foreach_abs</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_abs_.html">torch._foreach_abs_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_acos.html">torch._foreach_acos</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_acos_.html">torch._foreach_acos_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_asin.html">torch._foreach_asin</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_asin_.html">torch._foreach_asin_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_atan.html">torch._foreach_atan</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_atan_.html">torch._foreach_atan_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_ceil.html">torch._foreach_ceil</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_ceil_.html">torch._foreach_ceil_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_cos.html">torch._foreach_cos</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_cos_.html">torch._foreach_cos_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_cosh.html">torch._foreach_cosh</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_cosh_.html">torch._foreach_cosh_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_erf.html">torch._foreach_erf</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_erf_.html">torch._foreach_erf_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_erfc.html">torch._foreach_erfc</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_erfc_.html">torch._foreach_erfc_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_exp.html">torch._foreach_exp</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_exp_.html">torch._foreach_exp_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_expm1.html">torch._foreach_expm1</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_expm1_.html">torch._foreach_expm1_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_floor.html">torch._foreach_floor</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_floor_.html">torch._foreach_floor_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_log.html">torch._foreach_log</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_log_.html">torch._foreach_log_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_log10.html">torch._foreach_log10</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_log10_.html">torch._foreach_log10_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_log1p.html">torch._foreach_log1p</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_log1p_.html">torch._foreach_log1p_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_log2.html">torch._foreach_log2</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_log2_.html">torch._foreach_log2_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_neg.html">torch._foreach_neg</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_neg_.html">torch._foreach_neg_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_tan.html">torch._foreach_tan</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_tan_.html">torch._foreach_tan_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_sin.html">torch._foreach_sin</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_sin_.html">torch._foreach_sin_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_sinh.html">torch._foreach_sinh</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_sinh_.html">torch._foreach_sinh_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_round.html">torch._foreach_round</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_round_.html">torch._foreach_round_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_sqrt.html">torch._foreach_sqrt</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_sqrt_.html">torch._foreach_sqrt_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_lgamma.html">torch._foreach_lgamma</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_lgamma_.html">torch._foreach_lgamma_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_frac.html">torch._foreach_frac</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_frac_.html">torch._foreach_frac_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_reciprocal.html">torch._foreach_reciprocal</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_reciprocal_.html">torch._foreach_reciprocal_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_sigmoid.html">torch._foreach_sigmoid</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_sigmoid_.html">torch._foreach_sigmoid_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_trunc.html">torch._foreach_trunc</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_trunc_.html">torch._foreach_trunc_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_zero_.html">torch._foreach_zero_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.compiled_with_cxx11_abi.html">torch.compiled_with_cxx11_abi</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.result_type.html">torch.result_type</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.can_cast.html">torch.can_cast</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.promote_types.html">torch.promote_types</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.use_deterministic_algorithms.html">torch.use_deterministic_algorithms</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.are_deterministic_algorithms_enabled.html">torch.are_deterministic_algorithms_enabled</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.is_deterministic_algorithms_warn_only_enabled.html">torch.is_deterministic_algorithms_warn_only_enabled</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.set_deterministic_debug_mode.html">torch.set_deterministic_debug_mode</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.get_deterministic_debug_mode.html">torch.get_deterministic_debug_mode</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.set_float32_matmul_precision.html">torch.set_float32_matmul_precision</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.get_float32_matmul_precision.html">torch.get_float32_matmul_precision</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.set_warn_always.html">torch.set_warn_always</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.get_device_module.html">torch.get_device_module</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.is_warn_always_enabled.html">torch.is_warn_always_enabled</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.vmap.html">torch.vmap</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._assert.html">torch._assert</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.sym_float.html">torch.sym_float</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.sym_fresh_size.html">torch.sym_fresh_size</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.sym_int.html">torch.sym_int</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.sym_max.html">torch.sym_max</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.sym_min.html">torch.sym_min</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.sym_not.html">torch.sym_not</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.sym_ite.html">torch.sym_ite</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.sym_sum.html">torch.sym_sum</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cond.html">torch.cond</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.compile.html">torch.compile</a></li>
</ul>
</details></li>
<li class="toctree-l1 current active has-children"><a class="current reference internal" href="#">torch.Tensor</a><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.new_tensor.html">torch.Tensor.new_tensor</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.new_full.html">torch.Tensor.new_full</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.new_empty.html">torch.Tensor.new_empty</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.new_ones.html">torch.Tensor.new_ones</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.new_zeros.html">torch.Tensor.new_zeros</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.is_cuda.html">torch.Tensor.is_cuda</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.is_quantized.html">torch.Tensor.is_quantized</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.is_meta.html">torch.Tensor.is_meta</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.device.html">torch.Tensor.device</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.grad.html">torch.Tensor.grad</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.ndim.html">torch.Tensor.ndim</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.real.html">torch.Tensor.real</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.imag.html">torch.Tensor.imag</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.nbytes.html">torch.Tensor.nbytes</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.itemsize.html">torch.Tensor.itemsize</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.abs.html">torch.Tensor.abs</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.abs_.html">torch.Tensor.abs_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.absolute.html">torch.Tensor.absolute</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.absolute_.html">torch.Tensor.absolute_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.acos.html">torch.Tensor.acos</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.acos_.html">torch.Tensor.acos_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.arccos.html">torch.Tensor.arccos</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.arccos_.html">torch.Tensor.arccos_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.add.html">torch.Tensor.add</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.add_.html">torch.Tensor.add_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.addbmm.html">torch.Tensor.addbmm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.addbmm_.html">torch.Tensor.addbmm_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.addcdiv.html">torch.Tensor.addcdiv</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.addcdiv_.html">torch.Tensor.addcdiv_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.addcmul.html">torch.Tensor.addcmul</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.addcmul_.html">torch.Tensor.addcmul_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.addmm.html">torch.Tensor.addmm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.addmm_.html">torch.Tensor.addmm_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.sspaddmm.html">torch.Tensor.sspaddmm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.addmv.html">torch.Tensor.addmv</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.addmv_.html">torch.Tensor.addmv_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.addr.html">torch.Tensor.addr</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.addr_.html">torch.Tensor.addr_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.adjoint.html">torch.Tensor.adjoint</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.allclose.html">torch.Tensor.allclose</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.amax.html">torch.Tensor.amax</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.amin.html">torch.Tensor.amin</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.aminmax.html">torch.Tensor.aminmax</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.angle.html">torch.Tensor.angle</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.apply_.html">torch.Tensor.apply_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.argmax.html">torch.Tensor.argmax</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.argmin.html">torch.Tensor.argmin</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.argsort.html">torch.Tensor.argsort</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.argwhere.html">torch.Tensor.argwhere</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.asin.html">torch.Tensor.asin</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.asin_.html">torch.Tensor.asin_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.arcsin.html">torch.Tensor.arcsin</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.arcsin_.html">torch.Tensor.arcsin_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.as_strided.html">as_strided</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.atan.html">torch.Tensor.atan</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.atan_.html">torch.Tensor.atan_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.arctan.html">torch.Tensor.arctan</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.arctan_.html">torch.Tensor.arctan_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.atan2.html">torch.Tensor.atan2</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.atan2_.html">torch.Tensor.atan2_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.arctan2.html">torch.Tensor.arctan2</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.arctan2_.html">torch.Tensor.arctan2_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.all.html">torch.Tensor.all</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.any.html">torch.Tensor.any</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.backward.html">torch.Tensor.backward</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.baddbmm.html">torch.Tensor.baddbmm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.baddbmm_.html">torch.Tensor.baddbmm_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.bernoulli.html">torch.Tensor.bernoulli</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.bernoulli_.html">torch.Tensor.bernoulli_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.bfloat16.html">torch.Tensor.bfloat16</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.bincount.html">torch.Tensor.bincount</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.bitwise_not.html">torch.Tensor.bitwise_not</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.bitwise_not_.html">torch.Tensor.bitwise_not_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.bitwise_and.html">torch.Tensor.bitwise_and</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.bitwise_and_.html">torch.Tensor.bitwise_and_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.bitwise_or.html">torch.Tensor.bitwise_or</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.bitwise_or_.html">torch.Tensor.bitwise_or_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.bitwise_xor.html">torch.Tensor.bitwise_xor</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.bitwise_xor_.html">torch.Tensor.bitwise_xor_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.bitwise_left_shift.html">torch.Tensor.bitwise_left_shift</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.bitwise_left_shift_.html">torch.Tensor.bitwise_left_shift_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.bitwise_right_shift.html">torch.Tensor.bitwise_right_shift</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.bitwise_right_shift_.html">torch.Tensor.bitwise_right_shift_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.bmm.html">torch.Tensor.bmm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.bool.html">torch.Tensor.bool</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.byte.html">torch.Tensor.byte</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.broadcast_to.html">torch.Tensor.broadcast_to</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.cauchy_.html">torch.Tensor.cauchy_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.ceil.html">torch.Tensor.ceil</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.ceil_.html">torch.Tensor.ceil_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.char.html">torch.Tensor.char</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.cholesky.html">torch.Tensor.cholesky</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.cholesky_inverse.html">torch.Tensor.cholesky_inverse</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.cholesky_solve.html">torch.Tensor.cholesky_solve</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.chunk.html">torch.Tensor.chunk</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.clamp.html">torch.Tensor.clamp</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.clamp_.html">torch.Tensor.clamp_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.clip.html">torch.Tensor.clip</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.clip_.html">torch.Tensor.clip_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.clone.html">clone</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.contiguous.html">torch.Tensor.contiguous</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.copy_.html">copy</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.conj.html">torch.Tensor.conj</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.conj_physical.html">torch.Tensor.conj_physical</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.conj_physical_.html">torch.Tensor.conj_physical_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.resolve_conj.html">torch.Tensor.resolve_conj</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.resolve_neg.html">torch.Tensor.resolve_neg</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.copysign.html">torch.Tensor.copysign</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.copysign_.html">torch.Tensor.copysign_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.cos.html">torch.Tensor.cos</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.cos_.html">torch.Tensor.cos_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.cosh.html">torch.Tensor.cosh</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.cosh_.html">torch.Tensor.cosh_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.corrcoef.html">torch.Tensor.corrcoef</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.count_nonzero.html">torch.Tensor.count_nonzero</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.cov.html">torch.Tensor.cov</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.acosh.html">torch.Tensor.acosh</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.acosh_.html">torch.Tensor.acosh_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.arccosh.html">torch.Tensor.arccosh</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.arccosh_.html">torch.Tensor.arccosh_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.cpu.html">torch.Tensor.cpu</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.cross.html">torch.Tensor.cross</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.cuda.html">torch.Tensor.cuda</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.logcumsumexp.html">torch.Tensor.logcumsumexp</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.cummax.html">torch.Tensor.cummax</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.cummin.html">torch.Tensor.cummin</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.cumprod.html">torch.Tensor.cumprod</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.cumprod_.html">torch.Tensor.cumprod_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.cumsum.html">torch.Tensor.cumsum</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.cumsum_.html">torch.Tensor.cumsum_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.chalf.html">torch.Tensor.chalf</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.cfloat.html">torch.Tensor.cfloat</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.cdouble.html">torch.Tensor.cdouble</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.data_ptr.html">torch.Tensor.data_ptr</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.deg2rad.html">torch.Tensor.deg2rad</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.dequantize.html">dequantize</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.det.html">torch.Tensor.det</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.dense_dim.html">torch.Tensor.dense_dim</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.detach.html">torch.Tensor.detach</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.detach_.html">torch.Tensor.detach_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.diag.html">torch.Tensor.diag</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.diag_embed.html">torch.Tensor.diag_embed</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.diagflat.html">torch.Tensor.diagflat</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.diagonal.html">torch.Tensor.diagonal</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.diagonal_scatter.html">torch.Tensor.diagonal_scatter</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.fill_diagonal_.html">torch.Tensor.fill_diagonal_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.fmax.html">torch.Tensor.fmax</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.fmin.html">torch.Tensor.fmin</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.diff.html">torch.Tensor.diff</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.digamma.html">torch.Tensor.digamma</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.digamma_.html">torch.Tensor.digamma_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.dim.html">torch.Tensor.dim</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.dim_order.html">torch.Tensor.dim_order</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.dist.html">torch.Tensor.dist</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.div.html">torch.Tensor.div</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.div_.html">torch.Tensor.div_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.divide.html">torch.Tensor.divide</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.divide_.html">torch.Tensor.divide_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.dot.html">torch.Tensor.dot</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.double.html">torch.Tensor.double</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.dsplit.html">torch.Tensor.dsplit</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.element_size.html">torch.Tensor.element_size</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.eq.html">eq</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.eq_.html">torch.Tensor.eq_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.equal.html">equal</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.erf.html">torch.Tensor.erf</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.erf_.html">torch.Tensor.erf_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.erfc.html">torch.Tensor.erfc</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.erfc_.html">torch.Tensor.erfc_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.erfinv.html">torch.Tensor.erfinv</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.erfinv_.html">torch.Tensor.erfinv_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.exp.html">torch.Tensor.exp</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.exp_.html">torch.Tensor.exp_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.expm1.html">torch.Tensor.expm1</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.expm1_.html">torch.Tensor.expm1_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.expand.html">expand</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.expand_as.html">torch.Tensor.expand_as</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.exponential_.html">torch.Tensor.exponential_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.fix.html">torch.Tensor.fix</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.fix_.html">torch.Tensor.fix_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.fill_.html">torch.Tensor.fill_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.flatten.html">flatten</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.flip.html">torch.Tensor.flip</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.fliplr.html">torch.Tensor.fliplr</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.flipud.html">torch.Tensor.flipud</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.float.html">torch.Tensor.float</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.float_power.html">torch.Tensor.float_power</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.float_power_.html">torch.Tensor.float_power_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.floor.html">torch.Tensor.floor</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.floor_.html">torch.Tensor.floor_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.floor_divide.html">torch.Tensor.floor_divide</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.floor_divide_.html">torch.Tensor.floor_divide_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.fmod.html">torch.Tensor.fmod</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.fmod_.html">torch.Tensor.fmod_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.frac.html">torch.Tensor.frac</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.frac_.html">torch.Tensor.frac_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.frexp.html">torch.Tensor.frexp</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.gather.html">torch.Tensor.gather</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.gcd.html">torch.Tensor.gcd</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.gcd_.html">torch.Tensor.gcd_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.ge.html">ge</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.ge_.html">torch.Tensor.ge_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.greater_equal.html">torch.Tensor.greater_equal</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.greater_equal_.html">torch.Tensor.greater_equal_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.geometric_.html">torch.Tensor.geometric_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.geqrf.html">torch.Tensor.geqrf</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.ger.html">torch.Tensor.ger</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.get_device.html">torch.Tensor.get_device</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.gt.html">gt</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.gt_.html">torch.Tensor.gt_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.greater.html">torch.Tensor.greater</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.greater_.html">torch.Tensor.greater_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.half.html">torch.Tensor.half</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.hardshrink.html">torch.Tensor.hardshrink</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.heaviside.html">torch.Tensor.heaviside</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.histc.html">torch.Tensor.histc</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.histogram.html">torch.Tensor.histogram</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.hsplit.html">torch.Tensor.hsplit</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.hypot.html">torch.Tensor.hypot</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.hypot_.html">torch.Tensor.hypot_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.i0.html">torch.Tensor.i0</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.i0_.html">torch.Tensor.i0_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.igamma.html">torch.Tensor.igamma</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.igamma_.html">torch.Tensor.igamma_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.igammac.html">torch.Tensor.igammac</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.igammac_.html">torch.Tensor.igammac_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.index_add_.html">torch.Tensor.index_add_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.index_add.html">torch.Tensor.index_add</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.index_copy_.html">torch.Tensor.index_copy_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.index_copy.html">torch.Tensor.index_copy</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.index_fill_.html">torch.Tensor.index_fill_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.index_fill.html">torch.Tensor.index_fill</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.index_put_.html">torch.Tensor.index_put_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.index_put.html">torch.Tensor.index_put</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.index_reduce_.html">torch.Tensor.index_reduce_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.index_reduce.html">torch.Tensor.index_reduce</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.index_select.html">torch.Tensor.index_select</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.indices.html">torch.Tensor.indices</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.inner.html">torch.Tensor.inner</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.int.html">torch.Tensor.int</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.int_repr.html">int_repr</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.inverse.html">torch.Tensor.inverse</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.isclose.html">torch.Tensor.isclose</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.isfinite.html">torch.Tensor.isfinite</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.isinf.html">torch.Tensor.isinf</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.isposinf.html">torch.Tensor.isposinf</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.isneginf.html">torch.Tensor.isneginf</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.isnan.html">torch.Tensor.isnan</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.is_contiguous.html">torch.Tensor.is_contiguous</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.is_complex.html">torch.Tensor.is_complex</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.is_conj.html">torch.Tensor.is_conj</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.is_floating_point.html">torch.Tensor.is_floating_point</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.is_inference.html">torch.Tensor.is_inference</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.is_leaf.html">torch.Tensor.is_leaf</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.is_pinned.html">torch.Tensor.is_pinned</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.is_set_to.html">torch.Tensor.is_set_to</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.is_shared.html">torch.Tensor.is_shared</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.is_signed.html">torch.Tensor.is_signed</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.is_sparse.html">torch.Tensor.is_sparse</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.istft.html">torch.Tensor.istft</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.isreal.html">torch.Tensor.isreal</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.item.html">torch.Tensor.item</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.kthvalue.html">torch.Tensor.kthvalue</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.lcm.html">torch.Tensor.lcm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.lcm_.html">torch.Tensor.lcm_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.ldexp.html">torch.Tensor.ldexp</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.ldexp_.html">torch.Tensor.ldexp_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.le.html">le</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.le_.html">torch.Tensor.le_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.less_equal.html">torch.Tensor.less_equal</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.less_equal_.html">torch.Tensor.less_equal_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.lerp.html">torch.Tensor.lerp</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.lerp_.html">torch.Tensor.lerp_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.lgamma.html">torch.Tensor.lgamma</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.lgamma_.html">torch.Tensor.lgamma_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.log.html">torch.Tensor.log</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.log_.html">torch.Tensor.log_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.logdet.html">torch.Tensor.logdet</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.log10.html">torch.Tensor.log10</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.log10_.html">torch.Tensor.log10_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.log1p.html">torch.Tensor.log1p</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.log1p_.html">torch.Tensor.log1p_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.log2.html">torch.Tensor.log2</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.log2_.html">torch.Tensor.log2_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.log_normal_.html">torch.Tensor.log_normal_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.logaddexp.html">torch.Tensor.logaddexp</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.logaddexp2.html">torch.Tensor.logaddexp2</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.logsumexp.html">torch.Tensor.logsumexp</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.logical_and.html">torch.Tensor.logical_and</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.logical_and_.html">torch.Tensor.logical_and_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.logical_not.html">torch.Tensor.logical_not</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.logical_not_.html">torch.Tensor.logical_not_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.logical_or.html">torch.Tensor.logical_or</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.logical_or_.html">torch.Tensor.logical_or_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.logical_xor.html">torch.Tensor.logical_xor</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.logical_xor_.html">torch.Tensor.logical_xor_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.logit.html">torch.Tensor.logit</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.logit_.html">torch.Tensor.logit_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.long.html">torch.Tensor.long</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.lt.html">lt</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.lt_.html">torch.Tensor.lt_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.less.html">torch.Tensor.less</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.less_.html">torch.Tensor.less_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.lu.html">torch.Tensor.lu</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.lu_solve.html">torch.Tensor.lu_solve</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.as_subclass.html">torch.Tensor.as_subclass</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.map_.html">torch.Tensor.map_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.masked_scatter_.html">torch.Tensor.masked_scatter_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.masked_scatter.html">torch.Tensor.masked_scatter</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.masked_fill_.html">torch.Tensor.masked_fill_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.masked_fill.html">torch.Tensor.masked_fill</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.masked_select.html">torch.Tensor.masked_select</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.matmul.html">torch.Tensor.matmul</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.matrix_power.html">torch.Tensor.matrix_power</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.matrix_exp.html">torch.Tensor.matrix_exp</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.max.html">max</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.maximum.html">torch.Tensor.maximum</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.mean.html">mean</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.module_load.html">torch.Tensor.module_load</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.nanmean.html">torch.Tensor.nanmean</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.median.html">torch.Tensor.median</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.nanmedian.html">torch.Tensor.nanmedian</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.min.html">min</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.minimum.html">torch.Tensor.minimum</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.mm.html">torch.Tensor.mm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.smm.html">torch.Tensor.smm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.mode.html">torch.Tensor.mode</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.movedim.html">torch.Tensor.movedim</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.moveaxis.html">torch.Tensor.moveaxis</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.msort.html">torch.Tensor.msort</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.mul.html">torch.Tensor.mul</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.mul_.html">torch.Tensor.mul_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.multiply.html">torch.Tensor.multiply</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.multiply_.html">torch.Tensor.multiply_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.multinomial.html">torch.Tensor.multinomial</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.mv.html">torch.Tensor.mv</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.mvlgamma.html">torch.Tensor.mvlgamma</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.mvlgamma_.html">torch.Tensor.mvlgamma_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.nansum.html">torch.Tensor.nansum</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.narrow.html">torch.Tensor.narrow</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.narrow_copy.html">torch.Tensor.narrow_copy</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.ndimension.html">torch.Tensor.ndimension</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.nan_to_num.html">torch.Tensor.nan_to_num</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.nan_to_num_.html">torch.Tensor.nan_to_num_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.ne.html">ne</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.ne_.html">torch.Tensor.ne_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.not_equal.html">torch.Tensor.not_equal</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.not_equal_.html">torch.Tensor.not_equal_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.neg.html">torch.Tensor.neg</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.neg_.html">torch.Tensor.neg_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.negative.html">torch.Tensor.negative</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.negative_.html">torch.Tensor.negative_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.nelement.html">torch.Tensor.nelement</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.nextafter.html">torch.Tensor.nextafter</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.nextafter_.html">torch.Tensor.nextafter_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.nonzero.html">torch.Tensor.nonzero</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.norm.html">torch.Tensor.norm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.normal_.html">torch.Tensor.normal_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.numel.html">torch.Tensor.numel</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.numpy.html">torch.Tensor.numpy</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.orgqr.html">torch.Tensor.orgqr</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.ormqr.html">torch.Tensor.ormqr</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.outer.html">torch.Tensor.outer</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.permute.html">torch.Tensor.permute</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.pin_memory.html">torch.Tensor.pin_memory</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.pinverse.html">torch.Tensor.pinverse</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.polygamma.html">torch.Tensor.polygamma</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.polygamma_.html">torch.Tensor.polygamma_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.positive.html">torch.Tensor.positive</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.pow.html">torch.Tensor.pow</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.pow_.html">torch.Tensor.pow_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.prod.html">torch.Tensor.prod</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.put_.html">torch.Tensor.put_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.qr.html">torch.Tensor.qr</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.qscheme.html">torch.Tensor.qscheme</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.quantile.html">torch.Tensor.quantile</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.nanquantile.html">torch.Tensor.nanquantile</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.q_scale.html">q_scale</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.q_zero_point.html">q_zero_point</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.q_per_channel_scales.html">q_per_channel_scales</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.q_per_channel_zero_points.html">q_per_channel_zero_points</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.q_per_channel_axis.html">q_per_channel_axis</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.rad2deg.html">torch.Tensor.rad2deg</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.random_.html">torch.Tensor.random_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.ravel.html">torch.Tensor.ravel</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.reciprocal.html">torch.Tensor.reciprocal</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.reciprocal_.html">torch.Tensor.reciprocal_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.record_stream.html">torch.Tensor.record_stream</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.register_hook.html">torch.Tensor.register_hook</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.register_post_accumulate_grad_hook.html">torch.Tensor.register_post_accumulate_grad_hook</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.remainder.html">torch.Tensor.remainder</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.remainder_.html">torch.Tensor.remainder_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.renorm.html">torch.Tensor.renorm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.renorm_.html">torch.Tensor.renorm_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.repeat.html">torch.Tensor.repeat</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.repeat_interleave.html">torch.Tensor.repeat_interleave</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.requires_grad.html">torch.Tensor.requires_grad</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.requires_grad_.html">torch.Tensor.requires_grad_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.reshape.html">torch.Tensor.reshape</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.reshape_as.html">torch.Tensor.reshape_as</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.resize_.html">resize</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.resize_as_.html">torch.Tensor.resize_as_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.retain_grad.html">torch.Tensor.retain_grad</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.retains_grad.html">torch.Tensor.retains_grad</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.roll.html">torch.Tensor.roll</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.rot90.html">torch.Tensor.rot90</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.round.html">torch.Tensor.round</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.round_.html">torch.Tensor.round_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.rsqrt.html">torch.Tensor.rsqrt</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.rsqrt_.html">torch.Tensor.rsqrt_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.scatter.html">torch.Tensor.scatter</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.scatter_.html">torch.Tensor.scatter_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.scatter_add_.html">torch.Tensor.scatter_add_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.scatter_add.html">torch.Tensor.scatter_add</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.scatter_reduce_.html">torch.Tensor.scatter_reduce_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.scatter_reduce.html">torch.Tensor.scatter_reduce</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.select.html">select</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.select_scatter.html">torch.Tensor.select_scatter</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.set_.html">torch.Tensor.set_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.share_memory_.html">torch.Tensor.share_memory_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.short.html">torch.Tensor.short</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.sigmoid.html">torch.Tensor.sigmoid</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.sigmoid_.html">torch.Tensor.sigmoid_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.sign.html">torch.Tensor.sign</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.sign_.html">torch.Tensor.sign_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.signbit.html">torch.Tensor.signbit</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.sgn.html">torch.Tensor.sgn</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.sgn_.html">torch.Tensor.sgn_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.sin.html">torch.Tensor.sin</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.sin_.html">torch.Tensor.sin_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.sinc.html">torch.Tensor.sinc</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.sinc_.html">torch.Tensor.sinc_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.sinh.html">torch.Tensor.sinh</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.sinh_.html">torch.Tensor.sinh_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.asinh.html">torch.Tensor.asinh</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.asinh_.html">torch.Tensor.asinh_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.arcsinh.html">torch.Tensor.arcsinh</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.arcsinh_.html">torch.Tensor.arcsinh_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.shape.html">torch.Tensor.shape</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.size.html">torch.Tensor.size</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.slogdet.html">torch.Tensor.slogdet</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.slice_scatter.html">torch.Tensor.slice_scatter</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.softmax.html">torch.Tensor.softmax</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.sort.html">sort</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.split.html">torch.Tensor.split</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.sparse_mask.html">torch.Tensor.sparse_mask</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.sparse_dim.html">torch.Tensor.sparse_dim</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.sqrt.html">torch.Tensor.sqrt</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.sqrt_.html">torch.Tensor.sqrt_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.square.html">torch.Tensor.square</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.square_.html">torch.Tensor.square_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.squeeze.html">torch.Tensor.squeeze</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.squeeze_.html">torch.Tensor.squeeze_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.std.html">torch.Tensor.std</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.stft.html">torch.Tensor.stft</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.storage.html">torch.Tensor.storage</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.untyped_storage.html">torch.Tensor.untyped_storage</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.storage_offset.html">torch.Tensor.storage_offset</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.storage_type.html">torch.Tensor.storage_type</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.stride.html">torch.Tensor.stride</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.sub.html">torch.Tensor.sub</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.sub_.html">torch.Tensor.sub_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.subtract.html">torch.Tensor.subtract</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.subtract_.html">torch.Tensor.subtract_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.sum.html">torch.Tensor.sum</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.sum_to_size.html">torch.Tensor.sum_to_size</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.svd.html">torch.Tensor.svd</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.swapaxes.html">torch.Tensor.swapaxes</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.swapdims.html">torch.Tensor.swapdims</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.t.html">torch.Tensor.t</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.t_.html">torch.Tensor.t_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.tensor_split.html">torch.Tensor.tensor_split</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.tile.html">torch.Tensor.tile</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.to.html">torch.Tensor.to</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.to_mkldnn.html">torch.Tensor.to_mkldnn</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.take.html">torch.Tensor.take</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.take_along_dim.html">torch.Tensor.take_along_dim</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.tan.html">torch.Tensor.tan</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.tan_.html">torch.Tensor.tan_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.tanh.html">torch.Tensor.tanh</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.tanh_.html">torch.Tensor.tanh_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.atanh.html">torch.Tensor.atanh</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.atanh_.html">torch.Tensor.atanh_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.arctanh.html">torch.Tensor.arctanh</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.arctanh_.html">torch.Tensor.arctanh_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.tolist.html">torch.Tensor.tolist</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.topk.html">topk</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.to_dense.html">torch.Tensor.to_dense</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.to_sparse.html">torch.Tensor.to_sparse</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.to_sparse_csr.html">torch.Tensor.to_sparse_csr</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.to_sparse_csc.html">torch.Tensor.to_sparse_csc</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.to_sparse_bsr.html">torch.Tensor.to_sparse_bsr</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.to_sparse_bsc.html">torch.Tensor.to_sparse_bsc</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.trace.html">torch.Tensor.trace</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.transpose.html">torch.Tensor.transpose</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.transpose_.html">torch.Tensor.transpose_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.triangular_solve.html">torch.Tensor.triangular_solve</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.tril.html">torch.Tensor.tril</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.tril_.html">torch.Tensor.tril_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.triu.html">torch.Tensor.triu</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.triu_.html">torch.Tensor.triu_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.true_divide.html">torch.Tensor.true_divide</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.true_divide_.html">torch.Tensor.true_divide_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.trunc.html">torch.Tensor.trunc</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.trunc_.html">torch.Tensor.trunc_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.type.html">torch.Tensor.type</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.type_as.html">torch.Tensor.type_as</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.unbind.html">torch.Tensor.unbind</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.unflatten.html">torch.Tensor.unflatten</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.unfold.html">torch.Tensor.unfold</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.uniform_.html">torch.Tensor.uniform_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.unique.html">torch.Tensor.unique</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.unique_consecutive.html">torch.Tensor.unique_consecutive</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.unsqueeze.html">torch.Tensor.unsqueeze</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.unsqueeze_.html">torch.Tensor.unsqueeze_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.values.html">torch.Tensor.values</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.var.html">torch.Tensor.var</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.vdot.html">torch.Tensor.vdot</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.view.html">view</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.view_as.html">torch.Tensor.view_as</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.vsplit.html">torch.Tensor.vsplit</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.where.html">torch.Tensor.where</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.xlogy.html">torch.Tensor.xlogy</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.xlogy_.html">torch.Tensor.xlogy_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.xpu.html">torch.Tensor.xpu</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.zero_.html">torch.Tensor.zero_</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="tensor_attributes.html">Tensor Attributes</a></li>
<li class="toctree-l1"><a class="reference internal" href="tensor_view.html">Tensor Views</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="autograd.html">torch.autograd</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.backward.html">torch.autograd.backward</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.grad.html">torch.autograd.grad</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.forward_ad.dual_level.html">dual_level</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.forward_ad.make_dual.html">torch.autograd.forward_ad.make_dual</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.forward_ad.unpack_dual.html">torch.autograd.forward_ad.unpack_dual</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.forward_ad.enter_dual_level.html">torch.autograd.forward_ad.enter_dual_level</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.forward_ad.exit_dual_level.html">torch.autograd.forward_ad.exit_dual_level</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.forward_ad.UnpackedDualTensor.html">UnpackedDualTensor</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.functional.jacobian.html">torch.autograd.functional.jacobian</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.functional.hessian.html">torch.autograd.functional.hessian</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.functional.vjp.html">torch.autograd.functional.vjp</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.functional.jvp.html">torch.autograd.functional.jvp</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.functional.vhp.html">torch.autograd.functional.vhp</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.functional.hvp.html">torch.autograd.functional.hvp</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.Function.forward.html">torch.autograd.Function.forward</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.Function.backward.html">torch.autograd.Function.backward</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.Function.jvp.html">torch.autograd.Function.jvp</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.Function.vmap.html">torch.autograd.Function.vmap</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.function.FunctionCtx.mark_dirty.html">torch.autograd.function.FunctionCtx.mark_dirty</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.function.FunctionCtx.mark_non_differentiable.html">torch.autograd.function.FunctionCtx.mark_non_differentiable</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.function.FunctionCtx.save_for_backward.html">torch.autograd.function.FunctionCtx.save_for_backward</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.function.FunctionCtx.set_materialize_grads.html">torch.autograd.function.FunctionCtx.set_materialize_grads</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.function.once_differentiable.html">torch.autograd.function.once_differentiable</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.function.BackwardCFunction.html">BackwardCFunction</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.function.InplaceFunction.html">InplaceFunction</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.function.NestedIOFunction.html">NestedIOFunction</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.gradcheck.gradcheck.html">torch.autograd.gradcheck.gradcheck</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.gradcheck.gradgradcheck.html">torch.autograd.gradcheck.gradgradcheck</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.gradcheck.GradcheckError.html">torch.autograd.gradcheck.GradcheckError</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.profiler.profile.export_chrome_trace.html">torch.autograd.profiler.profile.export_chrome_trace</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.profiler.profile.key_averages.html">torch.autograd.profiler.profile.key_averages</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.profiler.profile.self_cpu_time_total.html">torch.autograd.profiler.profile.self_cpu_time_total</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.profiler.profile.total_average.html">torch.autograd.profiler.profile.total_average</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.profiler.parse_nvprof_trace.html">torch.autograd.profiler.parse_nvprof_trace</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.profiler.EnforceUnique.html">EnforceUnique</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.profiler.KinetoStepTracker.html">KinetoStepTracker</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.profiler.record_function.html">record_function</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.profiler_util.Interval.html">Interval</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.profiler_util.Kernel.html">Kernel</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.profiler_util.MemRecordsAcc.html">MemRecordsAcc</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.profiler_util.StringTable.html">StringTable</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.profiler.load_nvprof.html">torch.autograd.profiler.load_nvprof</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.grad_mode.set_multithreading_enabled.html">set_multithreading_enabled</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.graph.Node.name.html">torch.autograd.graph.Node.name</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.graph.Node.metadata.html">torch.autograd.graph.Node.metadata</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.graph.Node.next_functions.html">torch.autograd.graph.Node.next_functions</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.graph.Node.register_hook.html">torch.autograd.graph.Node.register_hook</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.graph.Node.register_prehook.html">torch.autograd.graph.Node.register_prehook</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.graph.increment_version.html">torch.autograd.graph.increment_version</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="size.html">torch.Size</a></li>
<li class="toctree-l1"><a class="reference internal" href="storage.html">torch.Storage</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Neural Networks</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="nn.html">torch.nn</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.parameter.Buffer.html">Buffer</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.parameter.Parameter.html">Parameter</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.parameter.UninitializedParameter.html">UninitializedParameter</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.parameter.UninitializedBuffer.html">UninitializedBuffer</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.Module.html">Module</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.Sequential.html">Sequential</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.ModuleList.html">ModuleList</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.ModuleDict.html">ModuleDict</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.ParameterList.html">ParameterList</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.ParameterDict.html">ParameterDict</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.module.register_module_forward_pre_hook.html">torch.nn.modules.module.register_module_forward_pre_hook</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.module.register_module_forward_hook.html">torch.nn.modules.module.register_module_forward_hook</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.module.register_module_backward_hook.html">torch.nn.modules.module.register_module_backward_hook</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.module.register_module_full_backward_pre_hook.html">torch.nn.modules.module.register_module_full_backward_pre_hook</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.module.register_module_full_backward_hook.html">torch.nn.modules.module.register_module_full_backward_hook</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.module.register_module_buffer_registration_hook.html">torch.nn.modules.module.register_module_buffer_registration_hook</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.module.register_module_module_registration_hook.html">torch.nn.modules.module.register_module_module_registration_hook</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.module.register_module_parameter_registration_hook.html">torch.nn.modules.module.register_module_parameter_registration_hook</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.Conv1d.html">Conv1d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.Conv2d.html">Conv2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.Conv3d.html">Conv3d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.ConvTranspose1d.html">ConvTranspose1d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.ConvTranspose2d.html">ConvTranspose2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.ConvTranspose3d.html">ConvTranspose3d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.LazyConv1d.html">LazyConv1d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.LazyConv2d.html">LazyConv2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.LazyConv3d.html">LazyConv3d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.LazyConvTranspose1d.html">LazyConvTranspose1d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.LazyConvTranspose2d.html">LazyConvTranspose2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.LazyConvTranspose3d.html">LazyConvTranspose3d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.Unfold.html">Unfold</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.Fold.html">Fold</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.MaxPool1d.html">MaxPool1d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.MaxPool2d.html">MaxPool2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.MaxPool3d.html">MaxPool3d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.MaxUnpool1d.html">MaxUnpool1d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.MaxUnpool2d.html">MaxUnpool2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.MaxUnpool3d.html">MaxUnpool3d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.AvgPool1d.html">AvgPool1d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.AvgPool2d.html">AvgPool2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.AvgPool3d.html">AvgPool3d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.FractionalMaxPool2d.html">FractionalMaxPool2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.FractionalMaxPool3d.html">FractionalMaxPool3d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.LPPool1d.html">LPPool1d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.LPPool2d.html">LPPool2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.LPPool3d.html">LPPool3d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.AdaptiveMaxPool1d.html">AdaptiveMaxPool1d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.AdaptiveMaxPool2d.html">AdaptiveMaxPool2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.AdaptiveMaxPool3d.html">AdaptiveMaxPool3d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.AdaptiveAvgPool1d.html">AdaptiveAvgPool1d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.AdaptiveAvgPool2d.html">AdaptiveAvgPool2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.AdaptiveAvgPool3d.html">AdaptiveAvgPool3d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.ReflectionPad1d.html">ReflectionPad1d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.ReflectionPad2d.html">ReflectionPad2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.ReflectionPad3d.html">ReflectionPad3d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.ReplicationPad1d.html">ReplicationPad1d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.ReplicationPad2d.html">ReplicationPad2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.ReplicationPad3d.html">ReplicationPad3d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.ZeroPad1d.html">ZeroPad1d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.ZeroPad2d.html">ZeroPad2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.ZeroPad3d.html">ZeroPad3d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.ConstantPad1d.html">ConstantPad1d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.ConstantPad2d.html">ConstantPad2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.ConstantPad3d.html">ConstantPad3d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.CircularPad1d.html">CircularPad1d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.CircularPad2d.html">CircularPad2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.CircularPad3d.html">CircularPad3d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.ELU.html">ELU</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.Hardshrink.html">Hardshrink</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.Hardsigmoid.html">Hardsigmoid</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.Hardtanh.html">Hardtanh</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.Hardswish.html">Hardswish</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.LeakyReLU.html">LeakyReLU</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.LogSigmoid.html">LogSigmoid</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.MultiheadAttention.html">MultiheadAttention</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.PReLU.html">PReLU</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.ReLU.html">ReLU</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.ReLU6.html">ReLU6</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.RReLU.html">RReLU</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.SELU.html">SELU</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.CELU.html">CELU</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.GELU.html">GELU</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.Sigmoid.html">Sigmoid</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.SiLU.html">SiLU</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.Mish.html">Mish</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.Softplus.html">Softplus</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.Softshrink.html">Softshrink</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.Softsign.html">Softsign</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.Tanh.html">Tanh</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.Tanhshrink.html">Tanhshrink</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.Threshold.html">Threshold</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.GLU.html">GLU</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.Softmin.html">Softmin</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.Softmax.html">Softmax</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.Softmax2d.html">Softmax2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.LogSoftmax.html">LogSoftmax</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.AdaptiveLogSoftmaxWithLoss.html">AdaptiveLogSoftmaxWithLoss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.BatchNorm1d.html">BatchNorm1d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.BatchNorm2d.html">BatchNorm2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.BatchNorm3d.html">BatchNorm3d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.LazyBatchNorm1d.html">LazyBatchNorm1d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.LazyBatchNorm2d.html">LazyBatchNorm2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.LazyBatchNorm3d.html">LazyBatchNorm3d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.GroupNorm.html">GroupNorm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.SyncBatchNorm.html">SyncBatchNorm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.InstanceNorm1d.html">InstanceNorm1d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.InstanceNorm2d.html">InstanceNorm2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.InstanceNorm3d.html">InstanceNorm3d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.LazyInstanceNorm1d.html">LazyInstanceNorm1d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.LazyInstanceNorm2d.html">LazyInstanceNorm2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.LazyInstanceNorm3d.html">LazyInstanceNorm3d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.LayerNorm.html">LayerNorm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.LocalResponseNorm.html">LocalResponseNorm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.RMSNorm.html">RMSNorm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.RNNBase.html">RNNBase</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.RNN.html">RNN</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.LSTM.html">LSTM</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.GRU.html">GRU</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.RNNCell.html">RNNCell</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.LSTMCell.html">LSTMCell</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.GRUCell.html">GRUCell</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.Transformer.html">Transformer</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.TransformerEncoder.html">TransformerEncoder</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.TransformerDecoder.html">TransformerDecoder</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.TransformerEncoderLayer.html">TransformerEncoderLayer</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.TransformerDecoderLayer.html">TransformerDecoderLayer</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.Identity.html">Identity</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.Linear.html">Linear</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.Bilinear.html">Bilinear</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.LazyLinear.html">LazyLinear</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.Dropout.html">Dropout</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.Dropout1d.html">Dropout1d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.Dropout2d.html">Dropout2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.Dropout3d.html">Dropout3d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.AlphaDropout.html">AlphaDropout</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.FeatureAlphaDropout.html">FeatureAlphaDropout</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.Embedding.html">Embedding</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.EmbeddingBag.html">EmbeddingBag</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.CosineSimilarity.html">CosineSimilarity</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.PairwiseDistance.html">PairwiseDistance</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.L1Loss.html">L1Loss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.MSELoss.html">MSELoss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.CrossEntropyLoss.html">CrossEntropyLoss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.CTCLoss.html">CTCLoss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.NLLLoss.html">NLLLoss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.PoissonNLLLoss.html">PoissonNLLLoss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.GaussianNLLLoss.html">GaussianNLLLoss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.KLDivLoss.html">KLDivLoss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.BCELoss.html">BCELoss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.BCEWithLogitsLoss.html">BCEWithLogitsLoss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.MarginRankingLoss.html">MarginRankingLoss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.HingeEmbeddingLoss.html">HingeEmbeddingLoss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.MultiLabelMarginLoss.html">MultiLabelMarginLoss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.HuberLoss.html">HuberLoss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.SmoothL1Loss.html">SmoothL1Loss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.SoftMarginLoss.html">SoftMarginLoss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.MultiLabelSoftMarginLoss.html">MultiLabelSoftMarginLoss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.CosineEmbeddingLoss.html">CosineEmbeddingLoss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.MultiMarginLoss.html">MultiMarginLoss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.TripletMarginLoss.html">TripletMarginLoss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.TripletMarginWithDistanceLoss.html">TripletMarginWithDistanceLoss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.PixelShuffle.html">PixelShuffle</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.PixelUnshuffle.html">PixelUnshuffle</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.Upsample.html">Upsample</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.UpsamplingNearest2d.html">UpsamplingNearest2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.UpsamplingBilinear2d.html">UpsamplingBilinear2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.ChannelShuffle.html">ChannelShuffle</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.DataParallel.html">DataParallel</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.parallel.DistributedDataParallel.html">DistributedDataParallel</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.clip_grad_norm_.html">torch.nn.utils.clip_grad_norm_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.clip_grad_norm.html">torch.nn.utils.clip_grad_norm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.clip_grad_value_.html">torch.nn.utils.clip_grad_value_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.get_total_norm.html">torch.nn.utils.get_total_norm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.clip_grads_with_norm_.html">torch.nn.utils.clip_grads_with_norm_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.parameters_to_vector.html">torch.nn.utils.parameters_to_vector</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.vector_to_parameters.html">torch.nn.utils.vector_to_parameters</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.fuse_conv_bn_eval.html">torch.nn.utils.fuse_conv_bn_eval</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.fuse_conv_bn_weights.html">torch.nn.utils.fuse_conv_bn_weights</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.fuse_linear_bn_eval.html">torch.nn.utils.fuse_linear_bn_eval</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.fuse_linear_bn_weights.html">torch.nn.utils.fuse_linear_bn_weights</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.convert_conv2d_weight_memory_format.html">torch.nn.utils.convert_conv2d_weight_memory_format</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.convert_conv3d_weight_memory_format.html">torch.nn.utils.convert_conv3d_weight_memory_format</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.weight_norm.html">torch.nn.utils.weight_norm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.remove_weight_norm.html">torch.nn.utils.remove_weight_norm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.spectral_norm.html">torch.nn.utils.spectral_norm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.remove_spectral_norm.html">torch.nn.utils.remove_spectral_norm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.skip_init.html">torch.nn.utils.skip_init</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.prune.BasePruningMethod.html">BasePruningMethod</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.prune.PruningContainer.html">PruningContainer</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.prune.Identity.html">torch.nn.utils.prune.identity</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.prune.RandomUnstructured.html">RandomUnstructured</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.prune.L1Unstructured.html">L1Unstructured</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.prune.RandomStructured.html">RandomStructured</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.prune.LnStructured.html">LnStructured</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.prune.CustomFromMask.html">CustomFromMask</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.prune.random_unstructured.html">torch.nn.utils.prune.random_unstructured</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.prune.l1_unstructured.html">torch.nn.utils.prune.l1_unstructured</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.prune.random_structured.html">torch.nn.utils.prune.random_structured</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.prune.ln_structured.html">torch.nn.utils.prune.ln_structured</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.prune.global_unstructured.html">torch.nn.utils.prune.global_unstructured</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.prune.custom_from_mask.html">torch.nn.utils.prune.custom_from_mask</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.prune.remove.html">torch.nn.utils.prune.remove</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.prune.is_pruned.html">torch.nn.utils.prune.is_pruned</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.parametrizations.orthogonal.html">torch.nn.utils.parametrizations.orthogonal</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.parametrizations.weight_norm.html">torch.nn.utils.parametrizations.weight_norm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.parametrizations.spectral_norm.html">torch.nn.utils.parametrizations.spectral_norm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.parametrize.register_parametrization.html">torch.nn.utils.parametrize.register_parametrization</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.parametrize.remove_parametrizations.html">torch.nn.utils.parametrize.remove_parametrizations</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.parametrize.cached.html">torch.nn.utils.parametrize.cached</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.parametrize.is_parametrized.html">torch.nn.utils.parametrize.is_parametrized</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.parametrize.ParametrizationList.html">ParametrizationList</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.stateless.functional_call.html">torch.nn.utils.stateless.functional_call</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.rnn.PackedSequence.html">PackedSequence</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.rnn.pack_padded_sequence.html">torch.nn.utils.rnn.pack_padded_sequence</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.rnn.pad_packed_sequence.html">torch.nn.utils.rnn.pad_packed_sequence</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.rnn.pad_sequence.html">torch.nn.utils.rnn.pad_sequence</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.rnn.pack_sequence.html">torch.nn.utils.rnn.pack_sequence</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.rnn.unpack_sequence.html">torch.nn.utils.rnn.unpack_sequence</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.rnn.unpad_sequence.html">torch.nn.utils.rnn.unpad_sequence</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.Flatten.html">Flatten</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.Unflatten.html">Unflatten</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.lazy.LazyModuleMixin.html">LazyModuleMixin</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.normalization.RMSNorm.html">RMSNorm</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="nn.functional.html">torch.nn.functional</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.conv1d.html">torch.nn.functional.conv1d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.conv2d.html">torch.nn.functional.conv2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.conv3d.html">torch.nn.functional.conv3d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.conv_transpose1d.html">torch.nn.functional.conv_transpose1d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.conv_transpose2d.html">torch.nn.functional.conv_transpose2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.conv_transpose3d.html">torch.nn.functional.conv_transpose3d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.unfold.html">torch.nn.functional.unfold</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.fold.html">torch.nn.functional.fold</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.avg_pool1d.html">torch.nn.functional.avg_pool1d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.avg_pool2d.html">torch.nn.functional.avg_pool2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.avg_pool3d.html">torch.nn.functional.avg_pool3d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.max_pool1d.html">torch.nn.functional.max_pool1d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.max_pool2d.html">torch.nn.functional.max_pool2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.max_pool3d.html">torch.nn.functional.max_pool3d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.max_unpool1d.html">torch.nn.functional.max_unpool1d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.max_unpool2d.html">torch.nn.functional.max_unpool2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.max_unpool3d.html">torch.nn.functional.max_unpool3d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.lp_pool1d.html">torch.nn.functional.lp_pool1d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.lp_pool2d.html">torch.nn.functional.lp_pool2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.lp_pool3d.html">torch.nn.functional.lp_pool3d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.adaptive_max_pool1d.html">torch.nn.functional.adaptive_max_pool1d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.adaptive_max_pool2d.html">torch.nn.functional.adaptive_max_pool2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.adaptive_max_pool3d.html">torch.nn.functional.adaptive_max_pool3d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.adaptive_avg_pool1d.html">torch.nn.functional.adaptive_avg_pool1d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.adaptive_avg_pool2d.html">torch.nn.functional.adaptive_avg_pool2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.adaptive_avg_pool3d.html">torch.nn.functional.adaptive_avg_pool3d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.fractional_max_pool2d.html">torch.nn.functional.fractional_max_pool2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.fractional_max_pool3d.html">torch.nn.functional.fractional_max_pool3d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.scaled_dot_product_attention.html">torch.nn.functional.scaled_dot_product_attention</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.threshold.html">torch.nn.functional.threshold</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.threshold_.html">torch.nn.functional.threshold_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.relu.html">torch.nn.functional.relu</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.relu_.html">torch.nn.functional.relu_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.hardtanh.html">torch.nn.functional.hardtanh</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.hardtanh_.html">torch.nn.functional.hardtanh_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.hardswish.html">torch.nn.functional.hardswish</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.relu6.html">torch.nn.functional.relu6</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.elu.html">torch.nn.functional.elu</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.elu_.html">torch.nn.functional.elu_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.selu.html">torch.nn.functional.selu</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.celu.html">torch.nn.functional.celu</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.leaky_relu.html">torch.nn.functional.leaky_relu</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.leaky_relu_.html">torch.nn.functional.leaky_relu_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.prelu.html">torch.nn.functional.prelu</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.rrelu.html">torch.nn.functional.rrelu</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.rrelu_.html">torch.nn.functional.rrelu_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.glu.html">torch.nn.functional.glu</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.gelu.html">torch.nn.functional.gelu</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.logsigmoid.html">torch.nn.functional.logsigmoid</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.hardshrink.html">torch.nn.functional.hardshrink</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.tanhshrink.html">torch.nn.functional.tanhshrink</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.softsign.html">torch.nn.functional.softsign</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.softplus.html">torch.nn.functional.softplus</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.softmin.html">torch.nn.functional.softmin</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.softmax.html">torch.nn.functional.softmax</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.softshrink.html">torch.nn.functional.softshrink</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.gumbel_softmax.html">torch.nn.functional.gumbel_softmax</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.log_softmax.html">torch.nn.functional.log_softmax</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.tanh.html">torch.nn.functional.tanh</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.sigmoid.html">torch.nn.functional.sigmoid</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.hardsigmoid.html">torch.nn.functional.hardsigmoid</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.silu.html">torch.nn.functional.silu</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.mish.html">torch.nn.functional.mish</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.batch_norm.html">torch.nn.functional.batch_norm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.group_norm.html">torch.nn.functional.group_norm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.instance_norm.html">torch.nn.functional.instance_norm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.layer_norm.html">torch.nn.functional.layer_norm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.local_response_norm.html">torch.nn.functional.local_response_norm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.rms_norm.html">torch.nn.functional.rms_norm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.normalize.html">torch.nn.functional.normalize</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.linear.html">torch.nn.functional.linear</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.bilinear.html">torch.nn.functional.bilinear</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.dropout.html">torch.nn.functional.dropout</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.alpha_dropout.html">torch.nn.functional.alpha_dropout</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.feature_alpha_dropout.html">torch.nn.functional.feature_alpha_dropout</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.dropout1d.html">torch.nn.functional.dropout1d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.dropout2d.html">torch.nn.functional.dropout2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.dropout3d.html">torch.nn.functional.dropout3d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.embedding.html">torch.nn.functional.embedding</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.embedding_bag.html">torch.nn.functional.embedding_bag</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.one_hot.html">torch.nn.functional.one_hot</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.pairwise_distance.html">torch.nn.functional.pairwise_distance</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.cosine_similarity.html">torch.nn.functional.cosine_similarity</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.pdist.html">torch.nn.functional.pdist</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.binary_cross_entropy.html">torch.nn.functional.binary_cross_entropy</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.binary_cross_entropy_with_logits.html">torch.nn.functional.binary_cross_entropy_with_logits</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.poisson_nll_loss.html">torch.nn.functional.poisson_nll_loss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.cosine_embedding_loss.html">torch.nn.functional.cosine_embedding_loss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.cross_entropy.html">torch.nn.functional.cross_entropy</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.ctc_loss.html">torch.nn.functional.ctc_loss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.gaussian_nll_loss.html">torch.nn.functional.gaussian_nll_loss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.hinge_embedding_loss.html">torch.nn.functional.hinge_embedding_loss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.kl_div.html">torch.nn.functional.kl_div</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.l1_loss.html">torch.nn.functional.l1_loss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.mse_loss.html">torch.nn.functional.mse_loss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.margin_ranking_loss.html">torch.nn.functional.margin_ranking_loss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.multilabel_margin_loss.html">torch.nn.functional.multilabel_margin_loss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.multilabel_soft_margin_loss.html">torch.nn.functional.multilabel_soft_margin_loss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.multi_margin_loss.html">torch.nn.functional.multi_margin_loss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.nll_loss.html">torch.nn.functional.nll_loss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.huber_loss.html">torch.nn.functional.huber_loss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.smooth_l1_loss.html">torch.nn.functional.smooth_l1_loss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.soft_margin_loss.html">torch.nn.functional.soft_margin_loss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.triplet_margin_loss.html">torch.nn.functional.triplet_margin_loss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.triplet_margin_with_distance_loss.html">torch.nn.functional.triplet_margin_with_distance_loss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.pixel_shuffle.html">torch.nn.functional.pixel_shuffle</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.pixel_unshuffle.html">torch.nn.functional.pixel_unshuffle</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.pad.html">torch.nn.functional.pad</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.interpolate.html">torch.nn.functional.interpolate</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.upsample.html">torch.nn.functional.upsample</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.upsample_nearest.html">torch.nn.functional.upsample_nearest</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.upsample_bilinear.html">torch.nn.functional.upsample_bilinear</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.grid_sample.html">torch.nn.functional.grid_sample</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.affine_grid.html">torch.nn.functional.affine_grid</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.torch.nn.parallel.data_parallel.html">torch.nn.functional.torch.nn.parallel.data_parallel</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="nn.init.html">torch.nn.init</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="nn.attention.html">torch.nn.attention</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.attention.sdpa_kernel.html">torch.nn.attention.sdpa_kernel</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.attention.SDPBackend.html">SDPBackend</a></li>
<li class="toctree-l2"><a class="reference internal" href="nn.attention.flex_attention.html">torch.nn.attention.flex_attention</a></li>
<li class="toctree-l2"><a class="reference internal" href="nn.attention.bias.html">torch.nn.attention.bias</a></li>

<li class="toctree-l2"><a class="reference internal" href="nn.attention.experimental.html">torch.nn.attention.experimental</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Device Management</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="cuda.html">torch.cuda</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.StreamContext.html">StreamContext</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.can_device_access_peer.html">torch.cuda.can_device_access_peer</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.current_blas_handle.html">torch.cuda.current_blas_handle</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.current_device.html">torch.cuda.current_device</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.current_stream.html">torch.cuda.current_stream</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.cudart.html">torch.cuda.cudart</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.default_stream.html">torch.cuda.default_stream</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.device.html">device</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.device_count.html">torch.cuda.device_count</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.device_memory_used.html">torch.cuda.device_memory_used</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.device_of.html">device_of</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.get_arch_list.html">torch.cuda.get_arch_list</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.get_device_capability.html">torch.cuda.get_device_capability</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.get_device_name.html">torch.cuda.get_device_name</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.get_device_properties.html">torch.cuda.get_device_properties</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.get_gencode_flags.html">torch.cuda.get_gencode_flags</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.get_sync_debug_mode.html">torch.cuda.get_sync_debug_mode</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.init.html">torch.cuda.init</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.ipc_collect.html">torch.cuda.ipc_collect</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.is_available.html">torch.cuda.is_available</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.is_initialized.html">torch.cuda.is_initialized</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.memory_usage.html">torch.cuda.memory_usage</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.set_device.html">torch.cuda.set_device</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.set_stream.html">torch.cuda.set_stream</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.set_sync_debug_mode.html">torch.cuda.set_sync_debug_mode</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.synchronize.html">torch.cuda.synchronize</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.utilization.html">torch.cuda.utilization</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.temperature.html">torch.cuda.temperature</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.power_draw.html">torch.cuda.power_draw</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.clock_rate.html">torch.cuda.clock_rate</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.OutOfMemoryError.html">torch.cuda.OutOfMemoryError</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.get_rng_state.html">torch.cuda.get_rng_state</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.get_rng_state_all.html">torch.cuda.get_rng_state_all</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.set_rng_state.html">torch.cuda.set_rng_state</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.set_rng_state_all.html">torch.cuda.set_rng_state_all</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.manual_seed.html">torch.cuda.manual_seed</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.manual_seed_all.html">torch.cuda.manual_seed_all</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.seed.html">torch.cuda.seed</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.seed_all.html">torch.cuda.seed_all</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.initial_seed.html">torch.cuda.initial_seed</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.comm.broadcast.html">torch.cuda.comm.broadcast</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.comm.broadcast_coalesced.html">torch.cuda.comm.broadcast_coalesced</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.comm.reduce_add.html">torch.cuda.comm.reduce_add</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.comm.scatter.html">torch.cuda.comm.scatter</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.comm.gather.html">torch.cuda.comm.gather</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.Stream.html">torch.cuda.stream</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.ExternalStream.html">ExternalStream</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.Event.html">Event</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.is_current_stream_capturing.html">torch.cuda.is_current_stream_capturing</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.graph_pool_handle.html">torch.cuda.graph_pool_handle</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.CUDAGraph.html">CUDAGraph</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.graph.html">graph</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.make_graphed_callables.html">torch.cuda.make_graphed_callables</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.empty_cache.html">torch.cuda.empty_cache</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.get_per_process_memory_fraction.html">torch.cuda.get_per_process_memory_fraction</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.list_gpu_processes.html">torch.cuda.list_gpu_processes</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.mem_get_info.html">torch.cuda.mem_get_info</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.memory_stats.html">torch.cuda.memory_stats</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.memory_summary.html">torch.cuda.memory_summary</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.memory_snapshot.html">torch.cuda.memory_snapshot</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.memory_allocated.html">torch.cuda.memory_allocated</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.max_memory_allocated.html">torch.cuda.max_memory_allocated</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.reset_max_memory_allocated.html">torch.cuda.reset_max_memory_allocated</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.memory_reserved.html">torch.cuda.memory_reserved</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.max_memory_reserved.html">torch.cuda.max_memory_reserved</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.set_per_process_memory_fraction.html">torch.cuda.set_per_process_memory_fraction</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.memory_cached.html">torch.cuda.memory_cached</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.max_memory_cached.html">torch.cuda.max_memory_cached</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.reset_max_memory_cached.html">torch.cuda.reset_max_memory_cached</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.reset_peak_memory_stats.html">torch.cuda.reset_peak_memory_stats</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.caching_allocator_alloc.html">torch.cuda.caching_allocator_alloc</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.caching_allocator_delete.html">torch.cuda.caching_allocator_delete</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.get_allocator_backend.html">torch.cuda.get_allocator_backend</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.CUDAPluggableAllocator.html">CUDAPluggableAllocator</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.change_current_allocator.html">torch.cuda.change_current_allocator</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.MemPool.html">MemPool</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.MemPoolContext.html">MemPoolContext</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.memory.caching_allocator_enable.html">torch.cuda.memory.caching_allocator_enable</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.nvtx.mark.html">torch.cuda.nvtx.mark</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.nvtx.range_push.html">torch.cuda.nvtx.range_push</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.nvtx.range_pop.html">torch.cuda.nvtx.range_pop</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.nvtx.range.html">torch.cuda.nvtx.range</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.jiterator._create_jit_fn.html">torch.cuda.jiterator._create_jit_fn</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.jiterator._create_multi_output_jit_fn.html">torch.cuda.jiterator._create_multi_output_jit_fn</a></li>
<li class="toctree-l2"><a class="reference internal" href="cuda.tunable.html">TunableOp</a></li>
<li class="toctree-l2"><a class="reference internal" href="cuda._sanitizer.html">CUDA Stream Sanitizer</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="cpu.html">torch.cpu</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cpu.current_device.html">torch.cpu.current_device</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cpu.current_stream.html">torch.cpu.current_stream</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cpu.is_available.html">torch.cpu.is_available</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cpu.synchronize.html">torch.cpu.synchronize</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cpu.set_device.html">torch.cpu.set_device</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cpu.device_count.html">torch.cpu.device_count</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cpu.StreamContext.html">StreamContext</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cpu.Stream.html">torch.cpu.stream</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="mps.html">torch.mps</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mps.device_count.html">torch.mps.device_count</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mps.synchronize.html">torch.mps.synchronize</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mps.get_rng_state.html">torch.mps.get_rng_state</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mps.set_rng_state.html">torch.mps.set_rng_state</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mps.manual_seed.html">torch.mps.manual_seed</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mps.seed.html">torch.mps.seed</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mps.empty_cache.html">torch.mps.empty_cache</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mps.set_per_process_memory_fraction.html">torch.mps.set_per_process_memory_fraction</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mps.current_allocated_memory.html">torch.mps.current_allocated_memory</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mps.driver_allocated_memory.html">torch.mps.driver_allocated_memory</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mps.recommended_max_memory.html">torch.mps.recommended_max_memory</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mps.profiler.start.html">torch.mps.profiler.start</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mps.profiler.stop.html">torch.mps.profiler.stop</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mps.profiler.profile.html">torch.mps.profiler.profile</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mps.event.Event.html">Event</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="xpu.html">torch.xpu</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xpu.StreamContext.html">StreamContext</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xpu.current_device.html">torch.xpu.current_device</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xpu.current_stream.html">torch.xpu.current_stream</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xpu.device.html">device</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xpu.device_count.html">torch.xpu.device_count</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xpu.device_of.html">device_of</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xpu.get_arch_list.html">torch.xpu.get_arch_list</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xpu.get_device_capability.html">torch.xpu.get_device_capability</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xpu.get_device_name.html">torch.xpu.get_device_name</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xpu.get_device_properties.html">torch.xpu.get_device_properties</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xpu.get_gencode_flags.html">torch.xpu.get_gencode_flags</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xpu.init.html">torch.xpu.init</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xpu.is_available.html">torch.xpu.is_available</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xpu.is_initialized.html">torch.xpu.is_initialized</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xpu.set_device.html">torch.xpu.set_device</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xpu.set_stream.html">torch.xpu.set_stream</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xpu.synchronize.html">torch.xpu.synchronize</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xpu.get_rng_state.html">torch.xpu.get_rng_state</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xpu.get_rng_state_all.html">torch.xpu.get_rng_state_all</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xpu.initial_seed.html">torch.xpu.initial_seed</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xpu.manual_seed.html">torch.xpu.manual_seed</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xpu.manual_seed_all.html">torch.xpu.manual_seed_all</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xpu.seed.html">torch.xpu.seed</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xpu.seed_all.html">torch.xpu.seed_all</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xpu.set_rng_state.html">torch.xpu.set_rng_state</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xpu.set_rng_state_all.html">torch.xpu.set_rng_state_all</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xpu.Event.html">Event</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xpu.Stream.html">torch.xpu.stream</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xpu.empty_cache.html">torch.xpu.empty_cache</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xpu.max_memory_allocated.html">torch.xpu.max_memory_allocated</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xpu.max_memory_reserved.html">torch.xpu.max_memory_reserved</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xpu.mem_get_info.html">torch.xpu.mem_get_info</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xpu.memory_allocated.html">torch.xpu.memory_allocated</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xpu.memory_reserved.html">torch.xpu.memory_reserved</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xpu.memory_stats.html">torch.xpu.memory_stats</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xpu.memory_stats_as_nested_dict.html">torch.xpu.memory_stats_as_nested_dict</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xpu.reset_accumulated_memory_stats.html">torch.xpu.reset_accumulated_memory_stats</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xpu.reset_peak_memory_stats.html">torch.xpu.reset_peak_memory_stats</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="accelerator.html">torch.accelerator</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.accelerator.device_count.html">torch.accelerator.device_count</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.accelerator.is_available.html">torch.accelerator.is_available</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.accelerator.current_accelerator.html">torch.accelerator.current_accelerator</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.accelerator.set_device_index.html">torch.accelerator.set_device_index</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.accelerator.set_device_idx.html">torch.accelerator.set_device_idx</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.accelerator.current_device_index.html">torch.accelerator.current_device_index</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.accelerator.current_device_idx.html">torch.accelerator.current_device_idx</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.accelerator.set_stream.html">torch.accelerator.set_stream</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.accelerator.current_stream.html">torch.accelerator.current_stream</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.accelerator.synchronize.html">torch.accelerator.synchronize</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="mtia.html">torch.mtia</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mtia.StreamContext.html">StreamContext</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mtia.current_device.html">torch.mtia.current_device</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mtia.current_stream.html">torch.mtia.current_stream</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mtia.default_stream.html">torch.mtia.default_stream</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mtia.device_count.html">torch.mtia.device_count</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mtia.init.html">torch.mtia.init</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mtia.is_available.html">torch.mtia.is_available</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mtia.is_initialized.html">torch.mtia.is_initialized</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mtia.memory_stats.html">torch.mtia.memory_stats</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mtia.get_device_capability.html">torch.mtia.get_device_capability</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mtia.empty_cache.html">torch.mtia.empty_cache</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mtia.set_device.html">torch.mtia.set_device</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mtia.set_stream.html">torch.mtia.set_stream</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mtia.synchronize.html">torch.mtia.synchronize</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mtia.device.html">device</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mtia.set_rng_state.html">torch.mtia.set_rng_state</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mtia.get_rng_state.html">torch.mtia.get_rng_state</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mtia.DeferredMtiaCallError.html">torch.mtia.DeferredMtiaCallError</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mtia.Event.html">Event</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mtia.Stream.html">torch.mtia.stream</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="mtia.memory.html">torch.mtia.memory</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mtia.memory.memory_stats.html">torch.mtia.memory.memory_stats</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="meta.html">Meta device</a></li>
<li class="toctree-l1"><a class="reference internal" href="torch_cuda_memory.html">Understanding CUDA Memory Usage</a></li>



</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Data Handling</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="data.html">torch.utils.data</a></li>
<li class="toctree-l1"><a class="reference internal" href="dlpack.html">torch.utils.dlpack</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_zoo.html">torch.utils.model_zoo</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Optimization</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="optim.html">torch.optim</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.Optimizer.add_param_group.html">torch.optim.Optimizer.add_param_group</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.Optimizer.load_state_dict.html">torch.optim.Optimizer.load_state_dict</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.Optimizer.register_load_state_dict_pre_hook.html">torch.optim.Optimizer.register_load_state_dict_pre_hook</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.Optimizer.register_load_state_dict_post_hook.html">torch.optim.Optimizer.register_load_state_dict_post_hook</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.Optimizer.state_dict.html">torch.optim.Optimizer.state_dict</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.Optimizer.register_state_dict_pre_hook.html">torch.optim.Optimizer.register_state_dict_pre_hook</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.Optimizer.register_state_dict_post_hook.html">torch.optim.Optimizer.register_state_dict_post_hook</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.Optimizer.step.html">torch.optim.Optimizer.step</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.Optimizer.register_step_pre_hook.html">torch.optim.Optimizer.register_step_pre_hook</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.Optimizer.register_step_post_hook.html">torch.optim.Optimizer.register_step_post_hook</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.Optimizer.zero_grad.html">torch.optim.Optimizer.zero_grad</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.Adadelta.html">Adadelta</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.Adafactor.html">Adafactor</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.Adagrad.html">Adagrad</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.Adam.html">Adam</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.AdamW.html">AdamW</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.SparseAdam.html">SparseAdam</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.Adamax.html">Adamax</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.ASGD.html">ASGD</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.LBFGS.html">LBFGS</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.NAdam.html">NAdam</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.RAdam.html">RAdam</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.RMSprop.html">RMSprop</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.Rprop.html">Rprop</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.SGD.html">SGD</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.lr_scheduler.LRScheduler.html">LRScheduler</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.lr_scheduler.LambdaLR.html">LambdaLR</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.lr_scheduler.MultiplicativeLR.html">MultiplicativeLR</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.lr_scheduler.StepLR.html">StepLR</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.lr_scheduler.MultiStepLR.html">MultiStepLR</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.lr_scheduler.ConstantLR.html">ConstantLR</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.lr_scheduler.LinearLR.html">LinearLR</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.lr_scheduler.ExponentialLR.html">ExponentialLR</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.lr_scheduler.PolynomialLR.html">PolynomialLR</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.lr_scheduler.CosineAnnealingLR.html">CosineAnnealingLR</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.lr_scheduler.ChainedScheduler.html">ChainedScheduler</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.lr_scheduler.SequentialLR.html">SequentialLR</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.lr_scheduler.ReduceLROnPlateau.html">ReduceLROnPlateau</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.lr_scheduler.CyclicLR.html">CyclicLR</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.lr_scheduler.OneCycleLR.html">OneCycleLR</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.lr_scheduler.CosineAnnealingWarmRestarts.html">CosineAnnealingWarmRestarts</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.swa_utils.AveragedModel.html">AveragedModel</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.swa_utils.SWALR.html">SWALR</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="distributed.optim.html">torch.distributed.optim</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Optimization and Compilation</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="torch.compiler.html">torch.compile</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="torch.compiler_get_started.html">Getting Started</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="torch.compiler_api.html">torch.compiler API reference</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.compiler.compile.html">torch.compiler.compile</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.compiler.reset.html">torch.compiler.reset</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.compiler.allow_in_graph.html">torch.compiler.allow_in_graph</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.compiler.substitute_in_graph.html">torch.compiler.substitute_in_graph</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.compiler.assume_constant_result.html">torch.compiler.assume_constant_result</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.compiler.list_backends.html">torch.compiler.list_backends</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.compiler.disable.html">torch.compiler.disable</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.compiler.set_stance.html">torch.compiler.set_stance</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.compiler.cudagraph_mark_step_begin.html">torch.compiler.cudagraph_mark_step_begin</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.compiler.is_compiling.html">torch.compiler.is_compiling</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.compiler.is_dynamo_compiling.html">torch.compiler.is_dynamo_compiling</a></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="torch.compiler.config.html">torch.compiler.config</a></li>
<li class="toctree-l2"><a class="reference internal" href="torch.compiler_fine_grain_apis.html">TorchDynamo APIs for fine-grained tracing</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="torch.compiler_aot_inductor.html">AOTInductor: Ahead-Of-Time Compilation for Torch.Export-ed Models</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3 has-children"><a class="reference internal" href="logging.html">torch._logging</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l4"><a class="reference internal" href="generated/torch._logging.set_logs.html">torch._logging.set_logs</a></li>
</ul>
</details></li>
<li class="toctree-l3"><a class="reference internal" href="torch.compiler_aot_inductor_minifier.html">AOTInductor Minifier</a></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="torch.compiler_inductor_profiling.html">TorchInductor GPU Profiling</a></li>
<li class="toctree-l2"><a class="reference internal" href="torch.compiler_profiling_torch_compile.html">Profiling to understand torch.compile performance</a></li>
<li class="toctree-l2"><a class="reference internal" href="torch.compiler_faq.html">Frequently Asked Questions</a></li>
<li class="toctree-l2"><a class="reference internal" href="torch.compiler_troubleshooting.html">torch.compile Troubleshooting</a></li>
<li class="toctree-l2"><a class="reference internal" href="torch.compiler_performance_dashboard.html">PyTorch 2.0 Performance Dashboard</a></li>
<li class="toctree-l2"><a class="reference internal" href="torch.compiler_dynamo_overview.html">Dynamo Overview</a></li>
<li class="toctree-l2"><a class="reference internal" href="torch.compiler_dynamo_deepdive.html">Dynamo Deep-Dive</a></li>
<li class="toctree-l2"><a class="reference internal" href="torch.compiler_dynamic_shapes.html">Dynamic shapes</a></li>
<li class="toctree-l2"><a class="reference internal" href="torch.compiler_nn_module.html">PyTorch 2.0 NNModule Support</a></li>
<li class="toctree-l2"><a class="reference internal" href="torch.compiler_best_practices_for_backends.html">Best Practices for Backends</a></li>
<li class="toctree-l2"><a class="reference internal" href="torch.compiler_cudagraph_trees.html">CUDAGraph Trees</a></li>
<li class="toctree-l2"><a class="reference internal" href="torch.compiler_fake_tensor.html">Fake tensor</a></li>
<li class="toctree-l2"><a class="reference internal" href="torch.compiler_custom_backends.html">Custom Backends</a></li>
<li class="toctree-l2"><a class="reference internal" href="torch.compiler_transformations.html">Writing Graph Transformations on ATen IR</a></li>
<li class="toctree-l2"><a class="reference internal" href="torch.compiler_ir.html">IRs</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="jit.html">torch.jit</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="jit_builtin_functions.html">torch.jit.supported_ops</a></li>
<li class="toctree-l2"><a class="reference internal" href="jit_language_reference.html">TorchScript Language Reference</a></li>
<li class="toctree-l2"><a class="reference internal" href="jit_language_reference_v2.html">TorchScript Language Reference</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.jit.script.html">torch.jit.script</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.jit.trace.html">torch.jit.trace</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.jit.script_if_tracing.html">torch.jit.script_if_tracing</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.jit.trace_module.html">torch.jit.trace_module</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.jit.fork.html">torch.jit.fork</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.jit.wait.html">torch.jit.wait</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.jit.ScriptModule.html">ScriptModule</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.jit.ScriptFunction.html">ScriptFunction</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.jit.freeze.html">torch.jit.freeze</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.jit.optimize_for_inference.html">torch.jit.optimize_for_inference</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.jit.enable_onednn_fusion.html">torch.jit.enable_onednn_fusion</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.jit.onednn_fusion_enabled.html">torch.jit.onednn_fusion_enabled</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.jit.set_fusion_strategy.html">torch.jit.set_fusion_strategy</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.jit.strict_fusion.html">strict_fusion</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.jit.save.html">torch.jit.save</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.jit.load.html">torch.jit.load</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.jit.ignore.html">torch.jit.ignore</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.jit.unused.html">torch.jit.unused</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.jit.interface.html">torch.jit.interface</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.jit.isinstance.html">torch.jit.isinstance</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.jit.Attribute.html">Attribute</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.jit.annotate.html">torch.jit.annotate</a></li>
<li class="toctree-l2"><a class="reference internal" href="jit_python_reference.html">Python Language Reference Coverage</a></li>
<li class="toctree-l2"><a class="reference internal" href="jit_unsupported.html">TorchScript Unsupported PyTorch Constructs</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="func.html">torch.func</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="func.whirlwind_tour.html">torch.func Whirlwind Tour</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="func.api.html">torch.func API Reference</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.func.vmap.html">torch.func.vmap</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.func.grad.html">torch.func.grad</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.func.grad_and_value.html">torch.func.grad_and_value</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.func.vjp.html">torch.func.vjp</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.func.jvp.html">torch.func.jvp</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.func.linearize.html">torch.func.linearize</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.func.jacrev.html">torch.func.jacrev</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.func.jacfwd.html">torch.func.jacfwd</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.func.hessian.html">torch.func.hessian</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.func.functionalize.html">torch.func.functionalize</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.func.functional_call.html">torch.func.functional_call</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.func.stack_module_state.html">torch.func.stack_module_state</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.func.replace_all_batch_norm_modules_.html">torch.func.replace_all_batch_norm_modules_</a></li>
<li class="toctree-l3"><a class="reference internal" href="func.batch_norm.html">Patching Batch Norm</a></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="func.ux_limitations.html">UX Limitations</a></li>
<li class="toctree-l2"><a class="reference internal" href="func.migrating.html">Migrating from functorch to torch.func</a></li>
</ul>
</details></li>
</ul>
</div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
      <div class="sidebar-primary-item">
<div id="ethical-ad-placement"
      class="flat"
      data-ea-publisher="readthedocs"
      data-ea-type="readthedocs-sidebar"
      data-ea-manual="true">
</div></div>
  </div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        
          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item">

<nav aria-label="Breadcrumb" class="d-print-none">
  <ul class="bd-breadcrumbs">
    
    <li class="breadcrumb-item breadcrumb-home">
      <a href="../index.html" class="nav-link" aria-label="Home">
        <i class="fa-solid fa-home"></i>
      </a>
    </li>
    
    <li class="breadcrumb-item"><a href="index.html" class="nav-link">Python API</a></li>
    
    <li class="breadcrumb-item active" aria-current="page"><span class="ellipsis">torch.Tensor</span></li>
  </ul>
</nav>
</div>
      
    </div>
  
  
</div>
</div>
              
              
              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section id="torch-tensor">
<span id="tensor-doc"></span><h1>torch.Tensor<a class="headerlink" href="#torch-tensor" title="Link to this heading">#</a></h1>
<p>A <a class="reference internal" href="#torch.Tensor" title="torch.Tensor"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Tensor</span></code></a> is a multi-dimensional matrix containing elements of
a single data type.</p>
<section id="data-types">
<h2>Data types<a class="headerlink" href="#data-types" title="Link to this heading">#</a></h2>
<p>Torch defines tensor types with the following data types:</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Data type</p></th>
<th class="head"><p>dtype</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>32-bit floating point</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">torch.float32</span></code> or <code class="docutils literal notranslate"><span class="pre">torch.float</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>64-bit floating point</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">torch.float64</span></code> or <code class="docutils literal notranslate"><span class="pre">torch.double</span></code></p></td>
</tr>
<tr class="row-even"><td><p>16-bit floating point <a class="footnote-reference brackets" href="#id9" id="id1" role="doc-noteref"><span class="fn-bracket">[</span>1<span class="fn-bracket">]</span></a></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">torch.float16</span></code> or <code class="docutils literal notranslate"><span class="pre">torch.half</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>16-bit floating point <a class="footnote-reference brackets" href="#id10" id="id2" role="doc-noteref"><span class="fn-bracket">[</span>2<span class="fn-bracket">]</span></a></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">torch.bfloat16</span></code></p></td>
</tr>
<tr class="row-even"><td><p>32-bit complex</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">torch.complex32</span></code> or <code class="docutils literal notranslate"><span class="pre">torch.chalf</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>64-bit complex</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">torch.complex64</span></code> or <code class="docutils literal notranslate"><span class="pre">torch.cfloat</span></code></p></td>
</tr>
<tr class="row-even"><td><p>128-bit complex</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">torch.complex128</span></code> or <code class="docutils literal notranslate"><span class="pre">torch.cdouble</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>8-bit integer (unsigned)</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">torch.uint8</span></code></p></td>
</tr>
<tr class="row-even"><td><p>16-bit integer (unsigned)</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">torch.uint16</span></code> (limited support) <a class="footnote-reference brackets" href="#id12" id="id3" role="doc-noteref"><span class="fn-bracket">[</span>4<span class="fn-bracket">]</span></a></p></td>
</tr>
<tr class="row-odd"><td><p>32-bit integer (unsigned)</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">torch.uint32</span></code> (limited support) <a class="footnote-reference brackets" href="#id12" id="id4" role="doc-noteref"><span class="fn-bracket">[</span>4<span class="fn-bracket">]</span></a></p></td>
</tr>
<tr class="row-even"><td><p>64-bit integer (unsigned)</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">torch.uint64</span></code> (limited support) <a class="footnote-reference brackets" href="#id12" id="id5" role="doc-noteref"><span class="fn-bracket">[</span>4<span class="fn-bracket">]</span></a></p></td>
</tr>
<tr class="row-odd"><td><p>8-bit integer (signed)</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">torch.int8</span></code></p></td>
</tr>
<tr class="row-even"><td><p>16-bit integer (signed)</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">torch.int16</span></code> or <code class="docutils literal notranslate"><span class="pre">torch.short</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>32-bit integer (signed)</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">torch.int32</span></code> or <code class="docutils literal notranslate"><span class="pre">torch.int</span></code></p></td>
</tr>
<tr class="row-even"><td><p>64-bit integer (signed)</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">torch.int64</span></code> or <code class="docutils literal notranslate"><span class="pre">torch.long</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>Boolean</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">torch.bool</span></code></p></td>
</tr>
<tr class="row-even"><td><p>quantized 8-bit integer (unsigned)</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">torch.quint8</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>quantized 8-bit integer (signed)</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">torch.qint8</span></code></p></td>
</tr>
<tr class="row-even"><td><p>quantized 32-bit integer (signed)</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">torch.qint32</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>quantized 4-bit integer (unsigned) <a class="footnote-reference brackets" href="#id11" id="id6" role="doc-noteref"><span class="fn-bracket">[</span>3<span class="fn-bracket">]</span></a></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">torch.quint4x2</span></code></p></td>
</tr>
<tr class="row-even"><td><p>8-bit floating point, e4m3 <a class="footnote-reference brackets" href="#id13" id="id7" role="doc-noteref"><span class="fn-bracket">[</span>5<span class="fn-bracket">]</span></a></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">torch.float8_e4m3fn</span></code> (limited support)</p></td>
</tr>
<tr class="row-odd"><td><p>8-bit floating point, e5m2 <a class="footnote-reference brackets" href="#id13" id="id8" role="doc-noteref"><span class="fn-bracket">[</span>5<span class="fn-bracket">]</span></a></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">torch.float8_e5m2</span></code> (limited support)</p></td>
</tr>
</tbody>
</table>
</div>
<aside class="footnote-list brackets">
<aside class="footnote brackets" id="id9" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id1">1</a><span class="fn-bracket">]</span></span>
<p>Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10
significand bits. Useful when precision is important at the expense of range.</p>
</aside>
<aside class="footnote brackets" id="id10" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id2">2</a><span class="fn-bracket">]</span></span>
<p>Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7
significand bits. Useful when range is important, since it has the same
number of exponent bits as <code class="docutils literal notranslate"><span class="pre">float32</span></code></p>
</aside>
<aside class="footnote brackets" id="id11" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id6">3</a><span class="fn-bracket">]</span></span>
<p>quantized 4-bit integer is stored as a 8-bit signed integer. Currently its only supported in EmbeddingBag operator.</p>
</aside>
<aside class="footnote brackets" id="id12" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span>4<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id3">1</a>,<a role="doc-backlink" href="#id4">2</a>,<a role="doc-backlink" href="#id5">3</a>)</span>
<p>Unsigned types asides from <code class="docutils literal notranslate"><span class="pre">uint8</span></code> are currently planned to only have
limited support in eager mode (they primarily exist to assist usage with
torch.compile); if you need eager support and the extra range is not needed,
we recommend using their signed variants instead.  See
<a class="github reference external" href="https://github.com/pytorch/pytorch/issues/58734">pytorch/pytorch#58734</a> for more details.</p>
</aside>
<aside class="footnote brackets" id="id13" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span>5<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id7">1</a>,<a role="doc-backlink" href="#id8">2</a>)</span>
<p><code class="docutils literal notranslate"><span class="pre">torch.float8_e4m3fn</span></code> and <code class="docutils literal notranslate"><span class="pre">torch.float8_e5m2</span></code> implement the spec for 8-bit
floating point types from <a class="reference external" href="https://arxiv.org/abs/2209.05433">https://arxiv.org/abs/2209.05433</a>. The op support
is very limited.</p>
</aside>
</aside>
<p>For backwards compatibility, we support the following alternate class names
for these data types:</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Data type</p></th>
<th class="head"><p>CPU tensor</p></th>
<th class="head"><p>GPU tensor</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>32-bit floating point</p></td>
<td><p><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code></p></td>
<td><p><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.cuda.FloatTensor</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>64-bit floating point</p></td>
<td><p><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.DoubleTensor</span></code></p></td>
<td><p><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.cuda.DoubleTensor</span></code></p></td>
</tr>
<tr class="row-even"><td><p>16-bit floating point</p></td>
<td><p><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.HalfTensor</span></code></p></td>
<td><p><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.cuda.HalfTensor</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>16-bit floating point</p></td>
<td><p><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.BFloat16Tensor</span></code></p></td>
<td><p><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.cuda.BFloat16Tensor</span></code></p></td>
</tr>
<tr class="row-even"><td><p>8-bit integer (unsigned)</p></td>
<td><p><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.ByteTensor</span></code></p></td>
<td><p><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.cuda.ByteTensor</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>8-bit integer (signed)</p></td>
<td><p><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.CharTensor</span></code></p></td>
<td><p><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.cuda.CharTensor</span></code></p></td>
</tr>
<tr class="row-even"><td><p>16-bit integer (signed)</p></td>
<td><p><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.ShortTensor</span></code></p></td>
<td><p><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.cuda.ShortTensor</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>32-bit integer (signed)</p></td>
<td><p><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.IntTensor</span></code></p></td>
<td><p><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.cuda.IntTensor</span></code></p></td>
</tr>
<tr class="row-even"><td><p>64-bit integer (signed)</p></td>
<td><p><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.LongTensor</span></code></p></td>
<td><p><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.cuda.LongTensor</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>Boolean</p></td>
<td><p><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.BoolTensor</span></code></p></td>
<td><p><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.cuda.BoolTensor</span></code></p></td>
</tr>
</tbody>
</table>
</div>
<p>However, to construct tensors, we recommend using factory functions such as
<a class="reference internal" href="generated/torch.empty.html#torch.empty" title="torch.empty"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.empty()</span></code></a> with the <code class="docutils literal notranslate"><span class="pre">dtype</span></code> argument instead.  The
<a class="reference internal" href="#torch.Tensor" title="torch.Tensor"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Tensor</span></code></a> constructor is an alias for the default tensor type
(<code class="xref py py-class docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code>).</p>
</section>
<section id="initializing-and-basic-operations">
<h2>Initializing and basic operations<a class="headerlink" href="#initializing-and-basic-operations" title="Link to this heading">#</a></h2>
<p>A tensor can be constructed from a Python <a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.13)"><code class="xref py py-class docutils literal notranslate"><span class="pre">list</span></code></a> or sequence using the
<a class="reference internal" href="generated/torch.tensor.html#torch.tensor" title="torch.tensor"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.tensor()</span></code></a> constructor:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">1.</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.</span><span class="p">],</span> <span class="p">[</span><span class="mf">1.</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.</span><span class="p">]])</span>
<span class="go">tensor([[ 1.0000, -1.0000],</span>
<span class="go">        [ 1.0000, -1.0000]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">]]))</span>
<span class="go">tensor([[ 1,  2,  3],</span>
<span class="go">        [ 4,  5,  6]])</span>
</pre></div>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p><a class="reference internal" href="generated/torch.tensor.html#torch.tensor" title="torch.tensor"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.tensor()</span></code></a> always copies <code class="xref py py-attr docutils literal notranslate"><span class="pre">data</span></code>. If you have a Tensor
<code class="xref py py-attr docutils literal notranslate"><span class="pre">data</span></code> and just want to change its <code class="docutils literal notranslate"><span class="pre">requires_grad</span></code> flag, use
<a class="reference internal" href="generated/torch.Tensor.requires_grad_.html#torch.Tensor.requires_grad_" title="torch.Tensor.requires_grad_"><code class="xref py py-meth docutils literal notranslate"><span class="pre">requires_grad_()</span></code></a> or
<a class="reference internal" href="generated/torch.Tensor.detach.html#torch.Tensor.detach" title="torch.Tensor.detach"><code class="xref py py-meth docutils literal notranslate"><span class="pre">detach()</span></code></a> to avoid a copy.
If you have a numpy array and want to avoid a copy, use
<a class="reference internal" href="generated/torch.as_tensor.html#torch.as_tensor" title="torch.as_tensor"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.as_tensor()</span></code></a>.</p>
</div>
<p>A tensor of specific data type can be constructed by passing a
<a class="reference internal" href="tensor_attributes.html#torch.dtype" title="torch.dtype"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.dtype</span></code></a> and/or a <a class="reference internal" href="tensor_attributes.html#torch.device" title="torch.device"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.device</span></code></a> to a
constructor or tensor creation op:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="go">tensor([[ 0,  0,  0,  0],</span>
<span class="go">        [ 0,  0,  0,  0]], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">cuda0</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">&#39;cuda:0&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float64</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">cuda0</span><span class="p">)</span>
<span class="go">tensor([[ 1.0000,  1.0000,  1.0000,  1.0000],</span>
<span class="go">        [ 1.0000,  1.0000,  1.0000,  1.0000]], dtype=torch.float64, device=&#39;cuda:0&#39;)</span>
</pre></div>
</div>
<p>For more information about building Tensors, see <a class="reference internal" href="torch.html#tensor-creation-ops"><span class="std std-ref">Creation Ops</span></a></p>
<p>The contents of a tensor can be accessed and modified using Pythons indexing
and slicing notation:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="mi">2</span><span class="p">])</span>
<span class="go">tensor(6)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="mi">8</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="go">tensor([[ 1,  8,  3],</span>
<span class="go">        [ 4,  5,  6]])</span>
</pre></div>
</div>
<p>Use <a class="reference internal" href="generated/torch.Tensor.item.html#torch.Tensor.item" title="torch.Tensor.item"><code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.Tensor.item()</span></code></a> to get a Python number from a tensor containing a
single value:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span>
<span class="go">tensor([[ 1]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
<span class="go">1</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">2.5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span>
<span class="go">tensor(2.5000)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
<span class="go">2.5</span>
</pre></div>
</div>
<p>For more information about indexing, see <a class="reference internal" href="torch.html#indexing-slicing-joining"><span class="std std-ref">Indexing, Slicing, Joining, Mutating Ops</span></a></p>
<p>A tensor can be created with <code class="xref py py-attr docutils literal notranslate"><span class="pre">requires_grad=True</span></code> so that
<a class="reference internal" href="autograd.html#module-torch.autograd" title="torch.autograd"><code class="xref py py-mod docutils literal notranslate"><span class="pre">torch.autograd</span></code></a> records operations on them for automatic differentiation.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">1.</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.</span><span class="p">],</span> <span class="p">[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">]],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">grad</span>
<span class="go">tensor([[ 2.0000, -2.0000],</span>
<span class="go">        [ 2.0000,  2.0000]])</span>
</pre></div>
</div>
<p>Each tensor has an associated <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Storage</span></code>, which holds its data.
The tensor class also provides multi-dimensional, <a class="reference external" href="https://en.wikipedia.org/wiki/Stride_of_an_array">strided</a>
view of a storage and defines numeric operations on it.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For more information on tensor views, see <a class="reference internal" href="tensor_view.html#tensor-view-doc"><span class="std std-ref">Tensor Views</span></a>.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For more information on the <a class="reference internal" href="tensor_attributes.html#torch.dtype" title="torch.dtype"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.dtype</span></code></a>, <a class="reference internal" href="tensor_attributes.html#torch.device" title="torch.device"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.device</span></code></a>, and
<a class="reference internal" href="tensor_attributes.html#torch.layout" title="torch.layout"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.layout</span></code></a> attributes of a <a class="reference internal" href="#torch.Tensor" title="torch.Tensor"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Tensor</span></code></a>, see
<a class="reference internal" href="tensor_attributes.html#tensor-attributes-doc"><span class="std std-ref">Tensor Attributes</span></a>.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Methods which mutate a tensor are marked with an underscore suffix.
For example, <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.FloatTensor.abs_()</span></code> computes the absolute value
in-place and returns the modified tensor, while <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.FloatTensor.abs()</span></code>
computes the result in a new tensor.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>To change an existing tensors <a class="reference internal" href="tensor_attributes.html#torch.device" title="torch.device"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.device</span></code></a> and/or <a class="reference internal" href="tensor_attributes.html#torch.dtype" title="torch.dtype"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.dtype</span></code></a>, consider using
<a class="reference internal" href="generated/torch.Tensor.to.html#torch.Tensor.to" title="torch.Tensor.to"><code class="xref py py-meth docutils literal notranslate"><span class="pre">to()</span></code></a> method on the tensor.</p>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Current implementation of <a class="reference internal" href="#torch.Tensor" title="torch.Tensor"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Tensor</span></code></a> introduces memory overhead,
thus it might lead to unexpectedly high memory usage in the applications with many tiny tensors.
If this is your case, consider using one large structure.</p>
</div>
</section>
<section id="tensor-class-reference">
<h2>Tensor class reference<a class="headerlink" href="#tensor-class-reference" title="Link to this heading">#</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="torch.Tensor">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torch.</span></span><span class="sig-name descname"><span class="pre">Tensor</span></span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/__init__.py#L80"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.Tensor" title="Link to this definition">#</a></dt>
<dd><p>There are a few main ways to create a tensor, depending on your use case.</p>
<ul class="simple">
<li><p>To create a tensor with pre-existing data, use <a class="reference internal" href="generated/torch.tensor.html#torch.tensor" title="torch.tensor"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.tensor()</span></code></a>.</p></li>
<li><p>To create a tensor with specific size, use <code class="docutils literal notranslate"><span class="pre">torch.*</span></code> tensor creation
ops (see <a class="reference internal" href="torch.html#tensor-creation-ops"><span class="std std-ref">Creation Ops</span></a>).</p></li>
<li><p>To create a tensor with the same size (and similar types) as another tensor,
use <code class="docutils literal notranslate"><span class="pre">torch.*_like</span></code> tensor creation ops
(see <a class="reference internal" href="torch.html#tensor-creation-ops"><span class="std std-ref">Creation Ops</span></a>).</p></li>
<li><p>To create a tensor with similar type but different size as another tensor,
use <code class="docutils literal notranslate"><span class="pre">tensor.new_*</span></code> creation ops.</p></li>
<li><p>There is a legacy constructor <code class="docutils literal notranslate"><span class="pre">torch.Tensor</span></code> whose use is discouraged.
Use <a class="reference internal" href="generated/torch.tensor.html#torch.tensor" title="torch.tensor"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.tensor()</span></code></a> instead.</p></li>
</ul>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.Tensor.__init__">
<span class="sig-prename descclassname"><span class="pre">Tensor.</span></span><span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">data</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.__init__" title="Link to this definition">#</a></dt>
<dd><p>This constructor is deprecated, we recommend using <a class="reference internal" href="generated/torch.tensor.html#torch.tensor" title="torch.tensor"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.tensor()</span></code></a> instead.
What this constructor does depends on the type of <code class="docutils literal notranslate"><span class="pre">data</span></code>.</p>
<ul class="simple">
<li><p>If <code class="docutils literal notranslate"><span class="pre">data</span></code> is a Tensor, returns an alias to the original Tensor.  Unlike
<a class="reference internal" href="generated/torch.tensor.html#torch.tensor" title="torch.tensor"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.tensor()</span></code></a>, this tracks autograd and will propagate gradients to
the original Tensor.  <code class="docutils literal notranslate"><span class="pre">device</span></code> kwarg is not supported for this <code class="docutils literal notranslate"><span class="pre">data</span></code> type.</p></li>
<li><p>If <code class="docutils literal notranslate"><span class="pre">data</span></code> is a sequence or nested sequence, create a tensor of the default
dtype (typically <code class="docutils literal notranslate"><span class="pre">torch.float32</span></code>) whose data is the values in the
sequences, performing coercions if necessary.  Notably, this differs from
<a class="reference internal" href="generated/torch.tensor.html#torch.tensor" title="torch.tensor"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.tensor()</span></code></a> in that this constructor will always construct a float
tensor, even if the inputs are all integers.</p></li>
<li><p>If <code class="docutils literal notranslate"><span class="pre">data</span></code> is a <a class="reference internal" href="size.html#torch.Size" title="torch.Size"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Size</span></code></a>, returns an empty tensor of that size.</p></li>
</ul>
<p>This constructor does not support explicitly specifying <code class="docutils literal notranslate"><span class="pre">dtype</span></code> or <code class="docutils literal notranslate"><span class="pre">device</span></code> of
the returned tensor.  We recommend using <a class="reference internal" href="generated/torch.tensor.html#torch.tensor" title="torch.tensor"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.tensor()</span></code></a> which provides this
functionality.</p>
<dl class="simple">
<dt>Args:</dt><dd><p>data (array_like): The tensor to construct from.</p>
</dd>
<dt>Keyword args:</dt><dd><dl class="simple">
<dt>device (<a class="reference internal" href="tensor_attributes.html#torch.device" title="torch.device"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.device</span></code></a>, optional): the desired device of returned tensor.</dt><dd><p>Default: if None, same <a class="reference internal" href="tensor_attributes.html#torch.device" title="torch.device"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.device</span></code></a> as this tensor.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="torch.Tensor.T">
<span class="sig-prename descclassname"><span class="pre">Tensor.</span></span><span class="sig-name descname"><span class="pre">T</span></span><a class="headerlink" href="#torch.Tensor.T" title="Link to this definition">#</a></dt>
<dd><p>Returns a view of this tensor with its dimensions reversed.</p>
<p>If <code class="docutils literal notranslate"><span class="pre">n</span></code> is the number of dimensions in <code class="docutils literal notranslate"><span class="pre">x</span></code>,
<code class="docutils literal notranslate"><span class="pre">x.T</span></code> is equivalent to <code class="docutils literal notranslate"><span class="pre">x.permute(n-1,</span> <span class="pre">n-2,</span> <span class="pre">...,</span> <span class="pre">0)</span></code>.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>The use of <a class="reference internal" href="#torch.Tensor.T" title="torch.Tensor.T"><code class="xref py py-func docutils literal notranslate"><span class="pre">Tensor.T()</span></code></a> on tensors of dimension other than 2 to reverse their shape
is deprecated and it will throw an error in a future release. Consider <a class="reference internal" href="#torch.Tensor.mT" title="torch.Tensor.mT"><code class="xref py py-attr docutils literal notranslate"><span class="pre">mT</span></code></a>
to transpose batches of matrices or <cite>x.permute(*torch.arange(x.ndim - 1, -1, -1))</cite> to reverse
the dimensions of a tensor.</p>
</div>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="torch.Tensor.H">
<span class="sig-prename descclassname"><span class="pre">Tensor.</span></span><span class="sig-name descname"><span class="pre">H</span></span><a class="headerlink" href="#torch.Tensor.H" title="Link to this definition">#</a></dt>
<dd><p>Returns a view of a matrix (2-D tensor) conjugated and transposed.</p>
<p><code class="docutils literal notranslate"><span class="pre">x.H</span></code> is equivalent to <code class="docutils literal notranslate"><span class="pre">x.transpose(0,</span> <span class="pre">1).conj()</span></code> for complex matrices and
<code class="docutils literal notranslate"><span class="pre">x.transpose(0,</span> <span class="pre">1)</span></code> for real matrices.</p>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p><a class="reference internal" href="#torch.Tensor.mH" title="torch.Tensor.mH"><code class="xref py py-attr docutils literal notranslate"><span class="pre">mH</span></code></a>: An attribute that also works on batches of matrices.</p>
</div>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="torch.Tensor.mT">
<span class="sig-prename descclassname"><span class="pre">Tensor.</span></span><span class="sig-name descname"><span class="pre">mT</span></span><a class="headerlink" href="#torch.Tensor.mT" title="Link to this definition">#</a></dt>
<dd><p>Returns a view of this tensor with the last two dimensions transposed.</p>
<p><code class="docutils literal notranslate"><span class="pre">x.mT</span></code> is equivalent to <code class="docutils literal notranslate"><span class="pre">x.transpose(-2,</span> <span class="pre">-1)</span></code>.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="torch.Tensor.mH">
<span class="sig-prename descclassname"><span class="pre">Tensor.</span></span><span class="sig-name descname"><span class="pre">mH</span></span><a class="headerlink" href="#torch.Tensor.mH" title="Link to this definition">#</a></dt>
<dd><p>Accessing this property is equivalent to calling <a class="reference internal" href="generated/torch.adjoint.html#torch.adjoint" title="torch.adjoint"><code class="xref py py-func docutils literal notranslate"><span class="pre">adjoint()</span></code></a>.</p>
</dd></dl>

<div class="pst-scrollable-table-container"><table class="autosummary longtable table autosummary">
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.new_tensor.html#torch.Tensor.new_tensor" title="torch.Tensor.new_tensor"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.new_tensor</span></code></a></p></td>
<td><p>Returns a new Tensor with <code class="xref py py-attr docutils literal notranslate"><span class="pre">data</span></code> as the tensor data.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.new_full.html#torch.Tensor.new_full" title="torch.Tensor.new_full"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.new_full</span></code></a></p></td>
<td><p>Returns a Tensor of size <code class="xref py py-attr docutils literal notranslate"><span class="pre">size</span></code> filled with <code class="xref py py-attr docutils literal notranslate"><span class="pre">fill_value</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.new_empty.html#torch.Tensor.new_empty" title="torch.Tensor.new_empty"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.new_empty</span></code></a></p></td>
<td><p>Returns a Tensor of size <code class="xref py py-attr docutils literal notranslate"><span class="pre">size</span></code> filled with uninitialized data.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.new_ones.html#torch.Tensor.new_ones" title="torch.Tensor.new_ones"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.new_ones</span></code></a></p></td>
<td><p>Returns a Tensor of size <code class="xref py py-attr docutils literal notranslate"><span class="pre">size</span></code> filled with <code class="docutils literal notranslate"><span class="pre">1</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.new_zeros.html#torch.Tensor.new_zeros" title="torch.Tensor.new_zeros"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.new_zeros</span></code></a></p></td>
<td><p>Returns a Tensor of size <code class="xref py py-attr docutils literal notranslate"><span class="pre">size</span></code> filled with <code class="docutils literal notranslate"><span class="pre">0</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.is_cuda.html#torch.Tensor.is_cuda" title="torch.Tensor.is_cuda"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.is_cuda</span></code></a></p></td>
<td><p>Is <code class="docutils literal notranslate"><span class="pre">True</span></code> if the Tensor is stored on the GPU, <code class="docutils literal notranslate"><span class="pre">False</span></code> otherwise.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.is_quantized.html#torch.Tensor.is_quantized" title="torch.Tensor.is_quantized"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.is_quantized</span></code></a></p></td>
<td><p>Is <code class="docutils literal notranslate"><span class="pre">True</span></code> if the Tensor is quantized, <code class="docutils literal notranslate"><span class="pre">False</span></code> otherwise.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.is_meta.html#torch.Tensor.is_meta" title="torch.Tensor.is_meta"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.is_meta</span></code></a></p></td>
<td><p>Is <code class="docutils literal notranslate"><span class="pre">True</span></code> if the Tensor is a meta tensor, <code class="docutils literal notranslate"><span class="pre">False</span></code> otherwise.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.device.html#torch.Tensor.device" title="torch.Tensor.device"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.device</span></code></a></p></td>
<td><p>Is the <a class="reference internal" href="tensor_attributes.html#torch.device" title="torch.device"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.device</span></code></a> where this Tensor is.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.grad.html#torch.Tensor.grad" title="torch.Tensor.grad"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.grad</span></code></a></p></td>
<td><p>This attribute is <code class="docutils literal notranslate"><span class="pre">None</span></code> by default and becomes a Tensor the first time a call to <code class="xref py py-func docutils literal notranslate"><span class="pre">backward()</span></code> computes gradients for <code class="docutils literal notranslate"><span class="pre">self</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.ndim.html#torch.Tensor.ndim" title="torch.Tensor.ndim"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.ndim</span></code></a></p></td>
<td><p>Alias for <a class="reference internal" href="generated/torch.Tensor.dim.html#torch.Tensor.dim" title="torch.Tensor.dim"><code class="xref py py-meth docutils literal notranslate"><span class="pre">dim()</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.real.html#torch.Tensor.real" title="torch.Tensor.real"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.real</span></code></a></p></td>
<td><p>Returns a new tensor containing real values of the <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor for a complex-valued input tensor.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.imag.html#torch.Tensor.imag" title="torch.Tensor.imag"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.imag</span></code></a></p></td>
<td><p>Returns a new tensor containing imaginary values of the <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.nbytes.html#torch.Tensor.nbytes" title="torch.Tensor.nbytes"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.nbytes</span></code></a></p></td>
<td><p>Returns the number of bytes consumed by the &quot;view&quot; of elements of the Tensor if the Tensor does not use sparse storage layout.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.itemsize.html#torch.Tensor.itemsize" title="torch.Tensor.itemsize"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.itemsize</span></code></a></p></td>
<td><p>Alias for <a class="reference internal" href="generated/torch.Tensor.element_size.html#torch.Tensor.element_size" title="torch.Tensor.element_size"><code class="xref py py-meth docutils literal notranslate"><span class="pre">element_size()</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.abs.html#torch.Tensor.abs" title="torch.Tensor.abs"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.abs</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.abs.html#torch.abs" title="torch.abs"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.abs()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.abs_.html#torch.Tensor.abs_" title="torch.Tensor.abs_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.abs_</span></code></a></p></td>
<td><p>In-place version of <a class="reference internal" href="generated/torch.Tensor.abs.html#torch.Tensor.abs" title="torch.Tensor.abs"><code class="xref py py-meth docutils literal notranslate"><span class="pre">abs()</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.absolute.html#torch.Tensor.absolute" title="torch.Tensor.absolute"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.absolute</span></code></a></p></td>
<td><p>Alias for <a class="reference internal" href="generated/torch.abs.html#torch.abs" title="torch.abs"><code class="xref py py-func docutils literal notranslate"><span class="pre">abs()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.absolute_.html#torch.Tensor.absolute_" title="torch.Tensor.absolute_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.absolute_</span></code></a></p></td>
<td><p>In-place version of <a class="reference internal" href="generated/torch.Tensor.absolute.html#torch.Tensor.absolute" title="torch.Tensor.absolute"><code class="xref py py-meth docutils literal notranslate"><span class="pre">absolute()</span></code></a> Alias for <code class="xref py py-func docutils literal notranslate"><span class="pre">abs_()</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.acos.html#torch.Tensor.acos" title="torch.Tensor.acos"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.acos</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.acos.html#torch.acos" title="torch.acos"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.acos()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.acos_.html#torch.Tensor.acos_" title="torch.Tensor.acos_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.acos_</span></code></a></p></td>
<td><p>In-place version of <a class="reference internal" href="generated/torch.Tensor.acos.html#torch.Tensor.acos" title="torch.Tensor.acos"><code class="xref py py-meth docutils literal notranslate"><span class="pre">acos()</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.arccos.html#torch.Tensor.arccos" title="torch.Tensor.arccos"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.arccos</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.arccos.html#torch.arccos" title="torch.arccos"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.arccos()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.arccos_.html#torch.Tensor.arccos_" title="torch.Tensor.arccos_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.arccos_</span></code></a></p></td>
<td><p>In-place version of <a class="reference internal" href="generated/torch.Tensor.arccos.html#torch.Tensor.arccos" title="torch.Tensor.arccos"><code class="xref py py-meth docutils literal notranslate"><span class="pre">arccos()</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.add.html#torch.Tensor.add" title="torch.Tensor.add"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.add</span></code></a></p></td>
<td><p>Add a scalar or tensor to <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.add_.html#torch.Tensor.add_" title="torch.Tensor.add_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.add_</span></code></a></p></td>
<td><p>In-place version of <a class="reference internal" href="generated/torch.Tensor.add.html#torch.Tensor.add" title="torch.Tensor.add"><code class="xref py py-meth docutils literal notranslate"><span class="pre">add()</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.addbmm.html#torch.Tensor.addbmm" title="torch.Tensor.addbmm"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.addbmm</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.addbmm.html#torch.addbmm" title="torch.addbmm"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.addbmm()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.addbmm_.html#torch.Tensor.addbmm_" title="torch.Tensor.addbmm_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.addbmm_</span></code></a></p></td>
<td><p>In-place version of <a class="reference internal" href="generated/torch.Tensor.addbmm.html#torch.Tensor.addbmm" title="torch.Tensor.addbmm"><code class="xref py py-meth docutils literal notranslate"><span class="pre">addbmm()</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.addcdiv.html#torch.Tensor.addcdiv" title="torch.Tensor.addcdiv"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.addcdiv</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.addcdiv.html#torch.addcdiv" title="torch.addcdiv"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.addcdiv()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.addcdiv_.html#torch.Tensor.addcdiv_" title="torch.Tensor.addcdiv_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.addcdiv_</span></code></a></p></td>
<td><p>In-place version of <a class="reference internal" href="generated/torch.Tensor.addcdiv.html#torch.Tensor.addcdiv" title="torch.Tensor.addcdiv"><code class="xref py py-meth docutils literal notranslate"><span class="pre">addcdiv()</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.addcmul.html#torch.Tensor.addcmul" title="torch.Tensor.addcmul"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.addcmul</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.addcmul.html#torch.addcmul" title="torch.addcmul"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.addcmul()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.addcmul_.html#torch.Tensor.addcmul_" title="torch.Tensor.addcmul_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.addcmul_</span></code></a></p></td>
<td><p>In-place version of <a class="reference internal" href="generated/torch.Tensor.addcmul.html#torch.Tensor.addcmul" title="torch.Tensor.addcmul"><code class="xref py py-meth docutils literal notranslate"><span class="pre">addcmul()</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.addmm.html#torch.Tensor.addmm" title="torch.Tensor.addmm"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.addmm</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.addmm.html#torch.addmm" title="torch.addmm"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.addmm()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.addmm_.html#torch.Tensor.addmm_" title="torch.Tensor.addmm_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.addmm_</span></code></a></p></td>
<td><p>In-place version of <a class="reference internal" href="generated/torch.Tensor.addmm.html#torch.Tensor.addmm" title="torch.Tensor.addmm"><code class="xref py py-meth docutils literal notranslate"><span class="pre">addmm()</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.sspaddmm.html#torch.Tensor.sspaddmm" title="torch.Tensor.sspaddmm"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.sspaddmm</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.sspaddmm.html#torch.sspaddmm" title="torch.sspaddmm"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.sspaddmm()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.addmv.html#torch.Tensor.addmv" title="torch.Tensor.addmv"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.addmv</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.addmv.html#torch.addmv" title="torch.addmv"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.addmv()</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.addmv_.html#torch.Tensor.addmv_" title="torch.Tensor.addmv_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.addmv_</span></code></a></p></td>
<td><p>In-place version of <a class="reference internal" href="generated/torch.Tensor.addmv.html#torch.Tensor.addmv" title="torch.Tensor.addmv"><code class="xref py py-meth docutils literal notranslate"><span class="pre">addmv()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.addr.html#torch.Tensor.addr" title="torch.Tensor.addr"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.addr</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.addr.html#torch.addr" title="torch.addr"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.addr()</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.addr_.html#torch.Tensor.addr_" title="torch.Tensor.addr_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.addr_</span></code></a></p></td>
<td><p>In-place version of <a class="reference internal" href="generated/torch.Tensor.addr.html#torch.Tensor.addr" title="torch.Tensor.addr"><code class="xref py py-meth docutils literal notranslate"><span class="pre">addr()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.adjoint.html#torch.Tensor.adjoint" title="torch.Tensor.adjoint"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.adjoint</span></code></a></p></td>
<td><p>Alias for <a class="reference internal" href="generated/torch.adjoint.html#torch.adjoint" title="torch.adjoint"><code class="xref py py-func docutils literal notranslate"><span class="pre">adjoint()</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.allclose.html#torch.Tensor.allclose" title="torch.Tensor.allclose"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.allclose</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.allclose.html#torch.allclose" title="torch.allclose"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.allclose()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.amax.html#torch.Tensor.amax" title="torch.Tensor.amax"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.amax</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.amax.html#torch.amax" title="torch.amax"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.amax()</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.amin.html#torch.Tensor.amin" title="torch.Tensor.amin"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.amin</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.amin.html#torch.amin" title="torch.amin"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.amin()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.aminmax.html#torch.Tensor.aminmax" title="torch.Tensor.aminmax"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.aminmax</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.aminmax.html#torch.aminmax" title="torch.aminmax"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.aminmax()</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.angle.html#torch.Tensor.angle" title="torch.Tensor.angle"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.angle</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.angle.html#torch.angle" title="torch.angle"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.angle()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.apply_.html#torch.Tensor.apply_" title="torch.Tensor.apply_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.apply_</span></code></a></p></td>
<td><p>Applies the function <code class="xref py py-attr docutils literal notranslate"><span class="pre">callable</span></code> to each element in the tensor, replacing each element with the value returned by <code class="xref py py-attr docutils literal notranslate"><span class="pre">callable</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.argmax.html#torch.Tensor.argmax" title="torch.Tensor.argmax"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.argmax</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.argmax.html#torch.argmax" title="torch.argmax"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.argmax()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.argmin.html#torch.Tensor.argmin" title="torch.Tensor.argmin"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.argmin</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.argmin.html#torch.argmin" title="torch.argmin"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.argmin()</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.argsort.html#torch.Tensor.argsort" title="torch.Tensor.argsort"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.argsort</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.argsort.html#torch.argsort" title="torch.argsort"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.argsort()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.argwhere.html#torch.Tensor.argwhere" title="torch.Tensor.argwhere"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.argwhere</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.argwhere.html#torch.argwhere" title="torch.argwhere"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.argwhere()</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.asin.html#torch.Tensor.asin" title="torch.Tensor.asin"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.asin</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.asin.html#torch.asin" title="torch.asin"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.asin()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.asin_.html#torch.Tensor.asin_" title="torch.Tensor.asin_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.asin_</span></code></a></p></td>
<td><p>In-place version of <a class="reference internal" href="generated/torch.Tensor.asin.html#torch.Tensor.asin" title="torch.Tensor.asin"><code class="xref py py-meth docutils literal notranslate"><span class="pre">asin()</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.arcsin.html#torch.Tensor.arcsin" title="torch.Tensor.arcsin"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.arcsin</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.arcsin.html#torch.arcsin" title="torch.arcsin"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.arcsin()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.arcsin_.html#torch.Tensor.arcsin_" title="torch.Tensor.arcsin_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.arcsin_</span></code></a></p></td>
<td><p>In-place version of <a class="reference internal" href="generated/torch.Tensor.arcsin.html#torch.Tensor.arcsin" title="torch.Tensor.arcsin"><code class="xref py py-meth docutils literal notranslate"><span class="pre">arcsin()</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.as_strided</span></code></p></td>
<td><p>See <a class="reference internal" href="generated/torch.as_strided.html#torch.as_strided" title="torch.as_strided"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.as_strided()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.atan.html#torch.Tensor.atan" title="torch.Tensor.atan"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.atan</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.atan.html#torch.atan" title="torch.atan"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.atan()</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.atan_.html#torch.Tensor.atan_" title="torch.Tensor.atan_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.atan_</span></code></a></p></td>
<td><p>In-place version of <a class="reference internal" href="generated/torch.Tensor.atan.html#torch.Tensor.atan" title="torch.Tensor.atan"><code class="xref py py-meth docutils literal notranslate"><span class="pre">atan()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.arctan.html#torch.Tensor.arctan" title="torch.Tensor.arctan"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.arctan</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.arctan.html#torch.arctan" title="torch.arctan"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.arctan()</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.arctan_.html#torch.Tensor.arctan_" title="torch.Tensor.arctan_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.arctan_</span></code></a></p></td>
<td><p>In-place version of <a class="reference internal" href="generated/torch.Tensor.arctan.html#torch.Tensor.arctan" title="torch.Tensor.arctan"><code class="xref py py-meth docutils literal notranslate"><span class="pre">arctan()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.atan2.html#torch.Tensor.atan2" title="torch.Tensor.atan2"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.atan2</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.atan2.html#torch.atan2" title="torch.atan2"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.atan2()</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.atan2_.html#torch.Tensor.atan2_" title="torch.Tensor.atan2_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.atan2_</span></code></a></p></td>
<td><p>In-place version of <a class="reference internal" href="generated/torch.Tensor.atan2.html#torch.Tensor.atan2" title="torch.Tensor.atan2"><code class="xref py py-meth docutils literal notranslate"><span class="pre">atan2()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.arctan2.html#torch.Tensor.arctan2" title="torch.Tensor.arctan2"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.arctan2</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.arctan2.html#torch.arctan2" title="torch.arctan2"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.arctan2()</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.arctan2_.html#torch.Tensor.arctan2_" title="torch.Tensor.arctan2_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.arctan2_</span></code></a></p></td>
<td><p>atan2_(other) -&gt; Tensor</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.all.html#torch.Tensor.all" title="torch.Tensor.all"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.all</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.all.html#torch.all" title="torch.all"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.all()</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.any.html#torch.Tensor.any" title="torch.Tensor.any"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.any</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.any.html#torch.any" title="torch.any"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.any()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.backward.html#torch.Tensor.backward" title="torch.Tensor.backward"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.backward</span></code></a></p></td>
<td><p>Computes the gradient of current tensor wrt graph leaves.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.baddbmm.html#torch.Tensor.baddbmm" title="torch.Tensor.baddbmm"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.baddbmm</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.baddbmm.html#torch.baddbmm" title="torch.baddbmm"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.baddbmm()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.baddbmm_.html#torch.Tensor.baddbmm_" title="torch.Tensor.baddbmm_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.baddbmm_</span></code></a></p></td>
<td><p>In-place version of <a class="reference internal" href="generated/torch.Tensor.baddbmm.html#torch.Tensor.baddbmm" title="torch.Tensor.baddbmm"><code class="xref py py-meth docutils literal notranslate"><span class="pre">baddbmm()</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.bernoulli.html#torch.Tensor.bernoulli" title="torch.Tensor.bernoulli"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.bernoulli</span></code></a></p></td>
<td><p>Returns a result tensor where each <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext mathvariant="monospace">result[i]</mtext></mrow><annotation encoding="application/x-tex">\texttt{result[i]}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7778em;vertical-align:-0.0833em;"></span><span class="mord text"><span class="mord texttt">result[i]</span></span></span></span></span></span> is independently sampled from <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>Bernoulli</mtext><mo stretchy="false">(</mo><mtext mathvariant="monospace">self[i]</mtext><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\text{Bernoulli}(\texttt{self[i]})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord text"><span class="mord">Bernoulli</span></span><span class="mopen">(</span><span class="mord text"><span class="mord texttt">self[i]</span></span><span class="mclose">)</span></span></span></span></span>.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.bernoulli_.html#torch.Tensor.bernoulli_" title="torch.Tensor.bernoulli_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.bernoulli_</span></code></a></p></td>
<td><p>Fills each location of <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> with an independent sample from <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>Bernoulli</mtext><mo stretchy="false">(</mo><mtext mathvariant="monospace">p</mtext><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\text{Bernoulli}(\texttt{p})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord text"><span class="mord">Bernoulli</span></span><span class="mopen">(</span><span class="mord text"><span class="mord texttt">p</span></span><span class="mclose">)</span></span></span></span></span>.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.bfloat16.html#torch.Tensor.bfloat16" title="torch.Tensor.bfloat16"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.bfloat16</span></code></a></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">self.bfloat16()</span></code> is equivalent to <code class="docutils literal notranslate"><span class="pre">self.to(torch.bfloat16)</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.bincount.html#torch.Tensor.bincount" title="torch.Tensor.bincount"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.bincount</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.bincount.html#torch.bincount" title="torch.bincount"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.bincount()</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.bitwise_not.html#torch.Tensor.bitwise_not" title="torch.Tensor.bitwise_not"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.bitwise_not</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.bitwise_not.html#torch.bitwise_not" title="torch.bitwise_not"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.bitwise_not()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.bitwise_not_.html#torch.Tensor.bitwise_not_" title="torch.Tensor.bitwise_not_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.bitwise_not_</span></code></a></p></td>
<td><p>In-place version of <a class="reference internal" href="generated/torch.Tensor.bitwise_not.html#torch.Tensor.bitwise_not" title="torch.Tensor.bitwise_not"><code class="xref py py-meth docutils literal notranslate"><span class="pre">bitwise_not()</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.bitwise_and.html#torch.Tensor.bitwise_and" title="torch.Tensor.bitwise_and"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.bitwise_and</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.bitwise_and.html#torch.bitwise_and" title="torch.bitwise_and"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.bitwise_and()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.bitwise_and_.html#torch.Tensor.bitwise_and_" title="torch.Tensor.bitwise_and_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.bitwise_and_</span></code></a></p></td>
<td><p>In-place version of <a class="reference internal" href="generated/torch.Tensor.bitwise_and.html#torch.Tensor.bitwise_and" title="torch.Tensor.bitwise_and"><code class="xref py py-meth docutils literal notranslate"><span class="pre">bitwise_and()</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.bitwise_or.html#torch.Tensor.bitwise_or" title="torch.Tensor.bitwise_or"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.bitwise_or</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.bitwise_or.html#torch.bitwise_or" title="torch.bitwise_or"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.bitwise_or()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.bitwise_or_.html#torch.Tensor.bitwise_or_" title="torch.Tensor.bitwise_or_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.bitwise_or_</span></code></a></p></td>
<td><p>In-place version of <a class="reference internal" href="generated/torch.Tensor.bitwise_or.html#torch.Tensor.bitwise_or" title="torch.Tensor.bitwise_or"><code class="xref py py-meth docutils literal notranslate"><span class="pre">bitwise_or()</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.bitwise_xor.html#torch.Tensor.bitwise_xor" title="torch.Tensor.bitwise_xor"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.bitwise_xor</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.bitwise_xor.html#torch.bitwise_xor" title="torch.bitwise_xor"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.bitwise_xor()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.bitwise_xor_.html#torch.Tensor.bitwise_xor_" title="torch.Tensor.bitwise_xor_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.bitwise_xor_</span></code></a></p></td>
<td><p>In-place version of <a class="reference internal" href="generated/torch.Tensor.bitwise_xor.html#torch.Tensor.bitwise_xor" title="torch.Tensor.bitwise_xor"><code class="xref py py-meth docutils literal notranslate"><span class="pre">bitwise_xor()</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.bitwise_left_shift.html#torch.Tensor.bitwise_left_shift" title="torch.Tensor.bitwise_left_shift"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.bitwise_left_shift</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.bitwise_left_shift.html#torch.bitwise_left_shift" title="torch.bitwise_left_shift"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.bitwise_left_shift()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.bitwise_left_shift_.html#torch.Tensor.bitwise_left_shift_" title="torch.Tensor.bitwise_left_shift_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.bitwise_left_shift_</span></code></a></p></td>
<td><p>In-place version of <a class="reference internal" href="generated/torch.Tensor.bitwise_left_shift.html#torch.Tensor.bitwise_left_shift" title="torch.Tensor.bitwise_left_shift"><code class="xref py py-meth docutils literal notranslate"><span class="pre">bitwise_left_shift()</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.bitwise_right_shift.html#torch.Tensor.bitwise_right_shift" title="torch.Tensor.bitwise_right_shift"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.bitwise_right_shift</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.bitwise_right_shift.html#torch.bitwise_right_shift" title="torch.bitwise_right_shift"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.bitwise_right_shift()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.bitwise_right_shift_.html#torch.Tensor.bitwise_right_shift_" title="torch.Tensor.bitwise_right_shift_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.bitwise_right_shift_</span></code></a></p></td>
<td><p>In-place version of <a class="reference internal" href="generated/torch.Tensor.bitwise_right_shift.html#torch.Tensor.bitwise_right_shift" title="torch.Tensor.bitwise_right_shift"><code class="xref py py-meth docutils literal notranslate"><span class="pre">bitwise_right_shift()</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.bmm.html#torch.Tensor.bmm" title="torch.Tensor.bmm"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.bmm</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.bmm.html#torch.bmm" title="torch.bmm"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.bmm()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.bool.html#torch.Tensor.bool" title="torch.Tensor.bool"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.bool</span></code></a></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">self.bool()</span></code> is equivalent to <code class="docutils literal notranslate"><span class="pre">self.to(torch.bool)</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.byte.html#torch.Tensor.byte" title="torch.Tensor.byte"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.byte</span></code></a></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">self.byte()</span></code> is equivalent to <code class="docutils literal notranslate"><span class="pre">self.to(torch.uint8)</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.broadcast_to.html#torch.Tensor.broadcast_to" title="torch.Tensor.broadcast_to"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.broadcast_to</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.broadcast_to.html#torch.broadcast_to" title="torch.broadcast_to"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.broadcast_to()</span></code></a>.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.cauchy_.html#torch.Tensor.cauchy_" title="torch.Tensor.cauchy_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.cauchy_</span></code></a></p></td>
<td><p>Fills the tensor with numbers drawn from the Cauchy distribution:</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.ceil.html#torch.Tensor.ceil" title="torch.Tensor.ceil"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.ceil</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.ceil.html#torch.ceil" title="torch.ceil"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.ceil()</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.ceil_.html#torch.Tensor.ceil_" title="torch.Tensor.ceil_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.ceil_</span></code></a></p></td>
<td><p>In-place version of <a class="reference internal" href="generated/torch.Tensor.ceil.html#torch.Tensor.ceil" title="torch.Tensor.ceil"><code class="xref py py-meth docutils literal notranslate"><span class="pre">ceil()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.char.html#torch.Tensor.char" title="torch.Tensor.char"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.char</span></code></a></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">self.char()</span></code> is equivalent to <code class="docutils literal notranslate"><span class="pre">self.to(torch.int8)</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.cholesky.html#torch.Tensor.cholesky" title="torch.Tensor.cholesky"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.cholesky</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.cholesky.html#torch.cholesky" title="torch.cholesky"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.cholesky()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.cholesky_inverse.html#torch.Tensor.cholesky_inverse" title="torch.Tensor.cholesky_inverse"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.cholesky_inverse</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.cholesky_inverse.html#torch.cholesky_inverse" title="torch.cholesky_inverse"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.cholesky_inverse()</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.cholesky_solve.html#torch.Tensor.cholesky_solve" title="torch.Tensor.cholesky_solve"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.cholesky_solve</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.cholesky_solve.html#torch.cholesky_solve" title="torch.cholesky_solve"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.cholesky_solve()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.chunk.html#torch.Tensor.chunk" title="torch.Tensor.chunk"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.chunk</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.chunk.html#torch.chunk" title="torch.chunk"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.chunk()</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.clamp.html#torch.Tensor.clamp" title="torch.Tensor.clamp"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.clamp</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.clamp.html#torch.clamp" title="torch.clamp"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.clamp()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.clamp_.html#torch.Tensor.clamp_" title="torch.Tensor.clamp_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.clamp_</span></code></a></p></td>
<td><p>In-place version of <a class="reference internal" href="generated/torch.Tensor.clamp.html#torch.Tensor.clamp" title="torch.Tensor.clamp"><code class="xref py py-meth docutils literal notranslate"><span class="pre">clamp()</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.clip.html#torch.Tensor.clip" title="torch.Tensor.clip"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.clip</span></code></a></p></td>
<td><p>Alias for <a class="reference internal" href="generated/torch.Tensor.clamp.html#torch.Tensor.clamp" title="torch.Tensor.clamp"><code class="xref py py-meth docutils literal notranslate"><span class="pre">clamp()</span></code></a>.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.clip_.html#torch.Tensor.clip_" title="torch.Tensor.clip_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.clip_</span></code></a></p></td>
<td><p>Alias for <a class="reference internal" href="generated/torch.Tensor.clamp_.html#torch.Tensor.clamp_" title="torch.Tensor.clamp_"><code class="xref py py-meth docutils literal notranslate"><span class="pre">clamp_()</span></code></a>.</p></td>
</tr>
<tr class="row-even"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.clone</span></code></p></td>
<td><p>See <a class="reference internal" href="generated/torch.clone.html#torch.clone" title="torch.clone"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.clone()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.contiguous.html#torch.Tensor.contiguous" title="torch.Tensor.contiguous"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.contiguous</span></code></a></p></td>
<td><p>Returns a contiguous in memory tensor containing the same data as <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor.</p></td>
</tr>
<tr class="row-even"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.copy_</span></code></p></td>
<td><p>Copies the elements from <code class="xref py py-attr docutils literal notranslate"><span class="pre">src</span></code> into <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor and returns <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.conj.html#torch.Tensor.conj" title="torch.Tensor.conj"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.conj</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.conj.html#torch.conj" title="torch.conj"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.conj()</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.conj_physical.html#torch.Tensor.conj_physical" title="torch.Tensor.conj_physical"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.conj_physical</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.conj_physical.html#torch.conj_physical" title="torch.conj_physical"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.conj_physical()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.conj_physical_.html#torch.Tensor.conj_physical_" title="torch.Tensor.conj_physical_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.conj_physical_</span></code></a></p></td>
<td><p>In-place version of <a class="reference internal" href="generated/torch.Tensor.conj_physical.html#torch.Tensor.conj_physical" title="torch.Tensor.conj_physical"><code class="xref py py-meth docutils literal notranslate"><span class="pre">conj_physical()</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.resolve_conj.html#torch.Tensor.resolve_conj" title="torch.Tensor.resolve_conj"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.resolve_conj</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.resolve_conj.html#torch.resolve_conj" title="torch.resolve_conj"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.resolve_conj()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.resolve_neg.html#torch.Tensor.resolve_neg" title="torch.Tensor.resolve_neg"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.resolve_neg</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.resolve_neg.html#torch.resolve_neg" title="torch.resolve_neg"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.resolve_neg()</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.copysign.html#torch.Tensor.copysign" title="torch.Tensor.copysign"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.copysign</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.copysign.html#torch.copysign" title="torch.copysign"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.copysign()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.copysign_.html#torch.Tensor.copysign_" title="torch.Tensor.copysign_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.copysign_</span></code></a></p></td>
<td><p>In-place version of <a class="reference internal" href="generated/torch.Tensor.copysign.html#torch.Tensor.copysign" title="torch.Tensor.copysign"><code class="xref py py-meth docutils literal notranslate"><span class="pre">copysign()</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.cos.html#torch.Tensor.cos" title="torch.Tensor.cos"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.cos</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.cos.html#torch.cos" title="torch.cos"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.cos()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.cos_.html#torch.Tensor.cos_" title="torch.Tensor.cos_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.cos_</span></code></a></p></td>
<td><p>In-place version of <a class="reference internal" href="generated/torch.Tensor.cos.html#torch.Tensor.cos" title="torch.Tensor.cos"><code class="xref py py-meth docutils literal notranslate"><span class="pre">cos()</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.cosh.html#torch.Tensor.cosh" title="torch.Tensor.cosh"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.cosh</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.cosh.html#torch.cosh" title="torch.cosh"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.cosh()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.cosh_.html#torch.Tensor.cosh_" title="torch.Tensor.cosh_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.cosh_</span></code></a></p></td>
<td><p>In-place version of <a class="reference internal" href="generated/torch.Tensor.cosh.html#torch.Tensor.cosh" title="torch.Tensor.cosh"><code class="xref py py-meth docutils literal notranslate"><span class="pre">cosh()</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.corrcoef.html#torch.Tensor.corrcoef" title="torch.Tensor.corrcoef"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.corrcoef</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.corrcoef.html#torch.corrcoef" title="torch.corrcoef"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.corrcoef()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.count_nonzero.html#torch.Tensor.count_nonzero" title="torch.Tensor.count_nonzero"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.count_nonzero</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.count_nonzero.html#torch.count_nonzero" title="torch.count_nonzero"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.count_nonzero()</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.cov.html#torch.Tensor.cov" title="torch.Tensor.cov"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.cov</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.cov.html#torch.cov" title="torch.cov"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.cov()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.acosh.html#torch.Tensor.acosh" title="torch.Tensor.acosh"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.acosh</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.acosh.html#torch.acosh" title="torch.acosh"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.acosh()</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.acosh_.html#torch.Tensor.acosh_" title="torch.Tensor.acosh_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.acosh_</span></code></a></p></td>
<td><p>In-place version of <a class="reference internal" href="generated/torch.Tensor.acosh.html#torch.Tensor.acosh" title="torch.Tensor.acosh"><code class="xref py py-meth docutils literal notranslate"><span class="pre">acosh()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.arccosh.html#torch.Tensor.arccosh" title="torch.Tensor.arccosh"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.arccosh</span></code></a></p></td>
<td><p>acosh() -&gt; Tensor</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.arccosh_.html#torch.Tensor.arccosh_" title="torch.Tensor.arccosh_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.arccosh_</span></code></a></p></td>
<td><p>acosh_() -&gt; Tensor</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.cpu.html#torch.Tensor.cpu" title="torch.Tensor.cpu"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.cpu</span></code></a></p></td>
<td><p>Returns a copy of this object in CPU memory.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.cross.html#torch.Tensor.cross" title="torch.Tensor.cross"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.cross</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.cross.html#torch.cross" title="torch.cross"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.cross()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.cuda.html#torch.Tensor.cuda" title="torch.Tensor.cuda"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.cuda</span></code></a></p></td>
<td><p>Returns a copy of this object in CUDA memory.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.logcumsumexp.html#torch.Tensor.logcumsumexp" title="torch.Tensor.logcumsumexp"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.logcumsumexp</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.logcumsumexp.html#torch.logcumsumexp" title="torch.logcumsumexp"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.logcumsumexp()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.cummax.html#torch.Tensor.cummax" title="torch.Tensor.cummax"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.cummax</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.cummax.html#torch.cummax" title="torch.cummax"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.cummax()</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.cummin.html#torch.Tensor.cummin" title="torch.Tensor.cummin"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.cummin</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.cummin.html#torch.cummin" title="torch.cummin"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.cummin()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.cumprod.html#torch.Tensor.cumprod" title="torch.Tensor.cumprod"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.cumprod</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.cumprod.html#torch.cumprod" title="torch.cumprod"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.cumprod()</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.cumprod_.html#torch.Tensor.cumprod_" title="torch.Tensor.cumprod_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.cumprod_</span></code></a></p></td>
<td><p>In-place version of <a class="reference internal" href="generated/torch.Tensor.cumprod.html#torch.Tensor.cumprod" title="torch.Tensor.cumprod"><code class="xref py py-meth docutils literal notranslate"><span class="pre">cumprod()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.cumsum.html#torch.Tensor.cumsum" title="torch.Tensor.cumsum"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.cumsum</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.cumsum.html#torch.cumsum" title="torch.cumsum"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.cumsum()</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.cumsum_.html#torch.Tensor.cumsum_" title="torch.Tensor.cumsum_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.cumsum_</span></code></a></p></td>
<td><p>In-place version of <a class="reference internal" href="generated/torch.Tensor.cumsum.html#torch.Tensor.cumsum" title="torch.Tensor.cumsum"><code class="xref py py-meth docutils literal notranslate"><span class="pre">cumsum()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.chalf.html#torch.Tensor.chalf" title="torch.Tensor.chalf"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.chalf</span></code></a></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">self.chalf()</span></code> is equivalent to <code class="docutils literal notranslate"><span class="pre">self.to(torch.complex32)</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.cfloat.html#torch.Tensor.cfloat" title="torch.Tensor.cfloat"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.cfloat</span></code></a></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">self.cfloat()</span></code> is equivalent to <code class="docutils literal notranslate"><span class="pre">self.to(torch.complex64)</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.cdouble.html#torch.Tensor.cdouble" title="torch.Tensor.cdouble"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.cdouble</span></code></a></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">self.cdouble()</span></code> is equivalent to <code class="docutils literal notranslate"><span class="pre">self.to(torch.complex128)</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.data_ptr.html#torch.Tensor.data_ptr" title="torch.Tensor.data_ptr"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.data_ptr</span></code></a></p></td>
<td><p>Returns the address of the first element of <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.deg2rad.html#torch.Tensor.deg2rad" title="torch.Tensor.deg2rad"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.deg2rad</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.deg2rad.html#torch.deg2rad" title="torch.deg2rad"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.deg2rad()</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.dequantize</span></code></p></td>
<td><p>Given a quantized Tensor, dequantize it and return the dequantized float Tensor.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.det.html#torch.Tensor.det" title="torch.Tensor.det"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.det</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.det.html#torch.det" title="torch.det"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.det()</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.dense_dim.html#torch.Tensor.dense_dim" title="torch.Tensor.dense_dim"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.dense_dim</span></code></a></p></td>
<td><p>Return the number of dense dimensions in a <a class="reference internal" href="sparse.html#sparse-docs"><span class="std std-ref">sparse tensor</span></a> <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.detach.html#torch.Tensor.detach" title="torch.Tensor.detach"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.detach</span></code></a></p></td>
<td><p>Returns a new Tensor, detached from the current graph.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.detach_.html#torch.Tensor.detach_" title="torch.Tensor.detach_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.detach_</span></code></a></p></td>
<td><p>Detaches the Tensor from the graph that created it, making it a leaf.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.diag.html#torch.Tensor.diag" title="torch.Tensor.diag"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.diag</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.diag.html#torch.diag" title="torch.diag"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.diag()</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.diag_embed.html#torch.Tensor.diag_embed" title="torch.Tensor.diag_embed"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.diag_embed</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.diag_embed.html#torch.diag_embed" title="torch.diag_embed"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.diag_embed()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.diagflat.html#torch.Tensor.diagflat" title="torch.Tensor.diagflat"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.diagflat</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.diagflat.html#torch.diagflat" title="torch.diagflat"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.diagflat()</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.diagonal.html#torch.Tensor.diagonal" title="torch.Tensor.diagonal"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.diagonal</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.diagonal.html#torch.diagonal" title="torch.diagonal"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.diagonal()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.diagonal_scatter.html#torch.Tensor.diagonal_scatter" title="torch.Tensor.diagonal_scatter"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.diagonal_scatter</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.diagonal_scatter.html#torch.diagonal_scatter" title="torch.diagonal_scatter"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.diagonal_scatter()</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.fill_diagonal_.html#torch.Tensor.fill_diagonal_" title="torch.Tensor.fill_diagonal_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.fill_diagonal_</span></code></a></p></td>
<td><p>Fill the main diagonal of a tensor that has at least 2-dimensions.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.fmax.html#torch.Tensor.fmax" title="torch.Tensor.fmax"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.fmax</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.fmax.html#torch.fmax" title="torch.fmax"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.fmax()</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.fmin.html#torch.Tensor.fmin" title="torch.Tensor.fmin"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.fmin</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.fmin.html#torch.fmin" title="torch.fmin"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.fmin()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.diff.html#torch.Tensor.diff" title="torch.Tensor.diff"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.diff</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.diff.html#torch.diff" title="torch.diff"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.diff()</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.digamma.html#torch.Tensor.digamma" title="torch.Tensor.digamma"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.digamma</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.digamma.html#torch.digamma" title="torch.digamma"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.digamma()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.digamma_.html#torch.Tensor.digamma_" title="torch.Tensor.digamma_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.digamma_</span></code></a></p></td>
<td><p>In-place version of <a class="reference internal" href="generated/torch.Tensor.digamma.html#torch.Tensor.digamma" title="torch.Tensor.digamma"><code class="xref py py-meth docutils literal notranslate"><span class="pre">digamma()</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.dim.html#torch.Tensor.dim" title="torch.Tensor.dim"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.dim</span></code></a></p></td>
<td><p>Returns the number of dimensions of <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.dim_order.html#torch.Tensor.dim_order" title="torch.Tensor.dim_order"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.dim_order</span></code></a></p></td>
<td><p>Returns the uniquely determined tuple of int describing the dim order or physical layout of <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.dist.html#torch.Tensor.dist" title="torch.Tensor.dist"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.dist</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.dist.html#torch.dist" title="torch.dist"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.dist()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.div.html#torch.Tensor.div" title="torch.Tensor.div"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.div</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.div.html#torch.div" title="torch.div"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.div()</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.div_.html#torch.Tensor.div_" title="torch.Tensor.div_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.div_</span></code></a></p></td>
<td><p>In-place version of <a class="reference internal" href="generated/torch.Tensor.div.html#torch.Tensor.div" title="torch.Tensor.div"><code class="xref py py-meth docutils literal notranslate"><span class="pre">div()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.divide.html#torch.Tensor.divide" title="torch.Tensor.divide"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.divide</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.divide.html#torch.divide" title="torch.divide"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.divide()</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.divide_.html#torch.Tensor.divide_" title="torch.Tensor.divide_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.divide_</span></code></a></p></td>
<td><p>In-place version of <a class="reference internal" href="generated/torch.Tensor.divide.html#torch.Tensor.divide" title="torch.Tensor.divide"><code class="xref py py-meth docutils literal notranslate"><span class="pre">divide()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.dot.html#torch.Tensor.dot" title="torch.Tensor.dot"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.dot</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.dot.html#torch.dot" title="torch.dot"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.dot()</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.double.html#torch.Tensor.double" title="torch.Tensor.double"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.double</span></code></a></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">self.double()</span></code> is equivalent to <code class="docutils literal notranslate"><span class="pre">self.to(torch.float64)</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.dsplit.html#torch.Tensor.dsplit" title="torch.Tensor.dsplit"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.dsplit</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.dsplit.html#torch.dsplit" title="torch.dsplit"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.dsplit()</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.element_size.html#torch.Tensor.element_size" title="torch.Tensor.element_size"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.element_size</span></code></a></p></td>
<td><p>Returns the size in bytes of an individual element.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.eq</span></code></p></td>
<td><p>See <a class="reference internal" href="generated/torch.eq.html#torch.eq" title="torch.eq"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.eq()</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.eq_.html#torch.Tensor.eq_" title="torch.Tensor.eq_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.eq_</span></code></a></p></td>
<td><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">eq()</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.equal</span></code></p></td>
<td><p>See <a class="reference internal" href="generated/torch.equal.html#torch.equal" title="torch.equal"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.equal()</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.erf.html#torch.Tensor.erf" title="torch.Tensor.erf"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.erf</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.erf.html#torch.erf" title="torch.erf"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.erf()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.erf_.html#torch.Tensor.erf_" title="torch.Tensor.erf_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.erf_</span></code></a></p></td>
<td><p>In-place version of <a class="reference internal" href="generated/torch.Tensor.erf.html#torch.Tensor.erf" title="torch.Tensor.erf"><code class="xref py py-meth docutils literal notranslate"><span class="pre">erf()</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.erfc.html#torch.Tensor.erfc" title="torch.Tensor.erfc"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.erfc</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.erfc.html#torch.erfc" title="torch.erfc"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.erfc()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.erfc_.html#torch.Tensor.erfc_" title="torch.Tensor.erfc_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.erfc_</span></code></a></p></td>
<td><p>In-place version of <a class="reference internal" href="generated/torch.Tensor.erfc.html#torch.Tensor.erfc" title="torch.Tensor.erfc"><code class="xref py py-meth docutils literal notranslate"><span class="pre">erfc()</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.erfinv.html#torch.Tensor.erfinv" title="torch.Tensor.erfinv"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.erfinv</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.erfinv.html#torch.erfinv" title="torch.erfinv"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.erfinv()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.erfinv_.html#torch.Tensor.erfinv_" title="torch.Tensor.erfinv_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.erfinv_</span></code></a></p></td>
<td><p>In-place version of <a class="reference internal" href="generated/torch.Tensor.erfinv.html#torch.Tensor.erfinv" title="torch.Tensor.erfinv"><code class="xref py py-meth docutils literal notranslate"><span class="pre">erfinv()</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.exp.html#torch.Tensor.exp" title="torch.Tensor.exp"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.exp</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.exp.html#torch.exp" title="torch.exp"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.exp()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.exp_.html#torch.Tensor.exp_" title="torch.Tensor.exp_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.exp_</span></code></a></p></td>
<td><p>In-place version of <a class="reference internal" href="generated/torch.Tensor.exp.html#torch.Tensor.exp" title="torch.Tensor.exp"><code class="xref py py-meth docutils literal notranslate"><span class="pre">exp()</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.expm1.html#torch.Tensor.expm1" title="torch.Tensor.expm1"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.expm1</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.expm1.html#torch.expm1" title="torch.expm1"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.expm1()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.expm1_.html#torch.Tensor.expm1_" title="torch.Tensor.expm1_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.expm1_</span></code></a></p></td>
<td><p>In-place version of <a class="reference internal" href="generated/torch.Tensor.expm1.html#torch.Tensor.expm1" title="torch.Tensor.expm1"><code class="xref py py-meth docutils literal notranslate"><span class="pre">expm1()</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.expand</span></code></p></td>
<td><p>Returns a new view of the <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor with singleton dimensions expanded to a larger size.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.expand_as.html#torch.Tensor.expand_as" title="torch.Tensor.expand_as"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.expand_as</span></code></a></p></td>
<td><p>Expand this tensor to the same size as <code class="xref py py-attr docutils literal notranslate"><span class="pre">other</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.exponential_.html#torch.Tensor.exponential_" title="torch.Tensor.exponential_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.exponential_</span></code></a></p></td>
<td><p>Fills <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor with elements drawn from the PDF (probability density function):</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.fix.html#torch.Tensor.fix" title="torch.Tensor.fix"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.fix</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.fix.html#torch.fix" title="torch.fix"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.fix()</span></code></a>.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.fix_.html#torch.Tensor.fix_" title="torch.Tensor.fix_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.fix_</span></code></a></p></td>
<td><p>In-place version of <a class="reference internal" href="generated/torch.Tensor.fix.html#torch.Tensor.fix" title="torch.Tensor.fix"><code class="xref py py-meth docutils literal notranslate"><span class="pre">fix()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.fill_.html#torch.Tensor.fill_" title="torch.Tensor.fill_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.fill_</span></code></a></p></td>
<td><p>Fills <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor with the specified value.</p></td>
</tr>
<tr class="row-even"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.flatten</span></code></p></td>
<td><p>See <a class="reference internal" href="generated/torch.flatten.html#torch.flatten" title="torch.flatten"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.flatten()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.flip.html#torch.Tensor.flip" title="torch.Tensor.flip"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.flip</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.flip.html#torch.flip" title="torch.flip"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.flip()</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.fliplr.html#torch.Tensor.fliplr" title="torch.Tensor.fliplr"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.fliplr</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.fliplr.html#torch.fliplr" title="torch.fliplr"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.fliplr()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.flipud.html#torch.Tensor.flipud" title="torch.Tensor.flipud"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.flipud</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.flipud.html#torch.flipud" title="torch.flipud"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.flipud()</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.float.html#torch.Tensor.float" title="torch.Tensor.float"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.float</span></code></a></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">self.float()</span></code> is equivalent to <code class="docutils literal notranslate"><span class="pre">self.to(torch.float32)</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.float_power.html#torch.Tensor.float_power" title="torch.Tensor.float_power"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.float_power</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.float_power.html#torch.float_power" title="torch.float_power"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.float_power()</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.float_power_.html#torch.Tensor.float_power_" title="torch.Tensor.float_power_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.float_power_</span></code></a></p></td>
<td><p>In-place version of <a class="reference internal" href="generated/torch.Tensor.float_power.html#torch.Tensor.float_power" title="torch.Tensor.float_power"><code class="xref py py-meth docutils literal notranslate"><span class="pre">float_power()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.floor.html#torch.Tensor.floor" title="torch.Tensor.floor"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.floor</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.floor.html#torch.floor" title="torch.floor"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.floor()</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.floor_.html#torch.Tensor.floor_" title="torch.Tensor.floor_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.floor_</span></code></a></p></td>
<td><p>In-place version of <a class="reference internal" href="generated/torch.Tensor.floor.html#torch.Tensor.floor" title="torch.Tensor.floor"><code class="xref py py-meth docutils literal notranslate"><span class="pre">floor()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.floor_divide.html#torch.Tensor.floor_divide" title="torch.Tensor.floor_divide"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.floor_divide</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.floor_divide.html#torch.floor_divide" title="torch.floor_divide"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.floor_divide()</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.floor_divide_.html#torch.Tensor.floor_divide_" title="torch.Tensor.floor_divide_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.floor_divide_</span></code></a></p></td>
<td><p>In-place version of <a class="reference internal" href="generated/torch.Tensor.floor_divide.html#torch.Tensor.floor_divide" title="torch.Tensor.floor_divide"><code class="xref py py-meth docutils literal notranslate"><span class="pre">floor_divide()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.fmod.html#torch.Tensor.fmod" title="torch.Tensor.fmod"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.fmod</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.fmod.html#torch.fmod" title="torch.fmod"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.fmod()</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.fmod_.html#torch.Tensor.fmod_" title="torch.Tensor.fmod_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.fmod_</span></code></a></p></td>
<td><p>In-place version of <a class="reference internal" href="generated/torch.Tensor.fmod.html#torch.Tensor.fmod" title="torch.Tensor.fmod"><code class="xref py py-meth docutils literal notranslate"><span class="pre">fmod()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.frac.html#torch.Tensor.frac" title="torch.Tensor.frac"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.frac</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.frac.html#torch.frac" title="torch.frac"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.frac()</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.frac_.html#torch.Tensor.frac_" title="torch.Tensor.frac_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.frac_</span></code></a></p></td>
<td><p>In-place version of <a class="reference internal" href="generated/torch.Tensor.frac.html#torch.Tensor.frac" title="torch.Tensor.frac"><code class="xref py py-meth docutils literal notranslate"><span class="pre">frac()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.frexp.html#torch.Tensor.frexp" title="torch.Tensor.frexp"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.frexp</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.frexp.html#torch.frexp" title="torch.frexp"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.frexp()</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.gather.html#torch.Tensor.gather" title="torch.Tensor.gather"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.gather</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.gather.html#torch.gather" title="torch.gather"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.gather()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.gcd.html#torch.Tensor.gcd" title="torch.Tensor.gcd"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.gcd</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.gcd.html#torch.gcd" title="torch.gcd"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.gcd()</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.gcd_.html#torch.Tensor.gcd_" title="torch.Tensor.gcd_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.gcd_</span></code></a></p></td>
<td><p>In-place version of <a class="reference internal" href="generated/torch.Tensor.gcd.html#torch.Tensor.gcd" title="torch.Tensor.gcd"><code class="xref py py-meth docutils literal notranslate"><span class="pre">gcd()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.ge</span></code></p></td>
<td><p>See <a class="reference internal" href="generated/torch.ge.html#torch.ge" title="torch.ge"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.ge()</span></code></a>.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.ge_.html#torch.Tensor.ge_" title="torch.Tensor.ge_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.ge_</span></code></a></p></td>
<td><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">ge()</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.greater_equal.html#torch.Tensor.greater_equal" title="torch.Tensor.greater_equal"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.greater_equal</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.greater_equal.html#torch.greater_equal" title="torch.greater_equal"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.greater_equal()</span></code></a>.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.greater_equal_.html#torch.Tensor.greater_equal_" title="torch.Tensor.greater_equal_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.greater_equal_</span></code></a></p></td>
<td><p>In-place version of <a class="reference internal" href="generated/torch.Tensor.greater_equal.html#torch.Tensor.greater_equal" title="torch.Tensor.greater_equal"><code class="xref py py-meth docutils literal notranslate"><span class="pre">greater_equal()</span></code></a>.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.geometric_.html#torch.Tensor.geometric_" title="torch.Tensor.geometric_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.geometric_</span></code></a></p></td>
<td><p>Fills <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor with elements drawn from the geometric distribution:</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.geqrf.html#torch.Tensor.geqrf" title="torch.Tensor.geqrf"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.geqrf</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.geqrf.html#torch.geqrf" title="torch.geqrf"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.geqrf()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.ger.html#torch.Tensor.ger" title="torch.Tensor.ger"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.ger</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.ger.html#torch.ger" title="torch.ger"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.ger()</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.get_device.html#torch.Tensor.get_device" title="torch.Tensor.get_device"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.get_device</span></code></a></p></td>
<td><p>For CUDA tensors, this function returns the device ordinal of the GPU on which the tensor resides.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.gt</span></code></p></td>
<td><p>See <a class="reference internal" href="generated/torch.gt.html#torch.gt" title="torch.gt"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.gt()</span></code></a>.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.gt_.html#torch.Tensor.gt_" title="torch.Tensor.gt_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.gt_</span></code></a></p></td>
<td><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">gt()</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.greater.html#torch.Tensor.greater" title="torch.Tensor.greater"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.greater</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.greater.html#torch.greater" title="torch.greater"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.greater()</span></code></a>.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.greater_.html#torch.Tensor.greater_" title="torch.Tensor.greater_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.greater_</span></code></a></p></td>
<td><p>In-place version of <a class="reference internal" href="generated/torch.Tensor.greater.html#torch.Tensor.greater" title="torch.Tensor.greater"><code class="xref py py-meth docutils literal notranslate"><span class="pre">greater()</span></code></a>.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.half.html#torch.Tensor.half" title="torch.Tensor.half"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.half</span></code></a></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">self.half()</span></code> is equivalent to <code class="docutils literal notranslate"><span class="pre">self.to(torch.float16)</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.hardshrink.html#torch.Tensor.hardshrink" title="torch.Tensor.hardshrink"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.hardshrink</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.nn.functional.hardshrink.html#torch.nn.functional.hardshrink" title="torch.nn.functional.hardshrink"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.nn.functional.hardshrink()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.heaviside.html#torch.Tensor.heaviside" title="torch.Tensor.heaviside"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.heaviside</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.heaviside.html#torch.heaviside" title="torch.heaviside"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.heaviside()</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.histc.html#torch.Tensor.histc" title="torch.Tensor.histc"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.histc</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.histc.html#torch.histc" title="torch.histc"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.histc()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.histogram.html#torch.Tensor.histogram" title="torch.Tensor.histogram"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.histogram</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.histogram.html#torch.histogram" title="torch.histogram"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.histogram()</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.hsplit.html#torch.Tensor.hsplit" title="torch.Tensor.hsplit"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.hsplit</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.hsplit.html#torch.hsplit" title="torch.hsplit"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.hsplit()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.hypot.html#torch.Tensor.hypot" title="torch.Tensor.hypot"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.hypot</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.hypot.html#torch.hypot" title="torch.hypot"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.hypot()</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.hypot_.html#torch.Tensor.hypot_" title="torch.Tensor.hypot_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.hypot_</span></code></a></p></td>
<td><p>In-place version of <a class="reference internal" href="generated/torch.Tensor.hypot.html#torch.Tensor.hypot" title="torch.Tensor.hypot"><code class="xref py py-meth docutils literal notranslate"><span class="pre">hypot()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.i0.html#torch.Tensor.i0" title="torch.Tensor.i0"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.i0</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.i0.html#torch.i0" title="torch.i0"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.i0()</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.i0_.html#torch.Tensor.i0_" title="torch.Tensor.i0_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.i0_</span></code></a></p></td>
<td><p>In-place version of <a class="reference internal" href="generated/torch.Tensor.i0.html#torch.Tensor.i0" title="torch.Tensor.i0"><code class="xref py py-meth docutils literal notranslate"><span class="pre">i0()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.igamma.html#torch.Tensor.igamma" title="torch.Tensor.igamma"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.igamma</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.igamma.html#torch.igamma" title="torch.igamma"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.igamma()</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.igamma_.html#torch.Tensor.igamma_" title="torch.Tensor.igamma_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.igamma_</span></code></a></p></td>
<td><p>In-place version of <a class="reference internal" href="generated/torch.Tensor.igamma.html#torch.Tensor.igamma" title="torch.Tensor.igamma"><code class="xref py py-meth docutils literal notranslate"><span class="pre">igamma()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.igammac.html#torch.Tensor.igammac" title="torch.Tensor.igammac"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.igammac</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.igammac.html#torch.igammac" title="torch.igammac"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.igammac()</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.igammac_.html#torch.Tensor.igammac_" title="torch.Tensor.igammac_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.igammac_</span></code></a></p></td>
<td><p>In-place version of <a class="reference internal" href="generated/torch.Tensor.igammac.html#torch.Tensor.igammac" title="torch.Tensor.igammac"><code class="xref py py-meth docutils literal notranslate"><span class="pre">igammac()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.index_add_.html#torch.Tensor.index_add_" title="torch.Tensor.index_add_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.index_add_</span></code></a></p></td>
<td><p>Accumulate the elements of <code class="xref py py-attr docutils literal notranslate"><span class="pre">alpha</span></code> times <code class="docutils literal notranslate"><span class="pre">source</span></code> into the <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor by adding to the indices in the order given in <code class="xref py py-attr docutils literal notranslate"><span class="pre">index</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.index_add.html#torch.Tensor.index_add" title="torch.Tensor.index_add"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.index_add</span></code></a></p></td>
<td><p>Out-of-place version of <a class="reference internal" href="generated/torch.Tensor.index_add_.html#torch.Tensor.index_add_" title="torch.Tensor.index_add_"><code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.Tensor.index_add_()</span></code></a>.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.index_copy_.html#torch.Tensor.index_copy_" title="torch.Tensor.index_copy_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.index_copy_</span></code></a></p></td>
<td><p>Copies the elements of <a class="reference internal" href="generated/torch.tensor.html#torch.tensor" title="torch.tensor"><code class="xref py py-attr docutils literal notranslate"><span class="pre">tensor</span></code></a> into the <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor by selecting the indices in the order given in <code class="xref py py-attr docutils literal notranslate"><span class="pre">index</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.index_copy.html#torch.Tensor.index_copy" title="torch.Tensor.index_copy"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.index_copy</span></code></a></p></td>
<td><p>Out-of-place version of <a class="reference internal" href="generated/torch.Tensor.index_copy_.html#torch.Tensor.index_copy_" title="torch.Tensor.index_copy_"><code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.Tensor.index_copy_()</span></code></a>.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.index_fill_.html#torch.Tensor.index_fill_" title="torch.Tensor.index_fill_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.index_fill_</span></code></a></p></td>
<td><p>Fills the elements of the <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor with value <code class="xref py py-attr docutils literal notranslate"><span class="pre">value</span></code> by selecting the indices in the order given in <code class="xref py py-attr docutils literal notranslate"><span class="pre">index</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.index_fill.html#torch.Tensor.index_fill" title="torch.Tensor.index_fill"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.index_fill</span></code></a></p></td>
<td><p>Out-of-place version of <a class="reference internal" href="generated/torch.Tensor.index_fill_.html#torch.Tensor.index_fill_" title="torch.Tensor.index_fill_"><code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.Tensor.index_fill_()</span></code></a>.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.index_put_.html#torch.Tensor.index_put_" title="torch.Tensor.index_put_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.index_put_</span></code></a></p></td>
<td><p>Puts values from the tensor <code class="xref py py-attr docutils literal notranslate"><span class="pre">values</span></code> into the tensor <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> using the indices specified in <code class="xref py py-attr docutils literal notranslate"><span class="pre">indices</span></code> (which is a tuple of Tensors).</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.index_put.html#torch.Tensor.index_put" title="torch.Tensor.index_put"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.index_put</span></code></a></p></td>
<td><p>Out-place version of <a class="reference internal" href="generated/torch.Tensor.index_put_.html#torch.Tensor.index_put_" title="torch.Tensor.index_put_"><code class="xref py py-meth docutils literal notranslate"><span class="pre">index_put_()</span></code></a>.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.index_reduce_.html#torch.Tensor.index_reduce_" title="torch.Tensor.index_reduce_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.index_reduce_</span></code></a></p></td>
<td><p>Accumulate the elements of <code class="docutils literal notranslate"><span class="pre">source</span></code> into the <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor by accumulating to the indices in the order given in <code class="xref py py-attr docutils literal notranslate"><span class="pre">index</span></code> using the reduction given by the <code class="docutils literal notranslate"><span class="pre">reduce</span></code> argument.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.index_reduce.html#torch.Tensor.index_reduce" title="torch.Tensor.index_reduce"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.index_reduce</span></code></a></p></td>
<td><p></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.index_select.html#torch.Tensor.index_select" title="torch.Tensor.index_select"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.index_select</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.index_select.html#torch.index_select" title="torch.index_select"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.index_select()</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.indices.html#torch.Tensor.indices" title="torch.Tensor.indices"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.indices</span></code></a></p></td>
<td><p>Return the indices tensor of a <a class="reference internal" href="sparse.html#sparse-coo-docs"><span class="std std-ref">sparse COO tensor</span></a>.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.inner.html#torch.Tensor.inner" title="torch.Tensor.inner"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.inner</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.inner.html#torch.inner" title="torch.inner"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.inner()</span></code></a>.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.int.html#torch.Tensor.int" title="torch.Tensor.int"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.int</span></code></a></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">self.int()</span></code> is equivalent to <code class="docutils literal notranslate"><span class="pre">self.to(torch.int32)</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.int_repr</span></code></p></td>
<td><p>Given a quantized Tensor, <code class="docutils literal notranslate"><span class="pre">self.int_repr()</span></code> returns a CPU Tensor with uint8_t as data type that stores the underlying uint8_t values of the given Tensor.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.inverse.html#torch.Tensor.inverse" title="torch.Tensor.inverse"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.inverse</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.inverse.html#torch.inverse" title="torch.inverse"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.inverse()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.isclose.html#torch.Tensor.isclose" title="torch.Tensor.isclose"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.isclose</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.isclose.html#torch.isclose" title="torch.isclose"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.isclose()</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.isfinite.html#torch.Tensor.isfinite" title="torch.Tensor.isfinite"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.isfinite</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.isfinite.html#torch.isfinite" title="torch.isfinite"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.isfinite()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.isinf.html#torch.Tensor.isinf" title="torch.Tensor.isinf"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.isinf</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.isinf.html#torch.isinf" title="torch.isinf"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.isinf()</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.isposinf.html#torch.Tensor.isposinf" title="torch.Tensor.isposinf"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.isposinf</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.isposinf.html#torch.isposinf" title="torch.isposinf"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.isposinf()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.isneginf.html#torch.Tensor.isneginf" title="torch.Tensor.isneginf"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.isneginf</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.isneginf.html#torch.isneginf" title="torch.isneginf"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.isneginf()</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.isnan.html#torch.Tensor.isnan" title="torch.Tensor.isnan"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.isnan</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.isnan.html#torch.isnan" title="torch.isnan"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.isnan()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.is_contiguous.html#torch.Tensor.is_contiguous" title="torch.Tensor.is_contiguous"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.is_contiguous</span></code></a></p></td>
<td><p>Returns True if <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor is contiguous in memory in the order specified by memory format.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.is_complex.html#torch.Tensor.is_complex" title="torch.Tensor.is_complex"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.is_complex</span></code></a></p></td>
<td><p>Returns True if the data type of <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> is a complex data type.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.is_conj.html#torch.Tensor.is_conj" title="torch.Tensor.is_conj"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.is_conj</span></code></a></p></td>
<td><p>Returns True if the conjugate bit of <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> is set to true.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.is_floating_point.html#torch.Tensor.is_floating_point" title="torch.Tensor.is_floating_point"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.is_floating_point</span></code></a></p></td>
<td><p>Returns True if the data type of <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> is a floating point data type.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.is_inference.html#torch.Tensor.is_inference" title="torch.Tensor.is_inference"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.is_inference</span></code></a></p></td>
<td><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.is_inference()</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.is_leaf.html#torch.Tensor.is_leaf" title="torch.Tensor.is_leaf"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.is_leaf</span></code></a></p></td>
<td><p>All Tensors that have <code class="xref py py-attr docutils literal notranslate"><span class="pre">requires_grad</span></code> which is <code class="docutils literal notranslate"><span class="pre">False</span></code> will be leaf Tensors by convention.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.is_pinned.html#torch.Tensor.is_pinned" title="torch.Tensor.is_pinned"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.is_pinned</span></code></a></p></td>
<td><p>Returns true if this tensor resides in pinned memory.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.is_set_to.html#torch.Tensor.is_set_to" title="torch.Tensor.is_set_to"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.is_set_to</span></code></a></p></td>
<td><p>Returns True if both tensors are pointing to the exact same memory (same storage, offset, size and stride).</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.is_shared.html#torch.Tensor.is_shared" title="torch.Tensor.is_shared"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.is_shared</span></code></a></p></td>
<td><p>Checks if tensor is in shared memory.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.is_signed.html#torch.Tensor.is_signed" title="torch.Tensor.is_signed"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.is_signed</span></code></a></p></td>
<td><p>Returns True if the data type of <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> is a signed data type.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.is_sparse.html#torch.Tensor.is_sparse" title="torch.Tensor.is_sparse"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.is_sparse</span></code></a></p></td>
<td><p>Is <code class="docutils literal notranslate"><span class="pre">True</span></code> if the Tensor uses sparse COO storage layout, <code class="docutils literal notranslate"><span class="pre">False</span></code> otherwise.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.istft.html#torch.Tensor.istft" title="torch.Tensor.istft"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.istft</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.istft.html#torch.istft" title="torch.istft"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.istft()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.isreal.html#torch.Tensor.isreal" title="torch.Tensor.isreal"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.isreal</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.isreal.html#torch.isreal" title="torch.isreal"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.isreal()</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.item.html#torch.Tensor.item" title="torch.Tensor.item"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.item</span></code></a></p></td>
<td><p>Returns the value of this tensor as a standard Python number.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.kthvalue.html#torch.Tensor.kthvalue" title="torch.Tensor.kthvalue"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.kthvalue</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.kthvalue.html#torch.kthvalue" title="torch.kthvalue"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.kthvalue()</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.lcm.html#torch.Tensor.lcm" title="torch.Tensor.lcm"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.lcm</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.lcm.html#torch.lcm" title="torch.lcm"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.lcm()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.lcm_.html#torch.Tensor.lcm_" title="torch.Tensor.lcm_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.lcm_</span></code></a></p></td>
<td><p>In-place version of <a class="reference internal" href="generated/torch.Tensor.lcm.html#torch.Tensor.lcm" title="torch.Tensor.lcm"><code class="xref py py-meth docutils literal notranslate"><span class="pre">lcm()</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.ldexp.html#torch.Tensor.ldexp" title="torch.Tensor.ldexp"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.ldexp</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.ldexp.html#torch.ldexp" title="torch.ldexp"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.ldexp()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.ldexp_.html#torch.Tensor.ldexp_" title="torch.Tensor.ldexp_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.ldexp_</span></code></a></p></td>
<td><p>In-place version of <a class="reference internal" href="generated/torch.Tensor.ldexp.html#torch.Tensor.ldexp" title="torch.Tensor.ldexp"><code class="xref py py-meth docutils literal notranslate"><span class="pre">ldexp()</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.le</span></code></p></td>
<td><p>See <a class="reference internal" href="generated/torch.le.html#torch.le" title="torch.le"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.le()</span></code></a>.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.le_.html#torch.Tensor.le_" title="torch.Tensor.le_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.le_</span></code></a></p></td>
<td><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">le()</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.less_equal.html#torch.Tensor.less_equal" title="torch.Tensor.less_equal"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.less_equal</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.less_equal.html#torch.less_equal" title="torch.less_equal"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.less_equal()</span></code></a>.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.less_equal_.html#torch.Tensor.less_equal_" title="torch.Tensor.less_equal_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.less_equal_</span></code></a></p></td>
<td><p>In-place version of <a class="reference internal" href="generated/torch.Tensor.less_equal.html#torch.Tensor.less_equal" title="torch.Tensor.less_equal"><code class="xref py py-meth docutils literal notranslate"><span class="pre">less_equal()</span></code></a>.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.lerp.html#torch.Tensor.lerp" title="torch.Tensor.lerp"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.lerp</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.lerp.html#torch.lerp" title="torch.lerp"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.lerp()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.lerp_.html#torch.Tensor.lerp_" title="torch.Tensor.lerp_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.lerp_</span></code></a></p></td>
<td><p>In-place version of <a class="reference internal" href="generated/torch.Tensor.lerp.html#torch.Tensor.lerp" title="torch.Tensor.lerp"><code class="xref py py-meth docutils literal notranslate"><span class="pre">lerp()</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.lgamma.html#torch.Tensor.lgamma" title="torch.Tensor.lgamma"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.lgamma</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.lgamma.html#torch.lgamma" title="torch.lgamma"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.lgamma()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.lgamma_.html#torch.Tensor.lgamma_" title="torch.Tensor.lgamma_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.lgamma_</span></code></a></p></td>
<td><p>In-place version of <a class="reference internal" href="generated/torch.Tensor.lgamma.html#torch.Tensor.lgamma" title="torch.Tensor.lgamma"><code class="xref py py-meth docutils literal notranslate"><span class="pre">lgamma()</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.log.html#torch.Tensor.log" title="torch.Tensor.log"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.log</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.log.html#torch.log" title="torch.log"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.log()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.log_.html#torch.Tensor.log_" title="torch.Tensor.log_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.log_</span></code></a></p></td>
<td><p>In-place version of <a class="reference internal" href="generated/torch.Tensor.log.html#torch.Tensor.log" title="torch.Tensor.log"><code class="xref py py-meth docutils literal notranslate"><span class="pre">log()</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.logdet.html#torch.Tensor.logdet" title="torch.Tensor.logdet"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.logdet</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.logdet.html#torch.logdet" title="torch.logdet"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.logdet()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.log10.html#torch.Tensor.log10" title="torch.Tensor.log10"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.log10</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.log10.html#torch.log10" title="torch.log10"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.log10()</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.log10_.html#torch.Tensor.log10_" title="torch.Tensor.log10_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.log10_</span></code></a></p></td>
<td><p>In-place version of <a class="reference internal" href="generated/torch.Tensor.log10.html#torch.Tensor.log10" title="torch.Tensor.log10"><code class="xref py py-meth docutils literal notranslate"><span class="pre">log10()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.log1p.html#torch.Tensor.log1p" title="torch.Tensor.log1p"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.log1p</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.log1p.html#torch.log1p" title="torch.log1p"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.log1p()</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.log1p_.html#torch.Tensor.log1p_" title="torch.Tensor.log1p_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.log1p_</span></code></a></p></td>
<td><p>In-place version of <a class="reference internal" href="generated/torch.Tensor.log1p.html#torch.Tensor.log1p" title="torch.Tensor.log1p"><code class="xref py py-meth docutils literal notranslate"><span class="pre">log1p()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.log2.html#torch.Tensor.log2" title="torch.Tensor.log2"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.log2</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.log2.html#torch.log2" title="torch.log2"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.log2()</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.log2_.html#torch.Tensor.log2_" title="torch.Tensor.log2_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.log2_</span></code></a></p></td>
<td><p>In-place version of <a class="reference internal" href="generated/torch.Tensor.log2.html#torch.Tensor.log2" title="torch.Tensor.log2"><code class="xref py py-meth docutils literal notranslate"><span class="pre">log2()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.log_normal_.html#torch.Tensor.log_normal_" title="torch.Tensor.log_normal_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.log_normal_</span></code></a></p></td>
<td><p>Fills <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor with numbers samples from the log-normal distribution parameterized by the given mean <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi></mi></mrow><annotation encoding="application/x-tex">\mu</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord mathnormal"></span></span></span></span></span> and standard deviation <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi></mi></mrow><annotation encoding="application/x-tex">\sigma</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;"></span></span></span></span></span>.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.logaddexp.html#torch.Tensor.logaddexp" title="torch.Tensor.logaddexp"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.logaddexp</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.logaddexp.html#torch.logaddexp" title="torch.logaddexp"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.logaddexp()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.logaddexp2.html#torch.Tensor.logaddexp2" title="torch.Tensor.logaddexp2"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.logaddexp2</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.logaddexp2.html#torch.logaddexp2" title="torch.logaddexp2"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.logaddexp2()</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.logsumexp.html#torch.Tensor.logsumexp" title="torch.Tensor.logsumexp"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.logsumexp</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.logsumexp.html#torch.logsumexp" title="torch.logsumexp"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.logsumexp()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.logical_and.html#torch.Tensor.logical_and" title="torch.Tensor.logical_and"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.logical_and</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.logical_and.html#torch.logical_and" title="torch.logical_and"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.logical_and()</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.logical_and_.html#torch.Tensor.logical_and_" title="torch.Tensor.logical_and_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.logical_and_</span></code></a></p></td>
<td><p>In-place version of <a class="reference internal" href="generated/torch.Tensor.logical_and.html#torch.Tensor.logical_and" title="torch.Tensor.logical_and"><code class="xref py py-meth docutils literal notranslate"><span class="pre">logical_and()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.logical_not.html#torch.Tensor.logical_not" title="torch.Tensor.logical_not"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.logical_not</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.logical_not.html#torch.logical_not" title="torch.logical_not"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.logical_not()</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.logical_not_.html#torch.Tensor.logical_not_" title="torch.Tensor.logical_not_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.logical_not_</span></code></a></p></td>
<td><p>In-place version of <a class="reference internal" href="generated/torch.Tensor.logical_not.html#torch.Tensor.logical_not" title="torch.Tensor.logical_not"><code class="xref py py-meth docutils literal notranslate"><span class="pre">logical_not()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.logical_or.html#torch.Tensor.logical_or" title="torch.Tensor.logical_or"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.logical_or</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.logical_or.html#torch.logical_or" title="torch.logical_or"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.logical_or()</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.logical_or_.html#torch.Tensor.logical_or_" title="torch.Tensor.logical_or_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.logical_or_</span></code></a></p></td>
<td><p>In-place version of <a class="reference internal" href="generated/torch.Tensor.logical_or.html#torch.Tensor.logical_or" title="torch.Tensor.logical_or"><code class="xref py py-meth docutils literal notranslate"><span class="pre">logical_or()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.logical_xor.html#torch.Tensor.logical_xor" title="torch.Tensor.logical_xor"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.logical_xor</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.logical_xor.html#torch.logical_xor" title="torch.logical_xor"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.logical_xor()</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.logical_xor_.html#torch.Tensor.logical_xor_" title="torch.Tensor.logical_xor_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.logical_xor_</span></code></a></p></td>
<td><p>In-place version of <a class="reference internal" href="generated/torch.Tensor.logical_xor.html#torch.Tensor.logical_xor" title="torch.Tensor.logical_xor"><code class="xref py py-meth docutils literal notranslate"><span class="pre">logical_xor()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.logit.html#torch.Tensor.logit" title="torch.Tensor.logit"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.logit</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.logit.html#torch.logit" title="torch.logit"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.logit()</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.logit_.html#torch.Tensor.logit_" title="torch.Tensor.logit_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.logit_</span></code></a></p></td>
<td><p>In-place version of <a class="reference internal" href="generated/torch.Tensor.logit.html#torch.Tensor.logit" title="torch.Tensor.logit"><code class="xref py py-meth docutils literal notranslate"><span class="pre">logit()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.long.html#torch.Tensor.long" title="torch.Tensor.long"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.long</span></code></a></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">self.long()</span></code> is equivalent to <code class="docutils literal notranslate"><span class="pre">self.to(torch.int64)</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.lt</span></code></p></td>
<td><p>See <a class="reference internal" href="generated/torch.lt.html#torch.lt" title="torch.lt"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.lt()</span></code></a>.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.lt_.html#torch.Tensor.lt_" title="torch.Tensor.lt_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.lt_</span></code></a></p></td>
<td><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">lt()</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.less.html#torch.Tensor.less" title="torch.Tensor.less"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.less</span></code></a></p></td>
<td><p>lt(other) -&gt; Tensor</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.less_.html#torch.Tensor.less_" title="torch.Tensor.less_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.less_</span></code></a></p></td>
<td><p>In-place version of <a class="reference internal" href="generated/torch.Tensor.less.html#torch.Tensor.less" title="torch.Tensor.less"><code class="xref py py-meth docutils literal notranslate"><span class="pre">less()</span></code></a>.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.lu.html#torch.Tensor.lu" title="torch.Tensor.lu"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.lu</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.lu.html#torch.lu" title="torch.lu"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.lu()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.lu_solve.html#torch.Tensor.lu_solve" title="torch.Tensor.lu_solve"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.lu_solve</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.lu_solve.html#torch.lu_solve" title="torch.lu_solve"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.lu_solve()</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.as_subclass.html#torch.Tensor.as_subclass" title="torch.Tensor.as_subclass"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.as_subclass</span></code></a></p></td>
<td><p>Makes a <code class="docutils literal notranslate"><span class="pre">cls</span></code> instance with the same data pointer as <code class="docutils literal notranslate"><span class="pre">self</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.map_.html#torch.Tensor.map_" title="torch.Tensor.map_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.map_</span></code></a></p></td>
<td><p>Applies <code class="xref py py-attr docutils literal notranslate"><span class="pre">callable</span></code> for each element in <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor and the given <a class="reference internal" href="generated/torch.tensor.html#torch.tensor" title="torch.tensor"><code class="xref py py-attr docutils literal notranslate"><span class="pre">tensor</span></code></a> and stores the results in <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.masked_scatter_.html#torch.Tensor.masked_scatter_" title="torch.Tensor.masked_scatter_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.masked_scatter_</span></code></a></p></td>
<td><p>Copies elements from <code class="xref py py-attr docutils literal notranslate"><span class="pre">source</span></code> into <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor at positions where the <code class="xref py py-attr docutils literal notranslate"><span class="pre">mask</span></code> is True.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.masked_scatter.html#torch.Tensor.masked_scatter" title="torch.Tensor.masked_scatter"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.masked_scatter</span></code></a></p></td>
<td><p>Out-of-place version of <a class="reference internal" href="generated/torch.Tensor.masked_scatter_.html#torch.Tensor.masked_scatter_" title="torch.Tensor.masked_scatter_"><code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.Tensor.masked_scatter_()</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.masked_fill_.html#torch.Tensor.masked_fill_" title="torch.Tensor.masked_fill_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.masked_fill_</span></code></a></p></td>
<td><p>Fills elements of <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor with <code class="xref py py-attr docutils literal notranslate"><span class="pre">value</span></code> where <code class="xref py py-attr docutils literal notranslate"><span class="pre">mask</span></code> is True.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.masked_fill.html#torch.Tensor.masked_fill" title="torch.Tensor.masked_fill"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.masked_fill</span></code></a></p></td>
<td><p>Out-of-place version of <a class="reference internal" href="generated/torch.Tensor.masked_fill_.html#torch.Tensor.masked_fill_" title="torch.Tensor.masked_fill_"><code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.Tensor.masked_fill_()</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.masked_select.html#torch.Tensor.masked_select" title="torch.Tensor.masked_select"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.masked_select</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.masked_select.html#torch.masked_select" title="torch.masked_select"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.masked_select()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.matmul.html#torch.Tensor.matmul" title="torch.Tensor.matmul"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.matmul</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.matmul.html#torch.matmul" title="torch.matmul"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.matmul()</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.matrix_power.html#torch.Tensor.matrix_power" title="torch.Tensor.matrix_power"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.matrix_power</span></code></a></p></td>
<td><p></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.matrix_exp.html#torch.Tensor.matrix_exp" title="torch.Tensor.matrix_exp"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.matrix_exp</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.matrix_exp.html#torch.matrix_exp" title="torch.matrix_exp"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.matrix_exp()</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.max</span></code></p></td>
<td><p>See <a class="reference internal" href="generated/torch.max.html#torch.max" title="torch.max"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.max()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.maximum.html#torch.Tensor.maximum" title="torch.Tensor.maximum"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.maximum</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.maximum.html#torch.maximum" title="torch.maximum"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.maximum()</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.mean</span></code></p></td>
<td><p>See <a class="reference internal" href="generated/torch.mean.html#torch.mean" title="torch.mean"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.mean()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.module_load.html#torch.Tensor.module_load" title="torch.Tensor.module_load"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.module_load</span></code></a></p></td>
<td><p>Defines how to transform <code class="docutils literal notranslate"><span class="pre">other</span></code> when loading it into <code class="docutils literal notranslate"><span class="pre">self</span></code> in <a class="reference internal" href="generated/torch.nn.Module.html#torch.nn.Module.load_state_dict" title="torch.nn.Module.load_state_dict"><code class="xref py py-meth docutils literal notranslate"><span class="pre">load_state_dict()</span></code></a>.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.nanmean.html#torch.Tensor.nanmean" title="torch.Tensor.nanmean"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.nanmean</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.nanmean.html#torch.nanmean" title="torch.nanmean"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.nanmean()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.median.html#torch.Tensor.median" title="torch.Tensor.median"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.median</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.median.html#torch.median" title="torch.median"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.median()</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.nanmedian.html#torch.Tensor.nanmedian" title="torch.Tensor.nanmedian"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.nanmedian</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.nanmedian.html#torch.nanmedian" title="torch.nanmedian"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.nanmedian()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.min</span></code></p></td>
<td><p>See <a class="reference internal" href="generated/torch.min.html#torch.min" title="torch.min"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.min()</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.minimum.html#torch.Tensor.minimum" title="torch.Tensor.minimum"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.minimum</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.minimum.html#torch.minimum" title="torch.minimum"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.minimum()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.mm.html#torch.Tensor.mm" title="torch.Tensor.mm"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.mm</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.mm.html#torch.mm" title="torch.mm"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.mm()</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.smm.html#torch.Tensor.smm" title="torch.Tensor.smm"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.smm</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.smm.html#torch.smm" title="torch.smm"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.smm()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.mode.html#torch.Tensor.mode" title="torch.Tensor.mode"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.mode</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.mode.html#torch.mode" title="torch.mode"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.mode()</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.movedim.html#torch.Tensor.movedim" title="torch.Tensor.movedim"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.movedim</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.movedim.html#torch.movedim" title="torch.movedim"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.movedim()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.moveaxis.html#torch.Tensor.moveaxis" title="torch.Tensor.moveaxis"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.moveaxis</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.moveaxis.html#torch.moveaxis" title="torch.moveaxis"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.moveaxis()</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.msort.html#torch.Tensor.msort" title="torch.Tensor.msort"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.msort</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.msort.html#torch.msort" title="torch.msort"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.msort()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.mul.html#torch.Tensor.mul" title="torch.Tensor.mul"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.mul</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.mul.html#torch.mul" title="torch.mul"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.mul()</span></code></a>.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.mul_.html#torch.Tensor.mul_" title="torch.Tensor.mul_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.mul_</span></code></a></p></td>
<td><p>In-place version of <a class="reference internal" href="generated/torch.Tensor.mul.html#torch.Tensor.mul" title="torch.Tensor.mul"><code class="xref py py-meth docutils literal notranslate"><span class="pre">mul()</span></code></a>.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.multiply.html#torch.Tensor.multiply" title="torch.Tensor.multiply"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.multiply</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.multiply.html#torch.multiply" title="torch.multiply"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.multiply()</span></code></a>.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.multiply_.html#torch.Tensor.multiply_" title="torch.Tensor.multiply_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.multiply_</span></code></a></p></td>
<td><p>In-place version of <a class="reference internal" href="generated/torch.Tensor.multiply.html#torch.Tensor.multiply" title="torch.Tensor.multiply"><code class="xref py py-meth docutils literal notranslate"><span class="pre">multiply()</span></code></a>.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.multinomial.html#torch.Tensor.multinomial" title="torch.Tensor.multinomial"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.multinomial</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.multinomial.html#torch.multinomial" title="torch.multinomial"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.multinomial()</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.mv.html#torch.Tensor.mv" title="torch.Tensor.mv"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.mv</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.mv.html#torch.mv" title="torch.mv"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.mv()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.mvlgamma.html#torch.Tensor.mvlgamma" title="torch.Tensor.mvlgamma"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.mvlgamma</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.mvlgamma.html#torch.mvlgamma" title="torch.mvlgamma"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.mvlgamma()</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.mvlgamma_.html#torch.Tensor.mvlgamma_" title="torch.Tensor.mvlgamma_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.mvlgamma_</span></code></a></p></td>
<td><p>In-place version of <a class="reference internal" href="generated/torch.Tensor.mvlgamma.html#torch.Tensor.mvlgamma" title="torch.Tensor.mvlgamma"><code class="xref py py-meth docutils literal notranslate"><span class="pre">mvlgamma()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.nansum.html#torch.Tensor.nansum" title="torch.Tensor.nansum"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.nansum</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.nansum.html#torch.nansum" title="torch.nansum"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.nansum()</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.narrow.html#torch.Tensor.narrow" title="torch.Tensor.narrow"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.narrow</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.narrow.html#torch.narrow" title="torch.narrow"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.narrow()</span></code></a>.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.narrow_copy.html#torch.Tensor.narrow_copy" title="torch.Tensor.narrow_copy"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.narrow_copy</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.narrow_copy.html#torch.narrow_copy" title="torch.narrow_copy"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.narrow_copy()</span></code></a>.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.ndimension.html#torch.Tensor.ndimension" title="torch.Tensor.ndimension"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.ndimension</span></code></a></p></td>
<td><p>Alias for <a class="reference internal" href="generated/torch.Tensor.dim.html#torch.Tensor.dim" title="torch.Tensor.dim"><code class="xref py py-meth docutils literal notranslate"><span class="pre">dim()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.nan_to_num.html#torch.Tensor.nan_to_num" title="torch.Tensor.nan_to_num"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.nan_to_num</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.nan_to_num.html#torch.nan_to_num" title="torch.nan_to_num"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.nan_to_num()</span></code></a>.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.nan_to_num_.html#torch.Tensor.nan_to_num_" title="torch.Tensor.nan_to_num_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.nan_to_num_</span></code></a></p></td>
<td><p>In-place version of <a class="reference internal" href="generated/torch.Tensor.nan_to_num.html#torch.Tensor.nan_to_num" title="torch.Tensor.nan_to_num"><code class="xref py py-meth docutils literal notranslate"><span class="pre">nan_to_num()</span></code></a>.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.ne</span></code></p></td>
<td><p>See <a class="reference internal" href="generated/torch.Tensor.ne.html#torch.ne" title="torch.ne"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.ne()</span></code></a>.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.ne_.html#torch.Tensor.ne_" title="torch.Tensor.ne_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.ne_</span></code></a></p></td>
<td><p>In-place version of <code class="xref py py-meth docutils literal notranslate"><span class="pre">ne()</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.not_equal.html#torch.Tensor.not_equal" title="torch.Tensor.not_equal"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.not_equal</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.not_equal.html#torch.not_equal" title="torch.not_equal"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.not_equal()</span></code></a>.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.not_equal_.html#torch.Tensor.not_equal_" title="torch.Tensor.not_equal_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.not_equal_</span></code></a></p></td>
<td><p>In-place version of <a class="reference internal" href="generated/torch.Tensor.not_equal.html#torch.Tensor.not_equal" title="torch.Tensor.not_equal"><code class="xref py py-meth docutils literal notranslate"><span class="pre">not_equal()</span></code></a>.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.neg.html#torch.Tensor.neg" title="torch.Tensor.neg"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.neg</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.neg.html#torch.neg" title="torch.neg"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.neg()</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.neg_.html#torch.Tensor.neg_" title="torch.Tensor.neg_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.neg_</span></code></a></p></td>
<td><p>In-place version of <a class="reference internal" href="generated/torch.Tensor.neg.html#torch.Tensor.neg" title="torch.Tensor.neg"><code class="xref py py-meth docutils literal notranslate"><span class="pre">neg()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.negative.html#torch.Tensor.negative" title="torch.Tensor.negative"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.negative</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.negative.html#torch.negative" title="torch.negative"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.negative()</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.negative_.html#torch.Tensor.negative_" title="torch.Tensor.negative_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.negative_</span></code></a></p></td>
<td><p>In-place version of <a class="reference internal" href="generated/torch.Tensor.negative.html#torch.Tensor.negative" title="torch.Tensor.negative"><code class="xref py py-meth docutils literal notranslate"><span class="pre">negative()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.nelement.html#torch.Tensor.nelement" title="torch.Tensor.nelement"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.nelement</span></code></a></p></td>
<td><p>Alias for <a class="reference internal" href="generated/torch.Tensor.numel.html#torch.Tensor.numel" title="torch.Tensor.numel"><code class="xref py py-meth docutils literal notranslate"><span class="pre">numel()</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.nextafter.html#torch.Tensor.nextafter" title="torch.Tensor.nextafter"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.nextafter</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.nextafter.html#torch.nextafter" title="torch.nextafter"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.nextafter()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.nextafter_.html#torch.Tensor.nextafter_" title="torch.Tensor.nextafter_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.nextafter_</span></code></a></p></td>
<td><p>In-place version of <a class="reference internal" href="generated/torch.Tensor.nextafter.html#torch.Tensor.nextafter" title="torch.Tensor.nextafter"><code class="xref py py-meth docutils literal notranslate"><span class="pre">nextafter()</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.nonzero.html#torch.Tensor.nonzero" title="torch.Tensor.nonzero"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.nonzero</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.nonzero.html#torch.nonzero" title="torch.nonzero"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.nonzero()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.norm.html#torch.Tensor.norm" title="torch.Tensor.norm"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.norm</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.norm.html#torch.norm" title="torch.norm"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.norm()</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.normal_.html#torch.Tensor.normal_" title="torch.Tensor.normal_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.normal_</span></code></a></p></td>
<td><p>Fills <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor with elements samples from the normal distribution parameterized by <a class="reference internal" href="generated/torch.mean.html#torch.mean" title="torch.mean"><code class="xref py py-attr docutils literal notranslate"><span class="pre">mean</span></code></a> and <a class="reference internal" href="generated/torch.std.html#torch.std" title="torch.std"><code class="xref py py-attr docutils literal notranslate"><span class="pre">std</span></code></a>.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.numel.html#torch.Tensor.numel" title="torch.Tensor.numel"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.numel</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.numel.html#torch.numel" title="torch.numel"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.numel()</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.numpy.html#torch.Tensor.numpy" title="torch.Tensor.numpy"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.numpy</span></code></a></p></td>
<td><p>Returns the tensor as a NumPy <code class="xref py py-class docutils literal notranslate"><span class="pre">ndarray</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.orgqr.html#torch.Tensor.orgqr" title="torch.Tensor.orgqr"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.orgqr</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.orgqr.html#torch.orgqr" title="torch.orgqr"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.orgqr()</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.ormqr.html#torch.Tensor.ormqr" title="torch.Tensor.ormqr"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.ormqr</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.ormqr.html#torch.ormqr" title="torch.ormqr"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.ormqr()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.outer.html#torch.Tensor.outer" title="torch.Tensor.outer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.outer</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.outer.html#torch.outer" title="torch.outer"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.outer()</span></code></a>.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.permute.html#torch.Tensor.permute" title="torch.Tensor.permute"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.permute</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.permute.html#torch.permute" title="torch.permute"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.permute()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.pin_memory.html#torch.Tensor.pin_memory" title="torch.Tensor.pin_memory"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.pin_memory</span></code></a></p></td>
<td><p>Copies the tensor to pinned memory, if it's not already pinned.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.pinverse.html#torch.Tensor.pinverse" title="torch.Tensor.pinverse"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.pinverse</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.pinverse.html#torch.pinverse" title="torch.pinverse"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.pinverse()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.polygamma.html#torch.Tensor.polygamma" title="torch.Tensor.polygamma"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.polygamma</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.polygamma.html#torch.polygamma" title="torch.polygamma"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.polygamma()</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.polygamma_.html#torch.Tensor.polygamma_" title="torch.Tensor.polygamma_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.polygamma_</span></code></a></p></td>
<td><p>In-place version of <a class="reference internal" href="generated/torch.Tensor.polygamma.html#torch.Tensor.polygamma" title="torch.Tensor.polygamma"><code class="xref py py-meth docutils literal notranslate"><span class="pre">polygamma()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.positive.html#torch.Tensor.positive" title="torch.Tensor.positive"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.positive</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.positive.html#torch.positive" title="torch.positive"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.positive()</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.pow.html#torch.Tensor.pow" title="torch.Tensor.pow"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.pow</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.pow.html#torch.pow" title="torch.pow"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.pow()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.pow_.html#torch.Tensor.pow_" title="torch.Tensor.pow_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.pow_</span></code></a></p></td>
<td><p>In-place version of <a class="reference internal" href="generated/torch.Tensor.pow.html#torch.Tensor.pow" title="torch.Tensor.pow"><code class="xref py py-meth docutils literal notranslate"><span class="pre">pow()</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.prod.html#torch.Tensor.prod" title="torch.Tensor.prod"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.prod</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.prod.html#torch.prod" title="torch.prod"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.prod()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.put_.html#torch.Tensor.put_" title="torch.Tensor.put_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.put_</span></code></a></p></td>
<td><p>Copies the elements from <code class="xref py py-attr docutils literal notranslate"><span class="pre">source</span></code> into the positions specified by <code class="xref py py-attr docutils literal notranslate"><span class="pre">index</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.qr.html#torch.Tensor.qr" title="torch.Tensor.qr"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.qr</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.qr.html#torch.qr" title="torch.qr"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.qr()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.qscheme.html#torch.Tensor.qscheme" title="torch.Tensor.qscheme"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.qscheme</span></code></a></p></td>
<td><p>Returns the quantization scheme of a given QTensor.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.quantile.html#torch.Tensor.quantile" title="torch.Tensor.quantile"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.quantile</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.quantile.html#torch.quantile" title="torch.quantile"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.quantile()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.nanquantile.html#torch.Tensor.nanquantile" title="torch.Tensor.nanquantile"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.nanquantile</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.nanquantile.html#torch.nanquantile" title="torch.nanquantile"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.nanquantile()</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.q_scale</span></code></p></td>
<td><p>Given a Tensor quantized by linear(affine) quantization, returns the scale of the underlying quantizer().</p></td>
</tr>
<tr class="row-odd"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.q_zero_point</span></code></p></td>
<td><p>Given a Tensor quantized by linear(affine) quantization, returns the zero_point of the underlying quantizer().</p></td>
</tr>
<tr class="row-even"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.q_per_channel_scales</span></code></p></td>
<td><p>Given a Tensor quantized by linear (affine) per-channel quantization, returns a Tensor of scales of the underlying quantizer.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.q_per_channel_zero_points</span></code></p></td>
<td><p>Given a Tensor quantized by linear (affine) per-channel quantization, returns a tensor of zero_points of the underlying quantizer.</p></td>
</tr>
<tr class="row-even"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.q_per_channel_axis</span></code></p></td>
<td><p>Given a Tensor quantized by linear (affine) per-channel quantization, returns the index of dimension on which per-channel quantization is applied.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.rad2deg.html#torch.Tensor.rad2deg" title="torch.Tensor.rad2deg"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.rad2deg</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.rad2deg.html#torch.rad2deg" title="torch.rad2deg"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.rad2deg()</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.random_.html#torch.Tensor.random_" title="torch.Tensor.random_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.random_</span></code></a></p></td>
<td><p>Fills <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor with numbers sampled from the discrete uniform distribution over <code class="docutils literal notranslate"><span class="pre">[from,</span> <span class="pre">to</span> <span class="pre">-</span> <span class="pre">1]</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.ravel.html#torch.Tensor.ravel" title="torch.Tensor.ravel"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.ravel</span></code></a></p></td>
<td><p>see <a class="reference internal" href="generated/torch.ravel.html#torch.ravel" title="torch.ravel"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.ravel()</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.reciprocal.html#torch.Tensor.reciprocal" title="torch.Tensor.reciprocal"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.reciprocal</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.reciprocal.html#torch.reciprocal" title="torch.reciprocal"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.reciprocal()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.reciprocal_.html#torch.Tensor.reciprocal_" title="torch.Tensor.reciprocal_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.reciprocal_</span></code></a></p></td>
<td><p>In-place version of <a class="reference internal" href="generated/torch.Tensor.reciprocal.html#torch.Tensor.reciprocal" title="torch.Tensor.reciprocal"><code class="xref py py-meth docutils literal notranslate"><span class="pre">reciprocal()</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.record_stream.html#torch.Tensor.record_stream" title="torch.Tensor.record_stream"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.record_stream</span></code></a></p></td>
<td><p>Marks the tensor as having been used by this stream.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.register_hook.html#torch.Tensor.register_hook" title="torch.Tensor.register_hook"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.register_hook</span></code></a></p></td>
<td><p>Registers a backward hook.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.register_post_accumulate_grad_hook.html#torch.Tensor.register_post_accumulate_grad_hook" title="torch.Tensor.register_post_accumulate_grad_hook"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.register_post_accumulate_grad_hook</span></code></a></p></td>
<td><p>Registers a backward hook that runs after grad accumulation.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.remainder.html#torch.Tensor.remainder" title="torch.Tensor.remainder"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.remainder</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.remainder.html#torch.remainder" title="torch.remainder"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.remainder()</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.remainder_.html#torch.Tensor.remainder_" title="torch.Tensor.remainder_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.remainder_</span></code></a></p></td>
<td><p>In-place version of <a class="reference internal" href="generated/torch.Tensor.remainder.html#torch.Tensor.remainder" title="torch.Tensor.remainder"><code class="xref py py-meth docutils literal notranslate"><span class="pre">remainder()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.renorm.html#torch.Tensor.renorm" title="torch.Tensor.renorm"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.renorm</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.renorm.html#torch.renorm" title="torch.renorm"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.renorm()</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.renorm_.html#torch.Tensor.renorm_" title="torch.Tensor.renorm_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.renorm_</span></code></a></p></td>
<td><p>In-place version of <a class="reference internal" href="generated/torch.Tensor.renorm.html#torch.Tensor.renorm" title="torch.Tensor.renorm"><code class="xref py py-meth docutils literal notranslate"><span class="pre">renorm()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.repeat.html#torch.Tensor.repeat" title="torch.Tensor.repeat"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.repeat</span></code></a></p></td>
<td><p>Repeats this tensor along the specified dimensions.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.repeat_interleave.html#torch.Tensor.repeat_interleave" title="torch.Tensor.repeat_interleave"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.repeat_interleave</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.repeat_interleave.html#torch.repeat_interleave" title="torch.repeat_interleave"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.repeat_interleave()</span></code></a>.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.requires_grad.html#torch.Tensor.requires_grad" title="torch.Tensor.requires_grad"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.requires_grad</span></code></a></p></td>
<td><p>Is <code class="docutils literal notranslate"><span class="pre">True</span></code> if gradients need to be computed for this Tensor, <code class="docutils literal notranslate"><span class="pre">False</span></code> otherwise.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.requires_grad_.html#torch.Tensor.requires_grad_" title="torch.Tensor.requires_grad_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.requires_grad_</span></code></a></p></td>
<td><p>Change if autograd should record operations on this tensor: sets this tensor's <code class="xref py py-attr docutils literal notranslate"><span class="pre">requires_grad</span></code> attribute in-place.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.reshape.html#torch.Tensor.reshape" title="torch.Tensor.reshape"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.reshape</span></code></a></p></td>
<td><p>Returns a tensor with the same data and number of elements as <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> but with the specified shape.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.reshape_as.html#torch.Tensor.reshape_as" title="torch.Tensor.reshape_as"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.reshape_as</span></code></a></p></td>
<td><p>Returns this tensor as the same shape as <code class="xref py py-attr docutils literal notranslate"><span class="pre">other</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.resize_</span></code></p></td>
<td><p>Resizes <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor to the specified size.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.resize_as_.html#torch.Tensor.resize_as_" title="torch.Tensor.resize_as_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.resize_as_</span></code></a></p></td>
<td><p>Resizes the <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor to be the same size as the specified <a class="reference internal" href="generated/torch.tensor.html#torch.tensor" title="torch.tensor"><code class="xref py py-attr docutils literal notranslate"><span class="pre">tensor</span></code></a>.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.retain_grad.html#torch.Tensor.retain_grad" title="torch.Tensor.retain_grad"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.retain_grad</span></code></a></p></td>
<td><p>Enables this Tensor to have their <code class="xref py py-attr docutils literal notranslate"><span class="pre">grad</span></code> populated during <code class="xref py py-func docutils literal notranslate"><span class="pre">backward()</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.retains_grad.html#torch.Tensor.retains_grad" title="torch.Tensor.retains_grad"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.retains_grad</span></code></a></p></td>
<td><p>Is <code class="docutils literal notranslate"><span class="pre">True</span></code> if this Tensor is non-leaf and its <code class="xref py py-attr docutils literal notranslate"><span class="pre">grad</span></code> is enabled to be populated during <code class="xref py py-func docutils literal notranslate"><span class="pre">backward()</span></code>, <code class="docutils literal notranslate"><span class="pre">False</span></code> otherwise.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.roll.html#torch.Tensor.roll" title="torch.Tensor.roll"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.roll</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.roll.html#torch.roll" title="torch.roll"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.roll()</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.rot90.html#torch.Tensor.rot90" title="torch.Tensor.rot90"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.rot90</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.rot90.html#torch.rot90" title="torch.rot90"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.rot90()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.round.html#torch.Tensor.round" title="torch.Tensor.round"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.round</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.round.html#torch.round" title="torch.round"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.round()</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.round_.html#torch.Tensor.round_" title="torch.Tensor.round_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.round_</span></code></a></p></td>
<td><p>In-place version of <a class="reference internal" href="generated/torch.Tensor.round.html#torch.Tensor.round" title="torch.Tensor.round"><code class="xref py py-meth docutils literal notranslate"><span class="pre">round()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.rsqrt.html#torch.Tensor.rsqrt" title="torch.Tensor.rsqrt"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.rsqrt</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.rsqrt.html#torch.rsqrt" title="torch.rsqrt"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.rsqrt()</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.rsqrt_.html#torch.Tensor.rsqrt_" title="torch.Tensor.rsqrt_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.rsqrt_</span></code></a></p></td>
<td><p>In-place version of <a class="reference internal" href="generated/torch.Tensor.rsqrt.html#torch.Tensor.rsqrt" title="torch.Tensor.rsqrt"><code class="xref py py-meth docutils literal notranslate"><span class="pre">rsqrt()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.scatter.html#torch.Tensor.scatter" title="torch.Tensor.scatter"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.scatter</span></code></a></p></td>
<td><p>Out-of-place version of <a class="reference internal" href="generated/torch.Tensor.scatter_.html#torch.Tensor.scatter_" title="torch.Tensor.scatter_"><code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.Tensor.scatter_()</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.scatter_.html#torch.Tensor.scatter_" title="torch.Tensor.scatter_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.scatter_</span></code></a></p></td>
<td><p>Writes all values from the tensor <code class="xref py py-attr docutils literal notranslate"><span class="pre">src</span></code> into <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> at the indices specified in the <code class="xref py py-attr docutils literal notranslate"><span class="pre">index</span></code> tensor.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.scatter_add_.html#torch.Tensor.scatter_add_" title="torch.Tensor.scatter_add_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.scatter_add_</span></code></a></p></td>
<td><p>Adds all values from the tensor <code class="xref py py-attr docutils literal notranslate"><span class="pre">src</span></code> into <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> at the indices specified in the <code class="xref py py-attr docutils literal notranslate"><span class="pre">index</span></code> tensor in a similar fashion as <a class="reference internal" href="generated/torch.Tensor.scatter_.html#torch.Tensor.scatter_" title="torch.Tensor.scatter_"><code class="xref py py-meth docutils literal notranslate"><span class="pre">scatter_()</span></code></a>.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.scatter_add.html#torch.Tensor.scatter_add" title="torch.Tensor.scatter_add"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.scatter_add</span></code></a></p></td>
<td><p>Out-of-place version of <a class="reference internal" href="generated/torch.Tensor.scatter_add_.html#torch.Tensor.scatter_add_" title="torch.Tensor.scatter_add_"><code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.Tensor.scatter_add_()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.scatter_reduce_.html#torch.Tensor.scatter_reduce_" title="torch.Tensor.scatter_reduce_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.scatter_reduce_</span></code></a></p></td>
<td><p>Reduces all values from the <code class="xref py py-attr docutils literal notranslate"><span class="pre">src</span></code> tensor to the indices specified in the <code class="xref py py-attr docutils literal notranslate"><span class="pre">index</span></code> tensor in the <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor using the applied reduction defined via the <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduce</span></code> argument (<code class="xref py py-obj docutils literal notranslate"><span class="pre">&quot;sum&quot;</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">&quot;prod&quot;</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">&quot;mean&quot;</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">&quot;amax&quot;</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">&quot;amin&quot;</span></code>).</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.scatter_reduce.html#torch.Tensor.scatter_reduce" title="torch.Tensor.scatter_reduce"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.scatter_reduce</span></code></a></p></td>
<td><p>Out-of-place version of <a class="reference internal" href="generated/torch.Tensor.scatter_reduce_.html#torch.Tensor.scatter_reduce_" title="torch.Tensor.scatter_reduce_"><code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.Tensor.scatter_reduce_()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.select</span></code></p></td>
<td><p>See <a class="reference internal" href="generated/torch.Tensor.select.html#torch.select" title="torch.select"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.select()</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.select_scatter.html#torch.Tensor.select_scatter" title="torch.Tensor.select_scatter"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.select_scatter</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.select_scatter.html#torch.select_scatter" title="torch.select_scatter"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.select_scatter()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.set_.html#torch.Tensor.set_" title="torch.Tensor.set_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.set_</span></code></a></p></td>
<td><p>Sets the underlying storage, size, and strides.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.share_memory_.html#torch.Tensor.share_memory_" title="torch.Tensor.share_memory_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.share_memory_</span></code></a></p></td>
<td><p>Moves the underlying storage to shared memory.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.short.html#torch.Tensor.short" title="torch.Tensor.short"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.short</span></code></a></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">self.short()</span></code> is equivalent to <code class="docutils literal notranslate"><span class="pre">self.to(torch.int16)</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.sigmoid.html#torch.Tensor.sigmoid" title="torch.Tensor.sigmoid"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.sigmoid</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.sigmoid.html#torch.sigmoid" title="torch.sigmoid"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.sigmoid()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.sigmoid_.html#torch.Tensor.sigmoid_" title="torch.Tensor.sigmoid_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.sigmoid_</span></code></a></p></td>
<td><p>In-place version of <a class="reference internal" href="generated/torch.Tensor.sigmoid.html#torch.Tensor.sigmoid" title="torch.Tensor.sigmoid"><code class="xref py py-meth docutils literal notranslate"><span class="pre">sigmoid()</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.sign.html#torch.Tensor.sign" title="torch.Tensor.sign"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.sign</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.sign.html#torch.sign" title="torch.sign"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.sign()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.sign_.html#torch.Tensor.sign_" title="torch.Tensor.sign_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.sign_</span></code></a></p></td>
<td><p>In-place version of <a class="reference internal" href="generated/torch.Tensor.sign.html#torch.Tensor.sign" title="torch.Tensor.sign"><code class="xref py py-meth docutils literal notranslate"><span class="pre">sign()</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.signbit.html#torch.Tensor.signbit" title="torch.Tensor.signbit"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.signbit</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.signbit.html#torch.signbit" title="torch.signbit"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.signbit()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.sgn.html#torch.Tensor.sgn" title="torch.Tensor.sgn"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.sgn</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.sgn.html#torch.sgn" title="torch.sgn"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.sgn()</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.sgn_.html#torch.Tensor.sgn_" title="torch.Tensor.sgn_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.sgn_</span></code></a></p></td>
<td><p>In-place version of <a class="reference internal" href="generated/torch.Tensor.sgn.html#torch.Tensor.sgn" title="torch.Tensor.sgn"><code class="xref py py-meth docutils literal notranslate"><span class="pre">sgn()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.sin.html#torch.Tensor.sin" title="torch.Tensor.sin"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.sin</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.sin.html#torch.sin" title="torch.sin"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.sin()</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.sin_.html#torch.Tensor.sin_" title="torch.Tensor.sin_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.sin_</span></code></a></p></td>
<td><p>In-place version of <a class="reference internal" href="generated/torch.Tensor.sin.html#torch.Tensor.sin" title="torch.Tensor.sin"><code class="xref py py-meth docutils literal notranslate"><span class="pre">sin()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.sinc.html#torch.Tensor.sinc" title="torch.Tensor.sinc"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.sinc</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.sinc.html#torch.sinc" title="torch.sinc"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.sinc()</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.sinc_.html#torch.Tensor.sinc_" title="torch.Tensor.sinc_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.sinc_</span></code></a></p></td>
<td><p>In-place version of <a class="reference internal" href="generated/torch.Tensor.sinc.html#torch.Tensor.sinc" title="torch.Tensor.sinc"><code class="xref py py-meth docutils literal notranslate"><span class="pre">sinc()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.sinh.html#torch.Tensor.sinh" title="torch.Tensor.sinh"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.sinh</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.sinh.html#torch.sinh" title="torch.sinh"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.sinh()</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.sinh_.html#torch.Tensor.sinh_" title="torch.Tensor.sinh_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.sinh_</span></code></a></p></td>
<td><p>In-place version of <a class="reference internal" href="generated/torch.Tensor.sinh.html#torch.Tensor.sinh" title="torch.Tensor.sinh"><code class="xref py py-meth docutils literal notranslate"><span class="pre">sinh()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.asinh.html#torch.Tensor.asinh" title="torch.Tensor.asinh"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.asinh</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.asinh.html#torch.asinh" title="torch.asinh"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.asinh()</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.asinh_.html#torch.Tensor.asinh_" title="torch.Tensor.asinh_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.asinh_</span></code></a></p></td>
<td><p>In-place version of <a class="reference internal" href="generated/torch.Tensor.asinh.html#torch.Tensor.asinh" title="torch.Tensor.asinh"><code class="xref py py-meth docutils literal notranslate"><span class="pre">asinh()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.arcsinh.html#torch.Tensor.arcsinh" title="torch.Tensor.arcsinh"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.arcsinh</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.arcsinh.html#torch.arcsinh" title="torch.arcsinh"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.arcsinh()</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.arcsinh_.html#torch.Tensor.arcsinh_" title="torch.Tensor.arcsinh_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.arcsinh_</span></code></a></p></td>
<td><p>In-place version of <a class="reference internal" href="generated/torch.Tensor.arcsinh.html#torch.Tensor.arcsinh" title="torch.Tensor.arcsinh"><code class="xref py py-meth docutils literal notranslate"><span class="pre">arcsinh()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.shape.html#torch.Tensor.shape" title="torch.Tensor.shape"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.shape</span></code></a></p></td>
<td><p>Returns the size of the <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.size.html#torch.Tensor.size" title="torch.Tensor.size"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.size</span></code></a></p></td>
<td><p>Returns the size of the <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.slogdet.html#torch.Tensor.slogdet" title="torch.Tensor.slogdet"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.slogdet</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.slogdet.html#torch.slogdet" title="torch.slogdet"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.slogdet()</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.slice_scatter.html#torch.Tensor.slice_scatter" title="torch.Tensor.slice_scatter"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.slice_scatter</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.slice_scatter.html#torch.slice_scatter" title="torch.slice_scatter"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.slice_scatter()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.softmax.html#torch.Tensor.softmax" title="torch.Tensor.softmax"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.softmax</span></code></a></p></td>
<td><p>Alias for <a class="reference internal" href="generated/torch.nn.functional.softmax.html#torch.nn.functional.softmax" title="torch.nn.functional.softmax"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.nn.functional.softmax()</span></code></a>.</p></td>
</tr>
<tr class="row-even"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.sort</span></code></p></td>
<td><p>See <a class="reference internal" href="generated/torch.Tensor.sort.html#torch.sort" title="torch.sort"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.sort()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.split.html#torch.Tensor.split" title="torch.Tensor.split"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.split</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.split.html#torch.split" title="torch.split"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.split()</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.sparse_mask.html#torch.Tensor.sparse_mask" title="torch.Tensor.sparse_mask"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.sparse_mask</span></code></a></p></td>
<td><p>Returns a new <a class="reference internal" href="sparse.html#sparse-docs"><span class="std std-ref">sparse tensor</span></a> with values from a strided tensor <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> filtered by the indices of the sparse tensor <code class="xref py py-attr docutils literal notranslate"><span class="pre">mask</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.sparse_dim.html#torch.Tensor.sparse_dim" title="torch.Tensor.sparse_dim"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.sparse_dim</span></code></a></p></td>
<td><p>Return the number of sparse dimensions in a <a class="reference internal" href="sparse.html#sparse-docs"><span class="std std-ref">sparse tensor</span></a> <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.sqrt.html#torch.Tensor.sqrt" title="torch.Tensor.sqrt"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.sqrt</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.sqrt.html#torch.sqrt" title="torch.sqrt"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.sqrt()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.sqrt_.html#torch.Tensor.sqrt_" title="torch.Tensor.sqrt_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.sqrt_</span></code></a></p></td>
<td><p>In-place version of <a class="reference internal" href="generated/torch.Tensor.sqrt.html#torch.Tensor.sqrt" title="torch.Tensor.sqrt"><code class="xref py py-meth docutils literal notranslate"><span class="pre">sqrt()</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.square.html#torch.Tensor.square" title="torch.Tensor.square"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.square</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.square.html#torch.square" title="torch.square"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.square()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.square_.html#torch.Tensor.square_" title="torch.Tensor.square_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.square_</span></code></a></p></td>
<td><p>In-place version of <a class="reference internal" href="generated/torch.Tensor.square.html#torch.Tensor.square" title="torch.Tensor.square"><code class="xref py py-meth docutils literal notranslate"><span class="pre">square()</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.squeeze.html#torch.Tensor.squeeze" title="torch.Tensor.squeeze"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.squeeze</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.squeeze.html#torch.squeeze" title="torch.squeeze"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.squeeze()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.squeeze_.html#torch.Tensor.squeeze_" title="torch.Tensor.squeeze_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.squeeze_</span></code></a></p></td>
<td><p>In-place version of <a class="reference internal" href="generated/torch.Tensor.squeeze.html#torch.Tensor.squeeze" title="torch.Tensor.squeeze"><code class="xref py py-meth docutils literal notranslate"><span class="pre">squeeze()</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.std.html#torch.Tensor.std" title="torch.Tensor.std"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.std</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.std.html#torch.std" title="torch.std"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.std()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.stft.html#torch.Tensor.stft" title="torch.Tensor.stft"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.stft</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.stft.html#torch.stft" title="torch.stft"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.stft()</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.storage.html#torch.Tensor.storage" title="torch.Tensor.storage"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.storage</span></code></a></p></td>
<td><p>Returns the underlying <a class="reference internal" href="storage.html#torch.TypedStorage" title="torch.TypedStorage"><code class="xref py py-class docutils literal notranslate"><span class="pre">TypedStorage</span></code></a>.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.untyped_storage.html#torch.Tensor.untyped_storage" title="torch.Tensor.untyped_storage"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.untyped_storage</span></code></a></p></td>
<td><p>Returns the underlying <a class="reference internal" href="storage.html#torch.UntypedStorage" title="torch.UntypedStorage"><code class="xref py py-class docutils literal notranslate"><span class="pre">UntypedStorage</span></code></a>.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.storage_offset.html#torch.Tensor.storage_offset" title="torch.Tensor.storage_offset"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.storage_offset</span></code></a></p></td>
<td><p>Returns <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor's offset in the underlying storage in terms of number of storage elements (not bytes).</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.storage_type.html#torch.Tensor.storage_type" title="torch.Tensor.storage_type"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.storage_type</span></code></a></p></td>
<td><p>Returns the type of the underlying storage.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.stride.html#torch.Tensor.stride" title="torch.Tensor.stride"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.stride</span></code></a></p></td>
<td><p>Returns the stride of <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.sub.html#torch.Tensor.sub" title="torch.Tensor.sub"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.sub</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.sub.html#torch.sub" title="torch.sub"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.sub()</span></code></a>.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.sub_.html#torch.Tensor.sub_" title="torch.Tensor.sub_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.sub_</span></code></a></p></td>
<td><p>In-place version of <a class="reference internal" href="generated/torch.Tensor.sub.html#torch.Tensor.sub" title="torch.Tensor.sub"><code class="xref py py-meth docutils literal notranslate"><span class="pre">sub()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.subtract.html#torch.Tensor.subtract" title="torch.Tensor.subtract"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.subtract</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.subtract.html#torch.subtract" title="torch.subtract"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.subtract()</span></code></a>.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.subtract_.html#torch.Tensor.subtract_" title="torch.Tensor.subtract_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.subtract_</span></code></a></p></td>
<td><p>In-place version of <a class="reference internal" href="generated/torch.Tensor.subtract.html#torch.Tensor.subtract" title="torch.Tensor.subtract"><code class="xref py py-meth docutils literal notranslate"><span class="pre">subtract()</span></code></a>.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.sum.html#torch.Tensor.sum" title="torch.Tensor.sum"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.sum</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.sum.html#torch.sum" title="torch.sum"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.sum()</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.sum_to_size.html#torch.Tensor.sum_to_size" title="torch.Tensor.sum_to_size"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.sum_to_size</span></code></a></p></td>
<td><p>Sum <code class="docutils literal notranslate"><span class="pre">this</span></code> tensor to <code class="xref py py-attr docutils literal notranslate"><span class="pre">size</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.svd.html#torch.Tensor.svd" title="torch.Tensor.svd"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.svd</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.svd.html#torch.svd" title="torch.svd"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.svd()</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.swapaxes.html#torch.Tensor.swapaxes" title="torch.Tensor.swapaxes"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.swapaxes</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.swapaxes.html#torch.swapaxes" title="torch.swapaxes"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.swapaxes()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.swapdims.html#torch.Tensor.swapdims" title="torch.Tensor.swapdims"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.swapdims</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.swapdims.html#torch.swapdims" title="torch.swapdims"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.swapdims()</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.t.html#torch.Tensor.t" title="torch.Tensor.t"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.t</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.t.html#torch.t" title="torch.t"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.t()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.t_.html#torch.Tensor.t_" title="torch.Tensor.t_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.t_</span></code></a></p></td>
<td><p>In-place version of <a class="reference internal" href="generated/torch.Tensor.t.html#torch.Tensor.t" title="torch.Tensor.t"><code class="xref py py-meth docutils literal notranslate"><span class="pre">t()</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.tensor_split.html#torch.Tensor.tensor_split" title="torch.Tensor.tensor_split"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.tensor_split</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.tensor_split.html#torch.tensor_split" title="torch.tensor_split"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.tensor_split()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.tile.html#torch.Tensor.tile" title="torch.Tensor.tile"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.tile</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.tile.html#torch.tile" title="torch.tile"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.tile()</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.to.html#torch.Tensor.to" title="torch.Tensor.to"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.to</span></code></a></p></td>
<td><p>Performs Tensor dtype and/or device conversion.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.to_mkldnn.html#torch.Tensor.to_mkldnn" title="torch.Tensor.to_mkldnn"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.to_mkldnn</span></code></a></p></td>
<td><p>Returns a copy of the tensor in <code class="docutils literal notranslate"><span class="pre">torch.mkldnn</span></code> layout.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.take.html#torch.Tensor.take" title="torch.Tensor.take"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.take</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.take.html#torch.take" title="torch.take"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.take()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.take_along_dim.html#torch.Tensor.take_along_dim" title="torch.Tensor.take_along_dim"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.take_along_dim</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.take_along_dim.html#torch.take_along_dim" title="torch.take_along_dim"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.take_along_dim()</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.tan.html#torch.Tensor.tan" title="torch.Tensor.tan"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.tan</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.tan.html#torch.tan" title="torch.tan"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.tan()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.tan_.html#torch.Tensor.tan_" title="torch.Tensor.tan_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.tan_</span></code></a></p></td>
<td><p>In-place version of <a class="reference internal" href="generated/torch.Tensor.tan.html#torch.Tensor.tan" title="torch.Tensor.tan"><code class="xref py py-meth docutils literal notranslate"><span class="pre">tan()</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.tanh.html#torch.Tensor.tanh" title="torch.Tensor.tanh"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.tanh</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.tanh.html#torch.tanh" title="torch.tanh"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.tanh()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.tanh_.html#torch.Tensor.tanh_" title="torch.Tensor.tanh_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.tanh_</span></code></a></p></td>
<td><p>In-place version of <a class="reference internal" href="generated/torch.Tensor.tanh.html#torch.Tensor.tanh" title="torch.Tensor.tanh"><code class="xref py py-meth docutils literal notranslate"><span class="pre">tanh()</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.atanh.html#torch.Tensor.atanh" title="torch.Tensor.atanh"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.atanh</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.atanh.html#torch.atanh" title="torch.atanh"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.atanh()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.atanh_.html#torch.Tensor.atanh_" title="torch.Tensor.atanh_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.atanh_</span></code></a></p></td>
<td><p>In-place version of <a class="reference internal" href="generated/torch.Tensor.atanh.html#torch.Tensor.atanh" title="torch.Tensor.atanh"><code class="xref py py-meth docutils literal notranslate"><span class="pre">atanh()</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.arctanh.html#torch.Tensor.arctanh" title="torch.Tensor.arctanh"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.arctanh</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.arctanh.html#torch.arctanh" title="torch.arctanh"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.arctanh()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.arctanh_.html#torch.Tensor.arctanh_" title="torch.Tensor.arctanh_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.arctanh_</span></code></a></p></td>
<td><p>In-place version of <a class="reference internal" href="generated/torch.Tensor.arctanh.html#torch.Tensor.arctanh" title="torch.Tensor.arctanh"><code class="xref py py-meth docutils literal notranslate"><span class="pre">arctanh()</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.tolist.html#torch.Tensor.tolist" title="torch.Tensor.tolist"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.tolist</span></code></a></p></td>
<td><p>Returns the tensor as a (nested) list.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.topk</span></code></p></td>
<td><p>See <a class="reference internal" href="generated/torch.Tensor.topk.html#torch.topk" title="torch.topk"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.topk()</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.to_dense.html#torch.Tensor.to_dense" title="torch.Tensor.to_dense"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.to_dense</span></code></a></p></td>
<td><p>Creates a strided copy of <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> if <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> is not a strided tensor, otherwise returns <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.to_sparse.html#torch.Tensor.to_sparse" title="torch.Tensor.to_sparse"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.to_sparse</span></code></a></p></td>
<td><p>Returns a sparse copy of the tensor.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.to_sparse_csr.html#torch.Tensor.to_sparse_csr" title="torch.Tensor.to_sparse_csr"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.to_sparse_csr</span></code></a></p></td>
<td><p>Convert a tensor to compressed row storage format (CSR).</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.to_sparse_csc.html#torch.Tensor.to_sparse_csc" title="torch.Tensor.to_sparse_csc"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.to_sparse_csc</span></code></a></p></td>
<td><p>Convert a tensor to compressed column storage (CSC) format.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.to_sparse_bsr.html#torch.Tensor.to_sparse_bsr" title="torch.Tensor.to_sparse_bsr"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.to_sparse_bsr</span></code></a></p></td>
<td><p>Convert a tensor to a block sparse row (BSR) storage format of given blocksize.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.to_sparse_bsc.html#torch.Tensor.to_sparse_bsc" title="torch.Tensor.to_sparse_bsc"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.to_sparse_bsc</span></code></a></p></td>
<td><p>Convert a tensor to a block sparse column (BSC) storage format of given blocksize.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.trace.html#torch.Tensor.trace" title="torch.Tensor.trace"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.trace</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.trace.html#torch.trace" title="torch.trace"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.trace()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.transpose.html#torch.Tensor.transpose" title="torch.Tensor.transpose"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.transpose</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.transpose.html#torch.transpose" title="torch.transpose"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.transpose()</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.transpose_.html#torch.Tensor.transpose_" title="torch.Tensor.transpose_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.transpose_</span></code></a></p></td>
<td><p>In-place version of <a class="reference internal" href="generated/torch.Tensor.transpose.html#torch.Tensor.transpose" title="torch.Tensor.transpose"><code class="xref py py-meth docutils literal notranslate"><span class="pre">transpose()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.triangular_solve.html#torch.Tensor.triangular_solve" title="torch.Tensor.triangular_solve"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.triangular_solve</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.triangular_solve.html#torch.triangular_solve" title="torch.triangular_solve"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.triangular_solve()</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.tril.html#torch.Tensor.tril" title="torch.Tensor.tril"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.tril</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.tril.html#torch.tril" title="torch.tril"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.tril()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.tril_.html#torch.Tensor.tril_" title="torch.Tensor.tril_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.tril_</span></code></a></p></td>
<td><p>In-place version of <a class="reference internal" href="generated/torch.Tensor.tril.html#torch.Tensor.tril" title="torch.Tensor.tril"><code class="xref py py-meth docutils literal notranslate"><span class="pre">tril()</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.triu.html#torch.Tensor.triu" title="torch.Tensor.triu"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.triu</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.triu.html#torch.triu" title="torch.triu"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.triu()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.triu_.html#torch.Tensor.triu_" title="torch.Tensor.triu_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.triu_</span></code></a></p></td>
<td><p>In-place version of <a class="reference internal" href="generated/torch.Tensor.triu.html#torch.Tensor.triu" title="torch.Tensor.triu"><code class="xref py py-meth docutils literal notranslate"><span class="pre">triu()</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.true_divide.html#torch.Tensor.true_divide" title="torch.Tensor.true_divide"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.true_divide</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.true_divide.html#torch.true_divide" title="torch.true_divide"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.true_divide()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.true_divide_.html#torch.Tensor.true_divide_" title="torch.Tensor.true_divide_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.true_divide_</span></code></a></p></td>
<td><p>In-place version of <a class="reference internal" href="generated/torch.Tensor.true_divide_.html#torch.Tensor.true_divide_" title="torch.Tensor.true_divide_"><code class="xref py py-meth docutils literal notranslate"><span class="pre">true_divide_()</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.trunc.html#torch.Tensor.trunc" title="torch.Tensor.trunc"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.trunc</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.trunc.html#torch.trunc" title="torch.trunc"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.trunc()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.trunc_.html#torch.Tensor.trunc_" title="torch.Tensor.trunc_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.trunc_</span></code></a></p></td>
<td><p>In-place version of <a class="reference internal" href="generated/torch.Tensor.trunc.html#torch.Tensor.trunc" title="torch.Tensor.trunc"><code class="xref py py-meth docutils literal notranslate"><span class="pre">trunc()</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.type.html#torch.Tensor.type" title="torch.Tensor.type"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.type</span></code></a></p></td>
<td><p>Returns the type if <cite>dtype</cite> is not provided, else casts this object to the specified type.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.type_as.html#torch.Tensor.type_as" title="torch.Tensor.type_as"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.type_as</span></code></a></p></td>
<td><p>Returns this tensor cast to the type of the given tensor.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.unbind.html#torch.Tensor.unbind" title="torch.Tensor.unbind"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.unbind</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.unbind.html#torch.unbind" title="torch.unbind"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.unbind()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.unflatten.html#torch.Tensor.unflatten" title="torch.Tensor.unflatten"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.unflatten</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.unflatten.html#torch.unflatten" title="torch.unflatten"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.unflatten()</span></code></a>.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.unfold.html#torch.Tensor.unfold" title="torch.Tensor.unfold"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.unfold</span></code></a></p></td>
<td><p>Returns a view of the original tensor which contains all slices of size <code class="xref py py-attr docutils literal notranslate"><span class="pre">size</span></code> from <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor in the dimension <code class="xref py py-attr docutils literal notranslate"><span class="pre">dimension</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.uniform_.html#torch.Tensor.uniform_" title="torch.Tensor.uniform_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.uniform_</span></code></a></p></td>
<td><p>Fills <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor with numbers sampled from the continuous uniform distribution:</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.unique.html#torch.Tensor.unique" title="torch.Tensor.unique"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.unique</span></code></a></p></td>
<td><p>Returns the unique elements of the input tensor.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.unique_consecutive.html#torch.Tensor.unique_consecutive" title="torch.Tensor.unique_consecutive"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.unique_consecutive</span></code></a></p></td>
<td><p>Eliminates all but the first element from every consecutive group of equivalent elements.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.unsqueeze.html#torch.Tensor.unsqueeze" title="torch.Tensor.unsqueeze"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.unsqueeze</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.unsqueeze.html#torch.unsqueeze" title="torch.unsqueeze"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.unsqueeze()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.unsqueeze_.html#torch.Tensor.unsqueeze_" title="torch.Tensor.unsqueeze_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.unsqueeze_</span></code></a></p></td>
<td><p>In-place version of <a class="reference internal" href="generated/torch.Tensor.unsqueeze.html#torch.Tensor.unsqueeze" title="torch.Tensor.unsqueeze"><code class="xref py py-meth docutils literal notranslate"><span class="pre">unsqueeze()</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.values.html#torch.Tensor.values" title="torch.Tensor.values"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.values</span></code></a></p></td>
<td><p>Return the values tensor of a <a class="reference internal" href="sparse.html#sparse-coo-docs"><span class="std std-ref">sparse COO tensor</span></a>.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.var.html#torch.Tensor.var" title="torch.Tensor.var"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.var</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.var.html#torch.var" title="torch.var"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.var()</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.vdot.html#torch.Tensor.vdot" title="torch.Tensor.vdot"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.vdot</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.vdot.html#torch.vdot" title="torch.vdot"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.vdot()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.view</span></code></p></td>
<td><p>Returns a new tensor with the same data as the <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor but of a different <code class="xref py py-attr docutils literal notranslate"><span class="pre">shape</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.view_as.html#torch.Tensor.view_as" title="torch.Tensor.view_as"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.view_as</span></code></a></p></td>
<td><p>View this tensor as the same size as <code class="xref py py-attr docutils literal notranslate"><span class="pre">other</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.vsplit.html#torch.Tensor.vsplit" title="torch.Tensor.vsplit"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.vsplit</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.vsplit.html#torch.vsplit" title="torch.vsplit"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.vsplit()</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.where.html#torch.Tensor.where" title="torch.Tensor.where"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.where</span></code></a></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">self.where(condition,</span> <span class="pre">y)</span></code> is equivalent to <code class="docutils literal notranslate"><span class="pre">torch.where(condition,</span> <span class="pre">self,</span> <span class="pre">y)</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.xlogy.html#torch.Tensor.xlogy" title="torch.Tensor.xlogy"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.xlogy</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.xlogy.html#torch.xlogy" title="torch.xlogy"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.xlogy()</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.xlogy_.html#torch.Tensor.xlogy_" title="torch.Tensor.xlogy_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.xlogy_</span></code></a></p></td>
<td><p>In-place version of <a class="reference internal" href="generated/torch.Tensor.xlogy.html#torch.Tensor.xlogy" title="torch.Tensor.xlogy"><code class="xref py py-meth docutils literal notranslate"><span class="pre">xlogy()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.xpu.html#torch.Tensor.xpu" title="torch.Tensor.xpu"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.xpu</span></code></a></p></td>
<td><p>Returns a copy of this object in XPU memory.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.zero_.html#torch.Tensor.zero_" title="torch.Tensor.zero_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.zero_</span></code></a></p></td>
<td><p>Fills <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor with zeros.</p></td>
</tr>
</tbody>
</table>
</div>
</section>
</section>


                </article>
              
              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="generated/torch.compile.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">torch.compile</p>
      </div>
    </a>
    <a class="right-next"
       href="generated/torch.Tensor.new_tensor.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">torch.Tensor.new_tensor</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <dialog id="pst-secondary-sidebar-modal"></dialog>
                <div id="pst-secondary-sidebar" class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
<div
    id="pst-page-navigation-heading-2"
    class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> On this page
  </div>
  <nav class="bd-toc-nav page-toc" aria-labelledby="pst-page-navigation-heading-2">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#data-types">Data types</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#initializing-and-basic-operations">Initializing and basic operations</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#tensor-class-reference">Tensor class reference</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.Tensor"><code class="docutils literal notranslate"><span class="pre">Tensor</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.Tensor.__init__"><code class="docutils literal notranslate"><span class="pre">Tensor.__init__()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.Tensor.T"><code class="docutils literal notranslate"><span class="pre">Tensor.T</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.Tensor.H"><code class="docutils literal notranslate"><span class="pre">Tensor.H</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.Tensor.mT"><code class="docutils literal notranslate"><span class="pre">Tensor.mT</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.Tensor.mH"><code class="docutils literal notranslate"><span class="pre">Tensor.mH</span></code></a></li>
</ul>
</li>
</ul>
  </nav></div>

  <div class="sidebar-secondary-item">

  
  <div class="tocsection editthispage">
    <a href="https://github.com/pytorch/pytorch/edit/main/docs/source/python-api/tensors.rst">
      <i class="fa-solid fa-pencil"></i>
      
      
        
          Edit on GitHub
        
      
    </a>
  </div>
</div>

  <div class="sidebar-secondary-item">
    <div class="tocsection sourcelink">
      <a href="../_sources/python-api/tensors.rst.txt">
        <i class="fa-solid fa-file-lines"></i> Show Source
      </a>
    </div>
</div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
          </footer>
        
      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script defer src="../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf"></script>
<script defer src="../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf"></script>


  <footer class="bd-footer">
<div class="bd-footer__inner bd-page-width">
  
    <div class="footer-items__start">
      
        <div class="footer-item">

  <p class="copyright">
    
       Copyright PyTorch Contributors.
      <br/>
    
  </p>
</div>
      
        <div class="footer-item">

  <p class="sphinx-version">
    Created using <a href="https://www.sphinx-doc.org/">Sphinx</a> 8.1.0.
    <br/>
  </p>
</div>
      
    </div>
  
  
  
    <div class="footer-items__end">
      
        <div class="footer-item">
<p class="theme-version">
  <!-- # L10n: Setting the PST URL as an argument as this does not need to be localized -->
  Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.16.1.
</p></div>
      
    </div>
  
</div>

  </footer>
  </body>
</html>