
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>TorchScript Language Reference &#8212; PyTorch main documentation</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  <!--
    this give us a css class that will be invisible only if js is disabled
  -->
  <noscript>
    <style>
      .pst-js-only { display: none !important; }

    </style>
  </noscript>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=949a1ff5" />
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/katex.min.css" />
    <link rel="stylesheet" type="text/css" href="../_static/katex-math.css?v=91adb8b6" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="../_static/css/jit.css?v=8de1ea5d" />
    <link rel="stylesheet" type="text/css" href="../_static/css/custom.css?v=3ccf5357" />
    <link rel="stylesheet" type="text/css" href="../_static/css/custom2.css?v=3539c01c" />
  
  <!-- So that users can add custom icons -->
  <script src="../_static/scripts/fontawesome.js?digest=8878045cc6db502f8baf"></script>
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf" />

    <script src="../_static/documentation_options.js?v=a8da1a53"></script>
    <script src="../_static/doctools.js?v=9bcbadda"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=940804e7"></script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'python-api/jit_language_reference_v2';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="torch.jit.script" href="generated/torch.jit.script.html" />
    <link rel="prev" title="TorchScript Language Reference" href="jit_language_reference.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  <meta name="docsearch:version" content="main (2.6.0 )" />
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>

          <li class="main-menu-item">
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Learn
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="">
                  <span class=dropdown-title>Get Started</span>
                  <p>Run PyTorch locally or get started quickly with one of the supported cloud platforms</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Tutorials</span>
                  <p>Whats new in PyTorch tutorials</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Learn the Basics</span>
                  <p>Familiarize yourself with PyTorch concepts and modules</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">PyTorch Recipes</span>
                  <p>Bite-size, ready-to-deploy PyTorch code examples</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Intro to PyTorch - YouTube Series</span>
                  <p>Master PyTorch basics with our engaging YouTube tutorial series</p>
                </a>
              </div>
            </div>
          </li>

          <li>
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Ecosystem
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Tools</span>
                  <p>Learn about the tools and frameworks in the PyTorch Ecosystem</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class=dropdown-title>Community</span>
                  <p>Join the PyTorch developer community to contribute, learn, and get your questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="" target="_blank">
                  <span class=dropdown-title>Forums</span>
                  <p>A place to discuss PyTorch code, issues, install, research</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class=dropdown-title>Developer Resources</span>
                  <p>Find resources and get questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Contributor Awards - 2024</span>
                  <p>Award winners announced at this year's PyTorch Conference</p>
                </a>
              </div>
            </div>
          </li>

          <li>
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Edge
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">About PyTorch Edge</span>
                  <p>Build innovative and privacy-aware AI experiences for edge devices</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">ExecuTorch</span>
                  <p>End-to-end solution for enabling on-device inference capabilities across mobile and edge devices</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">ExecuTorch Docs</span>
                </a>
              </div>
            </div>  
          </li>

          <li class="main-menu-item">
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Docs
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">PyTorch</span>
                  <p>Explore the documentation for comprehensive guidance on how to use PyTorch</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">PyTorch Domains</span>
                  <p>Read the PyTorch Domains documentation to learn more about domain-specific libraries</p>
                </a>
              </div>
            </div>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Blogs & News 
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">PyTorch Blog</span>
                  <p>Catch up on the latest technical news and happenings</p>
                </a>
                 <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Community Blog</span>
                  <p>Stories from the PyTorch ecosystem</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Videos</span>
                  <p>Learn about the latest PyTorch tutorials, new, and more </p>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Community Stories</span>
                  <p>Learn how our community solves real, everyday machine learning problems with PyTorch</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Events</span>
                  <p>Find events, webinars, and podcasts</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Newsletter</span>
                  <p>Stay up-to-date with the latest updates</p>
                </a>
            </div>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                About
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">PyTorch Foundation</span>
                  <p>Learn more about the PyTorch Foundation</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Governing Board</span>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Cloud Credit Program</span>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Technical Advisory Council</span>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Staff</span>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Contact Us</span>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
            <div class="no-dropdown">
              <a href="" data-cta="join">
                Become a Member
              </a>
            </div>
          </li>
          <li>
           <div class="main-menu-item">
             <a href="" class="github-icon">
             </a>
           </div>
          </li>
          <!--- TODO: This block adds the search icon to the nav bar. We will enable it later. 
          <li>
            <div class="main-menu-item">
             <a href="" class="search-icon">
             </a>
            </div>
          </li>
          --->
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>
  </div>
</div>

  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <dialog id="pst-search-dialog">
    
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </dialog>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
<div class="bd-header__inner bd-page-width">
  <button class="pst-navbar-icon sidebar-toggle primary-toggle" aria-label="Site navigation">
    <span class="fa-solid fa-bars"></span>
  </button>
  
  
  <div class=" navbar-header-items__start">
    
      <div class="navbar-item">
<div class="version-switcher__container dropdown pst-js-only">
  <button id="pst-version-switcher-button-2"
    type="button"
    class="version-switcher__button btn btn-sm dropdown-toggle"
    data-bs-toggle="dropdown"
    aria-haspopup="listbox"
    aria-controls="pst-version-switcher-list-2"
    aria-label="Version switcher list"
  >
    Choose version  <!-- this text may get changed later by javascript -->
    <span class="caret"></span>
  </button>
  <div id="pst-version-switcher-list-2"
    class="version-switcher__menu dropdown-menu list-group-flush py-0"
    role="listbox" aria-labelledby="pst-version-switcher-button-2">
    <!-- dropdown will be populated by javascript on page load -->
  </div>
</div></div>
    
      <div class="navbar-item">

  
    
  

<a class="navbar-brand logo" href="../index.html">
  
  
  
  
  
  
    <p class="title logo__title">Home</p>
  
</a></div>
    
  </div>
  
  <div class=" navbar-header-items">
    
    <div class="me-auto navbar-header-items__center">
      
        <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item current active">
  <a class="nav-link nav-internal" href="index.html">
    Python API
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../notes/index.html">
    Developer Notes
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../community/index.html">
    Community
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../lang-bindings/index.html">
    Language Bindings
  </a>
</li>

  </ul>
</nav></div>
      
    </div>
    
    
    <div class="navbar-header-items__end">
      
        <div class="navbar-item navbar-persistent--container">
          

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button>
        </div>
      
      
        <div class="navbar-item">

<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button></div>
      
        <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://x.com/PyTorch" title="X" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-x-twitter fa-lg" aria-hidden="true"></i>
            <span class="sr-only">X</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/pytorch/pytorch" title="GitHub" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">GitHub</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://pypi.org/project/torch" title="PyPI" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-custom fa-pypi fa-lg" aria-hidden="true"></i>
            <span class="sr-only">PyPI</span></a>
        </li>
</ul></div>
      
    </div>
    
  </div>
  
  
    <div class="navbar-persistent--mobile">

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button>
    </div>
  

  
    <button class="pst-navbar-icon sidebar-toggle secondary-toggle" aria-label="On this page">
      <span class="fa-solid fa-outdent"></span>
    </button>
  
</div>

    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <dialog id="pst-primary-sidebar-modal"></dialog>
      <div id="pst-primary-sidebar" class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
      <div class="sidebar-header-items__center">
        
          
          
            <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item current active">
  <a class="nav-link nav-internal" href="index.html">
    Python API
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../notes/index.html">
    Developer Notes
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../community/index.html">
    Community
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../lang-bindings/index.html">
    Language Bindings
  </a>
</li>

  </ul>
</nav></div>
          
        
      </div>
    
    
    
      <div class="sidebar-header-items__end">
        
          <div class="navbar-item">

<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button></div>
        
          <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://x.com/PyTorch" title="X" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-x-twitter fa-lg" aria-hidden="true"></i>
            <span class="sr-only">X</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/pytorch/pytorch" title="GitHub" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">GitHub</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://pypi.org/project/torch" title="PyPI" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-custom fa-pypi fa-lg" aria-hidden="true"></i>
            <span class="sr-only">PyPI</span></a>
        </li>
</ul></div>
        
      </div>
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
<nav class="bd-docs-nav bd-links"
     aria-label="Section Navigation">
  <p class="bd-links__title" role="heading" aria-level="1">Section Navigation</p>
  <div class="bd-toc-item navbar-nav"><p aria-level="2" class="caption" role="heading"><span class="caption-text">Core Modules</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="torch.html">torch</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.is_tensor.html">torch.is_tensor</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.is_storage.html">torch.is_storage</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.is_complex.html">torch.is_complex</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.is_conj.html">torch.is_conj</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.is_floating_point.html">torch.is_floating_point</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.is_nonzero.html">torch.is_nonzero</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.set_default_dtype.html">torch.set_default_dtype</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.get_default_dtype.html">torch.get_default_dtype</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.set_default_device.html">torch.set_default_device</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.get_default_device.html">torch.get_default_device</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.set_default_tensor_type.html">torch.set_default_tensor_type</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.numel.html">torch.numel</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.set_printoptions.html">torch.set_printoptions</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.set_flush_denormal.html">torch.set_flush_denormal</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.tensor.html">torch.tensor</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.sparse_coo_tensor.html">torch.sparse_coo_tensor</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.sparse_csr_tensor.html">torch.sparse_csr_tensor</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.sparse_csc_tensor.html">torch.sparse_csc_tensor</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.sparse_bsr_tensor.html">torch.sparse_bsr_tensor</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.sparse_bsc_tensor.html">torch.sparse_bsc_tensor</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.asarray.html">torch.asarray</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.as_tensor.html">torch.as_tensor</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.as_strided.html">torch.as_strided</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.from_file.html">torch.from_file</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.from_numpy.html">torch.from_numpy</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.from_dlpack.html">torch.from_dlpack</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.frombuffer.html">torch.frombuffer</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.zeros.html">torch.zeros</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.zeros_like.html">torch.zeros_like</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.ones.html">torch.ones</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.ones_like.html">torch.ones_like</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.arange.html">torch.arange</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.range.html">torch.range</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.linspace.html">torch.linspace</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.logspace.html">torch.logspace</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.eye.html">torch.eye</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.empty.html">torch.empty</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.empty_like.html">torch.empty_like</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.empty_strided.html">torch.empty_strided</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.full.html">torch.full</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.full_like.html">torch.full_like</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.quantize_per_tensor.html">quantize_per_tensor</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.quantize_per_channel.html">quantize_per_channel</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.dequantize.html">dequantize</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.complex.html">torch.complex</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.polar.html">torch.polar</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.heaviside.html">torch.heaviside</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.adjoint.html">torch.adjoint</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.argwhere.html">torch.argwhere</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cat.html">torch.cat</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.concat.html">torch.concat</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.concatenate.html">torch.concatenate</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.conj.html">torch.conj</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.chunk.html">torch.chunk</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.dsplit.html">torch.dsplit</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.column_stack.html">torch.column_stack</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.dstack.html">torch.dstack</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.gather.html">torch.gather</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.hsplit.html">torch.hsplit</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.hstack.html">torch.hstack</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.index_add.html">torch.index_add</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.index_copy.html">torch.index_copy</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.index_reduce.html">torch.index_reduce</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.index_select.html">torch.index_select</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.masked_select.html">torch.masked_select</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.movedim.html">torch.movedim</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.moveaxis.html">torch.moveaxis</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.narrow.html">torch.narrow</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.narrow_copy.html">torch.narrow_copy</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nonzero.html">torch.nonzero</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.permute.html">torch.permute</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.reshape.html">torch.reshape</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.row_stack.html">torch.row_stack</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.select.html">torch.select</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.scatter.html">torch.scatter</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.diagonal_scatter.html">torch.diagonal_scatter</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.select_scatter.html">torch.select_scatter</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.slice_scatter.html">torch.slice_scatter</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.scatter_add.html">torch.scatter_add</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.scatter_reduce.html">torch.scatter_reduce</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.split.html">torch.split</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.squeeze.html">torch.squeeze</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.stack.html">torch.stack</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.swapaxes.html">torch.swapaxes</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.swapdims.html">torch.swapdims</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.t.html">torch.t</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.take.html">torch.take</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.take_along_dim.html">torch.take_along_dim</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.tensor_split.html">torch.tensor_split</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.tile.html">torch.tile</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.transpose.html">torch.transpose</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.unbind.html">torch.unbind</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.unravel_index.html">torch.unravel_index</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.unsqueeze.html">torch.unsqueeze</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.vsplit.html">torch.vsplit</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.vstack.html">torch.vstack</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.where.html">torch.where</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Stream.html">Stream</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Event.html">Event</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Generator.html">Generator</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.seed.html">torch.seed</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.manual_seed.html">torch.manual_seed</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.initial_seed.html">torch.initial_seed</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.get_rng_state.html">torch.get_rng_state</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.set_rng_state.html">torch.set_rng_state</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.bernoulli.html">torch.bernoulli</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.multinomial.html">torch.multinomial</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.normal.html">torch.normal</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.poisson.html">torch.poisson</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.rand.html">torch.rand</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.rand_like.html">torch.rand_like</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.randint.html">torch.randint</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.randint_like.html">torch.randint_like</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.randn.html">torch.randn</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.randn_like.html">torch.randn_like</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.randperm.html">torch.randperm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.quasirandom.SobolEngine.html">SobolEngine</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.save.html">torch.save</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.load.html">torch.load</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.get_num_threads.html">torch.get_num_threads</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.set_num_threads.html">torch.set_num_threads</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.get_num_interop_threads.html">torch.get_num_interop_threads</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.set_num_interop_threads.html">torch.set_num_interop_threads</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.no_grad.html">no_grad</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.enable_grad.html">enable_grad</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.grad_mode.set_grad_enabled.html">set_grad_enabled</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.is_grad_enabled.html">torch.is_grad_enabled</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.grad_mode.inference_mode.html">inference_mode</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.is_inference_mode_enabled.html">torch.is_inference_mode_enabled</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.abs.html">torch.abs</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.absolute.html">torch.absolute</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.acos.html">torch.acos</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.arccos.html">torch.arccos</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.acosh.html">torch.acosh</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.arccosh.html">torch.arccosh</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.add.html">torch.add</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.addcdiv.html">torch.addcdiv</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.addcmul.html">torch.addcmul</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.angle.html">torch.angle</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.asin.html">torch.asin</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.arcsin.html">torch.arcsin</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.asinh.html">torch.asinh</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.arcsinh.html">torch.arcsinh</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.atan.html">torch.atan</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.arctan.html">torch.arctan</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.atanh.html">torch.atanh</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.arctanh.html">torch.arctanh</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.atan2.html">torch.atan2</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.arctan2.html">torch.arctan2</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.bitwise_not.html">torch.bitwise_not</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.bitwise_and.html">torch.bitwise_and</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.bitwise_or.html">torch.bitwise_or</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.bitwise_xor.html">torch.bitwise_xor</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.bitwise_left_shift.html">torch.bitwise_left_shift</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.bitwise_right_shift.html">torch.bitwise_right_shift</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.ceil.html">torch.ceil</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.clamp.html">torch.clamp</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.clip.html">torch.clip</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.conj_physical.html">torch.conj_physical</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.copysign.html">torch.copysign</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cos.html">torch.cos</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cosh.html">torch.cosh</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.deg2rad.html">torch.deg2rad</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.div.html">torch.div</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.divide.html">torch.divide</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.digamma.html">torch.digamma</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.erf.html">torch.erf</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.erfc.html">torch.erfc</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.erfinv.html">torch.erfinv</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.exp.html">torch.exp</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.exp2.html">torch.exp2</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.expm1.html">torch.expm1</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fake_quantize_per_channel_affine.html">torch.fake_quantize_per_channel_affine</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fake_quantize_per_tensor_affine.html">torch.fake_quantize_per_tensor_affine</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fix.html">torch.fix</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.float_power.html">torch.float_power</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.floor.html">torch.floor</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.floor_divide.html">torch.floor_divide</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fmod.html">torch.fmod</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.frac.html">torch.frac</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.frexp.html">torch.frexp</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.gradient.html">torch.gradient</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.imag.html">torch.imag</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.ldexp.html">torch.ldexp</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.lerp.html">torch.lerp</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.lgamma.html">torch.lgamma</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.log.html">torch.log</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.log10.html">torch.log10</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.log1p.html">torch.log1p</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.log2.html">torch.log2</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.logaddexp.html">torch.logaddexp</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.logaddexp2.html">torch.logaddexp2</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.logical_and.html">torch.logical_and</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.logical_not.html">torch.logical_not</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.logical_or.html">torch.logical_or</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.logical_xor.html">torch.logical_xor</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.logit.html">torch.logit</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.hypot.html">torch.hypot</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.i0.html">torch.i0</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.igamma.html">torch.igamma</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.igammac.html">torch.igammac</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mul.html">torch.mul</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.multiply.html">torch.multiply</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mvlgamma.html">torch.mvlgamma</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nan_to_num.html">torch.nan_to_num</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.neg.html">torch.neg</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.negative.html">torch.negative</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nextafter.html">torch.nextafter</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.polygamma.html">torch.polygamma</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.positive.html">torch.positive</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.pow.html">torch.pow</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.quantized_batch_norm.html">torch.quantized_batch_norm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.quantized_max_pool1d.html">torch.quantized_max_pool1d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.quantized_max_pool2d.html">torch.quantized_max_pool2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.rad2deg.html">torch.rad2deg</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.real.html">torch.real</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.reciprocal.html">torch.reciprocal</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.remainder.html">torch.remainder</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.round.html">torch.round</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.rsqrt.html">torch.rsqrt</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.sigmoid.html">torch.sigmoid</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.sign.html">torch.sign</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.sgn.html">torch.sgn</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.signbit.html">torch.signbit</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.sin.html">torch.sin</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.sinc.html">torch.sinc</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.sinh.html">torch.sinh</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.softmax.html">torch.softmax</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.sqrt.html">torch.sqrt</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.square.html">torch.square</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.sub.html">torch.sub</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.subtract.html">torch.subtract</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.tan.html">torch.tan</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.tanh.html">torch.tanh</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.true_divide.html">torch.true_divide</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.trunc.html">torch.trunc</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xlogy.html">torch.xlogy</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.argmax.html">torch.argmax</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.argmin.html">torch.argmin</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.amax.html">torch.amax</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.amin.html">torch.amin</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.aminmax.html">torch.aminmax</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.all.html">torch.all</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.any.html">torch.any</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.max.html">torch.max</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.min.html">torch.min</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.dist.html">torch.dist</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.logsumexp.html">torch.logsumexp</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mean.html">torch.mean</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nanmean.html">torch.nanmean</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.median.html">torch.median</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nanmedian.html">torch.nanmedian</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mode.html">torch.mode</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.norm.html">torch.norm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nansum.html">torch.nansum</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.prod.html">torch.prod</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.quantile.html">torch.quantile</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nanquantile.html">torch.nanquantile</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.std.html">torch.std</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.std_mean.html">torch.std_mean</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.sum.html">torch.sum</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.unique.html">torch.unique</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.unique_consecutive.html">torch.unique_consecutive</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.var.html">torch.var</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.var_mean.html">torch.var_mean</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.count_nonzero.html">torch.count_nonzero</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.allclose.html">torch.allclose</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.argsort.html">torch.argsort</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.eq.html">torch.eq</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.equal.html">torch.equal</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.ge.html">torch.ge</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.greater_equal.html">torch.greater_equal</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.gt.html">torch.gt</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.greater.html">torch.greater</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.isclose.html">torch.isclose</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.isfinite.html">torch.isfinite</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.isin.html">torch.isin</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.isinf.html">torch.isinf</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.isposinf.html">torch.isposinf</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.isneginf.html">torch.isneginf</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.isnan.html">torch.isnan</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.isreal.html">torch.isreal</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.kthvalue.html">torch.kthvalue</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.le.html">torch.le</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.less_equal.html">torch.less_equal</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.lt.html">torch.lt</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.less.html">torch.less</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.maximum.html">torch.maximum</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.minimum.html">torch.minimum</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fmax.html">torch.fmax</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fmin.html">torch.fmin</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.ne.html">torch.ne</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.not_equal.html">torch.not_equal</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.sort.html">torch.sort</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.topk.html">torch.topk</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.msort.html">torch.msort</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.stft.html">torch.stft</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.istft.html">torch.istft</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.bartlett_window.html">torch.bartlett_window</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.blackman_window.html">torch.blackman_window</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.hamming_window.html">torch.hamming_window</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.hann_window.html">torch.hann_window</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.kaiser_window.html">torch.kaiser_window</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.atleast_1d.html">torch.atleast_1d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.atleast_2d.html">torch.atleast_2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.atleast_3d.html">torch.atleast_3d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.bincount.html">torch.bincount</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.block_diag.html">torch.block_diag</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.broadcast_tensors.html">torch.broadcast_tensors</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.broadcast_to.html">torch.broadcast_to</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.broadcast_shapes.html">torch.broadcast_shapes</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.bucketize.html">torch.bucketize</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cartesian_prod.html">torch.cartesian_prod</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cdist.html">torch.cdist</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.clone.html">torch.clone</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.combinations.html">torch.combinations</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.corrcoef.html">torch.corrcoef</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cov.html">torch.cov</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cross.html">torch.cross</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cummax.html">torch.cummax</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cummin.html">torch.cummin</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cumprod.html">torch.cumprod</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cumsum.html">torch.cumsum</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.diag.html">torch.diag</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.diag_embed.html">torch.diag_embed</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.diagflat.html">torch.diagflat</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.diagonal.html">torch.diagonal</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.diff.html">torch.diff</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.einsum.html">torch.einsum</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.flatten.html">torch.flatten</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.flip.html">torch.flip</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fliplr.html">torch.fliplr</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.flipud.html">torch.flipud</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.kron.html">torch.kron</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.rot90.html">torch.rot90</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.gcd.html">torch.gcd</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.histc.html">torch.histc</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.histogram.html">torch.histogram</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.histogramdd.html">torch.histogramdd</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.meshgrid.html">torch.meshgrid</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.lcm.html">torch.lcm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.logcumsumexp.html">torch.logcumsumexp</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.ravel.html">torch.ravel</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.renorm.html">torch.renorm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.repeat_interleave.html">torch.repeat_interleave</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.roll.html">torch.roll</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.searchsorted.html">torch.searchsorted</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.tensordot.html">torch.tensordot</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.trace.html">torch.trace</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.tril.html">torch.tril</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.tril_indices.html">torch.tril_indices</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.triu.html">torch.triu</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.triu_indices.html">torch.triu_indices</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.unflatten.html">torch.unflatten</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.vander.html">torch.vander</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.view_as_real.html">torch.view_as_real</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.view_as_complex.html">torch.view_as_complex</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.resolve_conj.html">torch.resolve_conj</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.resolve_neg.html">torch.resolve_neg</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.addbmm.html">torch.addbmm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.addmm.html">torch.addmm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.addmv.html">torch.addmv</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.addr.html">torch.addr</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.baddbmm.html">torch.baddbmm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.bmm.html">torch.bmm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.chain_matmul.html">torch.chain_matmul</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cholesky.html">torch.cholesky</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cholesky_inverse.html">torch.cholesky_inverse</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cholesky_solve.html">torch.cholesky_solve</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.dot.html">torch.dot</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.geqrf.html">torch.geqrf</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.ger.html">torch.ger</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.inner.html">torch.inner</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.inverse.html">torch.inverse</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.det.html">torch.det</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.logdet.html">torch.logdet</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.slogdet.html">torch.slogdet</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.lu.html">torch.lu</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.lu_solve.html">torch.lu_solve</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.lu_unpack.html">torch.lu_unpack</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.matmul.html">torch.matmul</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.matrix_power.html">torch.matrix_power</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.matrix_exp.html">torch.matrix_exp</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mm.html">torch.mm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mv.html">torch.mv</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.orgqr.html">torch.orgqr</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.ormqr.html">torch.ormqr</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.outer.html">torch.outer</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.pinverse.html">torch.pinverse</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.qr.html">torch.qr</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.svd.html">torch.svd</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.svd_lowrank.html">torch.svd_lowrank</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.pca_lowrank.html">torch.pca_lowrank</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.lobpcg.html">torch.lobpcg</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.trapz.html">torch.trapz</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.trapezoid.html">torch.trapezoid</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cumulative_trapezoid.html">torch.cumulative_trapezoid</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.triangular_solve.html">torch.triangular_solve</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.vdot.html">torch.vdot</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_abs.html">torch._foreach_abs</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_abs_.html">torch._foreach_abs_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_acos.html">torch._foreach_acos</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_acos_.html">torch._foreach_acos_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_asin.html">torch._foreach_asin</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_asin_.html">torch._foreach_asin_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_atan.html">torch._foreach_atan</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_atan_.html">torch._foreach_atan_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_ceil.html">torch._foreach_ceil</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_ceil_.html">torch._foreach_ceil_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_cos.html">torch._foreach_cos</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_cos_.html">torch._foreach_cos_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_cosh.html">torch._foreach_cosh</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_cosh_.html">torch._foreach_cosh_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_erf.html">torch._foreach_erf</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_erf_.html">torch._foreach_erf_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_erfc.html">torch._foreach_erfc</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_erfc_.html">torch._foreach_erfc_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_exp.html">torch._foreach_exp</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_exp_.html">torch._foreach_exp_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_expm1.html">torch._foreach_expm1</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_expm1_.html">torch._foreach_expm1_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_floor.html">torch._foreach_floor</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_floor_.html">torch._foreach_floor_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_log.html">torch._foreach_log</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_log_.html">torch._foreach_log_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_log10.html">torch._foreach_log10</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_log10_.html">torch._foreach_log10_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_log1p.html">torch._foreach_log1p</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_log1p_.html">torch._foreach_log1p_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_log2.html">torch._foreach_log2</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_log2_.html">torch._foreach_log2_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_neg.html">torch._foreach_neg</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_neg_.html">torch._foreach_neg_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_tan.html">torch._foreach_tan</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_tan_.html">torch._foreach_tan_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_sin.html">torch._foreach_sin</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_sin_.html">torch._foreach_sin_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_sinh.html">torch._foreach_sinh</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_sinh_.html">torch._foreach_sinh_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_round.html">torch._foreach_round</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_round_.html">torch._foreach_round_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_sqrt.html">torch._foreach_sqrt</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_sqrt_.html">torch._foreach_sqrt_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_lgamma.html">torch._foreach_lgamma</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_lgamma_.html">torch._foreach_lgamma_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_frac.html">torch._foreach_frac</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_frac_.html">torch._foreach_frac_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_reciprocal.html">torch._foreach_reciprocal</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_reciprocal_.html">torch._foreach_reciprocal_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_sigmoid.html">torch._foreach_sigmoid</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_sigmoid_.html">torch._foreach_sigmoid_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_trunc.html">torch._foreach_trunc</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_trunc_.html">torch._foreach_trunc_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_zero_.html">torch._foreach_zero_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.compiled_with_cxx11_abi.html">torch.compiled_with_cxx11_abi</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.result_type.html">torch.result_type</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.can_cast.html">torch.can_cast</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.promote_types.html">torch.promote_types</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.use_deterministic_algorithms.html">torch.use_deterministic_algorithms</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.are_deterministic_algorithms_enabled.html">torch.are_deterministic_algorithms_enabled</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.is_deterministic_algorithms_warn_only_enabled.html">torch.is_deterministic_algorithms_warn_only_enabled</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.set_deterministic_debug_mode.html">torch.set_deterministic_debug_mode</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.get_deterministic_debug_mode.html">torch.get_deterministic_debug_mode</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.set_float32_matmul_precision.html">torch.set_float32_matmul_precision</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.get_float32_matmul_precision.html">torch.get_float32_matmul_precision</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.set_warn_always.html">torch.set_warn_always</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.get_device_module.html">torch.get_device_module</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.is_warn_always_enabled.html">torch.is_warn_always_enabled</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.vmap.html">torch.vmap</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._assert.html">torch._assert</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.sym_float.html">torch.sym_float</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.sym_fresh_size.html">torch.sym_fresh_size</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.sym_int.html">torch.sym_int</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.sym_max.html">torch.sym_max</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.sym_min.html">torch.sym_min</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.sym_not.html">torch.sym_not</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.sym_ite.html">torch.sym_ite</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.sym_sum.html">torch.sym_sum</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cond.html">torch.cond</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.compile.html">torch.compile</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="tensors.html">torch.Tensor</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.new_tensor.html">torch.Tensor.new_tensor</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.new_full.html">torch.Tensor.new_full</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.new_empty.html">torch.Tensor.new_empty</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.new_ones.html">torch.Tensor.new_ones</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.new_zeros.html">torch.Tensor.new_zeros</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.is_cuda.html">torch.Tensor.is_cuda</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.is_quantized.html">torch.Tensor.is_quantized</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.is_meta.html">torch.Tensor.is_meta</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.device.html">torch.Tensor.device</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.grad.html">torch.Tensor.grad</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.ndim.html">torch.Tensor.ndim</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.real.html">torch.Tensor.real</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.imag.html">torch.Tensor.imag</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.nbytes.html">torch.Tensor.nbytes</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.itemsize.html">torch.Tensor.itemsize</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.abs.html">torch.Tensor.abs</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.abs_.html">torch.Tensor.abs_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.absolute.html">torch.Tensor.absolute</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.absolute_.html">torch.Tensor.absolute_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.acos.html">torch.Tensor.acos</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.acos_.html">torch.Tensor.acos_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.arccos.html">torch.Tensor.arccos</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.arccos_.html">torch.Tensor.arccos_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.add.html">torch.Tensor.add</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.add_.html">torch.Tensor.add_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.addbmm.html">torch.Tensor.addbmm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.addbmm_.html">torch.Tensor.addbmm_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.addcdiv.html">torch.Tensor.addcdiv</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.addcdiv_.html">torch.Tensor.addcdiv_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.addcmul.html">torch.Tensor.addcmul</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.addcmul_.html">torch.Tensor.addcmul_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.addmm.html">torch.Tensor.addmm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.addmm_.html">torch.Tensor.addmm_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.sspaddmm.html">torch.Tensor.sspaddmm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.addmv.html">torch.Tensor.addmv</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.addmv_.html">torch.Tensor.addmv_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.addr.html">torch.Tensor.addr</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.addr_.html">torch.Tensor.addr_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.adjoint.html">torch.Tensor.adjoint</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.allclose.html">torch.Tensor.allclose</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.amax.html">torch.Tensor.amax</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.amin.html">torch.Tensor.amin</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.aminmax.html">torch.Tensor.aminmax</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.angle.html">torch.Tensor.angle</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.apply_.html">torch.Tensor.apply_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.argmax.html">torch.Tensor.argmax</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.argmin.html">torch.Tensor.argmin</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.argsort.html">torch.Tensor.argsort</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.argwhere.html">torch.Tensor.argwhere</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.asin.html">torch.Tensor.asin</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.asin_.html">torch.Tensor.asin_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.arcsin.html">torch.Tensor.arcsin</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.arcsin_.html">torch.Tensor.arcsin_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.as_strided.html">as_strided</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.atan.html">torch.Tensor.atan</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.atan_.html">torch.Tensor.atan_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.arctan.html">torch.Tensor.arctan</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.arctan_.html">torch.Tensor.arctan_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.atan2.html">torch.Tensor.atan2</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.atan2_.html">torch.Tensor.atan2_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.arctan2.html">torch.Tensor.arctan2</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.arctan2_.html">torch.Tensor.arctan2_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.all.html">torch.Tensor.all</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.any.html">torch.Tensor.any</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.backward.html">torch.Tensor.backward</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.baddbmm.html">torch.Tensor.baddbmm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.baddbmm_.html">torch.Tensor.baddbmm_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.bernoulli.html">torch.Tensor.bernoulli</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.bernoulli_.html">torch.Tensor.bernoulli_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.bfloat16.html">torch.Tensor.bfloat16</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.bincount.html">torch.Tensor.bincount</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.bitwise_not.html">torch.Tensor.bitwise_not</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.bitwise_not_.html">torch.Tensor.bitwise_not_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.bitwise_and.html">torch.Tensor.bitwise_and</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.bitwise_and_.html">torch.Tensor.bitwise_and_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.bitwise_or.html">torch.Tensor.bitwise_or</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.bitwise_or_.html">torch.Tensor.bitwise_or_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.bitwise_xor.html">torch.Tensor.bitwise_xor</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.bitwise_xor_.html">torch.Tensor.bitwise_xor_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.bitwise_left_shift.html">torch.Tensor.bitwise_left_shift</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.bitwise_left_shift_.html">torch.Tensor.bitwise_left_shift_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.bitwise_right_shift.html">torch.Tensor.bitwise_right_shift</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.bitwise_right_shift_.html">torch.Tensor.bitwise_right_shift_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.bmm.html">torch.Tensor.bmm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.bool.html">torch.Tensor.bool</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.byte.html">torch.Tensor.byte</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.broadcast_to.html">torch.Tensor.broadcast_to</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.cauchy_.html">torch.Tensor.cauchy_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.ceil.html">torch.Tensor.ceil</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.ceil_.html">torch.Tensor.ceil_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.char.html">torch.Tensor.char</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.cholesky.html">torch.Tensor.cholesky</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.cholesky_inverse.html">torch.Tensor.cholesky_inverse</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.cholesky_solve.html">torch.Tensor.cholesky_solve</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.chunk.html">torch.Tensor.chunk</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.clamp.html">torch.Tensor.clamp</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.clamp_.html">torch.Tensor.clamp_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.clip.html">torch.Tensor.clip</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.clip_.html">torch.Tensor.clip_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.clone.html">clone</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.contiguous.html">torch.Tensor.contiguous</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.copy_.html">copy</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.conj.html">torch.Tensor.conj</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.conj_physical.html">torch.Tensor.conj_physical</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.conj_physical_.html">torch.Tensor.conj_physical_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.resolve_conj.html">torch.Tensor.resolve_conj</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.resolve_neg.html">torch.Tensor.resolve_neg</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.copysign.html">torch.Tensor.copysign</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.copysign_.html">torch.Tensor.copysign_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.cos.html">torch.Tensor.cos</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.cos_.html">torch.Tensor.cos_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.cosh.html">torch.Tensor.cosh</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.cosh_.html">torch.Tensor.cosh_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.corrcoef.html">torch.Tensor.corrcoef</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.count_nonzero.html">torch.Tensor.count_nonzero</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.cov.html">torch.Tensor.cov</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.acosh.html">torch.Tensor.acosh</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.acosh_.html">torch.Tensor.acosh_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.arccosh.html">torch.Tensor.arccosh</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.arccosh_.html">torch.Tensor.arccosh_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.cpu.html">torch.Tensor.cpu</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.cross.html">torch.Tensor.cross</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.cuda.html">torch.Tensor.cuda</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.logcumsumexp.html">torch.Tensor.logcumsumexp</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.cummax.html">torch.Tensor.cummax</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.cummin.html">torch.Tensor.cummin</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.cumprod.html">torch.Tensor.cumprod</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.cumprod_.html">torch.Tensor.cumprod_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.cumsum.html">torch.Tensor.cumsum</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.cumsum_.html">torch.Tensor.cumsum_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.chalf.html">torch.Tensor.chalf</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.cfloat.html">torch.Tensor.cfloat</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.cdouble.html">torch.Tensor.cdouble</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.data_ptr.html">torch.Tensor.data_ptr</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.deg2rad.html">torch.Tensor.deg2rad</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.dequantize.html">dequantize</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.det.html">torch.Tensor.det</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.dense_dim.html">torch.Tensor.dense_dim</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.detach.html">torch.Tensor.detach</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.detach_.html">torch.Tensor.detach_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.diag.html">torch.Tensor.diag</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.diag_embed.html">torch.Tensor.diag_embed</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.diagflat.html">torch.Tensor.diagflat</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.diagonal.html">torch.Tensor.diagonal</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.diagonal_scatter.html">torch.Tensor.diagonal_scatter</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.fill_diagonal_.html">torch.Tensor.fill_diagonal_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.fmax.html">torch.Tensor.fmax</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.fmin.html">torch.Tensor.fmin</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.diff.html">torch.Tensor.diff</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.digamma.html">torch.Tensor.digamma</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.digamma_.html">torch.Tensor.digamma_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.dim.html">torch.Tensor.dim</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.dim_order.html">torch.Tensor.dim_order</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.dist.html">torch.Tensor.dist</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.div.html">torch.Tensor.div</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.div_.html">torch.Tensor.div_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.divide.html">torch.Tensor.divide</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.divide_.html">torch.Tensor.divide_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.dot.html">torch.Tensor.dot</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.double.html">torch.Tensor.double</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.dsplit.html">torch.Tensor.dsplit</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.element_size.html">torch.Tensor.element_size</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.eq.html">eq</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.eq_.html">torch.Tensor.eq_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.equal.html">equal</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.erf.html">torch.Tensor.erf</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.erf_.html">torch.Tensor.erf_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.erfc.html">torch.Tensor.erfc</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.erfc_.html">torch.Tensor.erfc_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.erfinv.html">torch.Tensor.erfinv</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.erfinv_.html">torch.Tensor.erfinv_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.exp.html">torch.Tensor.exp</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.exp_.html">torch.Tensor.exp_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.expm1.html">torch.Tensor.expm1</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.expm1_.html">torch.Tensor.expm1_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.expand.html">expand</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.expand_as.html">torch.Tensor.expand_as</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.exponential_.html">torch.Tensor.exponential_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.fix.html">torch.Tensor.fix</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.fix_.html">torch.Tensor.fix_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.fill_.html">torch.Tensor.fill_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.flatten.html">flatten</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.flip.html">torch.Tensor.flip</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.fliplr.html">torch.Tensor.fliplr</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.flipud.html">torch.Tensor.flipud</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.float.html">torch.Tensor.float</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.float_power.html">torch.Tensor.float_power</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.float_power_.html">torch.Tensor.float_power_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.floor.html">torch.Tensor.floor</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.floor_.html">torch.Tensor.floor_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.floor_divide.html">torch.Tensor.floor_divide</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.floor_divide_.html">torch.Tensor.floor_divide_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.fmod.html">torch.Tensor.fmod</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.fmod_.html">torch.Tensor.fmod_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.frac.html">torch.Tensor.frac</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.frac_.html">torch.Tensor.frac_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.frexp.html">torch.Tensor.frexp</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.gather.html">torch.Tensor.gather</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.gcd.html">torch.Tensor.gcd</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.gcd_.html">torch.Tensor.gcd_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.ge.html">ge</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.ge_.html">torch.Tensor.ge_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.greater_equal.html">torch.Tensor.greater_equal</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.greater_equal_.html">torch.Tensor.greater_equal_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.geometric_.html">torch.Tensor.geometric_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.geqrf.html">torch.Tensor.geqrf</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.ger.html">torch.Tensor.ger</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.get_device.html">torch.Tensor.get_device</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.gt.html">gt</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.gt_.html">torch.Tensor.gt_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.greater.html">torch.Tensor.greater</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.greater_.html">torch.Tensor.greater_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.half.html">torch.Tensor.half</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.hardshrink.html">torch.Tensor.hardshrink</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.heaviside.html">torch.Tensor.heaviside</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.histc.html">torch.Tensor.histc</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.histogram.html">torch.Tensor.histogram</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.hsplit.html">torch.Tensor.hsplit</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.hypot.html">torch.Tensor.hypot</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.hypot_.html">torch.Tensor.hypot_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.i0.html">torch.Tensor.i0</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.i0_.html">torch.Tensor.i0_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.igamma.html">torch.Tensor.igamma</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.igamma_.html">torch.Tensor.igamma_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.igammac.html">torch.Tensor.igammac</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.igammac_.html">torch.Tensor.igammac_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.index_add_.html">torch.Tensor.index_add_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.index_add.html">torch.Tensor.index_add</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.index_copy_.html">torch.Tensor.index_copy_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.index_copy.html">torch.Tensor.index_copy</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.index_fill_.html">torch.Tensor.index_fill_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.index_fill.html">torch.Tensor.index_fill</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.index_put_.html">torch.Tensor.index_put_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.index_put.html">torch.Tensor.index_put</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.index_reduce_.html">torch.Tensor.index_reduce_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.index_reduce.html">torch.Tensor.index_reduce</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.index_select.html">torch.Tensor.index_select</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.indices.html">torch.Tensor.indices</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.inner.html">torch.Tensor.inner</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.int.html">torch.Tensor.int</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.int_repr.html">int_repr</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.inverse.html">torch.Tensor.inverse</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.isclose.html">torch.Tensor.isclose</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.isfinite.html">torch.Tensor.isfinite</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.isinf.html">torch.Tensor.isinf</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.isposinf.html">torch.Tensor.isposinf</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.isneginf.html">torch.Tensor.isneginf</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.isnan.html">torch.Tensor.isnan</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.is_contiguous.html">torch.Tensor.is_contiguous</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.is_complex.html">torch.Tensor.is_complex</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.is_conj.html">torch.Tensor.is_conj</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.is_floating_point.html">torch.Tensor.is_floating_point</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.is_inference.html">torch.Tensor.is_inference</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.is_leaf.html">torch.Tensor.is_leaf</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.is_pinned.html">torch.Tensor.is_pinned</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.is_set_to.html">torch.Tensor.is_set_to</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.is_shared.html">torch.Tensor.is_shared</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.is_signed.html">torch.Tensor.is_signed</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.is_sparse.html">torch.Tensor.is_sparse</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.istft.html">torch.Tensor.istft</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.isreal.html">torch.Tensor.isreal</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.item.html">torch.Tensor.item</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.kthvalue.html">torch.Tensor.kthvalue</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.lcm.html">torch.Tensor.lcm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.lcm_.html">torch.Tensor.lcm_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.ldexp.html">torch.Tensor.ldexp</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.ldexp_.html">torch.Tensor.ldexp_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.le.html">le</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.le_.html">torch.Tensor.le_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.less_equal.html">torch.Tensor.less_equal</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.less_equal_.html">torch.Tensor.less_equal_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.lerp.html">torch.Tensor.lerp</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.lerp_.html">torch.Tensor.lerp_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.lgamma.html">torch.Tensor.lgamma</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.lgamma_.html">torch.Tensor.lgamma_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.log.html">torch.Tensor.log</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.log_.html">torch.Tensor.log_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.logdet.html">torch.Tensor.logdet</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.log10.html">torch.Tensor.log10</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.log10_.html">torch.Tensor.log10_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.log1p.html">torch.Tensor.log1p</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.log1p_.html">torch.Tensor.log1p_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.log2.html">torch.Tensor.log2</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.log2_.html">torch.Tensor.log2_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.log_normal_.html">torch.Tensor.log_normal_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.logaddexp.html">torch.Tensor.logaddexp</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.logaddexp2.html">torch.Tensor.logaddexp2</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.logsumexp.html">torch.Tensor.logsumexp</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.logical_and.html">torch.Tensor.logical_and</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.logical_and_.html">torch.Tensor.logical_and_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.logical_not.html">torch.Tensor.logical_not</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.logical_not_.html">torch.Tensor.logical_not_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.logical_or.html">torch.Tensor.logical_or</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.logical_or_.html">torch.Tensor.logical_or_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.logical_xor.html">torch.Tensor.logical_xor</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.logical_xor_.html">torch.Tensor.logical_xor_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.logit.html">torch.Tensor.logit</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.logit_.html">torch.Tensor.logit_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.long.html">torch.Tensor.long</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.lt.html">lt</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.lt_.html">torch.Tensor.lt_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.less.html">torch.Tensor.less</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.less_.html">torch.Tensor.less_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.lu.html">torch.Tensor.lu</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.lu_solve.html">torch.Tensor.lu_solve</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.as_subclass.html">torch.Tensor.as_subclass</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.map_.html">torch.Tensor.map_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.masked_scatter_.html">torch.Tensor.masked_scatter_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.masked_scatter.html">torch.Tensor.masked_scatter</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.masked_fill_.html">torch.Tensor.masked_fill_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.masked_fill.html">torch.Tensor.masked_fill</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.masked_select.html">torch.Tensor.masked_select</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.matmul.html">torch.Tensor.matmul</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.matrix_power.html">torch.Tensor.matrix_power</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.matrix_exp.html">torch.Tensor.matrix_exp</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.max.html">max</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.maximum.html">torch.Tensor.maximum</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.mean.html">mean</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.module_load.html">torch.Tensor.module_load</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.nanmean.html">torch.Tensor.nanmean</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.median.html">torch.Tensor.median</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.nanmedian.html">torch.Tensor.nanmedian</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.min.html">min</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.minimum.html">torch.Tensor.minimum</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.mm.html">torch.Tensor.mm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.smm.html">torch.Tensor.smm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.mode.html">torch.Tensor.mode</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.movedim.html">torch.Tensor.movedim</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.moveaxis.html">torch.Tensor.moveaxis</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.msort.html">torch.Tensor.msort</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.mul.html">torch.Tensor.mul</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.mul_.html">torch.Tensor.mul_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.multiply.html">torch.Tensor.multiply</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.multiply_.html">torch.Tensor.multiply_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.multinomial.html">torch.Tensor.multinomial</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.mv.html">torch.Tensor.mv</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.mvlgamma.html">torch.Tensor.mvlgamma</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.mvlgamma_.html">torch.Tensor.mvlgamma_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.nansum.html">torch.Tensor.nansum</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.narrow.html">torch.Tensor.narrow</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.narrow_copy.html">torch.Tensor.narrow_copy</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.ndimension.html">torch.Tensor.ndimension</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.nan_to_num.html">torch.Tensor.nan_to_num</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.nan_to_num_.html">torch.Tensor.nan_to_num_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.ne.html">ne</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.ne_.html">torch.Tensor.ne_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.not_equal.html">torch.Tensor.not_equal</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.not_equal_.html">torch.Tensor.not_equal_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.neg.html">torch.Tensor.neg</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.neg_.html">torch.Tensor.neg_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.negative.html">torch.Tensor.negative</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.negative_.html">torch.Tensor.negative_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.nelement.html">torch.Tensor.nelement</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.nextafter.html">torch.Tensor.nextafter</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.nextafter_.html">torch.Tensor.nextafter_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.nonzero.html">torch.Tensor.nonzero</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.norm.html">torch.Tensor.norm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.normal_.html">torch.Tensor.normal_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.numel.html">torch.Tensor.numel</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.numpy.html">torch.Tensor.numpy</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.orgqr.html">torch.Tensor.orgqr</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.ormqr.html">torch.Tensor.ormqr</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.outer.html">torch.Tensor.outer</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.permute.html">torch.Tensor.permute</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.pin_memory.html">torch.Tensor.pin_memory</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.pinverse.html">torch.Tensor.pinverse</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.polygamma.html">torch.Tensor.polygamma</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.polygamma_.html">torch.Tensor.polygamma_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.positive.html">torch.Tensor.positive</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.pow.html">torch.Tensor.pow</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.pow_.html">torch.Tensor.pow_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.prod.html">torch.Tensor.prod</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.put_.html">torch.Tensor.put_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.qr.html">torch.Tensor.qr</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.qscheme.html">torch.Tensor.qscheme</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.quantile.html">torch.Tensor.quantile</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.nanquantile.html">torch.Tensor.nanquantile</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.q_scale.html">q_scale</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.q_zero_point.html">q_zero_point</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.q_per_channel_scales.html">q_per_channel_scales</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.q_per_channel_zero_points.html">q_per_channel_zero_points</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.q_per_channel_axis.html">q_per_channel_axis</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.rad2deg.html">torch.Tensor.rad2deg</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.random_.html">torch.Tensor.random_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.ravel.html">torch.Tensor.ravel</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.reciprocal.html">torch.Tensor.reciprocal</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.reciprocal_.html">torch.Tensor.reciprocal_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.record_stream.html">torch.Tensor.record_stream</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.register_hook.html">torch.Tensor.register_hook</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.register_post_accumulate_grad_hook.html">torch.Tensor.register_post_accumulate_grad_hook</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.remainder.html">torch.Tensor.remainder</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.remainder_.html">torch.Tensor.remainder_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.renorm.html">torch.Tensor.renorm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.renorm_.html">torch.Tensor.renorm_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.repeat.html">torch.Tensor.repeat</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.repeat_interleave.html">torch.Tensor.repeat_interleave</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.requires_grad.html">torch.Tensor.requires_grad</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.requires_grad_.html">torch.Tensor.requires_grad_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.reshape.html">torch.Tensor.reshape</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.reshape_as.html">torch.Tensor.reshape_as</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.resize_.html">resize</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.resize_as_.html">torch.Tensor.resize_as_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.retain_grad.html">torch.Tensor.retain_grad</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.retains_grad.html">torch.Tensor.retains_grad</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.roll.html">torch.Tensor.roll</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.rot90.html">torch.Tensor.rot90</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.round.html">torch.Tensor.round</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.round_.html">torch.Tensor.round_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.rsqrt.html">torch.Tensor.rsqrt</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.rsqrt_.html">torch.Tensor.rsqrt_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.scatter.html">torch.Tensor.scatter</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.scatter_.html">torch.Tensor.scatter_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.scatter_add_.html">torch.Tensor.scatter_add_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.scatter_add.html">torch.Tensor.scatter_add</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.scatter_reduce_.html">torch.Tensor.scatter_reduce_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.scatter_reduce.html">torch.Tensor.scatter_reduce</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.select.html">select</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.select_scatter.html">torch.Tensor.select_scatter</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.set_.html">torch.Tensor.set_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.share_memory_.html">torch.Tensor.share_memory_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.short.html">torch.Tensor.short</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.sigmoid.html">torch.Tensor.sigmoid</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.sigmoid_.html">torch.Tensor.sigmoid_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.sign.html">torch.Tensor.sign</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.sign_.html">torch.Tensor.sign_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.signbit.html">torch.Tensor.signbit</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.sgn.html">torch.Tensor.sgn</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.sgn_.html">torch.Tensor.sgn_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.sin.html">torch.Tensor.sin</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.sin_.html">torch.Tensor.sin_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.sinc.html">torch.Tensor.sinc</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.sinc_.html">torch.Tensor.sinc_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.sinh.html">torch.Tensor.sinh</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.sinh_.html">torch.Tensor.sinh_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.asinh.html">torch.Tensor.asinh</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.asinh_.html">torch.Tensor.asinh_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.arcsinh.html">torch.Tensor.arcsinh</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.arcsinh_.html">torch.Tensor.arcsinh_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.shape.html">torch.Tensor.shape</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.size.html">torch.Tensor.size</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.slogdet.html">torch.Tensor.slogdet</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.slice_scatter.html">torch.Tensor.slice_scatter</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.softmax.html">torch.Tensor.softmax</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.sort.html">sort</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.split.html">torch.Tensor.split</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.sparse_mask.html">torch.Tensor.sparse_mask</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.sparse_dim.html">torch.Tensor.sparse_dim</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.sqrt.html">torch.Tensor.sqrt</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.sqrt_.html">torch.Tensor.sqrt_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.square.html">torch.Tensor.square</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.square_.html">torch.Tensor.square_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.squeeze.html">torch.Tensor.squeeze</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.squeeze_.html">torch.Tensor.squeeze_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.std.html">torch.Tensor.std</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.stft.html">torch.Tensor.stft</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.storage.html">torch.Tensor.storage</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.untyped_storage.html">torch.Tensor.untyped_storage</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.storage_offset.html">torch.Tensor.storage_offset</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.storage_type.html">torch.Tensor.storage_type</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.stride.html">torch.Tensor.stride</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.sub.html">torch.Tensor.sub</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.sub_.html">torch.Tensor.sub_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.subtract.html">torch.Tensor.subtract</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.subtract_.html">torch.Tensor.subtract_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.sum.html">torch.Tensor.sum</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.sum_to_size.html">torch.Tensor.sum_to_size</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.svd.html">torch.Tensor.svd</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.swapaxes.html">torch.Tensor.swapaxes</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.swapdims.html">torch.Tensor.swapdims</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.t.html">torch.Tensor.t</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.t_.html">torch.Tensor.t_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.tensor_split.html">torch.Tensor.tensor_split</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.tile.html">torch.Tensor.tile</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.to.html">torch.Tensor.to</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.to_mkldnn.html">torch.Tensor.to_mkldnn</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.take.html">torch.Tensor.take</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.take_along_dim.html">torch.Tensor.take_along_dim</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.tan.html">torch.Tensor.tan</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.tan_.html">torch.Tensor.tan_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.tanh.html">torch.Tensor.tanh</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.tanh_.html">torch.Tensor.tanh_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.atanh.html">torch.Tensor.atanh</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.atanh_.html">torch.Tensor.atanh_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.arctanh.html">torch.Tensor.arctanh</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.arctanh_.html">torch.Tensor.arctanh_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.tolist.html">torch.Tensor.tolist</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.topk.html">topk</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.to_dense.html">torch.Tensor.to_dense</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.to_sparse.html">torch.Tensor.to_sparse</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.to_sparse_csr.html">torch.Tensor.to_sparse_csr</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.to_sparse_csc.html">torch.Tensor.to_sparse_csc</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.to_sparse_bsr.html">torch.Tensor.to_sparse_bsr</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.to_sparse_bsc.html">torch.Tensor.to_sparse_bsc</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.trace.html">torch.Tensor.trace</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.transpose.html">torch.Tensor.transpose</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.transpose_.html">torch.Tensor.transpose_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.triangular_solve.html">torch.Tensor.triangular_solve</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.tril.html">torch.Tensor.tril</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.tril_.html">torch.Tensor.tril_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.triu.html">torch.Tensor.triu</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.triu_.html">torch.Tensor.triu_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.true_divide.html">torch.Tensor.true_divide</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.true_divide_.html">torch.Tensor.true_divide_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.trunc.html">torch.Tensor.trunc</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.trunc_.html">torch.Tensor.trunc_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.type.html">torch.Tensor.type</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.type_as.html">torch.Tensor.type_as</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.unbind.html">torch.Tensor.unbind</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.unflatten.html">torch.Tensor.unflatten</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.unfold.html">torch.Tensor.unfold</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.uniform_.html">torch.Tensor.uniform_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.unique.html">torch.Tensor.unique</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.unique_consecutive.html">torch.Tensor.unique_consecutive</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.unsqueeze.html">torch.Tensor.unsqueeze</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.unsqueeze_.html">torch.Tensor.unsqueeze_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.values.html">torch.Tensor.values</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.var.html">torch.Tensor.var</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.vdot.html">torch.Tensor.vdot</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.view.html">view</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.view_as.html">torch.Tensor.view_as</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.vsplit.html">torch.Tensor.vsplit</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.where.html">torch.Tensor.where</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.xlogy.html">torch.Tensor.xlogy</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.xlogy_.html">torch.Tensor.xlogy_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.xpu.html">torch.Tensor.xpu</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.zero_.html">torch.Tensor.zero_</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="tensor_attributes.html">Tensor Attributes</a></li>
<li class="toctree-l1"><a class="reference internal" href="tensor_view.html">Tensor Views</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="autograd.html">torch.autograd</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.backward.html">torch.autograd.backward</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.grad.html">torch.autograd.grad</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.forward_ad.dual_level.html">dual_level</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.forward_ad.make_dual.html">torch.autograd.forward_ad.make_dual</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.forward_ad.unpack_dual.html">torch.autograd.forward_ad.unpack_dual</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.forward_ad.enter_dual_level.html">torch.autograd.forward_ad.enter_dual_level</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.forward_ad.exit_dual_level.html">torch.autograd.forward_ad.exit_dual_level</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.forward_ad.UnpackedDualTensor.html">UnpackedDualTensor</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.functional.jacobian.html">torch.autograd.functional.jacobian</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.functional.hessian.html">torch.autograd.functional.hessian</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.functional.vjp.html">torch.autograd.functional.vjp</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.functional.jvp.html">torch.autograd.functional.jvp</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.functional.vhp.html">torch.autograd.functional.vhp</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.functional.hvp.html">torch.autograd.functional.hvp</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.Function.forward.html">torch.autograd.Function.forward</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.Function.backward.html">torch.autograd.Function.backward</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.Function.jvp.html">torch.autograd.Function.jvp</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.Function.vmap.html">torch.autograd.Function.vmap</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.function.FunctionCtx.mark_dirty.html">torch.autograd.function.FunctionCtx.mark_dirty</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.function.FunctionCtx.mark_non_differentiable.html">torch.autograd.function.FunctionCtx.mark_non_differentiable</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.function.FunctionCtx.save_for_backward.html">torch.autograd.function.FunctionCtx.save_for_backward</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.function.FunctionCtx.set_materialize_grads.html">torch.autograd.function.FunctionCtx.set_materialize_grads</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.function.once_differentiable.html">torch.autograd.function.once_differentiable</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.function.BackwardCFunction.html">BackwardCFunction</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.function.InplaceFunction.html">InplaceFunction</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.function.NestedIOFunction.html">NestedIOFunction</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.gradcheck.gradcheck.html">torch.autograd.gradcheck.gradcheck</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.gradcheck.gradgradcheck.html">torch.autograd.gradcheck.gradgradcheck</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.gradcheck.GradcheckError.html">torch.autograd.gradcheck.GradcheckError</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.profiler.profile.export_chrome_trace.html">torch.autograd.profiler.profile.export_chrome_trace</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.profiler.profile.key_averages.html">torch.autograd.profiler.profile.key_averages</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.profiler.profile.self_cpu_time_total.html">torch.autograd.profiler.profile.self_cpu_time_total</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.profiler.profile.total_average.html">torch.autograd.profiler.profile.total_average</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.profiler.parse_nvprof_trace.html">torch.autograd.profiler.parse_nvprof_trace</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.profiler.EnforceUnique.html">EnforceUnique</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.profiler.KinetoStepTracker.html">KinetoStepTracker</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.profiler.record_function.html">record_function</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.profiler_util.Interval.html">Interval</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.profiler_util.Kernel.html">Kernel</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.profiler_util.MemRecordsAcc.html">MemRecordsAcc</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.profiler_util.StringTable.html">StringTable</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.profiler.load_nvprof.html">torch.autograd.profiler.load_nvprof</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.grad_mode.set_multithreading_enabled.html">set_multithreading_enabled</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.graph.Node.name.html">torch.autograd.graph.Node.name</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.graph.Node.metadata.html">torch.autograd.graph.Node.metadata</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.graph.Node.next_functions.html">torch.autograd.graph.Node.next_functions</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.graph.Node.register_hook.html">torch.autograd.graph.Node.register_hook</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.graph.Node.register_prehook.html">torch.autograd.graph.Node.register_prehook</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.graph.increment_version.html">torch.autograd.graph.increment_version</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="size.html">torch.Size</a></li>
<li class="toctree-l1"><a class="reference internal" href="storage.html">torch.Storage</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Neural Networks</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="nn.html">torch.nn</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.parameter.Buffer.html">Buffer</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.parameter.Parameter.html">Parameter</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.parameter.UninitializedParameter.html">UninitializedParameter</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.parameter.UninitializedBuffer.html">UninitializedBuffer</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.Module.html">Module</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.Sequential.html">Sequential</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.ModuleList.html">ModuleList</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.ModuleDict.html">ModuleDict</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.ParameterList.html">ParameterList</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.ParameterDict.html">ParameterDict</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.module.register_module_forward_pre_hook.html">torch.nn.modules.module.register_module_forward_pre_hook</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.module.register_module_forward_hook.html">torch.nn.modules.module.register_module_forward_hook</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.module.register_module_backward_hook.html">torch.nn.modules.module.register_module_backward_hook</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.module.register_module_full_backward_pre_hook.html">torch.nn.modules.module.register_module_full_backward_pre_hook</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.module.register_module_full_backward_hook.html">torch.nn.modules.module.register_module_full_backward_hook</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.module.register_module_buffer_registration_hook.html">torch.nn.modules.module.register_module_buffer_registration_hook</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.module.register_module_module_registration_hook.html">torch.nn.modules.module.register_module_module_registration_hook</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.module.register_module_parameter_registration_hook.html">torch.nn.modules.module.register_module_parameter_registration_hook</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.Conv1d.html">Conv1d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.Conv2d.html">Conv2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.Conv3d.html">Conv3d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.ConvTranspose1d.html">ConvTranspose1d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.ConvTranspose2d.html">ConvTranspose2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.ConvTranspose3d.html">ConvTranspose3d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.LazyConv1d.html">LazyConv1d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.LazyConv2d.html">LazyConv2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.LazyConv3d.html">LazyConv3d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.LazyConvTranspose1d.html">LazyConvTranspose1d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.LazyConvTranspose2d.html">LazyConvTranspose2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.LazyConvTranspose3d.html">LazyConvTranspose3d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.Unfold.html">Unfold</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.Fold.html">Fold</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.MaxPool1d.html">MaxPool1d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.MaxPool2d.html">MaxPool2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.MaxPool3d.html">MaxPool3d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.MaxUnpool1d.html">MaxUnpool1d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.MaxUnpool2d.html">MaxUnpool2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.MaxUnpool3d.html">MaxUnpool3d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.AvgPool1d.html">AvgPool1d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.AvgPool2d.html">AvgPool2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.AvgPool3d.html">AvgPool3d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.FractionalMaxPool2d.html">FractionalMaxPool2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.FractionalMaxPool3d.html">FractionalMaxPool3d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.LPPool1d.html">LPPool1d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.LPPool2d.html">LPPool2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.LPPool3d.html">LPPool3d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.AdaptiveMaxPool1d.html">AdaptiveMaxPool1d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.AdaptiveMaxPool2d.html">AdaptiveMaxPool2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.AdaptiveMaxPool3d.html">AdaptiveMaxPool3d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.AdaptiveAvgPool1d.html">AdaptiveAvgPool1d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.AdaptiveAvgPool2d.html">AdaptiveAvgPool2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.AdaptiveAvgPool3d.html">AdaptiveAvgPool3d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.ReflectionPad1d.html">ReflectionPad1d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.ReflectionPad2d.html">ReflectionPad2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.ReflectionPad3d.html">ReflectionPad3d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.ReplicationPad1d.html">ReplicationPad1d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.ReplicationPad2d.html">ReplicationPad2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.ReplicationPad3d.html">ReplicationPad3d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.ZeroPad1d.html">ZeroPad1d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.ZeroPad2d.html">ZeroPad2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.ZeroPad3d.html">ZeroPad3d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.ConstantPad1d.html">ConstantPad1d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.ConstantPad2d.html">ConstantPad2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.ConstantPad3d.html">ConstantPad3d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.CircularPad1d.html">CircularPad1d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.CircularPad2d.html">CircularPad2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.CircularPad3d.html">CircularPad3d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.ELU.html">ELU</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.Hardshrink.html">Hardshrink</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.Hardsigmoid.html">Hardsigmoid</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.Hardtanh.html">Hardtanh</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.Hardswish.html">Hardswish</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.LeakyReLU.html">LeakyReLU</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.LogSigmoid.html">LogSigmoid</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.MultiheadAttention.html">MultiheadAttention</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.PReLU.html">PReLU</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.ReLU.html">ReLU</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.ReLU6.html">ReLU6</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.RReLU.html">RReLU</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.SELU.html">SELU</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.CELU.html">CELU</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.GELU.html">GELU</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.Sigmoid.html">Sigmoid</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.SiLU.html">SiLU</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.Mish.html">Mish</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.Softplus.html">Softplus</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.Softshrink.html">Softshrink</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.Softsign.html">Softsign</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.Tanh.html">Tanh</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.Tanhshrink.html">Tanhshrink</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.Threshold.html">Threshold</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.GLU.html">GLU</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.Softmin.html">Softmin</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.Softmax.html">Softmax</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.Softmax2d.html">Softmax2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.LogSoftmax.html">LogSoftmax</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.AdaptiveLogSoftmaxWithLoss.html">AdaptiveLogSoftmaxWithLoss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.BatchNorm1d.html">BatchNorm1d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.BatchNorm2d.html">BatchNorm2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.BatchNorm3d.html">BatchNorm3d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.LazyBatchNorm1d.html">LazyBatchNorm1d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.LazyBatchNorm2d.html">LazyBatchNorm2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.LazyBatchNorm3d.html">LazyBatchNorm3d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.GroupNorm.html">GroupNorm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.SyncBatchNorm.html">SyncBatchNorm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.InstanceNorm1d.html">InstanceNorm1d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.InstanceNorm2d.html">InstanceNorm2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.InstanceNorm3d.html">InstanceNorm3d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.LazyInstanceNorm1d.html">LazyInstanceNorm1d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.LazyInstanceNorm2d.html">LazyInstanceNorm2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.LazyInstanceNorm3d.html">LazyInstanceNorm3d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.LayerNorm.html">LayerNorm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.LocalResponseNorm.html">LocalResponseNorm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.RMSNorm.html">RMSNorm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.RNNBase.html">RNNBase</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.RNN.html">RNN</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.LSTM.html">LSTM</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.GRU.html">GRU</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.RNNCell.html">RNNCell</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.LSTMCell.html">LSTMCell</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.GRUCell.html">GRUCell</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.Transformer.html">Transformer</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.TransformerEncoder.html">TransformerEncoder</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.TransformerDecoder.html">TransformerDecoder</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.TransformerEncoderLayer.html">TransformerEncoderLayer</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.TransformerDecoderLayer.html">TransformerDecoderLayer</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.Identity.html">Identity</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.Linear.html">Linear</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.Bilinear.html">Bilinear</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.LazyLinear.html">LazyLinear</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.Dropout.html">Dropout</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.Dropout1d.html">Dropout1d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.Dropout2d.html">Dropout2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.Dropout3d.html">Dropout3d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.AlphaDropout.html">AlphaDropout</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.FeatureAlphaDropout.html">FeatureAlphaDropout</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.Embedding.html">Embedding</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.EmbeddingBag.html">EmbeddingBag</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.CosineSimilarity.html">CosineSimilarity</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.PairwiseDistance.html">PairwiseDistance</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.L1Loss.html">L1Loss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.MSELoss.html">MSELoss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.CrossEntropyLoss.html">CrossEntropyLoss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.CTCLoss.html">CTCLoss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.NLLLoss.html">NLLLoss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.PoissonNLLLoss.html">PoissonNLLLoss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.GaussianNLLLoss.html">GaussianNLLLoss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.KLDivLoss.html">KLDivLoss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.BCELoss.html">BCELoss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.BCEWithLogitsLoss.html">BCEWithLogitsLoss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.MarginRankingLoss.html">MarginRankingLoss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.HingeEmbeddingLoss.html">HingeEmbeddingLoss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.MultiLabelMarginLoss.html">MultiLabelMarginLoss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.HuberLoss.html">HuberLoss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.SmoothL1Loss.html">SmoothL1Loss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.SoftMarginLoss.html">SoftMarginLoss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.MultiLabelSoftMarginLoss.html">MultiLabelSoftMarginLoss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.CosineEmbeddingLoss.html">CosineEmbeddingLoss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.MultiMarginLoss.html">MultiMarginLoss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.TripletMarginLoss.html">TripletMarginLoss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.TripletMarginWithDistanceLoss.html">TripletMarginWithDistanceLoss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.PixelShuffle.html">PixelShuffle</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.PixelUnshuffle.html">PixelUnshuffle</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.Upsample.html">Upsample</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.UpsamplingNearest2d.html">UpsamplingNearest2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.UpsamplingBilinear2d.html">UpsamplingBilinear2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.ChannelShuffle.html">ChannelShuffle</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.DataParallel.html">DataParallel</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.parallel.DistributedDataParallel.html">DistributedDataParallel</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.clip_grad_norm_.html">torch.nn.utils.clip_grad_norm_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.clip_grad_norm.html">torch.nn.utils.clip_grad_norm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.clip_grad_value_.html">torch.nn.utils.clip_grad_value_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.get_total_norm.html">torch.nn.utils.get_total_norm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.clip_grads_with_norm_.html">torch.nn.utils.clip_grads_with_norm_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.parameters_to_vector.html">torch.nn.utils.parameters_to_vector</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.vector_to_parameters.html">torch.nn.utils.vector_to_parameters</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.fuse_conv_bn_eval.html">torch.nn.utils.fuse_conv_bn_eval</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.fuse_conv_bn_weights.html">torch.nn.utils.fuse_conv_bn_weights</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.fuse_linear_bn_eval.html">torch.nn.utils.fuse_linear_bn_eval</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.fuse_linear_bn_weights.html">torch.nn.utils.fuse_linear_bn_weights</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.convert_conv2d_weight_memory_format.html">torch.nn.utils.convert_conv2d_weight_memory_format</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.convert_conv3d_weight_memory_format.html">torch.nn.utils.convert_conv3d_weight_memory_format</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.weight_norm.html">torch.nn.utils.weight_norm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.remove_weight_norm.html">torch.nn.utils.remove_weight_norm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.spectral_norm.html">torch.nn.utils.spectral_norm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.remove_spectral_norm.html">torch.nn.utils.remove_spectral_norm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.skip_init.html">torch.nn.utils.skip_init</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.prune.BasePruningMethod.html">BasePruningMethod</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.prune.PruningContainer.html">PruningContainer</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.prune.Identity.html">torch.nn.utils.prune.identity</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.prune.RandomUnstructured.html">RandomUnstructured</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.prune.L1Unstructured.html">L1Unstructured</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.prune.RandomStructured.html">RandomStructured</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.prune.LnStructured.html">LnStructured</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.prune.CustomFromMask.html">CustomFromMask</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.prune.random_unstructured.html">torch.nn.utils.prune.random_unstructured</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.prune.l1_unstructured.html">torch.nn.utils.prune.l1_unstructured</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.prune.random_structured.html">torch.nn.utils.prune.random_structured</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.prune.ln_structured.html">torch.nn.utils.prune.ln_structured</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.prune.global_unstructured.html">torch.nn.utils.prune.global_unstructured</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.prune.custom_from_mask.html">torch.nn.utils.prune.custom_from_mask</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.prune.remove.html">torch.nn.utils.prune.remove</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.prune.is_pruned.html">torch.nn.utils.prune.is_pruned</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.parametrizations.orthogonal.html">torch.nn.utils.parametrizations.orthogonal</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.parametrizations.weight_norm.html">torch.nn.utils.parametrizations.weight_norm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.parametrizations.spectral_norm.html">torch.nn.utils.parametrizations.spectral_norm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.parametrize.register_parametrization.html">torch.nn.utils.parametrize.register_parametrization</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.parametrize.remove_parametrizations.html">torch.nn.utils.parametrize.remove_parametrizations</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.parametrize.cached.html">torch.nn.utils.parametrize.cached</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.parametrize.is_parametrized.html">torch.nn.utils.parametrize.is_parametrized</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.parametrize.ParametrizationList.html">ParametrizationList</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.stateless.functional_call.html">torch.nn.utils.stateless.functional_call</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.rnn.PackedSequence.html">PackedSequence</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.rnn.pack_padded_sequence.html">torch.nn.utils.rnn.pack_padded_sequence</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.rnn.pad_packed_sequence.html">torch.nn.utils.rnn.pad_packed_sequence</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.rnn.pad_sequence.html">torch.nn.utils.rnn.pad_sequence</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.rnn.pack_sequence.html">torch.nn.utils.rnn.pack_sequence</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.rnn.unpack_sequence.html">torch.nn.utils.rnn.unpack_sequence</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.rnn.unpad_sequence.html">torch.nn.utils.rnn.unpad_sequence</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.Flatten.html">Flatten</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.Unflatten.html">Unflatten</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.lazy.LazyModuleMixin.html">LazyModuleMixin</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.normalization.RMSNorm.html">RMSNorm</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="nn.functional.html">torch.nn.functional</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.conv1d.html">torch.nn.functional.conv1d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.conv2d.html">torch.nn.functional.conv2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.conv3d.html">torch.nn.functional.conv3d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.conv_transpose1d.html">torch.nn.functional.conv_transpose1d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.conv_transpose2d.html">torch.nn.functional.conv_transpose2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.conv_transpose3d.html">torch.nn.functional.conv_transpose3d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.unfold.html">torch.nn.functional.unfold</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.fold.html">torch.nn.functional.fold</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.avg_pool1d.html">torch.nn.functional.avg_pool1d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.avg_pool2d.html">torch.nn.functional.avg_pool2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.avg_pool3d.html">torch.nn.functional.avg_pool3d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.max_pool1d.html">torch.nn.functional.max_pool1d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.max_pool2d.html">torch.nn.functional.max_pool2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.max_pool3d.html">torch.nn.functional.max_pool3d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.max_unpool1d.html">torch.nn.functional.max_unpool1d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.max_unpool2d.html">torch.nn.functional.max_unpool2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.max_unpool3d.html">torch.nn.functional.max_unpool3d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.lp_pool1d.html">torch.nn.functional.lp_pool1d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.lp_pool2d.html">torch.nn.functional.lp_pool2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.lp_pool3d.html">torch.nn.functional.lp_pool3d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.adaptive_max_pool1d.html">torch.nn.functional.adaptive_max_pool1d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.adaptive_max_pool2d.html">torch.nn.functional.adaptive_max_pool2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.adaptive_max_pool3d.html">torch.nn.functional.adaptive_max_pool3d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.adaptive_avg_pool1d.html">torch.nn.functional.adaptive_avg_pool1d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.adaptive_avg_pool2d.html">torch.nn.functional.adaptive_avg_pool2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.adaptive_avg_pool3d.html">torch.nn.functional.adaptive_avg_pool3d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.fractional_max_pool2d.html">torch.nn.functional.fractional_max_pool2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.fractional_max_pool3d.html">torch.nn.functional.fractional_max_pool3d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.scaled_dot_product_attention.html">torch.nn.functional.scaled_dot_product_attention</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.threshold.html">torch.nn.functional.threshold</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.threshold_.html">torch.nn.functional.threshold_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.relu.html">torch.nn.functional.relu</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.relu_.html">torch.nn.functional.relu_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.hardtanh.html">torch.nn.functional.hardtanh</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.hardtanh_.html">torch.nn.functional.hardtanh_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.hardswish.html">torch.nn.functional.hardswish</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.relu6.html">torch.nn.functional.relu6</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.elu.html">torch.nn.functional.elu</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.elu_.html">torch.nn.functional.elu_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.selu.html">torch.nn.functional.selu</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.celu.html">torch.nn.functional.celu</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.leaky_relu.html">torch.nn.functional.leaky_relu</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.leaky_relu_.html">torch.nn.functional.leaky_relu_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.prelu.html">torch.nn.functional.prelu</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.rrelu.html">torch.nn.functional.rrelu</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.rrelu_.html">torch.nn.functional.rrelu_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.glu.html">torch.nn.functional.glu</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.gelu.html">torch.nn.functional.gelu</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.logsigmoid.html">torch.nn.functional.logsigmoid</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.hardshrink.html">torch.nn.functional.hardshrink</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.tanhshrink.html">torch.nn.functional.tanhshrink</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.softsign.html">torch.nn.functional.softsign</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.softplus.html">torch.nn.functional.softplus</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.softmin.html">torch.nn.functional.softmin</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.softmax.html">torch.nn.functional.softmax</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.softshrink.html">torch.nn.functional.softshrink</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.gumbel_softmax.html">torch.nn.functional.gumbel_softmax</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.log_softmax.html">torch.nn.functional.log_softmax</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.tanh.html">torch.nn.functional.tanh</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.sigmoid.html">torch.nn.functional.sigmoid</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.hardsigmoid.html">torch.nn.functional.hardsigmoid</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.silu.html">torch.nn.functional.silu</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.mish.html">torch.nn.functional.mish</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.batch_norm.html">torch.nn.functional.batch_norm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.group_norm.html">torch.nn.functional.group_norm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.instance_norm.html">torch.nn.functional.instance_norm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.layer_norm.html">torch.nn.functional.layer_norm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.local_response_norm.html">torch.nn.functional.local_response_norm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.rms_norm.html">torch.nn.functional.rms_norm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.normalize.html">torch.nn.functional.normalize</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.linear.html">torch.nn.functional.linear</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.bilinear.html">torch.nn.functional.bilinear</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.dropout.html">torch.nn.functional.dropout</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.alpha_dropout.html">torch.nn.functional.alpha_dropout</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.feature_alpha_dropout.html">torch.nn.functional.feature_alpha_dropout</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.dropout1d.html">torch.nn.functional.dropout1d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.dropout2d.html">torch.nn.functional.dropout2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.dropout3d.html">torch.nn.functional.dropout3d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.embedding.html">torch.nn.functional.embedding</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.embedding_bag.html">torch.nn.functional.embedding_bag</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.one_hot.html">torch.nn.functional.one_hot</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.pairwise_distance.html">torch.nn.functional.pairwise_distance</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.cosine_similarity.html">torch.nn.functional.cosine_similarity</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.pdist.html">torch.nn.functional.pdist</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.binary_cross_entropy.html">torch.nn.functional.binary_cross_entropy</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.binary_cross_entropy_with_logits.html">torch.nn.functional.binary_cross_entropy_with_logits</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.poisson_nll_loss.html">torch.nn.functional.poisson_nll_loss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.cosine_embedding_loss.html">torch.nn.functional.cosine_embedding_loss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.cross_entropy.html">torch.nn.functional.cross_entropy</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.ctc_loss.html">torch.nn.functional.ctc_loss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.gaussian_nll_loss.html">torch.nn.functional.gaussian_nll_loss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.hinge_embedding_loss.html">torch.nn.functional.hinge_embedding_loss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.kl_div.html">torch.nn.functional.kl_div</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.l1_loss.html">torch.nn.functional.l1_loss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.mse_loss.html">torch.nn.functional.mse_loss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.margin_ranking_loss.html">torch.nn.functional.margin_ranking_loss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.multilabel_margin_loss.html">torch.nn.functional.multilabel_margin_loss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.multilabel_soft_margin_loss.html">torch.nn.functional.multilabel_soft_margin_loss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.multi_margin_loss.html">torch.nn.functional.multi_margin_loss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.nll_loss.html">torch.nn.functional.nll_loss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.huber_loss.html">torch.nn.functional.huber_loss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.smooth_l1_loss.html">torch.nn.functional.smooth_l1_loss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.soft_margin_loss.html">torch.nn.functional.soft_margin_loss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.triplet_margin_loss.html">torch.nn.functional.triplet_margin_loss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.triplet_margin_with_distance_loss.html">torch.nn.functional.triplet_margin_with_distance_loss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.pixel_shuffle.html">torch.nn.functional.pixel_shuffle</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.pixel_unshuffle.html">torch.nn.functional.pixel_unshuffle</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.pad.html">torch.nn.functional.pad</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.interpolate.html">torch.nn.functional.interpolate</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.upsample.html">torch.nn.functional.upsample</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.upsample_nearest.html">torch.nn.functional.upsample_nearest</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.upsample_bilinear.html">torch.nn.functional.upsample_bilinear</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.grid_sample.html">torch.nn.functional.grid_sample</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.affine_grid.html">torch.nn.functional.affine_grid</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.torch.nn.parallel.data_parallel.html">torch.nn.functional.torch.nn.parallel.data_parallel</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="nn.init.html">torch.nn.init</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="nn.attention.html">torch.nn.attention</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.attention.sdpa_kernel.html">torch.nn.attention.sdpa_kernel</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.attention.SDPBackend.html">SDPBackend</a></li>
<li class="toctree-l2"><a class="reference internal" href="nn.attention.flex_attention.html">torch.nn.attention.flex_attention</a></li>
<li class="toctree-l2"><a class="reference internal" href="nn.attention.bias.html">torch.nn.attention.bias</a></li>

<li class="toctree-l2"><a class="reference internal" href="nn.attention.experimental.html">torch.nn.attention.experimental</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Device Management</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="cuda.html">torch.cuda</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.StreamContext.html">StreamContext</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.can_device_access_peer.html">torch.cuda.can_device_access_peer</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.current_blas_handle.html">torch.cuda.current_blas_handle</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.current_device.html">torch.cuda.current_device</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.current_stream.html">torch.cuda.current_stream</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.cudart.html">torch.cuda.cudart</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.default_stream.html">torch.cuda.default_stream</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.device.html">device</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.device_count.html">torch.cuda.device_count</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.device_memory_used.html">torch.cuda.device_memory_used</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.device_of.html">device_of</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.get_arch_list.html">torch.cuda.get_arch_list</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.get_device_capability.html">torch.cuda.get_device_capability</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.get_device_name.html">torch.cuda.get_device_name</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.get_device_properties.html">torch.cuda.get_device_properties</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.get_gencode_flags.html">torch.cuda.get_gencode_flags</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.get_sync_debug_mode.html">torch.cuda.get_sync_debug_mode</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.init.html">torch.cuda.init</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.ipc_collect.html">torch.cuda.ipc_collect</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.is_available.html">torch.cuda.is_available</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.is_initialized.html">torch.cuda.is_initialized</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.memory_usage.html">torch.cuda.memory_usage</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.set_device.html">torch.cuda.set_device</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.set_stream.html">torch.cuda.set_stream</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.set_sync_debug_mode.html">torch.cuda.set_sync_debug_mode</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.synchronize.html">torch.cuda.synchronize</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.utilization.html">torch.cuda.utilization</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.temperature.html">torch.cuda.temperature</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.power_draw.html">torch.cuda.power_draw</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.clock_rate.html">torch.cuda.clock_rate</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.OutOfMemoryError.html">torch.cuda.OutOfMemoryError</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.get_rng_state.html">torch.cuda.get_rng_state</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.get_rng_state_all.html">torch.cuda.get_rng_state_all</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.set_rng_state.html">torch.cuda.set_rng_state</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.set_rng_state_all.html">torch.cuda.set_rng_state_all</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.manual_seed.html">torch.cuda.manual_seed</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.manual_seed_all.html">torch.cuda.manual_seed_all</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.seed.html">torch.cuda.seed</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.seed_all.html">torch.cuda.seed_all</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.initial_seed.html">torch.cuda.initial_seed</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.comm.broadcast.html">torch.cuda.comm.broadcast</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.comm.broadcast_coalesced.html">torch.cuda.comm.broadcast_coalesced</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.comm.reduce_add.html">torch.cuda.comm.reduce_add</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.comm.scatter.html">torch.cuda.comm.scatter</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.comm.gather.html">torch.cuda.comm.gather</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.Stream.html">torch.cuda.stream</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.ExternalStream.html">ExternalStream</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.Event.html">Event</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.is_current_stream_capturing.html">torch.cuda.is_current_stream_capturing</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.graph_pool_handle.html">torch.cuda.graph_pool_handle</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.CUDAGraph.html">CUDAGraph</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.graph.html">graph</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.make_graphed_callables.html">torch.cuda.make_graphed_callables</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.empty_cache.html">torch.cuda.empty_cache</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.get_per_process_memory_fraction.html">torch.cuda.get_per_process_memory_fraction</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.list_gpu_processes.html">torch.cuda.list_gpu_processes</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.mem_get_info.html">torch.cuda.mem_get_info</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.memory_stats.html">torch.cuda.memory_stats</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.memory_summary.html">torch.cuda.memory_summary</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.memory_snapshot.html">torch.cuda.memory_snapshot</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.memory_allocated.html">torch.cuda.memory_allocated</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.max_memory_allocated.html">torch.cuda.max_memory_allocated</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.reset_max_memory_allocated.html">torch.cuda.reset_max_memory_allocated</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.memory_reserved.html">torch.cuda.memory_reserved</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.max_memory_reserved.html">torch.cuda.max_memory_reserved</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.set_per_process_memory_fraction.html">torch.cuda.set_per_process_memory_fraction</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.memory_cached.html">torch.cuda.memory_cached</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.max_memory_cached.html">torch.cuda.max_memory_cached</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.reset_max_memory_cached.html">torch.cuda.reset_max_memory_cached</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.reset_peak_memory_stats.html">torch.cuda.reset_peak_memory_stats</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.caching_allocator_alloc.html">torch.cuda.caching_allocator_alloc</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.caching_allocator_delete.html">torch.cuda.caching_allocator_delete</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.get_allocator_backend.html">torch.cuda.get_allocator_backend</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.CUDAPluggableAllocator.html">CUDAPluggableAllocator</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.change_current_allocator.html">torch.cuda.change_current_allocator</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.MemPool.html">MemPool</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.MemPoolContext.html">MemPoolContext</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.memory.caching_allocator_enable.html">torch.cuda.memory.caching_allocator_enable</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.nvtx.mark.html">torch.cuda.nvtx.mark</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.nvtx.range_push.html">torch.cuda.nvtx.range_push</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.nvtx.range_pop.html">torch.cuda.nvtx.range_pop</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.nvtx.range.html">torch.cuda.nvtx.range</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.jiterator._create_jit_fn.html">torch.cuda.jiterator._create_jit_fn</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.jiterator._create_multi_output_jit_fn.html">torch.cuda.jiterator._create_multi_output_jit_fn</a></li>
<li class="toctree-l2"><a class="reference internal" href="cuda.tunable.html">TunableOp</a></li>
<li class="toctree-l2"><a class="reference internal" href="cuda._sanitizer.html">CUDA Stream Sanitizer</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="cpu.html">torch.cpu</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cpu.current_device.html">torch.cpu.current_device</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cpu.current_stream.html">torch.cpu.current_stream</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cpu.is_available.html">torch.cpu.is_available</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cpu.synchronize.html">torch.cpu.synchronize</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cpu.set_device.html">torch.cpu.set_device</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cpu.device_count.html">torch.cpu.device_count</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cpu.StreamContext.html">StreamContext</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cpu.Stream.html">torch.cpu.stream</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="mps.html">torch.mps</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mps.device_count.html">torch.mps.device_count</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mps.synchronize.html">torch.mps.synchronize</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mps.get_rng_state.html">torch.mps.get_rng_state</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mps.set_rng_state.html">torch.mps.set_rng_state</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mps.manual_seed.html">torch.mps.manual_seed</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mps.seed.html">torch.mps.seed</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mps.empty_cache.html">torch.mps.empty_cache</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mps.set_per_process_memory_fraction.html">torch.mps.set_per_process_memory_fraction</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mps.current_allocated_memory.html">torch.mps.current_allocated_memory</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mps.driver_allocated_memory.html">torch.mps.driver_allocated_memory</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mps.recommended_max_memory.html">torch.mps.recommended_max_memory</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mps.profiler.start.html">torch.mps.profiler.start</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mps.profiler.stop.html">torch.mps.profiler.stop</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mps.profiler.profile.html">torch.mps.profiler.profile</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mps.event.Event.html">Event</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="xpu.html">torch.xpu</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xpu.StreamContext.html">StreamContext</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xpu.current_device.html">torch.xpu.current_device</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xpu.current_stream.html">torch.xpu.current_stream</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xpu.device.html">device</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xpu.device_count.html">torch.xpu.device_count</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xpu.device_of.html">device_of</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xpu.get_arch_list.html">torch.xpu.get_arch_list</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xpu.get_device_capability.html">torch.xpu.get_device_capability</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xpu.get_device_name.html">torch.xpu.get_device_name</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xpu.get_device_properties.html">torch.xpu.get_device_properties</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xpu.get_gencode_flags.html">torch.xpu.get_gencode_flags</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xpu.init.html">torch.xpu.init</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xpu.is_available.html">torch.xpu.is_available</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xpu.is_initialized.html">torch.xpu.is_initialized</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xpu.set_device.html">torch.xpu.set_device</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xpu.set_stream.html">torch.xpu.set_stream</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xpu.synchronize.html">torch.xpu.synchronize</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xpu.get_rng_state.html">torch.xpu.get_rng_state</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xpu.get_rng_state_all.html">torch.xpu.get_rng_state_all</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xpu.initial_seed.html">torch.xpu.initial_seed</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xpu.manual_seed.html">torch.xpu.manual_seed</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xpu.manual_seed_all.html">torch.xpu.manual_seed_all</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xpu.seed.html">torch.xpu.seed</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xpu.seed_all.html">torch.xpu.seed_all</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xpu.set_rng_state.html">torch.xpu.set_rng_state</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xpu.set_rng_state_all.html">torch.xpu.set_rng_state_all</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xpu.Event.html">Event</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xpu.Stream.html">torch.xpu.stream</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xpu.empty_cache.html">torch.xpu.empty_cache</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xpu.max_memory_allocated.html">torch.xpu.max_memory_allocated</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xpu.max_memory_reserved.html">torch.xpu.max_memory_reserved</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xpu.mem_get_info.html">torch.xpu.mem_get_info</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xpu.memory_allocated.html">torch.xpu.memory_allocated</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xpu.memory_reserved.html">torch.xpu.memory_reserved</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xpu.memory_stats.html">torch.xpu.memory_stats</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xpu.memory_stats_as_nested_dict.html">torch.xpu.memory_stats_as_nested_dict</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xpu.reset_accumulated_memory_stats.html">torch.xpu.reset_accumulated_memory_stats</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xpu.reset_peak_memory_stats.html">torch.xpu.reset_peak_memory_stats</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="accelerator.html">torch.accelerator</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.accelerator.device_count.html">torch.accelerator.device_count</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.accelerator.is_available.html">torch.accelerator.is_available</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.accelerator.current_accelerator.html">torch.accelerator.current_accelerator</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.accelerator.set_device_index.html">torch.accelerator.set_device_index</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.accelerator.set_device_idx.html">torch.accelerator.set_device_idx</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.accelerator.current_device_index.html">torch.accelerator.current_device_index</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.accelerator.current_device_idx.html">torch.accelerator.current_device_idx</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.accelerator.set_stream.html">torch.accelerator.set_stream</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.accelerator.current_stream.html">torch.accelerator.current_stream</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.accelerator.synchronize.html">torch.accelerator.synchronize</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="mtia.html">torch.mtia</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mtia.StreamContext.html">StreamContext</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mtia.current_device.html">torch.mtia.current_device</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mtia.current_stream.html">torch.mtia.current_stream</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mtia.default_stream.html">torch.mtia.default_stream</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mtia.device_count.html">torch.mtia.device_count</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mtia.init.html">torch.mtia.init</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mtia.is_available.html">torch.mtia.is_available</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mtia.is_initialized.html">torch.mtia.is_initialized</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mtia.memory_stats.html">torch.mtia.memory_stats</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mtia.get_device_capability.html">torch.mtia.get_device_capability</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mtia.empty_cache.html">torch.mtia.empty_cache</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mtia.set_device.html">torch.mtia.set_device</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mtia.set_stream.html">torch.mtia.set_stream</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mtia.synchronize.html">torch.mtia.synchronize</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mtia.device.html">device</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mtia.set_rng_state.html">torch.mtia.set_rng_state</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mtia.get_rng_state.html">torch.mtia.get_rng_state</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mtia.DeferredMtiaCallError.html">torch.mtia.DeferredMtiaCallError</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mtia.Event.html">Event</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mtia.Stream.html">torch.mtia.stream</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="mtia.memory.html">torch.mtia.memory</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mtia.memory.memory_stats.html">torch.mtia.memory.memory_stats</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="meta.html">Meta device</a></li>
<li class="toctree-l1"><a class="reference internal" href="torch_cuda_memory.html">Understanding CUDA Memory Usage</a></li>



</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Data Handling</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="data.html">torch.utils.data</a></li>
<li class="toctree-l1"><a class="reference internal" href="dlpack.html">torch.utils.dlpack</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_zoo.html">torch.utils.model_zoo</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Optimization</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="optim.html">torch.optim</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.Optimizer.add_param_group.html">torch.optim.Optimizer.add_param_group</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.Optimizer.load_state_dict.html">torch.optim.Optimizer.load_state_dict</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.Optimizer.register_load_state_dict_pre_hook.html">torch.optim.Optimizer.register_load_state_dict_pre_hook</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.Optimizer.register_load_state_dict_post_hook.html">torch.optim.Optimizer.register_load_state_dict_post_hook</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.Optimizer.state_dict.html">torch.optim.Optimizer.state_dict</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.Optimizer.register_state_dict_pre_hook.html">torch.optim.Optimizer.register_state_dict_pre_hook</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.Optimizer.register_state_dict_post_hook.html">torch.optim.Optimizer.register_state_dict_post_hook</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.Optimizer.step.html">torch.optim.Optimizer.step</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.Optimizer.register_step_pre_hook.html">torch.optim.Optimizer.register_step_pre_hook</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.Optimizer.register_step_post_hook.html">torch.optim.Optimizer.register_step_post_hook</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.Optimizer.zero_grad.html">torch.optim.Optimizer.zero_grad</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.Adadelta.html">Adadelta</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.Adafactor.html">Adafactor</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.Adagrad.html">Adagrad</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.Adam.html">Adam</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.AdamW.html">AdamW</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.SparseAdam.html">SparseAdam</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.Adamax.html">Adamax</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.ASGD.html">ASGD</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.LBFGS.html">LBFGS</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.NAdam.html">NAdam</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.RAdam.html">RAdam</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.RMSprop.html">RMSprop</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.Rprop.html">Rprop</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.SGD.html">SGD</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.lr_scheduler.LRScheduler.html">LRScheduler</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.lr_scheduler.LambdaLR.html">LambdaLR</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.lr_scheduler.MultiplicativeLR.html">MultiplicativeLR</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.lr_scheduler.StepLR.html">StepLR</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.lr_scheduler.MultiStepLR.html">MultiStepLR</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.lr_scheduler.ConstantLR.html">ConstantLR</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.lr_scheduler.LinearLR.html">LinearLR</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.lr_scheduler.ExponentialLR.html">ExponentialLR</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.lr_scheduler.PolynomialLR.html">PolynomialLR</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.lr_scheduler.CosineAnnealingLR.html">CosineAnnealingLR</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.lr_scheduler.ChainedScheduler.html">ChainedScheduler</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.lr_scheduler.SequentialLR.html">SequentialLR</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.lr_scheduler.ReduceLROnPlateau.html">ReduceLROnPlateau</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.lr_scheduler.CyclicLR.html">CyclicLR</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.lr_scheduler.OneCycleLR.html">OneCycleLR</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.lr_scheduler.CosineAnnealingWarmRestarts.html">CosineAnnealingWarmRestarts</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.swa_utils.AveragedModel.html">AveragedModel</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.swa_utils.SWALR.html">SWALR</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="distributed.optim.html">torch.distributed.optim</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Optimization and Compilation</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="torch.compiler.html">torch.compile</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="torch.compiler_get_started.html">Getting Started</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="torch.compiler_api.html">torch.compiler API reference</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.compiler.compile.html">torch.compiler.compile</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.compiler.reset.html">torch.compiler.reset</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.compiler.allow_in_graph.html">torch.compiler.allow_in_graph</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.compiler.substitute_in_graph.html">torch.compiler.substitute_in_graph</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.compiler.assume_constant_result.html">torch.compiler.assume_constant_result</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.compiler.list_backends.html">torch.compiler.list_backends</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.compiler.disable.html">torch.compiler.disable</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.compiler.set_stance.html">torch.compiler.set_stance</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.compiler.cudagraph_mark_step_begin.html">torch.compiler.cudagraph_mark_step_begin</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.compiler.is_compiling.html">torch.compiler.is_compiling</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.compiler.is_dynamo_compiling.html">torch.compiler.is_dynamo_compiling</a></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="torch.compiler.config.html">torch.compiler.config</a></li>
<li class="toctree-l2"><a class="reference internal" href="torch.compiler_fine_grain_apis.html">TorchDynamo APIs for fine-grained tracing</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="torch.compiler_aot_inductor.html">AOTInductor: Ahead-Of-Time Compilation for Torch.Export-ed Models</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3 has-children"><a class="reference internal" href="logging.html">torch._logging</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l4"><a class="reference internal" href="generated/torch._logging.set_logs.html">torch._logging.set_logs</a></li>
</ul>
</details></li>
<li class="toctree-l3"><a class="reference internal" href="torch.compiler_aot_inductor_minifier.html">AOTInductor Minifier</a></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="torch.compiler_inductor_profiling.html">TorchInductor GPU Profiling</a></li>
<li class="toctree-l2"><a class="reference internal" href="torch.compiler_profiling_torch_compile.html">Profiling to understand torch.compile performance</a></li>
<li class="toctree-l2"><a class="reference internal" href="torch.compiler_faq.html">Frequently Asked Questions</a></li>
<li class="toctree-l2"><a class="reference internal" href="torch.compiler_troubleshooting.html">torch.compile Troubleshooting</a></li>
<li class="toctree-l2"><a class="reference internal" href="torch.compiler_performance_dashboard.html">PyTorch 2.0 Performance Dashboard</a></li>
<li class="toctree-l2"><a class="reference internal" href="torch.compiler_dynamo_overview.html">Dynamo Overview</a></li>
<li class="toctree-l2"><a class="reference internal" href="torch.compiler_dynamo_deepdive.html">Dynamo Deep-Dive</a></li>
<li class="toctree-l2"><a class="reference internal" href="torch.compiler_dynamic_shapes.html">Dynamic shapes</a></li>
<li class="toctree-l2"><a class="reference internal" href="torch.compiler_nn_module.html">PyTorch 2.0 NNModule Support</a></li>
<li class="toctree-l2"><a class="reference internal" href="torch.compiler_best_practices_for_backends.html">Best Practices for Backends</a></li>
<li class="toctree-l2"><a class="reference internal" href="torch.compiler_cudagraph_trees.html">CUDAGraph Trees</a></li>
<li class="toctree-l2"><a class="reference internal" href="torch.compiler_fake_tensor.html">Fake tensor</a></li>
<li class="toctree-l2"><a class="reference internal" href="torch.compiler_custom_backends.html">Custom Backends</a></li>
<li class="toctree-l2"><a class="reference internal" href="torch.compiler_transformations.html">Writing Graph Transformations on ATen IR</a></li>
<li class="toctree-l2"><a class="reference internal" href="torch.compiler_ir.html">IRs</a></li>
</ul>
</details></li>
<li class="toctree-l1 current active has-children"><a class="reference internal" href="jit.html">torch.jit</a><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="jit_builtin_functions.html">torch.jit.supported_ops</a></li>
<li class="toctree-l2"><a class="reference internal" href="jit_language_reference.html">TorchScript Language Reference</a></li>
<li class="toctree-l2 current active"><a class="current reference internal" href="#">TorchScript Language Reference</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.jit.script.html">torch.jit.script</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.jit.trace.html">torch.jit.trace</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.jit.script_if_tracing.html">torch.jit.script_if_tracing</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.jit.trace_module.html">torch.jit.trace_module</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.jit.fork.html">torch.jit.fork</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.jit.wait.html">torch.jit.wait</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.jit.ScriptModule.html">ScriptModule</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.jit.ScriptFunction.html">ScriptFunction</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.jit.freeze.html">torch.jit.freeze</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.jit.optimize_for_inference.html">torch.jit.optimize_for_inference</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.jit.enable_onednn_fusion.html">torch.jit.enable_onednn_fusion</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.jit.onednn_fusion_enabled.html">torch.jit.onednn_fusion_enabled</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.jit.set_fusion_strategy.html">torch.jit.set_fusion_strategy</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.jit.strict_fusion.html">strict_fusion</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.jit.save.html">torch.jit.save</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.jit.load.html">torch.jit.load</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.jit.ignore.html">torch.jit.ignore</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.jit.unused.html">torch.jit.unused</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.jit.interface.html">torch.jit.interface</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.jit.isinstance.html">torch.jit.isinstance</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.jit.Attribute.html">Attribute</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.jit.annotate.html">torch.jit.annotate</a></li>
<li class="toctree-l2"><a class="reference internal" href="jit_python_reference.html">Python Language Reference Coverage</a></li>
<li class="toctree-l2"><a class="reference internal" href="jit_unsupported.html">TorchScript Unsupported PyTorch Constructs</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="func.html">torch.func</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="func.whirlwind_tour.html">torch.func Whirlwind Tour</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="func.api.html">torch.func API Reference</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.func.vmap.html">torch.func.vmap</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.func.grad.html">torch.func.grad</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.func.grad_and_value.html">torch.func.grad_and_value</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.func.vjp.html">torch.func.vjp</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.func.jvp.html">torch.func.jvp</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.func.linearize.html">torch.func.linearize</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.func.jacrev.html">torch.func.jacrev</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.func.jacfwd.html">torch.func.jacfwd</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.func.hessian.html">torch.func.hessian</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.func.functionalize.html">torch.func.functionalize</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.func.functional_call.html">torch.func.functional_call</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.func.stack_module_state.html">torch.func.stack_module_state</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.func.replace_all_batch_norm_modules_.html">torch.func.replace_all_batch_norm_modules_</a></li>
<li class="toctree-l3"><a class="reference internal" href="func.batch_norm.html">Patching Batch Norm</a></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="func.ux_limitations.html">UX Limitations</a></li>
<li class="toctree-l2"><a class="reference internal" href="func.migrating.html">Migrating from functorch to torch.func</a></li>
</ul>
</details></li>
</ul>
</div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
      <div class="sidebar-primary-item">
<div id="ethical-ad-placement"
      class="flat"
      data-ea-publisher="readthedocs"
      data-ea-type="readthedocs-sidebar"
      data-ea-manual="true">
</div></div>
  </div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        
          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item">

<nav aria-label="Breadcrumb" class="d-print-none">
  <ul class="bd-breadcrumbs">
    
    <li class="breadcrumb-item breadcrumb-home">
      <a href="../index.html" class="nav-link" aria-label="Home">
        <i class="fa-solid fa-home"></i>
      </a>
    </li>
    
    <li class="breadcrumb-item"><a href="index.html" class="nav-link">Python API</a></li>
    
    
    <li class="breadcrumb-item"><a href="jit.html" class="nav-link">TorchScript</a></li>
    
    <li class="breadcrumb-item active" aria-current="page"><span class="ellipsis">TorchScript Language Reference</span></li>
  </ul>
</nav>
</div>
      
    </div>
  
  
</div>
</div>
              
              
              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section id="torchscript-language-reference">
<span id="language-reference-v2"></span><h1>TorchScript Language Reference<a class="headerlink" href="#torchscript-language-reference" title="Link to this heading">#</a></h1>
<p>This reference manual describes the syntax and core semantics of the TorchScript language.
TorchScript is a statically typed subset of the Python language. This document explains the supported features of
Python in TorchScript and also how the language diverges from regular Python. Any features of Python that are not mentioned in
this reference manual are not part of TorchScript. TorchScript focuses specifically on the features of Python that are needed to
represent neural network models in PyTorch.</p>
<nav class="contents local" id="contents">
<ul class="simple">
<li><p><a class="reference internal" href="#terminology" id="id7">Terminology</a></p></li>
<li><p><a class="reference internal" href="#id1" id="id8">Type System</a></p></li>
<li><p><a class="reference internal" href="#type-annotation" id="id9">Type Annotation</a></p></li>
<li><p><a class="reference internal" href="#expressions" id="id10">Expressions</a></p></li>
<li><p><a class="reference internal" href="#simple-statements" id="id11">Simple Statements</a></p></li>
<li><p><a class="reference internal" href="#compound-statements" id="id12">Compound Statements</a></p></li>
<li><p><a class="reference internal" href="#python-values" id="id13">Python Values</a></p></li>
<li><p><a class="reference internal" href="#torch-apis" id="id14">torch.* APIs</a></p></li>
</ul>
</nav>
<section id="terminology">
<span id="type-system"></span><h2><a class="toc-backref" href="#id7" role="doc-backlink">Terminology</a><a class="headerlink" href="#terminology" title="Link to this heading">#</a></h2>
<p>This document uses the following terminologies:</p>
<div class="pst-scrollable-table-container"><table class="table">
<colgroup>
<col style="width: 50.0%" />
<col style="width: 50.0%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Pattern</p></th>
<th class="head"><p>Notes</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">::=</span></code></p></td>
<td><p>Indicates that the given symbol is defined as.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">&quot;</span> <span class="pre">&quot;</span></code></p></td>
<td><p>Represents real keywords and delimiters that are part of the syntax.</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">A</span> <span class="pre">|</span> <span class="pre">B</span></code></p></td>
<td><p>Indicates either A or B.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">(</span> <span class="pre">)</span></code></p></td>
<td><p>Indicates grouping.</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">[]</span></code></p></td>
<td><p>Indicates optional.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">A+</span></code></p></td>
<td><p>Indicates a regular expression where term A is repeated at least once.</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">A*</span></code></p></td>
<td><p>Indicates a regular expression where term A is repeated zero or more times.</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="id1">
<h2><a class="toc-backref" href="#id8" role="doc-backlink">Type System</a><a class="headerlink" href="#id1" title="Link to this heading">#</a></h2>
<p>TorchScript is a statically typed subset of Python. The largest difference between TorchScript and the full Python language is that TorchScript only supports a small set of types that are needed to express
neural net models.</p>
<section id="torchscript-types">
<h3>TorchScript Types<a class="headerlink" href="#torchscript-types" title="Link to this heading">#</a></h3>
<p>The TorchScript type system consists of <code class="docutils literal notranslate"><span class="pre">TSType</span></code> and <code class="docutils literal notranslate"><span class="pre">TSModuleType</span></code> as defined below.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">TSAllType</span> <span class="p">:</span><span class="o">:=</span> <span class="n">TSType</span> <span class="o">|</span> <span class="n">TSModuleType</span>
<span class="n">TSType</span>    <span class="p">:</span><span class="o">:=</span> <span class="n">TSMetaType</span> <span class="o">|</span> <span class="n">TSPrimitiveType</span> <span class="o">|</span> <span class="n">TSStructuralType</span> <span class="o">|</span> <span class="n">TSNominalType</span>
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">TSType</span></code> represents the majority of TorchScript types that are composable and that can be used in TorchScript type annotations.
<code class="docutils literal notranslate"><span class="pre">TSType</span></code> refers to any of the following:</p>
<ul class="simple">
<li><p>Meta Types, e.g., <code class="docutils literal notranslate"><span class="pre">Any</span></code></p></li>
<li><p>Primitive Types, e.g., <code class="docutils literal notranslate"><span class="pre">int</span></code>, <code class="docutils literal notranslate"><span class="pre">float</span></code>, and <code class="docutils literal notranslate"><span class="pre">str</span></code></p></li>
<li><p>Structural Types, e.g., <code class="docutils literal notranslate"><span class="pre">Optional[int]</span></code> or <code class="docutils literal notranslate"><span class="pre">List[MyClass]</span></code></p></li>
<li><p>Nominal Types (Python classes), e.g., <code class="docutils literal notranslate"><span class="pre">MyClass</span></code> (user-defined), <code class="docutils literal notranslate"><span class="pre">torch.tensor</span></code> (built-in)</p></li>
</ul>
<p><code class="docutils literal notranslate"><span class="pre">TSModuleType</span></code> represents <code class="docutils literal notranslate"><span class="pre">torch.nn.Module</span></code> and its subclasses. It is treated differently from <code class="docutils literal notranslate"><span class="pre">TSType</span></code> because its type schema is inferred partly from the object instance and partly from the class definition.
As such, instances of a <code class="docutils literal notranslate"><span class="pre">TSModuleType</span></code> may not follow the same static type schema. <code class="docutils literal notranslate"><span class="pre">TSModuleType</span></code> cannot be used as a TorchScript type annotation or be composed with <code class="docutils literal notranslate"><span class="pre">TSType</span></code> for type safety considerations.</p>
</section>
<section id="meta-types">
<h3>Meta Types<a class="headerlink" href="#meta-types" title="Link to this heading">#</a></h3>
<p>Meta types are so abstract that they are more like type constraints than concrete types.
Currently TorchScript defines one meta-type, <code class="docutils literal notranslate"><span class="pre">Any</span></code>, that represents any TorchScript type.</p>
<section id="any-type">
<h4><code class="docutils literal notranslate"><span class="pre">Any</span></code> Type<a class="headerlink" href="#any-type" title="Link to this heading">#</a></h4>
<p>The <code class="docutils literal notranslate"><span class="pre">Any</span></code> type represents any TorchScript type. <code class="docutils literal notranslate"><span class="pre">Any</span></code> specifies no type constraints, thus there is no type-checking on <code class="docutils literal notranslate"><span class="pre">Any</span></code>.
As such it can be bound to any Python or TorchScript data types (e.g., <code class="docutils literal notranslate"><span class="pre">int</span></code>, TorchScript <code class="docutils literal notranslate"><span class="pre">tuple</span></code>, or an arbitrary Python class that is not scripted).</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">TSMetaType</span> <span class="p">:</span><span class="o">:=</span> <span class="s2">&quot;Any&quot;</span>
</pre></div>
</div>
<p>Where:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">Any</span></code> is the Python class name from the typing module. Therefore, to use the <code class="docutils literal notranslate"><span class="pre">Any</span></code> type, you must import it from <code class="docutils literal notranslate"><span class="pre">typing</span></code> (e.g., <code class="docutils literal notranslate"><span class="pre">from</span> <span class="pre">typing</span> <span class="pre">import</span> <span class="pre">Any</span></code>).</p></li>
<li><p>Since <code class="docutils literal notranslate"><span class="pre">Any</span></code> can represent any TorchScript type, the set of operators that are allowed to operate on values of this type on <code class="docutils literal notranslate"><span class="pre">Any</span></code> is limited.</p></li>
</ul>
</section>
<section id="operators-supported-for-any-type">
<h4>Operators Supported for <code class="docutils literal notranslate"><span class="pre">Any</span></code> Type<a class="headerlink" href="#operators-supported-for-any-type" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p>Assignment to data of <code class="docutils literal notranslate"><span class="pre">Any</span></code> type.</p></li>
<li><p>Binding to parameter or return of <code class="docutils literal notranslate"><span class="pre">Any</span></code> type.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">x</span> <span class="pre">is</span></code>, <code class="docutils literal notranslate"><span class="pre">x</span> <span class="pre">is</span> <span class="pre">not</span></code> where <code class="docutils literal notranslate"><span class="pre">x</span></code> is of <code class="docutils literal notranslate"><span class="pre">Any</span></code> type.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">isinstance(x,</span> <span class="pre">Type)</span></code> where <code class="docutils literal notranslate"><span class="pre">x</span></code> is of <code class="docutils literal notranslate"><span class="pre">Any</span></code> type.</p></li>
<li><p>Data of <code class="docutils literal notranslate"><span class="pre">Any</span></code> type is printable.</p></li>
<li><p>Data of <code class="docutils literal notranslate"><span class="pre">List[Any]</span></code> type may be sortable if the data is a list of values of the same type <code class="docutils literal notranslate"><span class="pre">T</span></code> and that <code class="docutils literal notranslate"><span class="pre">T</span></code> supports comparison operators.</p></li>
</ul>
<p><strong>Compared to Python</strong></p>
<p><code class="docutils literal notranslate"><span class="pre">Any</span></code> is the least constrained type in the TorchScript type system. In that sense, it is quite similar to the
<code class="docutils literal notranslate"><span class="pre">Object</span></code> class in Python. However, <code class="docutils literal notranslate"><span class="pre">Any</span></code> only supports a subset of the operators and methods that are supported by <code class="docutils literal notranslate"><span class="pre">Object</span></code>.</p>
</section>
<section id="design-notes">
<h4>Design Notes<a class="headerlink" href="#design-notes" title="Link to this heading">#</a></h4>
<p>When we script a PyTorch module, we may encounter data that is not involved in the execution of the script. Nevertheless, it has to be described
by a type schema. It is not only cumbersome to describe static types for unused data (in the context of the script), but also may lead to unnecessary
scripting failures. <code class="docutils literal notranslate"><span class="pre">Any</span></code> is introduced to describe the type of the data where precise static types are not necessary for compilation.</p>
<p><strong>Example 1</strong></p>
<p>This example illustrates how <code class="docutils literal notranslate"><span class="pre">Any</span></code> can be used to allow the second element of the tuple parameter to be of any type. This is possible
because <code class="docutils literal notranslate"><span class="pre">x[1]</span></code> is not involved in any computation that requires knowing its precise type.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">typing</span><span class="w"> </span><span class="kn">import</span> <span class="n">Tuple</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">typing</span><span class="w"> </span><span class="kn">import</span> <span class="n">Any</span>

<span class="nd">@torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">export</span>
<span class="k">def</span><span class="w"> </span><span class="nf">inc_first_element</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">Any</span><span class="p">]):</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>

<span class="n">m</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">script</span><span class="p">(</span><span class="n">inc_first_element</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">m</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span><span class="mf">2.0</span><span class="p">)))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">m</span><span class="p">((</span><span class="mi">1</span><span class="p">,(</span><span class="mi">100</span><span class="p">,</span><span class="mi">200</span><span class="p">))))</span>
</pre></div>
</div>
<p>The example above produces the following output:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>(2, 2.0)
(2, (100, 200))
</pre></div>
</div>
<p>The second element of the tuple is of <code class="docutils literal notranslate"><span class="pre">Any</span></code> type, thus can bind to multiple types.
For example, <code class="docutils literal notranslate"><span class="pre">(1,</span> <span class="pre">2.0)</span></code> binds a float type to <code class="docutils literal notranslate"><span class="pre">Any</span></code> as in <code class="docutils literal notranslate"><span class="pre">Tuple[int,</span> <span class="pre">Any]</span></code>,
whereas <code class="docutils literal notranslate"><span class="pre">(1,</span> <span class="pre">(100,</span> <span class="pre">200))</span></code> binds a tuple to <code class="docutils literal notranslate"><span class="pre">Any</span></code> in the second invocation.</p>
<p><strong>Example 2</strong></p>
<p>This example illustrates how we can use <code class="docutils literal notranslate"><span class="pre">isinstance</span></code> to dynamically check the type of the data that is annotated as <code class="docutils literal notranslate"><span class="pre">Any</span></code> type:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">typing</span><span class="w"> </span><span class="kn">import</span> <span class="n">Any</span>

<span class="k">def</span><span class="w"> </span><span class="nf">f</span><span class="p">(</span><span class="n">a</span><span class="p">:</span><span class="n">Any</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">))</span>

<span class="n">ones</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">2</span><span class="p">])</span>
<span class="n">m</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">script</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">m</span><span class="p">(</span><span class="n">ones</span><span class="p">))</span>
</pre></div>
</div>
<p>The example above produces the following output:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span> 1
 1
[ CPUFloatType{2} ]
True
</pre></div>
</div>
</section>
</section>
<section id="primitive-types">
<h3>Primitive Types<a class="headerlink" href="#primitive-types" title="Link to this heading">#</a></h3>
<p>Primitive TorchScript types are types that represent a single type of value and go with a single pre-defined
type name.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">TSPrimitiveType</span> <span class="p">:</span><span class="o">:=</span> <span class="s2">&quot;int&quot;</span> <span class="o">|</span> <span class="s2">&quot;float&quot;</span> <span class="o">|</span> <span class="s2">&quot;double&quot;</span> <span class="o">|</span> <span class="s2">&quot;complex&quot;</span> <span class="o">|</span> <span class="s2">&quot;bool&quot;</span> <span class="o">|</span> <span class="s2">&quot;str&quot;</span> <span class="o">|</span> <span class="s2">&quot;None&quot;</span>
</pre></div>
</div>
</section>
<section id="structural-types">
<h3>Structural Types<a class="headerlink" href="#structural-types" title="Link to this heading">#</a></h3>
<p>Structural types are types that are structurally defined without a user-defined name (unlike nominal types),
such as <code class="docutils literal notranslate"><span class="pre">Future[int]</span></code>. Structural types are composable with any <code class="docutils literal notranslate"><span class="pre">TSType</span></code>.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">TSStructuralType</span> <span class="p">:</span><span class="o">:=</span>  <span class="n">TSTuple</span> <span class="o">|</span> <span class="n">TSNamedTuple</span> <span class="o">|</span> <span class="n">TSList</span> <span class="o">|</span> <span class="n">TSDict</span> <span class="o">|</span>
                    <span class="n">TSOptional</span> <span class="o">|</span> <span class="n">TSUnion</span> <span class="o">|</span> <span class="n">TSFuture</span> <span class="o">|</span> <span class="n">TSRRef</span> <span class="o">|</span> <span class="n">TSAwait</span>

<span class="n">TSTuple</span>          <span class="p">:</span><span class="o">:=</span> <span class="s2">&quot;Tuple&quot;</span> <span class="s2">&quot;[&quot;</span> <span class="p">(</span><span class="n">TSType</span> <span class="s2">&quot;,&quot;</span><span class="p">)</span><span class="o">*</span> <span class="n">TSType</span> <span class="s2">&quot;]&quot;</span>
<span class="n">TSNamedTuple</span>     <span class="p">:</span><span class="o">:=</span> <span class="s2">&quot;namedtuple&quot;</span> <span class="s2">&quot;(&quot;</span> <span class="p">(</span><span class="n">TSType</span> <span class="s2">&quot;,&quot;</span><span class="p">)</span><span class="o">*</span> <span class="n">TSType</span> <span class="s2">&quot;)&quot;</span>
<span class="n">TSList</span>           <span class="p">:</span><span class="o">:=</span> <span class="s2">&quot;List&quot;</span> <span class="s2">&quot;[&quot;</span> <span class="n">TSType</span> <span class="s2">&quot;]&quot;</span>
<span class="n">TSOptional</span>       <span class="p">:</span><span class="o">:=</span> <span class="s2">&quot;Optional&quot;</span> <span class="s2">&quot;[&quot;</span> <span class="n">TSType</span> <span class="s2">&quot;]&quot;</span>
<span class="n">TSUnion</span>          <span class="p">:</span><span class="o">:=</span> <span class="s2">&quot;Union&quot;</span> <span class="s2">&quot;[&quot;</span> <span class="p">(</span><span class="n">TSType</span> <span class="s2">&quot;,&quot;</span><span class="p">)</span><span class="o">*</span> <span class="n">TSType</span> <span class="s2">&quot;]&quot;</span>
<span class="n">TSFuture</span>         <span class="p">:</span><span class="o">:=</span> <span class="s2">&quot;Future&quot;</span> <span class="s2">&quot;[&quot;</span> <span class="n">TSType</span> <span class="s2">&quot;]&quot;</span>
<span class="n">TSRRef</span>           <span class="p">:</span><span class="o">:=</span> <span class="s2">&quot;RRef&quot;</span> <span class="s2">&quot;[&quot;</span> <span class="n">TSType</span> <span class="s2">&quot;]&quot;</span>
<span class="n">TSAwait</span>          <span class="p">:</span><span class="o">:=</span> <span class="s2">&quot;Await&quot;</span> <span class="s2">&quot;[&quot;</span> <span class="n">TSType</span> <span class="s2">&quot;]&quot;</span>
<span class="n">TSDict</span>           <span class="p">:</span><span class="o">:=</span> <span class="s2">&quot;Dict&quot;</span> <span class="s2">&quot;[&quot;</span> <span class="n">KeyType</span> <span class="s2">&quot;,&quot;</span> <span class="n">TSType</span> <span class="s2">&quot;]&quot;</span>
<span class="n">KeyType</span>          <span class="p">:</span><span class="o">:=</span> <span class="s2">&quot;str&quot;</span> <span class="o">|</span> <span class="s2">&quot;int&quot;</span> <span class="o">|</span> <span class="s2">&quot;float&quot;</span> <span class="o">|</span> <span class="s2">&quot;bool&quot;</span> <span class="o">|</span> <span class="n">TensorType</span> <span class="o">|</span> <span class="s2">&quot;Any&quot;</span>
</pre></div>
</div>
<p>Where:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">Tuple</span></code>, <code class="docutils literal notranslate"><span class="pre">List</span></code>, <code class="docutils literal notranslate"><span class="pre">Optional</span></code>, <code class="docutils literal notranslate"><span class="pre">Union</span></code>, <code class="docutils literal notranslate"><span class="pre">Future</span></code>, <code class="docutils literal notranslate"><span class="pre">Dict</span></code> represent Python type class names that are defined in the module <code class="docutils literal notranslate"><span class="pre">typing</span></code>. To use these type names, you must import them from <code class="docutils literal notranslate"><span class="pre">typing</span></code> (e.g., <code class="docutils literal notranslate"><span class="pre">from</span> <span class="pre">typing</span> <span class="pre">import</span> <span class="pre">Tuple</span></code>).</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">namedtuple</span></code> represents the Python class <code class="docutils literal notranslate"><span class="pre">collections.namedtuple</span></code> or <code class="docutils literal notranslate"><span class="pre">typing.NamedTuple</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Future</span></code> and <code class="docutils literal notranslate"><span class="pre">RRef</span></code> represent the Python classes <code class="docutils literal notranslate"><span class="pre">torch.futures</span></code> and <code class="docutils literal notranslate"><span class="pre">torch.distributed.rpc</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Await</span></code> represent the Python class <code class="docutils literal notranslate"><span class="pre">torch._awaits._Await</span></code></p></li>
</ul>
<p><strong>Compared to Python</strong></p>
<p>Apart from being composable with TorchScript types, these TorchScript structural types often support a common subset of the operators and methods of their Python counterparts.</p>
<p><strong>Example 1</strong></p>
<p>This example uses <code class="docutils literal notranslate"><span class="pre">typing.NamedTuple</span></code> syntax to define a tuple:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">typing</span><span class="w"> </span><span class="kn">import</span> <span class="n">NamedTuple</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">typing</span><span class="w"> </span><span class="kn">import</span> <span class="n">Tuple</span>

<span class="k">class</span><span class="w"> </span><span class="nc">MyTuple</span><span class="p">(</span><span class="n">NamedTuple</span><span class="p">):</span>
    <span class="n">first</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">second</span><span class="p">:</span> <span class="nb">int</span>

<span class="k">def</span><span class="w"> </span><span class="nf">inc</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">MyTuple</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]:</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">first</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">second</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>

<span class="n">t</span> <span class="o">=</span> <span class="n">MyTuple</span><span class="p">(</span><span class="n">first</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">second</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">scripted_inc</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">script</span><span class="p">(</span><span class="n">inc</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;TorchScript:&quot;</span><span class="p">,</span> <span class="n">scripted_inc</span><span class="p">(</span><span class="n">t</span><span class="p">))</span>
</pre></div>
</div>
<p>The example above produces the following output:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>TorchScript: (2, 3)
</pre></div>
</div>
<p><strong>Example 2</strong></p>
<p>This example uses <code class="docutils literal notranslate"><span class="pre">collections.namedtuple</span></code> syntax to define a tuple:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">typing</span><span class="w"> </span><span class="kn">import</span> <span class="n">NamedTuple</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">typing</span><span class="w"> </span><span class="kn">import</span> <span class="n">Tuple</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">collections</span><span class="w"> </span><span class="kn">import</span> <span class="n">namedtuple</span>

<span class="n">_AnnotatedNamedTuple</span> <span class="o">=</span> <span class="n">NamedTuple</span><span class="p">(</span><span class="s1">&#39;_NamedTupleAnnotated&#39;</span><span class="p">,</span> <span class="p">[(</span><span class="s1">&#39;first&#39;</span><span class="p">,</span> <span class="nb">int</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;second&#39;</span><span class="p">,</span> <span class="nb">int</span><span class="p">)])</span>
<span class="n">_UnannotatedNamedTuple</span> <span class="o">=</span> <span class="n">namedtuple</span><span class="p">(</span><span class="s1">&#39;_NamedTupleAnnotated&#39;</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;first&#39;</span><span class="p">,</span> <span class="s1">&#39;second&#39;</span><span class="p">])</span>

<span class="k">def</span><span class="w"> </span><span class="nf">inc</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">_AnnotatedNamedTuple</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]:</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">first</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">second</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>

<span class="n">m</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">script</span><span class="p">(</span><span class="n">inc</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">inc</span><span class="p">(</span><span class="n">_UnannotatedNamedTuple</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">)))</span>
</pre></div>
</div>
<p>The example above produces the following output:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>(2, 3)
</pre></div>
</div>
<p><strong>Example 3</strong></p>
<p>This example illustrates a common mistake of annotating structural types, i.e., not importing the composite type
classes from the <code class="docutils literal notranslate"><span class="pre">typing</span></code> module:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>

<span class="c1"># ERROR: Tuple not recognized because not imported from typing</span>
<span class="nd">@torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">export</span>
<span class="k">def</span><span class="w"> </span><span class="nf">inc</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]):</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>

<span class="n">m</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">script</span><span class="p">(</span><span class="n">inc</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">m</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">)))</span>
</pre></div>
</div>
<p>Running the above code yields the following scripting error:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">File</span> <span class="s2">&quot;test-tuple.py&quot;</span><span class="p">,</span> <span class="n">line</span> <span class="mi">5</span><span class="p">,</span> <span class="ow">in</span> <span class="o">&lt;</span><span class="n">module</span><span class="o">&gt;</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">inc</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]):</span>
<span class="ne">NameError</span><span class="p">:</span> <span class="n">name</span> <span class="s1">&#39;Tuple&#39;</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">defined</span>
</pre></div>
</div>
<p>The remedy is to add the line <code class="docutils literal notranslate"><span class="pre">from</span> <span class="pre">typing</span> <span class="pre">import</span> <span class="pre">Tuple</span></code> to the beginning of the code.</p>
</section>
<section id="nominal-types">
<h3>Nominal Types<a class="headerlink" href="#nominal-types" title="Link to this heading">#</a></h3>
<p>Nominal TorchScript types are Python classes. These types are called nominal because they are declared with a custom
name and are compared using class names. Nominal classes are further classified into the following categories:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">TSNominalType</span> <span class="p">:</span><span class="o">:=</span> <span class="n">TSBuiltinClasses</span> <span class="o">|</span> <span class="n">TSCustomClass</span> <span class="o">|</span> <span class="n">TSEnum</span>
</pre></div>
</div>
<p>Among them, <code class="docutils literal notranslate"><span class="pre">TSCustomClass</span></code> and <code class="docutils literal notranslate"><span class="pre">TSEnum</span></code> must be compilable to TorchScript Intermediate Representation (IR). This is enforced by the type-checker.</p>
</section>
<section id="built-in-class">
<h3>Built-in Class<a class="headerlink" href="#built-in-class" title="Link to this heading">#</a></h3>
<p>Built-in nominal types are Python classes whose semantics are built into the TorchScript system (e.g., tensor types).
TorchScript defines the semantics of these built-in nominal types, and often supports only a subset of the methods or
attributes of its Python class definition.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">TSBuiltinClass</span> <span class="p">:</span><span class="o">:=</span> <span class="n">TSTensor</span> <span class="o">|</span> <span class="s2">&quot;torch.device&quot;</span> <span class="o">|</span> <span class="s2">&quot;torch.Stream&quot;</span> <span class="o">|</span> <span class="s2">&quot;torch.dtype&quot;</span> <span class="o">|</span>
                   <span class="s2">&quot;torch.nn.ModuleList&quot;</span> <span class="o">|</span> <span class="s2">&quot;torch.nn.ModuleDict&quot;</span> <span class="o">|</span> <span class="o">...</span>
<span class="n">TSTensor</span>       <span class="p">:</span><span class="o">:=</span> <span class="s2">&quot;torch.Tensor&quot;</span> <span class="o">|</span> <span class="s2">&quot;common.SubTensor&quot;</span> <span class="o">|</span> <span class="s2">&quot;common.SubWithTorchFunction&quot;</span> <span class="o">|</span>
                   <span class="s2">&quot;torch.nn.parameter.Parameter&quot;</span> <span class="o">|</span> <span class="ow">and</span> <span class="n">subclasses</span> <span class="n">of</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>
</pre></div>
</div>
<section id="special-note-on-torch-nn-modulelist-and-torch-nn-moduledict">
<h4>Special Note on torch.nn.ModuleList and torch.nn.ModuleDict<a class="headerlink" href="#special-note-on-torch-nn-modulelist-and-torch-nn-moduledict" title="Link to this heading">#</a></h4>
<p>Although <code class="docutils literal notranslate"><span class="pre">torch.nn.ModuleList</span></code> and <code class="docutils literal notranslate"><span class="pre">torch.nn.ModuleDict</span></code> are defined as a list and dictionary in Python,
they behave more like tuples in TorchScript:</p>
<ul class="simple">
<li><p>In TorchScript, instances of <code class="docutils literal notranslate"><span class="pre">torch.nn.ModuleList</span></code>  or <code class="docutils literal notranslate"><span class="pre">torch.nn.ModuleDict</span></code> are immutable.</p></li>
<li><p>Code that iterates over <code class="docutils literal notranslate"><span class="pre">torch.nn.ModuleList</span></code> or <code class="docutils literal notranslate"><span class="pre">torch.nn.ModuleDict</span></code> is completely unrolled so that elements of <code class="docutils literal notranslate"><span class="pre">torch.nn.ModuleList</span></code> or keys of <code class="docutils literal notranslate"><span class="pre">torch.nn.ModuleDict</span></code> can be of different subclasses of <code class="docutils literal notranslate"><span class="pre">torch.nn.Module</span></code>.</p></li>
</ul>
<p><strong>Example</strong></p>
<p>The following example highlights the use of a few built-in Torchscript classes (<code class="docutils literal notranslate"><span class="pre">torch.*</span></code>):</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>

<span class="nd">@torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">script</span>
<span class="k">class</span><span class="w"> </span><span class="nc">A</span><span class="p">:</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">f</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">x</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">y</span><span class="p">)</span>

<span class="k">def</span><span class="w"> </span><span class="nf">g</span><span class="p">():</span>
    <span class="n">a</span> <span class="o">=</span> <span class="n">A</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">a</span><span class="o">.</span><span class="n">f</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cpu&quot;</span><span class="p">))</span>

<span class="n">script_g</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">script</span><span class="p">(</span><span class="n">g</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">script_g</span><span class="o">.</span><span class="n">graph</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
<section id="custom-class">
<h3>Custom Class<a class="headerlink" href="#custom-class" title="Link to this heading">#</a></h3>
<p>Unlike built-in classes, semantics of custom classes are user-defined and the entire class definition must be compilable to TorchScript IR and subject to TorchScript type-checking rules.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">TSClassDef</span> <span class="p">:</span><span class="o">:=</span> <span class="p">[</span> <span class="s2">&quot;@torch.jit.script&quot;</span> <span class="p">]</span>
                 <span class="s2">&quot;class&quot;</span> <span class="n">ClassName</span> <span class="p">[</span> <span class="s2">&quot;(object)&quot;</span> <span class="p">]</span>  <span class="s2">&quot;:&quot;</span>
                    <span class="n">MethodDefinition</span> <span class="o">|</span>
                <span class="p">[</span> <span class="s2">&quot;@torch.jit.ignore&quot;</span> <span class="p">]</span> <span class="o">|</span> <span class="p">[</span> <span class="s2">&quot;@torch.jit.unused&quot;</span> <span class="p">]</span>
                    <span class="n">MethodDefinition</span>
</pre></div>
</div>
<p>Where:</p>
<ul class="simple">
<li><p>Classes must be new-style classes. Python 3 supports only new-style classes. In Python 2.x, a new-style class is specified by subclassing from the object.</p></li>
<li><p>Instance data attributes are statically typed, and instance attributes must be declared by assignments inside the <code class="docutils literal notranslate"><span class="pre">__init__()</span></code> method.</p></li>
<li><p>Method overloading is not supported (i.e., you cannot have multiple methods with the same method name).</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">MethodDefinition</span></code> must be compilable to TorchScript IR and adhere to TorchScript’s type-checking rules, (i.e., all methods must be valid TorchScript functions and class attribute definitions must be valid TorchScript statements).</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">torch.jit.ignore</span></code> and <code class="docutils literal notranslate"><span class="pre">torch.jit.unused</span></code> can be used to ignore the method or function that is not fully torchscriptable or should be ignored by the compiler.</p></li>
</ul>
<p><strong>Compared to Python</strong></p>
<p>TorchScript custom classes are quite limited compared to their Python counterpart. Torchscript custom classes:</p>
<ul class="simple">
<li><p>Do not support class attributes.</p></li>
<li><p>Do not support subclassing except for subclassing an interface type or object.</p></li>
<li><p>Do not support method overloading.</p></li>
<li><p>Must initialize all its instance attributes in  <code class="docutils literal notranslate"><span class="pre">__init__()</span></code>; this is because TorchScript constructs a static schema of the class by inferring attribute types in <code class="docutils literal notranslate"><span class="pre">__init__()</span></code>.</p></li>
<li><p>Must contain only methods that satisfy TorchScript type-checking rules and are compilable to TorchScript IRs.</p></li>
</ul>
<p><strong>Example 1</strong></p>
<p>Python classes can be used in TorchScript if they are annotated with <code class="docutils literal notranslate"><span class="pre">&#64;torch.jit.script</span></code>, similar to how a TorchScript function would be declared:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nd">@torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">script</span>
<span class="k">class</span><span class="w"> </span><span class="nc">MyClass</span><span class="p">:</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">x</span> <span class="o">=</span> <span class="n">x</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">inc</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">val</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">x</span> <span class="o">+=</span> <span class="n">val</span>
</pre></div>
</div>
<p><strong>Example 2</strong></p>
<p>A TorchScript custom class type must “declare” all its instance attributes by assignments in <code class="docutils literal notranslate"><span class="pre">__init__()</span></code>. If an instance attribute is not defined in <code class="docutils literal notranslate"><span class="pre">__init__()</span></code> but accessed in other methods of the class, the class cannot be compiled as a TorchScript class, as shown in the following example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>

<span class="nd">@torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">script</span>
<span class="k">class</span><span class="w"> </span><span class="nc">foo</span><span class="p">:</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">y</span> <span class="o">=</span> <span class="mi">1</span>

<span class="c1"># ERROR: self.x is not defined in __init__</span>
<span class="k">def</span><span class="w"> </span><span class="nf">assign_x</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
</pre></div>
</div>
<p>The class will fail to compile and issue the following error:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>RuntimeError:
Tried to set nonexistent attribute: x. Did you forget to initialize it in __init__()?:
def assign_x(self):
    self.x = torch.rand(2, 3)
    ~~~~~~~~~~~~~~~~~~~~~~~~ &lt;--- HERE
</pre></div>
</div>
<p><strong>Example 3</strong></p>
<p>In this example, a TorchScript custom class defines a class variable name, which is not allowed:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>

<span class="nd">@torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">script</span>
<span class="k">class</span><span class="w"> </span><span class="nc">MyClass</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;MyClass&quot;</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">x</span> <span class="o">=</span> <span class="n">x</span>

<span class="k">def</span><span class="w"> </span><span class="nf">fn</span><span class="p">(</span><span class="n">a</span><span class="p">:</span> <span class="n">MyClass</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">a</span><span class="o">.</span><span class="n">name</span>
</pre></div>
</div>
<p>It leads to the following compile-time error:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>RuntimeError:
&#39;__torch__.MyClass&#39; object has no attribute or method &#39;name&#39;. Did you forget to initialize an attribute in __init__()?:
    File &quot;test-class2.py&quot;, line 10
def fn(a: MyClass):
    return a.name
        ~~~~~~ &lt;--- HERE
</pre></div>
</div>
</section>
<section id="enum-type">
<h3>Enum Type<a class="headerlink" href="#enum-type" title="Link to this heading">#</a></h3>
<p>Like custom classes, semantics of the enum type are user-defined and the entire class definition must be compilable to TorchScript IR and adhere to TorchScript type-checking rules.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">TSEnumDef</span> <span class="p">:</span><span class="o">:=</span> <span class="s2">&quot;class&quot;</span> <span class="n">Identifier</span> <span class="s2">&quot;(enum.Enum | TSEnumType)&quot;</span> <span class="s2">&quot;:&quot;</span>
               <span class="p">(</span> <span class="n">MemberIdentifier</span> <span class="s2">&quot;=&quot;</span> <span class="n">Value</span> <span class="p">)</span><span class="o">+</span>
               <span class="p">(</span> <span class="n">MethodDefinition</span> <span class="p">)</span><span class="o">*</span>
</pre></div>
</div>
<p>Where:</p>
<ul class="simple">
<li><p>Value must be a TorchScript literal of type <code class="docutils literal notranslate"><span class="pre">int</span></code>, <code class="docutils literal notranslate"><span class="pre">float</span></code>, or <code class="docutils literal notranslate"><span class="pre">str</span></code>, and must be of the same TorchScript type.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">TSEnumType</span></code> is the name of a TorchScript enumerated type. Similar to Python enum, TorchScript allows restricted <code class="docutils literal notranslate"><span class="pre">Enum</span></code> subclassing, that is, subclassing an enumerated is allowed only if it does not define any members.</p></li>
</ul>
<p><strong>Compared to Python</strong></p>
<ul class="simple">
<li><p>TorchScript supports only <code class="docutils literal notranslate"><span class="pre">enum.Enum</span></code>. It does not support other variations such as <code class="docutils literal notranslate"><span class="pre">enum.IntEnum</span></code>, <code class="docutils literal notranslate"><span class="pre">enum.Flag</span></code>, <code class="docutils literal notranslate"><span class="pre">enum.IntFlag</span></code>, and <code class="docutils literal notranslate"><span class="pre">enum.auto</span></code>.</p></li>
<li><p>Values of TorchScript enum members must be of the same type and can only be <code class="docutils literal notranslate"><span class="pre">int</span></code>, <code class="docutils literal notranslate"><span class="pre">float</span></code>, or <code class="docutils literal notranslate"><span class="pre">str</span></code> types, whereas Python enum members can be of any type.</p></li>
<li><p>Enums containing methods are ignored in TorchScript.</p></li>
</ul>
<p><strong>Example 1</strong></p>
<p>The following example defines the class <code class="docutils literal notranslate"><span class="pre">Color</span></code> as an <code class="docutils literal notranslate"><span class="pre">Enum</span></code> type:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">enum</span><span class="w"> </span><span class="kn">import</span> <span class="n">Enum</span>

<span class="k">class</span><span class="w"> </span><span class="nc">Color</span><span class="p">(</span><span class="n">Enum</span><span class="p">):</span>
    <span class="n">RED</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="n">GREEN</span> <span class="o">=</span> <span class="mi">2</span>

<span class="k">def</span><span class="w"> </span><span class="nf">enum_fn</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">Color</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">Color</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">x</span> <span class="o">==</span> <span class="n">Color</span><span class="o">.</span><span class="n">RED</span><span class="p">:</span>
        <span class="k">return</span> <span class="kc">True</span>
    <span class="k">return</span> <span class="n">x</span> <span class="o">==</span> <span class="n">y</span>

<span class="n">m</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">script</span><span class="p">(</span><span class="n">enum_fn</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Eager: &quot;</span><span class="p">,</span> <span class="n">enum_fn</span><span class="p">(</span><span class="n">Color</span><span class="o">.</span><span class="n">RED</span><span class="p">,</span> <span class="n">Color</span><span class="o">.</span><span class="n">GREEN</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;TorchScript: &quot;</span><span class="p">,</span> <span class="n">m</span><span class="p">(</span><span class="n">Color</span><span class="o">.</span><span class="n">RED</span><span class="p">,</span> <span class="n">Color</span><span class="o">.</span><span class="n">GREEN</span><span class="p">))</span>
</pre></div>
</div>
<p><strong>Example 2</strong></p>
<p>The following example shows the case of restricted enum subclassing, where <code class="docutils literal notranslate"><span class="pre">BaseColor</span></code> does not define any member, thus can be subclassed by <code class="docutils literal notranslate"><span class="pre">Color</span></code>:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">enum</span><span class="w"> </span><span class="kn">import</span> <span class="n">Enum</span>

<span class="k">class</span><span class="w"> </span><span class="nc">BaseColor</span><span class="p">(</span><span class="n">Enum</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">foo</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

<span class="k">class</span><span class="w"> </span><span class="nc">Color</span><span class="p">(</span><span class="n">BaseColor</span><span class="p">):</span>
    <span class="n">RED</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="n">GREEN</span> <span class="o">=</span> <span class="mi">2</span>

<span class="k">def</span><span class="w"> </span><span class="nf">enum_fn</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">Color</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">Color</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">x</span> <span class="o">==</span> <span class="n">Color</span><span class="o">.</span><span class="n">RED</span><span class="p">:</span>
        <span class="k">return</span> <span class="kc">True</span>
    <span class="k">return</span> <span class="n">x</span> <span class="o">==</span> <span class="n">y</span>

<span class="n">m</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">script</span><span class="p">(</span><span class="n">enum_fn</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;TorchScript: &quot;</span><span class="p">,</span> <span class="n">m</span><span class="p">(</span><span class="n">Color</span><span class="o">.</span><span class="n">RED</span><span class="p">,</span> <span class="n">Color</span><span class="o">.</span><span class="n">GREEN</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Eager: &quot;</span><span class="p">,</span> <span class="n">enum_fn</span><span class="p">(</span><span class="n">Color</span><span class="o">.</span><span class="n">RED</span><span class="p">,</span> <span class="n">Color</span><span class="o">.</span><span class="n">GREEN</span><span class="p">))</span>
</pre></div>
</div>
</section>
<section id="torchscript-module-class">
<h3>TorchScript Module Class<a class="headerlink" href="#torchscript-module-class" title="Link to this heading">#</a></h3>
<p><code class="docutils literal notranslate"><span class="pre">TSModuleType</span></code> is a special class type that is inferred from object instances that are created outside TorchScript. <code class="docutils literal notranslate"><span class="pre">TSModuleType</span></code> is named by the Python class of the object instance. The <code class="docutils literal notranslate"><span class="pre">__init__()</span></code> method of the Python class is not considered a TorchScript method, so it does not have to comply with TorchScript’s type-checking rules.</p>
<p>The type schema of a module instance class is constructed directly from an instance object (created outside the scope of TorchScript) rather than inferred from <code class="docutils literal notranslate"><span class="pre">__init__()</span></code> like custom classes. It is possible that two objects of the same instance class type follow two different type schemas.</p>
<p>In this sense, <code class="docutils literal notranslate"><span class="pre">TSModuleType</span></code> is not really a static type. Therefore, for type safety considerations, <code class="docutils literal notranslate"><span class="pre">TSModuleType</span></code> cannot be used in a TorchScript type annotation or be composed with <code class="docutils literal notranslate"><span class="pre">TSType</span></code>.</p>
</section>
<section id="module-instance-class">
<h3>Module Instance Class<a class="headerlink" href="#module-instance-class" title="Link to this heading">#</a></h3>
<p>TorchScript module type represents the type schema of a user-defined PyTorch module instance.  When scripting a PyTorch module, the module object is always created outside TorchScript (i.e., passed in as parameter to <code class="docutils literal notranslate"><span class="pre">forward</span></code>). The Python module class is treated as a module instance class, so the <code class="docutils literal notranslate"><span class="pre">__init__()</span></code> method of the Python module class is not subject to the type-checking rules of TorchScript.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">TSModuleType</span> <span class="p">:</span><span class="o">:=</span> <span class="s2">&quot;class&quot;</span> <span class="n">Identifier</span> <span class="s2">&quot;(torch.nn.Module)&quot;</span> <span class="s2">&quot;:&quot;</span>
                    <span class="n">ClassBodyDefinition</span>
</pre></div>
</div>
<p>Where:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">forward()</span></code> and other methods decorated with <code class="docutils literal notranslate"><span class="pre">&#64;torch.jit.export</span></code> must be compilable to TorchScript IR and subject to TorchScript’s type-checking rules.</p></li>
</ul>
<p>Unlike custom classes, only the forward method and other methods decorated with <code class="docutils literal notranslate"><span class="pre">&#64;torch.jit.export</span></code>  of the module type need to be compilable. Most notably, <code class="docutils literal notranslate"><span class="pre">__init__()</span></code> is not considered a TorchScript method. Consequently, module type constructors cannot be invoked within the scope of TorchScript. Instead, TorchScript module objects are always constructed outside and passed into <code class="docutils literal notranslate"><span class="pre">torch.jit.script(ModuleObj)</span></code>.</p>
<p><strong>Example 1</strong></p>
<p>This example illustrates a few features of module types:</p>
<ul class="simple">
<li><p>The <code class="docutils literal notranslate"><span class="pre">TestModule</span></code> instance is created outside the scope of TorchScript (i.e., before invoking <code class="docutils literal notranslate"><span class="pre">torch.jit.script</span></code>).</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">__init__()</span></code> is not considered a TorchScript method, therefore, it does not have to be annotated and can contain arbitrary Python code. In addition, the <code class="docutils literal notranslate"><span class="pre">__init__()</span></code> method of an instance class cannot be invoked in TorchScript code. Because <code class="docutils literal notranslate"><span class="pre">TestModule</span></code> instances are instantiated in Python, in this example, <code class="docutils literal notranslate"><span class="pre">TestModule(2.0)</span></code> and <code class="docutils literal notranslate"><span class="pre">TestModule(2)</span></code> create two instances with different types for its data attributes. <code class="docutils literal notranslate"><span class="pre">self.x</span></code> is of type <code class="docutils literal notranslate"><span class="pre">float</span></code> for <code class="docutils literal notranslate"><span class="pre">TestModule(2.0)</span></code>, whereas <code class="docutils literal notranslate"><span class="pre">self.y</span></code> is of type <code class="docutils literal notranslate"><span class="pre">int</span></code> for <code class="docutils literal notranslate"><span class="pre">TestModule(2.0)</span></code>.</p></li>
<li><p>TorchScript automatically compiles other methods (e.g., <code class="docutils literal notranslate"><span class="pre">mul()</span></code>) invoked by methods annotated via <code class="docutils literal notranslate"><span class="pre">&#64;torch.jit.export</span></code> or <code class="docutils literal notranslate"><span class="pre">forward()</span></code> methods.</p></li>
<li><p>Entry-points to a TorchScript program are either <code class="docutils literal notranslate"><span class="pre">forward()</span></code> of a module type, functions annotated as <code class="docutils literal notranslate"><span class="pre">torch.jit.script</span></code>, or methods annotated as <code class="docutils literal notranslate"><span class="pre">torch.jit.export</span></code>.</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>

<span class="k">class</span><span class="w"> </span><span class="nc">TestModule</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">v</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">x</span> <span class="o">=</span> <span class="n">v</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inc</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">x</span> <span class="o">+</span> <span class="n">inc</span>

<span class="n">m</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">script</span><span class="p">(</span><span class="n">TestModule</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;First instance: </span><span class="si">{</span><span class="n">m</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="n">m</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">script</span><span class="p">(</span><span class="n">TestModule</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">5</span><span class="p">])))</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Second instance: </span><span class="si">{</span><span class="n">m</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>The example above produces the following output:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>First instance: 4
Second instance: tensor([4., 4., 4., 4., 4.])
</pre></div>
</div>
<p><strong>Example 2</strong></p>
<p>The following example shows an incorrect usage of module type. Specifically, this example invokes the constructor of <code class="docutils literal notranslate"><span class="pre">TestModule</span></code> inside the scope of TorchScript:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>

<span class="k">class</span><span class="w"> </span><span class="nc">TestModule</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">v</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">x</span> <span class="o">=</span> <span class="n">v</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">x</span> <span class="o">+</span> <span class="n">x</span>

<span class="k">class</span><span class="w"> </span><span class="nc">MyModel</span><span class="p">:</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">v</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">val</span> <span class="o">=</span> <span class="n">v</span>

    <span class="nd">@torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">export</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">doSomething</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">val</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
        <span class="c1"># error: should not invoke the constructor of module type</span>
        <span class="n">myModel</span> <span class="o">=</span> <span class="n">TestModule</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">val</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">myModel</span><span class="p">(</span><span class="n">val</span><span class="p">)</span>

<span class="c1"># m = torch.jit.script(MyModel(2)) # Results in below RuntimeError</span>
<span class="c1"># RuntimeError: Could not get name of python class object</span>
</pre></div>
</div>
</section>
</section>
<section id="type-annotation">
<span id="id2"></span><h2><a class="toc-backref" href="#id9" role="doc-backlink">Type Annotation</a><a class="headerlink" href="#type-annotation" title="Link to this heading">#</a></h2>
<p>Since TorchScript is statically typed, programmers need to annotate types at <em>strategic points</em> of TorchScript code so that every local variable or
instance data attribute has a static type, and every function and method has a statically typed signature.</p>
<section id="when-to-annotate-types">
<h3>When to Annotate Types<a class="headerlink" href="#when-to-annotate-types" title="Link to this heading">#</a></h3>
<p>In general, type annotations are only needed in places where static types cannot be automatically inferred (e.g., parameters or sometimes return types to
methods or functions). Types of local variables and data attributes are often automatically inferred from their assignment statements. Sometimes an inferred type
may be too restrictive, e.g., <code class="docutils literal notranslate"><span class="pre">x</span></code> being inferred as <code class="docutils literal notranslate"><span class="pre">NoneType</span></code> through assignment <code class="docutils literal notranslate"><span class="pre">x</span> <span class="pre">=</span> <span class="pre">None</span></code>, whereas <code class="docutils literal notranslate"><span class="pre">x</span></code> is actually used as an <code class="docutils literal notranslate"><span class="pre">Optional</span></code>. In such
cases, type annotations may be needed to overwrite auto inference, e.g., <code class="docutils literal notranslate"><span class="pre">x:</span> <span class="pre">Optional[int]</span> <span class="pre">=</span> <span class="pre">None</span></code>. Note that it is always safe to type annotate a local variable
or data attribute even if its type can be automatically inferred. The annotated type must be congruent with TorchScript’s type-checking.</p>
<p>When a parameter, local variable, or data attribute is not type annotated and its type cannot be automatically inferred, TorchScript assumes it to be a
default type of <code class="docutils literal notranslate"><span class="pre">TensorType</span></code>, <code class="docutils literal notranslate"><span class="pre">List[TensorType]</span></code>, or <code class="docutils literal notranslate"><span class="pre">Dict[str,</span> <span class="pre">TensorType]</span></code>.</p>
</section>
<section id="annotate-function-signature">
<h3>Annotate Function Signature<a class="headerlink" href="#annotate-function-signature" title="Link to this heading">#</a></h3>
<p>Since a parameter may not be automatically inferred from the body of the function (including both functions and methods), they need to be type annotated. Otherwise, they assume the default type <code class="docutils literal notranslate"><span class="pre">TensorType</span></code>.</p>
<p>TorchScript supports two styles for method and function signature type annotation:</p>
<ul class="simple">
<li><p><strong>Python3-style</strong> annotates types directly on the signature. As such, it allows individual parameters to be left unannotated (whose type will be the default type of <code class="docutils literal notranslate"><span class="pre">TensorType</span></code>), or allows the return type to be left unannotated (whose type will be automatically inferred).</p></li>
</ul>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Python3Annotation</span> <span class="p">:</span><span class="o">:=</span> <span class="s2">&quot;def&quot;</span> <span class="n">Identifier</span> <span class="p">[</span> <span class="s2">&quot;(&quot;</span> <span class="n">ParamAnnot</span><span class="o">*</span> <span class="s2">&quot;)&quot;</span> <span class="p">]</span> <span class="p">[</span><span class="n">ReturnAnnot</span><span class="p">]</span> <span class="s2">&quot;:&quot;</span>
                            <span class="n">FuncOrMethodBody</span>
<span class="n">ParamAnnot</span>        <span class="p">:</span><span class="o">:=</span> <span class="n">Identifier</span> <span class="p">[</span> <span class="s2">&quot;:&quot;</span> <span class="n">TSType</span> <span class="p">]</span> <span class="s2">&quot;,&quot;</span>
<span class="n">ReturnAnnot</span>       <span class="p">:</span><span class="o">:=</span> <span class="s2">&quot;-&gt;&quot;</span> <span class="n">TSType</span>
</pre></div>
</div>
<p>Note that when using Python3 style, the type <code class="docutils literal notranslate"><span class="pre">self</span></code> is automatically inferred and should not be annotated.</p>
<ul class="simple">
<li><p><strong>Mypy style</strong> annotates types as a comment right below the function/method declaration. In the Mypy style, since parameter names do not appear in the annotation, all parameters have to be annotated.</p></li>
</ul>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">MyPyAnnotation</span> <span class="p">:</span><span class="o">:=</span> <span class="s2">&quot;# type:&quot;</span> <span class="s2">&quot;(&quot;</span> <span class="n">ParamAnnot</span><span class="o">*</span> <span class="s2">&quot;)&quot;</span> <span class="p">[</span> <span class="n">ReturnAnnot</span> <span class="p">]</span>
<span class="n">ParamAnnot</span>     <span class="p">:</span><span class="o">:=</span> <span class="n">TSType</span> <span class="s2">&quot;,&quot;</span>
<span class="n">ReturnAnnot</span>    <span class="p">:</span><span class="o">:=</span> <span class="s2">&quot;-&gt;&quot;</span> <span class="n">TSType</span>
</pre></div>
</div>
<p><strong>Example 1</strong></p>
<p>In this example:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">a</span></code> is not annotated and assumes the default type of <code class="docutils literal notranslate"><span class="pre">TensorType</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">b</span></code> is annotated as type <code class="docutils literal notranslate"><span class="pre">int</span></code>.</p></li>
<li><p>The return type is not annotated and is automatically inferred as type <code class="docutils literal notranslate"><span class="pre">TensorType</span></code> (based on the type of the value being returned).</p></li>
</ul>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>

<span class="k">def</span><span class="w"> </span><span class="nf">f</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">a</span><span class="o">+</span><span class="n">b</span>

<span class="n">m</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">script</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;TorchScript:&quot;</span><span class="p">,</span> <span class="n">m</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">6</span><span class="p">]),</span> <span class="mi">100</span><span class="p">))</span>
</pre></div>
</div>
<p><strong>Example 2</strong></p>
<p>The following example uses Mypy style annotation. Note that parameters or return values must be annotated even if some of
them assume the default type.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>

<span class="k">def</span><span class="w"> </span><span class="nf">f</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
    <span class="c1"># type: (torch.Tensor, int) → torch.Tensor</span>
    <span class="k">return</span> <span class="n">a</span><span class="o">+</span><span class="n">b</span>

<span class="n">m</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">script</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;TorchScript:&quot;</span><span class="p">,</span> <span class="n">m</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">6</span><span class="p">]),</span> <span class="mi">100</span><span class="p">))</span>
</pre></div>
</div>
</section>
<section id="annotate-variables-and-data-attributes">
<h3>Annotate Variables and Data Attributes<a class="headerlink" href="#annotate-variables-and-data-attributes" title="Link to this heading">#</a></h3>
<p>In general, types of data attributes (including class and instance data attributes) and local variables can be automatically inferred from assignment statements.
Sometimes, however, if a variable or attribute is associated with values of different types (e.g., as <code class="docutils literal notranslate"><span class="pre">None</span></code> or <code class="docutils literal notranslate"><span class="pre">TensorType</span></code>), then they may need to be explicitly
type annotated as a <em>wider</em> type such as <code class="docutils literal notranslate"><span class="pre">Optional[int]</span></code> or <code class="docutils literal notranslate"><span class="pre">Any</span></code>.</p>
<section id="local-variables">
<h4>Local Variables<a class="headerlink" href="#local-variables" title="Link to this heading">#</a></h4>
<p>Local variables can be annotated according to Python3 typing module annotation rules, i.e.,</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">LocalVarAnnotation</span> <span class="p">:</span><span class="o">:=</span> <span class="n">Identifier</span> <span class="p">[</span><span class="s2">&quot;:&quot;</span> <span class="n">TSType</span><span class="p">]</span> <span class="s2">&quot;=&quot;</span> <span class="n">Expr</span>
</pre></div>
</div>
<p>In general, types of local variables can be automatically inferred. In some cases, however, you may need to annotate a multi-type for local variables
that may be associated with different concrete types. Typical multi-types include <code class="docutils literal notranslate"><span class="pre">Optional[T]</span></code> and <code class="docutils literal notranslate"><span class="pre">Any</span></code>.</p>
<p><strong>Example</strong></p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>

<span class="k">def</span><span class="w"> </span><span class="nf">f</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">setVal</span><span class="p">:</span> <span class="nb">bool</span><span class="p">):</span>
    <span class="n">value</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">if</span> <span class="n">setVal</span><span class="p">:</span>
        <span class="n">value</span> <span class="o">=</span> <span class="n">a</span>
    <span class="k">return</span> <span class="n">value</span>

<span class="n">ones</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">6</span><span class="p">])</span>
<span class="n">m</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">script</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;TorchScript:&quot;</span><span class="p">,</span> <span class="n">m</span><span class="p">(</span><span class="n">ones</span><span class="p">,</span> <span class="kc">True</span><span class="p">),</span> <span class="n">m</span><span class="p">(</span><span class="n">ones</span><span class="p">,</span> <span class="kc">False</span><span class="p">))</span>
</pre></div>
</div>
</section>
<section id="instance-data-attributes">
<h4>Instance Data Attributes<a class="headerlink" href="#instance-data-attributes" title="Link to this heading">#</a></h4>
<p>For <code class="docutils literal notranslate"><span class="pre">ModuleType</span></code> classes, instance data attributes can be annotated according to Python3 typing module annotation rules. Instance data attributes can be annotated (optionally) as final
via <code class="docutils literal notranslate"><span class="pre">Final</span></code>.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="s2">&quot;class&quot;</span> <span class="n">ClassIdentifier</span> <span class="s2">&quot;(torch.nn.Module):&quot;</span>
<span class="n">InstanceAttrIdentifier</span> <span class="s2">&quot;:&quot;</span> <span class="p">[</span><span class="s2">&quot;Final(&quot;</span><span class="p">]</span> <span class="n">TSType</span> <span class="p">[</span><span class="s2">&quot;)&quot;</span><span class="p">]</span>
<span class="o">...</span>
</pre></div>
</div>
<p>Where:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">InstanceAttrIdentifier</span></code> is the name of an instance attribute.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Final</span></code> indicates that the attribute cannot be re-assigned outside of <code class="docutils literal notranslate"><span class="pre">__init__</span></code> or overridden in subclasses.</p></li>
</ul>
<p><strong>Example</strong></p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>

<span class="k">class</span><span class="w"> </span><span class="nc">MyModule</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="n">offset_</span><span class="p">:</span> <span class="nb">int</span>

<span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">offset</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">offset_</span> <span class="o">=</span> <span class="n">offset</span>

<span class="o">...</span>
</pre></div>
</div>
</section>
</section>
<section id="type-annotation-apis">
<h3>Type Annotation APIs<a class="headerlink" href="#type-annotation-apis" title="Link to this heading">#</a></h3>
<section id="torch-jit-annotate-t-expr">
<h4><code class="docutils literal notranslate"><span class="pre">torch.jit.annotate(T,</span> <span class="pre">expr)</span></code><a class="headerlink" href="#torch-jit-annotate-t-expr" title="Link to this heading">#</a></h4>
<p>This API annotates type <code class="docutils literal notranslate"><span class="pre">T</span></code> to an expression <code class="docutils literal notranslate"><span class="pre">expr</span></code>. This is often used when the default type of an expression is not the type intended by the programmer.
For instance, an empty list (dictionary) has the default type of <code class="docutils literal notranslate"><span class="pre">List[TensorType]</span></code> (<code class="docutils literal notranslate"><span class="pre">Dict[TensorType,</span> <span class="pre">TensorType]</span></code>), but sometimes it may be used to initialize
a list of some other types. Another common use case is for annotating the return type of <code class="docutils literal notranslate"><span class="pre">tensor.tolist()</span></code>. Note, however, that it cannot be used to annotate
the type of a module attribute in <cite>__init__</cite>; <code class="docutils literal notranslate"><span class="pre">torch.jit.Attribute</span></code> should be used for this instead.</p>
<p><strong>Example</strong></p>
<p>In this example, <code class="docutils literal notranslate"><span class="pre">[]</span></code> is declared as a list of integers via <code class="docutils literal notranslate"><span class="pre">torch.jit.annotate</span></code> (instead of assuming <code class="docutils literal notranslate"><span class="pre">[]</span></code> to be the default type of <code class="docutils literal notranslate"><span class="pre">List[TensorType]</span></code>):</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">typing</span><span class="w"> </span><span class="kn">import</span> <span class="n">List</span>

<span class="k">def</span><span class="w"> </span><span class="nf">g</span><span class="p">(</span><span class="n">l</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="n">val</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
    <span class="n">l</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">val</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">l</span>

<span class="k">def</span><span class="w"> </span><span class="nf">f</span><span class="p">(</span><span class="n">val</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
    <span class="n">l</span> <span class="o">=</span> <span class="n">g</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="p">[]),</span> <span class="n">val</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">l</span>

<span class="n">m</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">script</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Eager:&quot;</span><span class="p">,</span> <span class="n">f</span><span class="p">(</span><span class="mi">3</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;TorchScript:&quot;</span><span class="p">,</span> <span class="n">m</span><span class="p">(</span><span class="mi">3</span><span class="p">))</span>
</pre></div>
</div>
<p>See <a class="reference internal" href="generated/torch.jit.annotate.html#torch.jit.annotate" title="torch.jit.annotate"><code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.jit.annotate()</span></code></a> for more information.</p>
</section>
</section>
<section id="type-annotation-appendix">
<h3>Type Annotation Appendix<a class="headerlink" href="#type-annotation-appendix" title="Link to this heading">#</a></h3>
<section id="torchscript-type-system-definition">
<h4>TorchScript Type System Definition<a class="headerlink" href="#torchscript-type-system-definition" title="Link to this heading">#</a></h4>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">TSAllType</span>       <span class="p">:</span><span class="o">:=</span> <span class="n">TSType</span> <span class="o">|</span> <span class="n">TSModuleType</span>
<span class="n">TSType</span>          <span class="p">:</span><span class="o">:=</span> <span class="n">TSMetaType</span> <span class="o">|</span> <span class="n">TSPrimitiveType</span> <span class="o">|</span> <span class="n">TSStructuralType</span> <span class="o">|</span> <span class="n">TSNominalType</span>

<span class="n">TSMetaType</span>      <span class="p">:</span><span class="o">:=</span> <span class="s2">&quot;Any&quot;</span>
<span class="n">TSPrimitiveType</span> <span class="p">:</span><span class="o">:=</span> <span class="s2">&quot;int&quot;</span> <span class="o">|</span> <span class="s2">&quot;float&quot;</span> <span class="o">|</span> <span class="s2">&quot;double&quot;</span> <span class="o">|</span> <span class="s2">&quot;complex&quot;</span> <span class="o">|</span> <span class="s2">&quot;bool&quot;</span> <span class="o">|</span> <span class="s2">&quot;str&quot;</span> <span class="o">|</span> <span class="s2">&quot;None&quot;</span>

<span class="n">TSStructuralType</span> <span class="p">:</span><span class="o">:=</span> <span class="n">TSTuple</span> <span class="o">|</span> <span class="n">TSNamedTuple</span> <span class="o">|</span> <span class="n">TSList</span> <span class="o">|</span> <span class="n">TSDict</span> <span class="o">|</span> <span class="n">TSOptional</span> <span class="o">|</span>
                     <span class="n">TSUnion</span> <span class="o">|</span> <span class="n">TSFuture</span> <span class="o">|</span> <span class="n">TSRRef</span> <span class="o">|</span> <span class="n">TSAwait</span>
<span class="n">TSTuple</span>         <span class="p">:</span><span class="o">:=</span> <span class="s2">&quot;Tuple&quot;</span> <span class="s2">&quot;[&quot;</span> <span class="p">(</span><span class="n">TSType</span> <span class="s2">&quot;,&quot;</span><span class="p">)</span><span class="o">*</span> <span class="n">TSType</span> <span class="s2">&quot;]&quot;</span>
<span class="n">TSNamedTuple</span>    <span class="p">:</span><span class="o">:=</span> <span class="s2">&quot;namedtuple&quot;</span> <span class="s2">&quot;(&quot;</span> <span class="p">(</span><span class="n">TSType</span> <span class="s2">&quot;,&quot;</span><span class="p">)</span><span class="o">*</span> <span class="n">TSType</span> <span class="s2">&quot;)&quot;</span>
<span class="n">TSList</span>          <span class="p">:</span><span class="o">:=</span> <span class="s2">&quot;List&quot;</span> <span class="s2">&quot;[&quot;</span> <span class="n">TSType</span> <span class="s2">&quot;]&quot;</span>
<span class="n">TSOptional</span>      <span class="p">:</span><span class="o">:=</span> <span class="s2">&quot;Optional&quot;</span> <span class="s2">&quot;[&quot;</span> <span class="n">TSType</span> <span class="s2">&quot;]&quot;</span>
<span class="n">TSUnion</span>         <span class="p">:</span><span class="o">:=</span> <span class="s2">&quot;Union&quot;</span> <span class="s2">&quot;[&quot;</span> <span class="p">(</span><span class="n">TSType</span> <span class="s2">&quot;,&quot;</span><span class="p">)</span><span class="o">*</span> <span class="n">TSType</span> <span class="s2">&quot;]&quot;</span>
<span class="n">TSFuture</span>        <span class="p">:</span><span class="o">:=</span> <span class="s2">&quot;Future&quot;</span> <span class="s2">&quot;[&quot;</span> <span class="n">TSType</span> <span class="s2">&quot;]&quot;</span>
<span class="n">TSRRef</span>          <span class="p">:</span><span class="o">:=</span> <span class="s2">&quot;RRef&quot;</span> <span class="s2">&quot;[&quot;</span> <span class="n">TSType</span> <span class="s2">&quot;]&quot;</span>
<span class="n">TSAwait</span>         <span class="p">:</span><span class="o">:=</span> <span class="s2">&quot;Await&quot;</span> <span class="s2">&quot;[&quot;</span> <span class="n">TSType</span> <span class="s2">&quot;]&quot;</span>
<span class="n">TSDict</span>          <span class="p">:</span><span class="o">:=</span> <span class="s2">&quot;Dict&quot;</span> <span class="s2">&quot;[&quot;</span> <span class="n">KeyType</span> <span class="s2">&quot;,&quot;</span> <span class="n">TSType</span> <span class="s2">&quot;]&quot;</span>
<span class="n">KeyType</span>         <span class="p">:</span><span class="o">:=</span> <span class="s2">&quot;str&quot;</span> <span class="o">|</span> <span class="s2">&quot;int&quot;</span> <span class="o">|</span> <span class="s2">&quot;float&quot;</span> <span class="o">|</span> <span class="s2">&quot;bool&quot;</span> <span class="o">|</span> <span class="n">TensorType</span> <span class="o">|</span> <span class="s2">&quot;Any&quot;</span>

<span class="n">TSNominalType</span>   <span class="p">:</span><span class="o">:=</span> <span class="n">TSBuiltinClasses</span> <span class="o">|</span> <span class="n">TSCustomClass</span> <span class="o">|</span> <span class="n">TSEnum</span>
<span class="n">TSBuiltinClass</span>  <span class="p">:</span><span class="o">:=</span> <span class="n">TSTensor</span> <span class="o">|</span> <span class="s2">&quot;torch.device&quot;</span> <span class="o">|</span> <span class="s2">&quot;torch.stream&quot;</span><span class="o">|</span>
                    <span class="s2">&quot;torch.dtype&quot;</span> <span class="o">|</span> <span class="s2">&quot;torch.nn.ModuleList&quot;</span> <span class="o">|</span>
                    <span class="s2">&quot;torch.nn.ModuleDict&quot;</span> <span class="o">|</span> <span class="o">...</span>
<span class="n">TSTensor</span>        <span class="p">:</span><span class="o">:=</span> <span class="s2">&quot;torch.tensor&quot;</span> <span class="ow">and</span> <span class="n">subclasses</span>
</pre></div>
</div>
</section>
<section id="unsupported-typing-constructs">
<h4>Unsupported Typing Constructs<a class="headerlink" href="#unsupported-typing-constructs" title="Link to this heading">#</a></h4>
<p>TorchScript does not support all features and types of the Python3 <a class="reference external" href="https://docs.python.org/3/library/typing.html#module-typing">typing</a> module.
Any functionality from the <a class="reference external" href="https://docs.python.org/3/library/typing.html#module-typing">typing</a> module that is not explicitly specified in this
documentation is unsupported. The following table summarizes <code class="docutils literal notranslate"><span class="pre">typing</span></code> constructs that are either unsupported or supported with restrictions in TorchScript.</p>
<div class="pst-scrollable-table-container"><table class="table">
<tbody>
<tr class="row-odd"><td><p>Item</p></td>
<td><p>Description</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">typing.Any</span></code></p></td>
<td><p>In development</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">typing.NoReturn</span></code></p></td>
<td><p>Not supported</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">typing.Callable</span></code></p></td>
<td><p>Not supported</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">typing.Literal</span></code></p></td>
<td><p>Not supported</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">typing.ClassVar</span></code></p></td>
<td><p>Not supported</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">typing.Final</span></code></p></td>
<td><p>Supported for module attributes, class attribute, and annotations, but not for functions.</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">typing.AnyStr</span></code></p></td>
<td><p>Not supported</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">typing.overload</span></code></p></td>
<td><p>In development</p></td>
</tr>
<tr class="row-even"><td><p>Type aliases</p></td>
<td><p>Not supported</p></td>
</tr>
<tr class="row-odd"><td><p>Nominal typing</p></td>
<td><p>In development</p></td>
</tr>
<tr class="row-even"><td><p>Structural typing</p></td>
<td><p>Not supported</p></td>
</tr>
<tr class="row-odd"><td><p>NewType</p></td>
<td><p>Not supported</p></td>
</tr>
<tr class="row-even"><td><p>Generics</p></td>
<td><p>Not supported</p></td>
</tr>
</tbody>
</table>
</div>
</section>
</section>
</section>
<section id="expressions">
<span id="id4"></span><h2><a class="toc-backref" href="#id10" role="doc-backlink">Expressions</a><a class="headerlink" href="#expressions" title="Link to this heading">#</a></h2>
<p>The following section describes the grammar of expressions that are supported in TorchScript.
It is modeled after <a class="reference external" href="https://docs.python.org/3/reference/expressions.html">the expressions chapter of the Python language reference</a>.</p>
<section id="arithmetic-conversions">
<h3>Arithmetic Conversions<a class="headerlink" href="#arithmetic-conversions" title="Link to this heading">#</a></h3>
<p>There are a number of implicit type conversions that are performed in TorchScript:</p>
<ul class="simple">
<li><p>A <code class="docutils literal notranslate"><span class="pre">Tensor</span></code> with a <code class="docutils literal notranslate"><span class="pre">float</span></code> or <code class="docutils literal notranslate"><span class="pre">int</span></code> data type can be implicitly converted to an instance of <code class="docutils literal notranslate"><span class="pre">FloatType</span></code> or <code class="docutils literal notranslate"><span class="pre">IntType</span></code> provided that it has a size of 0, does not have <code class="docutils literal notranslate"><span class="pre">require_grad</span></code> set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, and will not require narrowing.</p></li>
<li><p>Instances of <code class="docutils literal notranslate"><span class="pre">StringType</span></code> can be implicitly converted to <code class="docutils literal notranslate"><span class="pre">DeviceType</span></code>.</p></li>
<li><p>The implicit conversion rules from the two bullet points above can be applied to instances of <code class="docutils literal notranslate"><span class="pre">TupleType</span></code> to produce instances of <code class="docutils literal notranslate"><span class="pre">ListType</span></code> with the appropriate contained type.</p></li>
</ul>
<p>Explicit conversions can be invoked using the <code class="docutils literal notranslate"><span class="pre">float</span></code>, <code class="docutils literal notranslate"><span class="pre">int</span></code>, <code class="docutils literal notranslate"><span class="pre">bool</span></code>, and <code class="docutils literal notranslate"><span class="pre">str</span></code> built-in functions
that accept primitive data types as arguments and can accept user-defined types if they implement
<code class="docutils literal notranslate"><span class="pre">__bool__</span></code>, <code class="docutils literal notranslate"><span class="pre">__str__</span></code>, etc.</p>
</section>
<section id="atoms">
<h3>Atoms<a class="headerlink" href="#atoms" title="Link to this heading">#</a></h3>
<p>Atoms are the most basic elements of expressions.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">atom</span>      <span class="p">:</span><span class="o">:=</span>  <span class="n">identifier</span> <span class="o">|</span> <span class="n">literal</span> <span class="o">|</span> <span class="n">enclosure</span>
<span class="n">enclosure</span> <span class="p">:</span><span class="o">:=</span>  <span class="n">parenth_form</span> <span class="o">|</span> <span class="n">list_display</span> <span class="o">|</span> <span class="n">dict_display</span>
</pre></div>
</div>
<section id="identifiers">
<h4>Identifiers<a class="headerlink" href="#identifiers" title="Link to this heading">#</a></h4>
<p>The rules that dictate what is a legal identifier in TorchScript are the same as
their <a class="reference external" href="https://docs.python.org/3/reference/lexical_analysis.html#identifiers">Python counterparts</a>.</p>
</section>
<section id="literals">
<h4>Literals<a class="headerlink" href="#literals" title="Link to this heading">#</a></h4>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">literal</span> <span class="p">:</span><span class="o">:=</span>  <span class="n">stringliteral</span> <span class="o">|</span> <span class="n">integer</span> <span class="o">|</span> <span class="n">floatnumber</span>
</pre></div>
</div>
<p>Evaluation of a literal yields an object of the appropriate type with the specific value
(with approximations applied as necessary for floats). Literals are immutable, and multiple evaluations
of identical literals may obtain the same object or distinct objects with the same value.
<a class="reference external" href="https://docs.python.org/3/reference/lexical_analysis.html#string-and-bytes-literals">stringliteral</a>,
<a class="reference external" href="https://docs.python.org/3/reference/lexical_analysis.html#integer-literals">integer</a>, and
<a class="reference external" href="https://docs.python.org/3/reference/lexical_analysis.html#floating-point-literals">floatnumber</a>
are defined in the same way as their Python counterparts.</p>
</section>
<section id="parenthesized-forms">
<h4>Parenthesized Forms<a class="headerlink" href="#parenthesized-forms" title="Link to this heading">#</a></h4>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">parenth_form</span> <span class="p">:</span><span class="o">:=</span>  <span class="s1">&#39;(&#39;</span> <span class="p">[</span><span class="n">expression_list</span><span class="p">]</span> <span class="s1">&#39;)&#39;</span>
</pre></div>
</div>
<p>A parenthesized expression list yields whatever the expression list yields. If the list contains at least one
comma, it yields a <code class="docutils literal notranslate"><span class="pre">Tuple</span></code>; otherwise, it yields the single expression inside the expression list. An empty
pair of parentheses yields an empty <code class="docutils literal notranslate"><span class="pre">Tuple</span></code> object (<code class="docutils literal notranslate"><span class="pre">Tuple[]</span></code>).</p>
</section>
<section id="list-and-dictionary-displays">
<h4>List and Dictionary Displays<a class="headerlink" href="#list-and-dictionary-displays" title="Link to this heading">#</a></h4>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">list_comprehension</span> <span class="p">:</span><span class="o">:=</span>  <span class="n">expression</span> <span class="n">comp_for</span>
<span class="n">comp_for</span>           <span class="p">:</span><span class="o">:=</span>  <span class="s1">&#39;for&#39;</span> <span class="n">target_list</span> <span class="s1">&#39;in&#39;</span> <span class="n">or_expr</span>
<span class="n">list_display</span>       <span class="p">:</span><span class="o">:=</span>  <span class="s1">&#39;[&#39;</span> <span class="p">[</span><span class="n">expression_list</span> <span class="o">|</span> <span class="n">list_comprehension</span><span class="p">]</span> <span class="s1">&#39;]&#39;</span>
<span class="n">dict_display</span>       <span class="p">:</span><span class="o">:=</span>  <span class="s1">&#39;{&#39;</span> <span class="p">[</span><span class="n">key_datum_list</span> <span class="o">|</span> <span class="n">dict_comprehension</span><span class="p">]</span> <span class="s1">&#39;}&#39;</span>
<span class="n">key_datum_list</span>     <span class="p">:</span><span class="o">:=</span>  <span class="n">key_datum</span> <span class="p">(</span><span class="s1">&#39;,&#39;</span> <span class="n">key_datum</span><span class="p">)</span><span class="o">*</span>
<span class="n">key_datum</span>          <span class="p">:</span><span class="o">:=</span>  <span class="n">expression</span> <span class="s1">&#39;:&#39;</span> <span class="n">expression</span>
<span class="n">dict_comprehension</span> <span class="p">:</span><span class="o">:=</span>  <span class="n">key_datum</span> <span class="n">comp_for</span>
</pre></div>
</div>
<p>Lists and dicts can be constructed by either listing the container contents explicitly or by providing
instructions on how to compute them via a set of looping instructions (i.e. a <em>comprehension</em>). A comprehension
is semantically equivalent to using a for loop and appending to an ongoing list.
Comprehensions implicitly create their own scope to make sure that the items of the target list do not leak into the
enclosing scope. In the case that container items are explicitly listed, the expressions in the expression list
are evaluated left-to-right. If a key is repeated in a <code class="docutils literal notranslate"><span class="pre">dict_display</span></code> that has a <code class="docutils literal notranslate"><span class="pre">key_datum_list</span></code>, the
resultant dictionary uses the value from the rightmost datum in the list that uses the repeated key.</p>
</section>
</section>
<section id="primaries">
<h3>Primaries<a class="headerlink" href="#primaries" title="Link to this heading">#</a></h3>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">primary</span> <span class="p">:</span><span class="o">:=</span>  <span class="n">atom</span> <span class="o">|</span> <span class="n">attributeref</span> <span class="o">|</span> <span class="n">subscription</span> <span class="o">|</span> <span class="n">slicing</span> <span class="o">|</span> <span class="n">call</span>
</pre></div>
</div>
<section id="attribute-references">
<h4>Attribute References<a class="headerlink" href="#attribute-references" title="Link to this heading">#</a></h4>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">attributeref</span> <span class="p">:</span><span class="o">:=</span>  <span class="n">primary</span> <span class="s1">&#39;.&#39;</span> <span class="n">identifier</span>
</pre></div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">primary</span></code> must evaluate to an object of a type that supports attribute references that have an attribute named
<code class="docutils literal notranslate"><span class="pre">identifier</span></code>.</p>
</section>
<section id="subscriptions">
<h4>Subscriptions<a class="headerlink" href="#subscriptions" title="Link to this heading">#</a></h4>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">subscription</span> <span class="p">:</span><span class="o">:=</span>  <span class="n">primary</span> <span class="s1">&#39;[&#39;</span> <span class="n">expression_list</span> <span class="s1">&#39;]&#39;</span>
</pre></div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">primary</span></code> must evaluate to an object that supports subscription.</p>
<ul class="simple">
<li><p>If the primary is a <code class="docutils literal notranslate"><span class="pre">List</span></code>, <code class="docutils literal notranslate"><span class="pre">Tuple</span></code>, or <code class="docutils literal notranslate"><span class="pre">str</span></code>, the expression list must evaluate to an integer or slice.</p></li>
<li><p>If the primary is a <code class="docutils literal notranslate"><span class="pre">Dict</span></code>, the expression list must evaluate to an object of the same type as the key type of the <code class="docutils literal notranslate"><span class="pre">Dict</span></code>.</p></li>
<li><p>If the primary is a <code class="docutils literal notranslate"><span class="pre">ModuleList</span></code>, the expression list must be an <code class="docutils literal notranslate"><span class="pre">integer</span></code> literal.</p></li>
<li><p>If the primary is a <code class="docutils literal notranslate"><span class="pre">ModuleDict</span></code>, the expression must be a <code class="docutils literal notranslate"><span class="pre">stringliteral</span></code>.</p></li>
</ul>
</section>
<section id="slicings">
<h4>Slicings<a class="headerlink" href="#slicings" title="Link to this heading">#</a></h4>
<p>A slicing selects a range of items in a <code class="docutils literal notranslate"><span class="pre">str</span></code>, <code class="docutils literal notranslate"><span class="pre">Tuple</span></code>, <code class="docutils literal notranslate"><span class="pre">List</span></code>, or <code class="docutils literal notranslate"><span class="pre">Tensor</span></code>. Slicings may be used as
expressions or targets in assignment or <code class="docutils literal notranslate"><span class="pre">del</span></code> statements.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">slicing</span>      <span class="p">:</span><span class="o">:=</span>  <span class="n">primary</span> <span class="s1">&#39;[&#39;</span> <span class="n">slice_list</span> <span class="s1">&#39;]&#39;</span>
<span class="n">slice_list</span>   <span class="p">:</span><span class="o">:=</span>  <span class="n">slice_item</span> <span class="p">(</span><span class="s1">&#39;,&#39;</span> <span class="n">slice_item</span><span class="p">)</span><span class="o">*</span> <span class="p">[</span><span class="s1">&#39;,&#39;</span><span class="p">]</span>
<span class="n">slice_item</span>   <span class="p">:</span><span class="o">:=</span>  <span class="n">expression</span> <span class="o">|</span> <span class="n">proper_slice</span>
<span class="n">proper_slice</span> <span class="p">:</span><span class="o">:=</span>  <span class="p">[</span><span class="n">expression</span><span class="p">]</span> <span class="s1">&#39;:&#39;</span> <span class="p">[</span><span class="n">expression</span><span class="p">]</span> <span class="p">[</span><span class="s1">&#39;:&#39;</span> <span class="p">[</span><span class="n">expression</span><span class="p">]</span> <span class="p">]</span>
</pre></div>
</div>
<p>Slicings with more than one slice item in their slice lists can only be used with primaries that evaluate to an
object of type <code class="docutils literal notranslate"><span class="pre">Tensor</span></code>.</p>
</section>
<section id="calls">
<h4>Calls<a class="headerlink" href="#calls" title="Link to this heading">#</a></h4>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">call</span>          <span class="p">:</span><span class="o">:=</span>  <span class="n">primary</span> <span class="s1">&#39;(&#39;</span> <span class="n">argument_list</span> <span class="s1">&#39;)&#39;</span>
<span class="n">argument_list</span> <span class="p">:</span><span class="o">:=</span>  <span class="n">args</span> <span class="p">[</span><span class="s1">&#39;,&#39;</span> <span class="n">kwargs</span><span class="p">]</span> <span class="o">|</span> <span class="n">kwargs</span>
<span class="n">args</span>          <span class="p">:</span><span class="o">:=</span>  <span class="p">[</span><span class="n">arg</span> <span class="p">(</span><span class="s1">&#39;,&#39;</span> <span class="n">arg</span><span class="p">)</span><span class="o">*</span><span class="p">]</span>
<span class="n">kwargs</span>        <span class="p">:</span><span class="o">:=</span>  <span class="p">[</span><span class="n">kwarg</span> <span class="p">(</span><span class="s1">&#39;,&#39;</span> <span class="n">kwarg</span><span class="p">)</span><span class="o">*</span><span class="p">]</span>
<span class="n">kwarg</span>         <span class="p">:</span><span class="o">:=</span>  <span class="n">arg</span> <span class="s1">&#39;=&#39;</span> <span class="n">expression</span>
<span class="n">arg</span>           <span class="p">:</span><span class="o">:=</span>  <span class="n">identifier</span>
</pre></div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">primary</span></code> must desugar or evaluate to a callable object. All argument expressions are evaluated
before the call is attempted.</p>
</section>
</section>
<section id="power-operator">
<h3>Power Operator<a class="headerlink" href="#power-operator" title="Link to this heading">#</a></h3>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">power</span> <span class="p">:</span><span class="o">:=</span>  <span class="n">primary</span> <span class="p">[</span><span class="s1">&#39;**&#39;</span> <span class="n">u_expr</span><span class="p">]</span>
</pre></div>
</div>
<p>The power operator has the same semantics as the built-in pow function (not supported); it computes its
left argument raised to the power of its right argument. It binds more tightly than unary operators on the
left, but less tightly than unary operators on the right; i.e. <code class="docutils literal notranslate"><span class="pre">-2</span> <span class="pre">**</span> <span class="pre">-3</span> <span class="pre">==</span> <span class="pre">-(2</span> <span class="pre">**</span> <span class="pre">(-3))</span></code>.  The left and right
operands can be <code class="docutils literal notranslate"><span class="pre">int</span></code>, <code class="docutils literal notranslate"><span class="pre">float</span></code> or <code class="docutils literal notranslate"><span class="pre">Tensor</span></code>. Scalars are broadcast in the case of scalar-tensor/tensor-scalar
exponentiation operations, and tensor-tensor exponentiation is done elementwise without any broadcasting.</p>
</section>
<section id="unary-and-arithmetic-bitwise-operations">
<h3>Unary and Arithmetic Bitwise Operations<a class="headerlink" href="#unary-and-arithmetic-bitwise-operations" title="Link to this heading">#</a></h3>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">u_expr</span> <span class="p">:</span><span class="o">:=</span>  <span class="n">power</span> <span class="o">|</span> <span class="s1">&#39;-&#39;</span> <span class="n">power</span> <span class="o">|</span> <span class="s1">&#39;~&#39;</span> <span class="n">power</span>
</pre></div>
</div>
<p>The unary <code class="docutils literal notranslate"><span class="pre">-</span></code> operator yields the negation of its argument. The unary <code class="docutils literal notranslate"><span class="pre">~</span></code> operator yields the bitwise inversion
of its argument. <code class="docutils literal notranslate"><span class="pre">-</span></code> can be used with <code class="docutils literal notranslate"><span class="pre">int</span></code>, <code class="docutils literal notranslate"><span class="pre">float</span></code>, and <code class="docutils literal notranslate"><span class="pre">Tensor</span></code> of <code class="docutils literal notranslate"><span class="pre">int</span></code> and <code class="docutils literal notranslate"><span class="pre">float</span></code>.
<code class="docutils literal notranslate"><span class="pre">~</span></code> can only be used with <code class="docutils literal notranslate"><span class="pre">int</span></code> and <code class="docutils literal notranslate"><span class="pre">Tensor</span></code> of <code class="docutils literal notranslate"><span class="pre">int</span></code>.</p>
</section>
<section id="binary-arithmetic-operations">
<h3>Binary Arithmetic Operations<a class="headerlink" href="#binary-arithmetic-operations" title="Link to this heading">#</a></h3>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">m_expr</span> <span class="p">:</span><span class="o">:=</span>  <span class="n">u_expr</span> <span class="o">|</span> <span class="n">m_expr</span> <span class="s1">&#39;*&#39;</span> <span class="n">u_expr</span> <span class="o">|</span> <span class="n">m_expr</span> <span class="s1">&#39;@&#39;</span> <span class="n">m_expr</span> <span class="o">|</span> <span class="n">m_expr</span> <span class="s1">&#39;//&#39;</span> <span class="n">u_expr</span> <span class="o">|</span> <span class="n">m_expr</span> <span class="s1">&#39;/&#39;</span> <span class="n">u_expr</span> <span class="o">|</span> <span class="n">m_expr</span> <span class="s1">&#39;%&#39;</span> <span class="n">u_expr</span>
<span class="n">a_expr</span> <span class="p">:</span><span class="o">:=</span>  <span class="n">m_expr</span> <span class="o">|</span> <span class="n">a_expr</span> <span class="s1">&#39;+&#39;</span> <span class="n">m_expr</span> <span class="o">|</span> <span class="n">a_expr</span> <span class="s1">&#39;-&#39;</span> <span class="n">m_expr</span>
</pre></div>
</div>
<p>The binary arithmetic operators can operate on <code class="docutils literal notranslate"><span class="pre">Tensor</span></code>, <code class="docutils literal notranslate"><span class="pre">int</span></code>, and <code class="docutils literal notranslate"><span class="pre">float</span></code>. For tensor-tensor ops, both arguments must
have the same shape. For scalar-tensor or tensor-scalar ops, the scalar is usually broadcast to the size of the
tensor. Division ops can only accept scalars as their right-hand side argument, and do not support broadcasting.
The <code class="docutils literal notranslate"><span class="pre">&#64;</span></code> operator is for matrix multiplication and only operates on <code class="docutils literal notranslate"><span class="pre">Tensor</span></code> arguments. The multiplication operator
(<code class="docutils literal notranslate"><span class="pre">*</span></code>) can be used with a list and integer in order to get a result that is the original list repeated a certain
number of times.</p>
</section>
<section id="shifting-operations">
<h3>Shifting Operations<a class="headerlink" href="#shifting-operations" title="Link to this heading">#</a></h3>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">shift_expr</span> <span class="p">:</span><span class="o">:=</span>  <span class="n">a_expr</span> <span class="o">|</span> <span class="n">shift_expr</span> <span class="p">(</span> <span class="s1">&#39;&lt;&lt;&#39;</span> <span class="o">|</span> <span class="s1">&#39;&gt;&gt;&#39;</span> <span class="p">)</span> <span class="n">a_expr</span>
</pre></div>
</div>
<p>These operators accept two <code class="docutils literal notranslate"><span class="pre">int</span></code> arguments, two <code class="docutils literal notranslate"><span class="pre">Tensor</span></code> arguments, or a <code class="docutils literal notranslate"><span class="pre">Tensor</span></code> argument and an <code class="docutils literal notranslate"><span class="pre">int</span></code> or
<code class="docutils literal notranslate"><span class="pre">float</span></code> argument. In all cases, a right shift by <code class="docutils literal notranslate"><span class="pre">n</span></code> is defined as floor division by <code class="docutils literal notranslate"><span class="pre">pow(2,</span> <span class="pre">n)</span></code>, and a left shift
by <code class="docutils literal notranslate"><span class="pre">n</span></code> is defined as multiplication by <code class="docutils literal notranslate"><span class="pre">pow(2,</span> <span class="pre">n)</span></code>. When both arguments are <code class="docutils literal notranslate"><span class="pre">Tensors</span></code>, they must have the same
shape. When one is a scalar and the other is a <code class="docutils literal notranslate"><span class="pre">Tensor</span></code>, the scalar is logically broadcast to match the size of
the <code class="docutils literal notranslate"><span class="pre">Tensor</span></code>.</p>
</section>
<section id="binary-bitwise-operations">
<h3>Binary Bitwise Operations<a class="headerlink" href="#binary-bitwise-operations" title="Link to this heading">#</a></h3>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">and_expr</span> <span class="p">:</span><span class="o">:=</span>  <span class="n">shift_expr</span> <span class="o">|</span> <span class="n">and_expr</span> <span class="s1">&#39;&amp;&#39;</span> <span class="n">shift_expr</span>
<span class="n">xor_expr</span> <span class="p">:</span><span class="o">:=</span>  <span class="n">and_expr</span> <span class="o">|</span> <span class="n">xor_expr</span> <span class="s1">&#39;^&#39;</span> <span class="n">and_expr</span>
<span class="n">or_expr</span>  <span class="p">:</span><span class="o">:=</span>  <span class="n">xor_expr</span> <span class="o">|</span> <span class="n">or_expr</span> <span class="s1">&#39;|&#39;</span> <span class="n">xor_expr</span>
</pre></div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">&amp;</span></code> operator computes the bitwise AND of its arguments, the <code class="docutils literal notranslate"><span class="pre">^</span></code> the bitwise XOR, and the <code class="docutils literal notranslate"><span class="pre">|</span></code> the bitwise OR.
Both operands must be <code class="docutils literal notranslate"><span class="pre">int</span></code> or <code class="docutils literal notranslate"><span class="pre">Tensor</span></code>, or the left operand must be <code class="docutils literal notranslate"><span class="pre">Tensor</span></code> and the right operand must be
<code class="docutils literal notranslate"><span class="pre">int</span></code>. When both operands are <code class="docutils literal notranslate"><span class="pre">Tensor</span></code>, they must have the same shape. When the right operand is <code class="docutils literal notranslate"><span class="pre">int</span></code>, and
the left operand is <code class="docutils literal notranslate"><span class="pre">Tensor</span></code>, the right operand is logically broadcast to match the shape of the <code class="docutils literal notranslate"><span class="pre">Tensor</span></code>.</p>
</section>
<section id="comparisons">
<h3>Comparisons<a class="headerlink" href="#comparisons" title="Link to this heading">#</a></h3>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">comparison</span>    <span class="p">:</span><span class="o">:=</span>  <span class="n">or_expr</span> <span class="p">(</span><span class="n">comp_operator</span> <span class="n">or_expr</span><span class="p">)</span><span class="o">*</span>
<span class="n">comp_operator</span> <span class="p">:</span><span class="o">:=</span>  <span class="s1">&#39;&lt;&#39;</span> <span class="o">|</span> <span class="s1">&#39;&gt;&#39;</span> <span class="o">|</span> <span class="s1">&#39;==&#39;</span> <span class="o">|</span> <span class="s1">&#39;&gt;=&#39;</span> <span class="o">|</span> <span class="s1">&#39;&lt;=&#39;</span> <span class="o">|</span> <span class="s1">&#39;!=&#39;</span> <span class="o">|</span> <span class="s1">&#39;is&#39;</span> <span class="p">[</span><span class="s1">&#39;not&#39;</span><span class="p">]</span> <span class="o">|</span> <span class="p">[</span><span class="s1">&#39;not&#39;</span><span class="p">]</span> <span class="s1">&#39;in&#39;</span>
</pre></div>
</div>
<p>A comparison yields a boolean value (<code class="docutils literal notranslate"><span class="pre">True</span></code> or <code class="docutils literal notranslate"><span class="pre">False</span></code>), or if one of the operands is a <code class="docutils literal notranslate"><span class="pre">Tensor</span></code>, a boolean
<code class="docutils literal notranslate"><span class="pre">Tensor</span></code>. Comparisons can be chained arbitrarily as long as they do not yield boolean <code class="docutils literal notranslate"><span class="pre">Tensors</span></code> that have more
than one element. <code class="docutils literal notranslate"><span class="pre">a</span> <span class="pre">op1</span> <span class="pre">b</span> <span class="pre">op2</span> <span class="pre">c</span> <span class="pre">...</span></code> is equivalent to <code class="docutils literal notranslate"><span class="pre">a</span> <span class="pre">op1</span> <span class="pre">b</span> <span class="pre">and</span> <span class="pre">b</span> <span class="pre">op2</span> <span class="pre">c</span> <span class="pre">and</span> <span class="pre">...</span></code>.</p>
<section id="value-comparisons">
<h4>Value Comparisons<a class="headerlink" href="#value-comparisons" title="Link to this heading">#</a></h4>
<p>The operators <code class="docutils literal notranslate"><span class="pre">&lt;</span></code>, <code class="docutils literal notranslate"><span class="pre">&gt;</span></code>, <code class="docutils literal notranslate"><span class="pre">==</span></code>, <code class="docutils literal notranslate"><span class="pre">&gt;=</span></code>, <code class="docutils literal notranslate"><span class="pre">&lt;=</span></code>, and <code class="docutils literal notranslate"><span class="pre">!=</span></code> compare the values of two objects. The two objects generally need to be of
the same type, unless there is an implicit type conversion available between the objects. User-defined types can
be compared if rich comparison methods (e.g., <code class="docutils literal notranslate"><span class="pre">__lt__</span></code>) are defined on them. Built-in type comparison works like
Python:</p>
<ul class="simple">
<li><p>Numbers are compared mathematically.</p></li>
<li><p>Strings are compared lexicographically.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">lists</span></code>, <code class="docutils literal notranslate"><span class="pre">tuples</span></code>, and <code class="docutils literal notranslate"><span class="pre">dicts</span></code> can be compared only to other <code class="docutils literal notranslate"><span class="pre">lists</span></code>, <code class="docutils literal notranslate"><span class="pre">tuples</span></code>, and <code class="docutils literal notranslate"><span class="pre">dicts</span></code> of the same type and are compared using the comparison operator of corresponding elements.</p></li>
</ul>
</section>
<section id="membership-test-operations">
<h4>Membership Test Operations<a class="headerlink" href="#membership-test-operations" title="Link to this heading">#</a></h4>
<p>The operators <code class="docutils literal notranslate"><span class="pre">in</span></code> and <code class="docutils literal notranslate"><span class="pre">not</span> <span class="pre">in</span></code> test for membership. <code class="docutils literal notranslate"><span class="pre">x</span> <span class="pre">in</span> <span class="pre">s</span></code> evaluates to <code class="docutils literal notranslate"><span class="pre">True</span></code> if <code class="docutils literal notranslate"><span class="pre">x</span></code> is a member of <code class="docutils literal notranslate"><span class="pre">s</span></code> and <code class="docutils literal notranslate"><span class="pre">False</span></code> otherwise.
<code class="docutils literal notranslate"><span class="pre">x</span> <span class="pre">not</span> <span class="pre">in</span> <span class="pre">s</span></code> is equivalent to <code class="docutils literal notranslate"><span class="pre">not</span> <span class="pre">x</span> <span class="pre">in</span> <span class="pre">s</span></code>. This operator is supported for <code class="docutils literal notranslate"><span class="pre">lists</span></code>, <code class="docutils literal notranslate"><span class="pre">dicts</span></code>, and <code class="docutils literal notranslate"><span class="pre">tuples</span></code>, and can be used with
user-defined types if they implement the <code class="docutils literal notranslate"><span class="pre">__contains__</span></code> method.</p>
</section>
<section id="identity-comparisons">
<h4>Identity Comparisons<a class="headerlink" href="#identity-comparisons" title="Link to this heading">#</a></h4>
<p>For all types except <code class="docutils literal notranslate"><span class="pre">int</span></code>, <code class="docutils literal notranslate"><span class="pre">double</span></code>, <code class="docutils literal notranslate"><span class="pre">bool</span></code>, and <code class="docutils literal notranslate"><span class="pre">torch.device</span></code>, operators <code class="docutils literal notranslate"><span class="pre">is</span></code> and <code class="docutils literal notranslate"><span class="pre">is</span> <span class="pre">not</span></code> test for the object’s identity;
<code class="docutils literal notranslate"><span class="pre">x</span> <span class="pre">is</span> <span class="pre">y</span></code> is <code class="docutils literal notranslate"><span class="pre">True</span></code> if and only if <code class="docutils literal notranslate"><span class="pre">x</span></code> and <code class="docutils literal notranslate"><span class="pre">y</span></code> are the same object. For all other types, <code class="docutils literal notranslate"><span class="pre">is</span></code> is equivalent to
comparing them using <code class="docutils literal notranslate"><span class="pre">==</span></code>. <code class="docutils literal notranslate"><span class="pre">x</span> <span class="pre">is</span> <span class="pre">not</span> <span class="pre">y</span></code> yields the inverse of <code class="docutils literal notranslate"><span class="pre">x</span> <span class="pre">is</span> <span class="pre">y</span></code>.</p>
</section>
</section>
<section id="boolean-operations">
<h3>Boolean Operations<a class="headerlink" href="#boolean-operations" title="Link to this heading">#</a></h3>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">or_test</span>  <span class="p">:</span><span class="o">:=</span>  <span class="n">and_test</span> <span class="o">|</span> <span class="n">or_test</span> <span class="s1">&#39;or&#39;</span> <span class="n">and_test</span>
<span class="n">and_test</span> <span class="p">:</span><span class="o">:=</span>  <span class="n">not_test</span> <span class="o">|</span> <span class="n">and_test</span> <span class="s1">&#39;and&#39;</span> <span class="n">not_test</span>
<span class="n">not_test</span> <span class="p">:</span><span class="o">:=</span>  <span class="s1">&#39;bool&#39;</span> <span class="s1">&#39;(&#39;</span> <span class="n">or_expr</span> <span class="s1">&#39;)&#39;</span> <span class="o">|</span> <span class="n">comparison</span> <span class="o">|</span> <span class="s1">&#39;not&#39;</span> <span class="n">not_test</span>
</pre></div>
</div>
<p>User-defined objects can customize their conversion to <code class="docutils literal notranslate"><span class="pre">bool</span></code> by implementing a <code class="docutils literal notranslate"><span class="pre">__bool__</span></code> method. The operator <code class="docutils literal notranslate"><span class="pre">not</span></code>
yields <code class="docutils literal notranslate"><span class="pre">True</span></code> if its operand is false, <code class="docutils literal notranslate"><span class="pre">False</span></code> otherwise. The expression <code class="docutils literal notranslate"><span class="pre">x</span></code> and <code class="docutils literal notranslate"><span class="pre">y</span></code> first evaluates <code class="docutils literal notranslate"><span class="pre">x</span></code>; if it is <code class="docutils literal notranslate"><span class="pre">False</span></code>, its
value (<code class="docutils literal notranslate"><span class="pre">False</span></code>) is returned; otherwise, <code class="docutils literal notranslate"><span class="pre">y</span></code> is evaluated and its value is returned (<code class="docutils literal notranslate"><span class="pre">False</span></code> or <code class="docutils literal notranslate"><span class="pre">True</span></code>). The expression <code class="docutils literal notranslate"><span class="pre">x</span></code> or <code class="docutils literal notranslate"><span class="pre">y</span></code>
first evaluates <code class="docutils literal notranslate"><span class="pre">x</span></code>; if it is <code class="docutils literal notranslate"><span class="pre">True</span></code>, its value (<code class="docutils literal notranslate"><span class="pre">True</span></code>) is returned; otherwise, <code class="docutils literal notranslate"><span class="pre">y</span></code> is evaluated and its value is returned
(<code class="docutils literal notranslate"><span class="pre">False</span></code> or <code class="docutils literal notranslate"><span class="pre">True</span></code>).</p>
</section>
<section id="conditional-expressions">
<h3>Conditional Expressions<a class="headerlink" href="#conditional-expressions" title="Link to this heading">#</a></h3>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">conditional_expression</span> <span class="p">:</span><span class="o">:=</span>  <span class="n">or_expr</span> <span class="p">[</span><span class="s1">&#39;if&#39;</span> <span class="n">or_test</span> <span class="s1">&#39;else&#39;</span> <span class="n">conditional_expression</span><span class="p">]</span>
 <span class="n">expression</span>            <span class="p">:</span><span class="o">:=</span>  <span class="n">conditional_expression</span>
</pre></div>
</div>
<p>The expression <code class="docutils literal notranslate"><span class="pre">x</span> <span class="pre">if</span> <span class="pre">c</span> <span class="pre">else</span> <span class="pre">y</span></code> first evaluates the condition <code class="docutils literal notranslate"><span class="pre">c</span></code> rather than x. If <code class="docutils literal notranslate"><span class="pre">c</span></code> is <code class="docutils literal notranslate"><span class="pre">True</span></code>, <code class="docutils literal notranslate"><span class="pre">x</span></code> is
evaluated and its value is returned; otherwise, <code class="docutils literal notranslate"><span class="pre">y</span></code> is evaluated and its value is returned. As with if-statements,
<code class="docutils literal notranslate"><span class="pre">x</span></code> and <code class="docutils literal notranslate"><span class="pre">y</span></code> must evaluate to a value of the same type.</p>
</section>
<section id="expression-lists">
<h3>Expression Lists<a class="headerlink" href="#expression-lists" title="Link to this heading">#</a></h3>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">expression_list</span> <span class="p">:</span><span class="o">:=</span>  <span class="n">expression</span> <span class="p">(</span><span class="s1">&#39;,&#39;</span> <span class="n">expression</span><span class="p">)</span><span class="o">*</span> <span class="p">[</span><span class="s1">&#39;,&#39;</span><span class="p">]</span>
<span class="n">starred_item</span>    <span class="p">:</span><span class="o">:=</span>  <span class="s1">&#39;*&#39;</span> <span class="n">primary</span>
</pre></div>
</div>
<p>A starred item can only appear on the left-hand side of an assignment statement, e.g., <code class="docutils literal notranslate"><span class="pre">a,</span> <span class="pre">*b,</span> <span class="pre">c</span> <span class="pre">=</span> <span class="pre">...</span></code>.</p>
</section>
</section>
<section id="simple-statements">
<h2><a class="toc-backref" href="#id11" role="doc-backlink">Simple Statements</a><a class="headerlink" href="#simple-statements" title="Link to this heading">#</a></h2>
<p>The following section describes the syntax of simple statements that are supported in TorchScript.
It is modeled after <a class="reference external" href="https://docs.python.org/3/reference/simple_stmts.html">the simple statements chapter of the Python language reference</a>.</p>
<section id="expression-statements">
<h3>Expression Statements<a class="headerlink" href="#expression-statements" title="Link to this heading">#</a></h3>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">expression_stmt</span>    <span class="p">:</span><span class="o">:=</span>  <span class="n">starred_expression</span>
<span class="n">starred_expression</span> <span class="p">:</span><span class="o">:=</span>  <span class="n">expression</span> <span class="o">|</span> <span class="p">(</span><span class="n">starred_item</span> <span class="s2">&quot;,&quot;</span><span class="p">)</span><span class="o">*</span> <span class="p">[</span><span class="n">starred_item</span><span class="p">]</span>
<span class="n">starred_item</span>       <span class="p">:</span><span class="o">:=</span>  <span class="n">assignment_expression</span> <span class="o">|</span> <span class="s2">&quot;*&quot;</span> <span class="n">or_expr</span>
</pre></div>
</div>
</section>
<section id="assignment-statements">
<h3>Assignment Statements<a class="headerlink" href="#assignment-statements" title="Link to this heading">#</a></h3>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">assignment_stmt</span> <span class="p">:</span><span class="o">:=</span>  <span class="p">(</span><span class="n">target_list</span> <span class="s2">&quot;=&quot;</span><span class="p">)</span><span class="o">+</span> <span class="p">(</span><span class="n">starred_expression</span><span class="p">)</span>
<span class="n">target_list</span>     <span class="p">:</span><span class="o">:=</span>  <span class="n">target</span> <span class="p">(</span><span class="s2">&quot;,&quot;</span> <span class="n">target</span><span class="p">)</span><span class="o">*</span> <span class="p">[</span><span class="s2">&quot;,&quot;</span><span class="p">]</span>
<span class="n">target</span>          <span class="p">:</span><span class="o">:=</span>  <span class="n">identifier</span>
                     <span class="o">|</span> <span class="s2">&quot;(&quot;</span> <span class="p">[</span><span class="n">target_list</span><span class="p">]</span> <span class="s2">&quot;)&quot;</span>
                     <span class="o">|</span> <span class="s2">&quot;[&quot;</span> <span class="p">[</span><span class="n">target_list</span><span class="p">]</span> <span class="s2">&quot;]&quot;</span>
                     <span class="o">|</span> <span class="n">attributeref</span>
                     <span class="o">|</span> <span class="n">subscription</span>
                     <span class="o">|</span> <span class="n">slicing</span>
                     <span class="o">|</span> <span class="s2">&quot;*&quot;</span> <span class="n">target</span>
</pre></div>
</div>
</section>
<section id="augmented-assignment-statements">
<h3>Augmented Assignment Statements<a class="headerlink" href="#augmented-assignment-statements" title="Link to this heading">#</a></h3>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">augmented_assignment_stmt</span> <span class="p">:</span><span class="o">:=</span> <span class="n">augtarget</span> <span class="n">augop</span> <span class="p">(</span><span class="n">expression_list</span><span class="p">)</span>
<span class="n">augtarget</span>                 <span class="p">:</span><span class="o">:=</span> <span class="n">identifier</span> <span class="o">|</span> <span class="n">attributeref</span> <span class="o">|</span> <span class="n">subscription</span>
<span class="n">augop</span>                     <span class="p">:</span><span class="o">:=</span> <span class="s2">&quot;+=&quot;</span> <span class="o">|</span> <span class="s2">&quot;-=&quot;</span> <span class="o">|</span> <span class="s2">&quot;*=&quot;</span> <span class="o">|</span> <span class="s2">&quot;/=&quot;</span> <span class="o">|</span> <span class="s2">&quot;//=&quot;</span> <span class="o">|</span> <span class="s2">&quot;%=&quot;</span> <span class="o">|</span>
                              <span class="s2">&quot;**=&quot;</span><span class="o">|</span> <span class="s2">&quot;&gt;&gt;=&quot;</span> <span class="o">|</span> <span class="s2">&quot;&lt;&lt;=&quot;</span> <span class="o">|</span> <span class="s2">&quot;&amp;=&quot;</span> <span class="o">|</span> <span class="s2">&quot;^=&quot;</span> <span class="o">|</span> <span class="s2">&quot;|=&quot;</span>
</pre></div>
</div>
</section>
<section id="annotated-assignment-statements">
<h3>Annotated Assignment Statements<a class="headerlink" href="#annotated-assignment-statements" title="Link to this heading">#</a></h3>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">annotated_assignment_stmt</span> <span class="p">:</span><span class="o">:=</span> <span class="n">augtarget</span> <span class="s2">&quot;:&quot;</span> <span class="n">expression</span>
                              <span class="p">[</span><span class="s2">&quot;=&quot;</span> <span class="p">(</span><span class="n">starred_expression</span><span class="p">)]</span>
</pre></div>
</div>
</section>
<section id="the-raise-statement">
<h3>The <code class="docutils literal notranslate"><span class="pre">raise</span></code> Statement<a class="headerlink" href="#the-raise-statement" title="Link to this heading">#</a></h3>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">raise_stmt</span> <span class="p">:</span><span class="o">:=</span>  <span class="s2">&quot;raise&quot;</span> <span class="p">[</span><span class="n">expression</span> <span class="p">[</span><span class="s2">&quot;from&quot;</span> <span class="n">expression</span><span class="p">]]</span>
</pre></div>
</div>
<p>Raise statements in TorchScript do not support <code class="docutils literal notranslate"><span class="pre">try\except\finally</span></code>.</p>
</section>
<section id="the-assert-statement">
<h3>The <code class="docutils literal notranslate"><span class="pre">assert</span></code> Statement<a class="headerlink" href="#the-assert-statement" title="Link to this heading">#</a></h3>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">assert_stmt</span> <span class="p">:</span><span class="o">:=</span>  <span class="s2">&quot;assert&quot;</span> <span class="n">expression</span> <span class="p">[</span><span class="s2">&quot;,&quot;</span> <span class="n">expression</span><span class="p">]</span>
</pre></div>
</div>
<p>Assert statements in TorchScript do not support <code class="docutils literal notranslate"><span class="pre">try\except\finally</span></code>.</p>
</section>
<section id="the-return-statement">
<h3>The <code class="docutils literal notranslate"><span class="pre">return</span></code> Statement<a class="headerlink" href="#the-return-statement" title="Link to this heading">#</a></h3>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">return_stmt</span> <span class="p">:</span><span class="o">:=</span>  <span class="s2">&quot;return&quot;</span> <span class="p">[</span><span class="n">expression_list</span><span class="p">]</span>
</pre></div>
</div>
<p>Return statements in TorchScript do not support <code class="docutils literal notranslate"><span class="pre">try\except\finally</span></code>.</p>
</section>
<section id="the-del-statement">
<h3>The <code class="docutils literal notranslate"><span class="pre">del</span></code> Statement<a class="headerlink" href="#the-del-statement" title="Link to this heading">#</a></h3>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">del_stmt</span> <span class="p">:</span><span class="o">:=</span>  <span class="s2">&quot;del&quot;</span> <span class="n">target_list</span>
</pre></div>
</div>
</section>
<section id="the-pass-statement">
<h3>The <code class="docutils literal notranslate"><span class="pre">pass</span></code> Statement<a class="headerlink" href="#the-pass-statement" title="Link to this heading">#</a></h3>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">pass_stmt</span> <span class="p">:</span><span class="o">:=</span> <span class="s2">&quot;pass&quot;</span>
</pre></div>
</div>
</section>
<section id="the-print-statement">
<h3>The <code class="docutils literal notranslate"><span class="pre">print</span></code> Statement<a class="headerlink" href="#the-print-statement" title="Link to this heading">#</a></h3>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">print_stmt</span> <span class="p">:</span><span class="o">:=</span> <span class="s2">&quot;print&quot;</span> <span class="s2">&quot;(&quot;</span> <span class="n">expression</span>  <span class="p">[,</span> <span class="n">expression</span><span class="p">]</span> <span class="p">[</span><span class="o">.</span><span class="n">format</span><span class="p">{</span><span class="n">expression_list</span><span class="p">}]</span> <span class="s2">&quot;)&quot;</span>
</pre></div>
</div>
</section>
<section id="the-break-statement">
<h3>The <code class="docutils literal notranslate"><span class="pre">break</span></code> Statement<a class="headerlink" href="#the-break-statement" title="Link to this heading">#</a></h3>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">break_stmt</span> <span class="p">:</span><span class="o">:=</span> <span class="s2">&quot;break&quot;</span>
</pre></div>
</div>
</section>
<section id="the-continue-statement">
<h3>The <code class="docutils literal notranslate"><span class="pre">continue</span></code> Statement:<a class="headerlink" href="#the-continue-statement" title="Link to this heading">#</a></h3>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">continue_stmt</span> <span class="p">:</span><span class="o">:=</span> <span class="s2">&quot;continue&quot;</span>
</pre></div>
</div>
</section>
</section>
<section id="compound-statements">
<h2><a class="toc-backref" href="#id12" role="doc-backlink">Compound Statements</a><a class="headerlink" href="#compound-statements" title="Link to this heading">#</a></h2>
<p>The following section describes the syntax of compound statements that are supported in TorchScript.
The section also highlights how Torchscript differs from regular Python statements.
It is modeled after <a class="reference external" href="https://docs.python.org/3/reference/compound_stmts.html">the compound statements chapter of the Python language reference</a>.</p>
<section id="the-if-statement">
<h3>The <code class="docutils literal notranslate"><span class="pre">if</span></code> Statement<a class="headerlink" href="#the-if-statement" title="Link to this heading">#</a></h3>
<p>Torchscript supports both basic <code class="docutils literal notranslate"><span class="pre">if/else</span></code> and ternary <code class="docutils literal notranslate"><span class="pre">if/else</span></code>.</p>
<section id="basic-if-else-statement">
<h4>Basic <code class="docutils literal notranslate"><span class="pre">if/else</span></code> Statement<a class="headerlink" href="#basic-if-else-statement" title="Link to this heading">#</a></h4>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">if_stmt</span> <span class="p">:</span><span class="o">:=</span> <span class="s2">&quot;if&quot;</span> <span class="n">assignment_expression</span> <span class="s2">&quot;:&quot;</span> <span class="n">suite</span>
            <span class="p">(</span><span class="s2">&quot;elif&quot;</span> <span class="n">assignment_expression</span> <span class="s2">&quot;:&quot;</span> <span class="n">suite</span><span class="p">)</span>
            <span class="p">[</span><span class="s2">&quot;else&quot;</span> <span class="s2">&quot;:&quot;</span> <span class="n">suite</span><span class="p">]</span>
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">elif</span></code> statements can repeat for an arbitrary number of times, but it needs to be before <code class="docutils literal notranslate"><span class="pre">else</span></code> statement.</p>
</section>
<section id="ternary-if-else-statement">
<h4>Ternary <code class="docutils literal notranslate"><span class="pre">if/else</span></code> Statement<a class="headerlink" href="#ternary-if-else-statement" title="Link to this heading">#</a></h4>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">if_stmt</span> <span class="p">:</span><span class="o">:=</span> <span class="k">return</span> <span class="p">[</span><span class="n">expression_list</span><span class="p">]</span> <span class="s2">&quot;if&quot;</span> <span class="n">assignment_expression</span> <span class="s2">&quot;else&quot;</span> <span class="p">[</span><span class="n">expression_list</span><span class="p">]</span>
</pre></div>
</div>
<p><strong>Example 1</strong></p>
<p>A <code class="docutils literal notranslate"><span class="pre">tensor</span></code> with 1 dimension is promoted to <code class="docutils literal notranslate"><span class="pre">bool</span></code>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>

<span class="nd">@torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">script</span>
<span class="k">def</span><span class="w"> </span><span class="nf">fn</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">x</span><span class="p">:</span> <span class="c1"># The tensor gets promoted to bool</span>
        <span class="k">return</span> <span class="kc">True</span>
    <span class="k">return</span> <span class="kc">False</span>
<span class="nb">print</span><span class="p">(</span><span class="n">fn</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">1</span><span class="p">)))</span>
</pre></div>
</div>
<p>The example above produces the following output:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>True
</pre></div>
</div>
<p><strong>Example 2</strong></p>
<p>A <code class="docutils literal notranslate"><span class="pre">tensor</span></code> with multi dimensions are not promoted to <code class="docutils literal notranslate"><span class="pre">bool</span></code>:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>

<span class="c1"># Multi dimensional Tensors error out.</span>

<span class="nd">@torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">script</span>
<span class="k">def</span><span class="w"> </span><span class="nf">fn</span><span class="p">():</span>
    <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">2</span><span class="p">):</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Tensor is available&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">6</span><span class="p">):</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Tensor is available&quot;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">fn</span><span class="p">())</span>
</pre></div>
</div>
<p>Running the above code yields the following <code class="docutils literal notranslate"><span class="pre">RuntimeError</span></code>.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="ne">RuntimeError</span><span class="p">:</span> <span class="n">The</span> <span class="n">following</span> <span class="n">operation</span> <span class="n">failed</span> <span class="ow">in</span> <span class="n">the</span> <span class="n">TorchScript</span> <span class="n">interpreter</span><span class="o">.</span>
<span class="n">Traceback</span> <span class="n">of</span> <span class="n">TorchScript</span> <span class="p">(</span><span class="n">most</span> <span class="n">recent</span> <span class="n">call</span> <span class="n">last</span><span class="p">):</span>
<span class="nd">@torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">script</span>
<span class="k">def</span><span class="w"> </span><span class="nf">fn</span><span class="p">():</span>
    <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">2</span><span class="p">):</span>
       <span class="o">~~~~~~~~~~~~</span> <span class="o">&lt;---</span> <span class="n">HERE</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Tensor is available&quot;</span><span class="p">)</span>
<span class="ne">RuntimeError</span><span class="p">:</span> <span class="n">Boolean</span> <span class="n">value</span> <span class="n">of</span> <span class="n">Tensor</span> <span class="k">with</span> <span class="n">more</span> <span class="n">than</span> <span class="n">one</span> <span class="n">value</span> <span class="ow">is</span> <span class="n">ambiguous</span>
</pre></div>
</div>
<p>If a conditional variable is annotated as <code class="docutils literal notranslate"><span class="pre">final</span></code>, either the true or false branch is evaluated depending on the evaluation of the conditional variable.</p>
<p><strong>Example 3</strong></p>
<p>In this example, only the True branch is evaluated, since <code class="docutils literal notranslate"><span class="pre">a</span></code> is annotated as <code class="docutils literal notranslate"><span class="pre">final</span></code> and set to <code class="docutils literal notranslate"><span class="pre">True</span></code>:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>

<span class="n">a</span> <span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">final</span><span class="p">[</span><span class="n">Bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>

<span class="k">if</span> <span class="n">a</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="k">return</span> <span class="p">[]</span>
</pre></div>
</div>
</section>
</section>
<section id="the-while-statement">
<h3>The <code class="docutils literal notranslate"><span class="pre">while</span></code> Statement<a class="headerlink" href="#the-while-statement" title="Link to this heading">#</a></h3>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">while_stmt</span> <span class="p">:</span><span class="o">:=</span>  <span class="s2">&quot;while&quot;</span> <span class="n">assignment_expression</span> <span class="s2">&quot;:&quot;</span> <span class="n">suite</span>
</pre></div>
</div>
<p><cite>while…else</cite> statements are not supported in Torchscript. It results in a <code class="docutils literal notranslate"><span class="pre">RuntimeError</span></code>.</p>
</section>
<section id="the-for-in-statement">
<h3>The <code class="docutils literal notranslate"><span class="pre">for-in</span></code> Statement<a class="headerlink" href="#the-for-in-statement" title="Link to this heading">#</a></h3>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">for_stmt</span> <span class="p">:</span><span class="o">:=</span>  <span class="s2">&quot;for&quot;</span> <span class="n">target_list</span> <span class="s2">&quot;in&quot;</span> <span class="n">expression_list</span> <span class="s2">&quot;:&quot;</span> <span class="n">suite</span>
              <span class="p">[</span><span class="s2">&quot;else&quot;</span> <span class="s2">&quot;:&quot;</span> <span class="n">suite</span><span class="p">]</span>
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">for...else</span></code> statements are not supported in Torchscript. It results in a <code class="docutils literal notranslate"><span class="pre">RuntimeError</span></code>.</p>
<p><strong>Example 1</strong></p>
<p>For loops on tuples: these unroll the loop, generating a body for each member of the tuple. The body must type-check correctly for each member.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">typing</span><span class="w"> </span><span class="kn">import</span> <span class="n">Tuple</span>

<span class="nd">@torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">script</span>
<span class="k">def</span><span class="w"> </span><span class="nf">fn</span><span class="p">():</span>
    <span class="n">tup</span> <span class="o">=</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">4</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">tup</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="n">fn</span><span class="p">()</span>
</pre></div>
</div>
<p>The example above produces the following output:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>3
 1
 1
 1
 1
[ CPUFloatType{4} ]
</pre></div>
</div>
<p><strong>Example 2</strong></p>
<p>For loops on lists: for loops over a <code class="docutils literal notranslate"><span class="pre">nn.ModuleList</span></code> will unroll the body of the loop at compile time, with each member of the module list.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">SubModule</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span> <span class="o">+</span> <span class="nb">input</span>

<span class="k">class</span><span class="w"> </span><span class="nc">MyModule</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mods</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">([</span><span class="n">SubModule</span><span class="p">()</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">)])</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">v</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">module</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">mods</span><span class="p">:</span>
            <span class="n">v</span> <span class="o">=</span> <span class="n">module</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">v</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">script</span><span class="p">(</span><span class="n">MyModule</span><span class="p">())</span>
</pre></div>
</div>
</section>
<section id="the-with-statement">
<h3>The <code class="docutils literal notranslate"><span class="pre">with</span></code> Statement<a class="headerlink" href="#the-with-statement" title="Link to this heading">#</a></h3>
<p>The <code class="docutils literal notranslate"><span class="pre">with</span></code> statement is used to wrap the execution of a block with methods defined by a context manager.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">with_stmt</span> <span class="p">:</span><span class="o">:=</span>  <span class="s2">&quot;with&quot;</span> <span class="n">with_item</span> <span class="p">(</span><span class="s2">&quot;,&quot;</span> <span class="n">with_item</span><span class="p">)</span> <span class="s2">&quot;:&quot;</span> <span class="n">suite</span>
<span class="n">with_item</span> <span class="p">:</span><span class="o">:=</span>  <span class="n">expression</span> <span class="p">[</span><span class="s2">&quot;as&quot;</span> <span class="n">target</span><span class="p">]</span>
</pre></div>
</div>
<ul class="simple">
<li><p>If a target was included in the <code class="docutils literal notranslate"><span class="pre">with</span></code> statement, the return value from the context manager’s <code class="docutils literal notranslate"><span class="pre">__enter__()</span></code> is assigned to it. Unlike python, if an exception caused the suite to be exited, its type, value, and traceback are not passed as arguments to <code class="docutils literal notranslate"><span class="pre">__exit__()</span></code>. Three <code class="docutils literal notranslate"><span class="pre">None</span></code> arguments are supplied.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">try</span></code>, <code class="docutils literal notranslate"><span class="pre">except</span></code>, and <code class="docutils literal notranslate"><span class="pre">finally</span></code> statements are not supported inside <code class="docutils literal notranslate"><span class="pre">with</span></code> blocks.</p></li>
<li><p>Exceptions raised within <code class="docutils literal notranslate"><span class="pre">with</span></code> block cannot be suppressed.</p></li>
</ul>
</section>
<section id="the-tuple-statement">
<h3>The <code class="docutils literal notranslate"><span class="pre">tuple</span></code> Statement<a class="headerlink" href="#the-tuple-statement" title="Link to this heading">#</a></h3>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">tuple_stmt</span> <span class="p">:</span><span class="o">:=</span> <span class="nb">tuple</span><span class="p">([</span><span class="n">iterables</span><span class="p">])</span>
</pre></div>
</div>
<ul class="simple">
<li><p>Iterable types in TorchScript include <code class="docutils literal notranslate"><span class="pre">Tensors</span></code>, <code class="docutils literal notranslate"><span class="pre">lists</span></code>, <code class="docutils literal notranslate"><span class="pre">tuples</span></code>, <code class="docutils literal notranslate"><span class="pre">dictionaries</span></code>, <code class="docutils literal notranslate"><span class="pre">strings</span></code>, <code class="docutils literal notranslate"><span class="pre">torch.nn.ModuleList</span></code>, and <code class="docutils literal notranslate"><span class="pre">torch.nn.ModuleDict</span></code>.</p></li>
<li><p>You cannot convert a List to Tuple by using this built-in function.</p></li>
</ul>
<p>Unpacking all outputs into a tuple is covered by:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">abc</span> <span class="o">=</span> <span class="n">func</span><span class="p">()</span> <span class="c1"># Function that returns a tuple</span>
<span class="n">a</span><span class="p">,</span><span class="n">b</span> <span class="o">=</span> <span class="n">func</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section id="the-getattr-statement">
<h3>The <code class="docutils literal notranslate"><span class="pre">getattr</span></code> Statement<a class="headerlink" href="#the-getattr-statement" title="Link to this heading">#</a></h3>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">getattr_stmt</span> <span class="p">:</span><span class="o">:=</span> <span class="nb">getattr</span><span class="p">(</span><span class="nb">object</span><span class="p">,</span> <span class="n">name</span><span class="p">[,</span> <span class="n">default</span><span class="p">])</span>
</pre></div>
</div>
<ul class="simple">
<li><p>Attribute name must be a literal string.</p></li>
<li><p>Module type object is not supported (e.g., torch._C).</p></li>
<li><p>Custom class object is not supported (e.g., torch.classes.*).</p></li>
</ul>
</section>
<section id="the-hasattr-statement">
<h3>The <code class="docutils literal notranslate"><span class="pre">hasattr</span></code> Statement<a class="headerlink" href="#the-hasattr-statement" title="Link to this heading">#</a></h3>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">hasattr_stmt</span> <span class="p">:</span><span class="o">:=</span> <span class="nb">hasattr</span><span class="p">(</span><span class="nb">object</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>
</pre></div>
</div>
<ul class="simple">
<li><p>Attribute name must be a literal string.</p></li>
<li><p>Module type object is not supported (e.g., torch._C).</p></li>
<li><p>Custom class object is not supported (e.g., torch.classes.*).</p></li>
</ul>
</section>
<section id="the-zip-statement">
<h3>The <code class="docutils literal notranslate"><span class="pre">zip</span></code> Statement<a class="headerlink" href="#the-zip-statement" title="Link to this heading">#</a></h3>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">zip_stmt</span> <span class="p">:</span><span class="o">:=</span> <span class="nb">zip</span><span class="p">(</span><span class="n">iterable1</span><span class="p">,</span> <span class="n">iterable2</span><span class="p">)</span>
</pre></div>
</div>
<ul class="simple">
<li><p>Arguments must be iterables.</p></li>
<li><p>Two iterables of same outer container type but different length are supported.</p></li>
</ul>
<p><strong>Example 1</strong></p>
<p>Both the iterables must be of the same container type:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">a</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span> <span class="c1"># List</span>
<span class="n">b</span> <span class="o">=</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]</span> <span class="c1"># List</span>
<span class="nb">zip</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span> <span class="c1"># works</span>
</pre></div>
</div>
<p><strong>Example 2</strong></p>
<p>This example fails because the iterables are of different container types:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">a</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> <span class="c1"># Tuple</span>
<span class="n">b</span> <span class="o">=</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]</span> <span class="c1"># List</span>
<span class="nb">zip</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span> <span class="c1"># Runtime error</span>
</pre></div>
</div>
<p>Running the above code yields the following <code class="docutils literal notranslate"><span class="pre">RuntimeError</span></code>.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="ne">RuntimeError</span><span class="p">:</span> <span class="n">Can</span> <span class="ow">not</span> <span class="n">iterate</span> <span class="n">over</span> <span class="n">a</span> <span class="n">module</span> <span class="nb">list</span> <span class="ow">or</span>
    <span class="nb">tuple</span> <span class="k">with</span> <span class="n">a</span> <span class="n">value</span> <span class="n">that</span> <span class="n">does</span> <span class="ow">not</span> <span class="n">have</span> <span class="n">a</span> <span class="n">statically</span> <span class="n">determinable</span> <span class="n">length</span><span class="o">.</span>
</pre></div>
</div>
<p><strong>Example 3</strong></p>
<p>Two iterables of the same container Type but different data type is supported:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">a</span> <span class="o">=</span> <span class="p">[</span><span class="mf">1.3</span><span class="p">,</span> <span class="mf">2.4</span><span class="p">]</span>
<span class="n">b</span> <span class="o">=</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]</span>
<span class="nb">zip</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span> <span class="c1"># Works</span>
</pre></div>
</div>
<p>Iterable types in TorchScript include <code class="docutils literal notranslate"><span class="pre">Tensors</span></code>, <code class="docutils literal notranslate"><span class="pre">lists</span></code>, <code class="docutils literal notranslate"><span class="pre">tuples</span></code>, <code class="docutils literal notranslate"><span class="pre">dictionaries</span></code>, <code class="docutils literal notranslate"><span class="pre">strings</span></code>, <code class="docutils literal notranslate"><span class="pre">torch.nn.ModuleList</span></code>, and <code class="docutils literal notranslate"><span class="pre">torch.nn.ModuleDict</span></code>.</p>
</section>
<section id="the-enumerate-statement">
<h3>The <code class="docutils literal notranslate"><span class="pre">enumerate</span></code> Statement<a class="headerlink" href="#the-enumerate-statement" title="Link to this heading">#</a></h3>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">enumerate_stmt</span> <span class="p">:</span><span class="o">:=</span> <span class="nb">enumerate</span><span class="p">([</span><span class="n">iterable</span><span class="p">])</span>
</pre></div>
</div>
<ul class="simple">
<li><p>Arguments must be iterables.</p></li>
<li><p>Iterable types in TorchScript include <code class="docutils literal notranslate"><span class="pre">Tensors</span></code>, <code class="docutils literal notranslate"><span class="pre">lists</span></code>, <code class="docutils literal notranslate"><span class="pre">tuples</span></code>, <code class="docutils literal notranslate"><span class="pre">dictionaries</span></code>, <code class="docutils literal notranslate"><span class="pre">strings</span></code>, <code class="docutils literal notranslate"><span class="pre">torch.nn.ModuleList</span></code> and <code class="docutils literal notranslate"><span class="pre">torch.nn.ModuleDict</span></code>.</p></li>
</ul>
</section>
</section>
<section id="python-values">
<span id="python-values-torch-script"></span><h2><a class="toc-backref" href="#id13" role="doc-backlink">Python Values</a><a class="headerlink" href="#python-values" title="Link to this heading">#</a></h2>
<section id="resolution-rules">
<span id="python-builtin-functions-values-resolution"></span><h3>Resolution Rules<a class="headerlink" href="#resolution-rules" title="Link to this heading">#</a></h3>
<p>When given a Python value, TorchScript attempts to resolve it in the following five different ways:</p>
<ul class="simple">
<li><dl class="simple">
<dt>Compilable Python Implementation:</dt><dd><ul>
<li><p>When a Python value is backed by a Python implementation that can be compiled by TorchScript, TorchScript compiles and uses the underlying Python implementation.</p></li>
<li><p>Example: <code class="docutils literal notranslate"><span class="pre">torch.jit.Attribute</span></code></p></li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>Op Python Wrapper:</dt><dd><ul>
<li><p>When a Python value is a wrapper of a native PyTorch op, TorchScript emits the corresponding operator.</p></li>
<li><p>Example: <code class="docutils literal notranslate"><span class="pre">torch.jit._logging.add_stat_value</span></code></p></li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>Python Object Identity Match:</dt><dd><ul>
<li><p>For a limited set of <code class="docutils literal notranslate"><span class="pre">torch.*</span></code> API calls (in the form of Python values) that TorchScript supports, TorchScript attempts to match a Python value against each item in the set.</p></li>
<li><p>When matched, TorchScript generates a corresponding <code class="docutils literal notranslate"><span class="pre">SugaredValue</span></code> instance that contains lowering logic for these values.</p></li>
<li><p>Example: <code class="docutils literal notranslate"><span class="pre">torch.jit.isinstance()</span></code></p></li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>Name Match:</dt><dd><ul>
<li><p>For Python built-in functions and constants, TorchScript identifies them by name, and creates a corresponding <code class="docutils literal notranslate"><span class="pre">SugaredValue</span></code> instance that implements their functionality.</p></li>
<li><p>Example: <code class="docutils literal notranslate"><span class="pre">all()</span></code></p></li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>Value Snapshot:</dt><dd><ul>
<li><p>For Python values from unrecognized modules, TorchScript attempts to take a snapshot of the value and converts it to a constant in the graph of the function(s) or method(s) that are being compiled.</p></li>
<li><p>Example: <code class="docutils literal notranslate"><span class="pre">math.pi</span></code></p></li>
</ul>
</dd>
</dl>
</li>
</ul>
</section>
<section id="python-built-in-functions-support">
<span id="python-builtin-functions-support"></span><h3>Python Built-in Functions Support<a class="headerlink" href="#python-built-in-functions-support" title="Link to this heading">#</a></h3>
<div class="pst-scrollable-table-container"><table class="table" id="id5">
<caption><span class="caption-text">TorchScript Support for Python Built-in Functions</span><a class="headerlink" href="#id5" title="Link to this table">#</a></caption>
<colgroup>
<col style="width: 25.0%" />
<col style="width: 25.0%" />
<col style="width: 50.0%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Built-in Function</p></th>
<th class="head"><p>Support Level</p></th>
<th class="head"><p>Notes</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">abs()</span></code></p></td>
<td><p>Partial</p></td>
<td><p>Only supports <code class="docutils literal notranslate"><span class="pre">Tensor</span></code>/<code class="docutils literal notranslate"><span class="pre">Int</span></code>/<code class="docutils literal notranslate"><span class="pre">Float</span></code> type inputs. | Doesn’t honor <code class="docutils literal notranslate"><span class="pre">__abs__</span></code> override.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">all()</span></code></p></td>
<td><p>Full</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">any()</span></code></p></td>
<td><p>Full</p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">ascii()</span></code></p></td>
<td><p>None</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">bin()</span></code></p></td>
<td><p>Partial</p></td>
<td><p>Only supports <code class="docutils literal notranslate"><span class="pre">Int</span></code> type input.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">bool()</span></code></p></td>
<td><p>Partial</p></td>
<td><p>Only supports <code class="docutils literal notranslate"><span class="pre">Tensor</span></code>/<code class="docutils literal notranslate"><span class="pre">Int</span></code>/<code class="docutils literal notranslate"><span class="pre">Float</span></code> type inputs.</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">breakpoint()</span></code></p></td>
<td><p>None</p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">bytearray()</span></code></p></td>
<td><p>None</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">bytes()</span></code></p></td>
<td><p>None</p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">callable()</span></code></p></td>
<td><p>None</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">chr()</span></code></p></td>
<td><p>Partial</p></td>
<td><p>Only ASCII character set is supported.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">classmethod()</span></code></p></td>
<td><p>Full</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">compile()</span></code></p></td>
<td><p>None</p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">complex()</span></code></p></td>
<td><p>None</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">delattr()</span></code></p></td>
<td><p>None</p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">dict()</span></code></p></td>
<td><p>Full</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">dir()</span></code></p></td>
<td><p>None</p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">divmod()</span></code></p></td>
<td><p>Full</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">enumerate()</span></code></p></td>
<td><p>Full</p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">eval()</span></code></p></td>
<td><p>None</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">exec()</span></code></p></td>
<td><p>None</p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">filter()</span></code></p></td>
<td><p>None</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">float()</span></code></p></td>
<td><p>Partial</p></td>
<td><p>Doesn’t honor <code class="docutils literal notranslate"><span class="pre">__index__</span></code> override.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">format()</span></code></p></td>
<td><p>Partial</p></td>
<td><p>Manual index specification not supported. | Format type modifier not supported.</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">frozenset()</span></code></p></td>
<td><p>None</p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">getattr()</span></code></p></td>
<td><p>Partial</p></td>
<td><p>Attribute name must be string literal.</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">globals()</span></code></p></td>
<td><p>None</p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">hasattr()</span></code></p></td>
<td><p>Partial</p></td>
<td><p>Attribute name must be string literal.</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">hash()</span></code></p></td>
<td><p>Full</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Tensor</span></code>’s hash is based on identity not numeric value.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">hex()</span></code></p></td>
<td><p>Partial</p></td>
<td><p>Only supports <code class="docutils literal notranslate"><span class="pre">Int</span></code> type input.</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">id()</span></code></p></td>
<td><p>Full</p></td>
<td><p>Only supports <code class="docutils literal notranslate"><span class="pre">Int</span></code> type input.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">input()</span></code></p></td>
<td><p>None</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">int()</span></code></p></td>
<td><p>Partial</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">base</span></code> argument not supported. | Doesn’t honor <code class="docutils literal notranslate"><span class="pre">__index__</span></code> override.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">isinstance()</span></code></p></td>
<td><p>Full</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">torch.jit.isintance</span></code> provides better support when checking against container types like <code class="docutils literal notranslate"><span class="pre">Dict[str,</span> <span class="pre">int]</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">issubclass()</span></code></p></td>
<td><p>None</p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">iter()</span></code></p></td>
<td><p>None</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">len()</span></code></p></td>
<td><p>Full</p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">list()</span></code></p></td>
<td><p>Full</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">ord()</span></code></p></td>
<td><p>Partial</p></td>
<td><p>Only ASCII character set is supported.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">pow()</span></code></p></td>
<td><p>Full</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">print()</span></code></p></td>
<td><p>Partial</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">separate</span></code>, <code class="docutils literal notranslate"><span class="pre">end</span></code> and <code class="docutils literal notranslate"><span class="pre">file</span></code> arguments are not supported.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">property()</span></code></p></td>
<td><p>None</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">range()</span></code></p></td>
<td><p>Full</p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">repr()</span></code></p></td>
<td><p>None</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">reversed()</span></code></p></td>
<td><p>None</p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">round()</span></code></p></td>
<td><p>Partial</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">ndigits</span></code> argument is not supported.</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">set()</span></code></p></td>
<td><p>None</p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">setattr()</span></code></p></td>
<td><p>None</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">slice()</span></code></p></td>
<td><p>Full</p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">sorted()</span></code></p></td>
<td><p>Partial</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">key</span></code> argument is not supported.</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">staticmethod()</span></code></p></td>
<td><p>Full</p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">str()</span></code></p></td>
<td><p>Partial</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">encoding</span></code> and <code class="docutils literal notranslate"><span class="pre">errors</span></code> arguments are not supported.</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">sum()</span></code></p></td>
<td><p>Full</p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">super()</span></code></p></td>
<td><p>Partial</p></td>
<td><p>It can only be used in <code class="docutils literal notranslate"><span class="pre">nn.Module</span></code>’s <code class="docutils literal notranslate"><span class="pre">__init__</span></code> method.</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">type()</span></code></p></td>
<td><p>None</p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">vars()</span></code></p></td>
<td><p>None</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">zip()</span></code></p></td>
<td><p>Full</p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">__import__()</span></code></p></td>
<td><p>None</p></td>
<td></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="python-built-in-values-support">
<span id="python-builtin-values-support"></span><h3>Python Built-in Values Support<a class="headerlink" href="#python-built-in-values-support" title="Link to this heading">#</a></h3>
<div class="pst-scrollable-table-container"><table class="table" id="id6">
<caption><span class="caption-text">TorchScript Support for Python Built-in Values</span><a class="headerlink" href="#id6" title="Link to this table">#</a></caption>
<colgroup>
<col style="width: 25.0%" />
<col style="width: 25.0%" />
<col style="width: 50.0%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Built-in Value</p></th>
<th class="head"><p>Support Level</p></th>
<th class="head"><p>Notes</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">False</span></code></p></td>
<td><p>Full</p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">True</span></code></p></td>
<td><p>Full</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">None</span></code></p></td>
<td><p>Full</p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">NotImplemented</span></code></p></td>
<td><p>None</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">Ellipsis</span></code></p></td>
<td><p>Full</p></td>
<td></td>
</tr>
</tbody>
</table>
</div>
</section>
</section>
<section id="torch-apis">
<span id="torch-apis-in-torchscript"></span><h2><a class="toc-backref" href="#id14" role="doc-backlink">torch.* APIs</a><a class="headerlink" href="#torch-apis" title="Link to this heading">#</a></h2>
<section id="remote-procedure-calls">
<span id="torch-apis-in-torchscript-rpc"></span><h3>Remote Procedure Calls<a class="headerlink" href="#remote-procedure-calls" title="Link to this heading">#</a></h3>
<p>TorchScript supports a subset of RPC APIs that supports running a function on
a specified remote worker instead of locally.</p>
<p>Specifically, following APIs are fully supported:</p>
<ul class="simple">
<li><dl class="simple">
<dt><code class="docutils literal notranslate"><span class="pre">torch.distributed.rpc.rpc_sync()</span></code></dt><dd><ul>
<li><p><code class="docutils literal notranslate"><span class="pre">rpc_sync()</span></code> makes a blocking RPC call to run a function on a remote worker. RPC messages are sent and received in parallel to execution of Python code.</p></li>
<li><p>More details about its usage and examples can be found in <a class="reference internal" href="rpc.html#torch.distributed.rpc.rpc_sync" title="torch.distributed.rpc.rpc_sync"><code class="xref py py-meth docutils literal notranslate"><span class="pre">rpc_sync()</span></code></a>.</p></li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt><code class="docutils literal notranslate"><span class="pre">torch.distributed.rpc.rpc_async()</span></code></dt><dd><ul>
<li><p><code class="docutils literal notranslate"><span class="pre">rpc_async()</span></code> makes a non-blocking RPC call to run a function on a remote worker. RPC messages are sent and received in parallel to execution of Python code.</p></li>
<li><p>More details about its usage and examples can be found in <a class="reference internal" href="rpc.html#torch.distributed.rpc.rpc_async" title="torch.distributed.rpc.rpc_async"><code class="xref py py-meth docutils literal notranslate"><span class="pre">rpc_async()</span></code></a>.</p></li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt><code class="docutils literal notranslate"><span class="pre">torch.distributed.rpc.remote()</span></code></dt><dd><ul>
<li><p><code class="docutils literal notranslate"><span class="pre">remote.()</span></code> executes a remote call on a worker and gets a Remote Reference <code class="docutils literal notranslate"><span class="pre">RRef</span></code> as the return value.</p></li>
<li><p>More details about its usage and examples can be found in <a class="reference internal" href="rpc.html#torch.distributed.rpc.remote" title="torch.distributed.rpc.remote"><code class="xref py py-meth docutils literal notranslate"><span class="pre">remote()</span></code></a>.</p></li>
</ul>
</dd>
</dl>
</li>
</ul>
</section>
<section id="asynchronous-execution">
<span id="torch-apis-in-torchscript-async"></span><h3>Asynchronous Execution<a class="headerlink" href="#asynchronous-execution" title="Link to this heading">#</a></h3>
<p>TorchScript enables you to create asynchronous computation tasks to make better use
of computation resources. This is done via supporting a list of APIs that are
only usable within TorchScript:</p>
<ul class="simple">
<li><dl class="simple">
<dt><code class="docutils literal notranslate"><span class="pre">torch.jit.fork()</span></code></dt><dd><ul>
<li><p>Creates an asynchronous task executing func and a reference to the value of the result of this execution. Fork will return immediately.</p></li>
<li><p>Synonymous to <code class="docutils literal notranslate"><span class="pre">torch.jit._fork()</span></code>, which is only kept for backward compatibility reasons.</p></li>
<li><p>More details about its usage and examples can be found in <a class="reference internal" href="generated/torch.jit.fork.html#torch.jit.fork" title="torch.jit.fork"><code class="xref py py-meth docutils literal notranslate"><span class="pre">fork()</span></code></a>.</p></li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt><code class="docutils literal notranslate"><span class="pre">torch.jit.wait()</span></code></dt><dd><ul>
<li><p>Forces completion of a <code class="docutils literal notranslate"><span class="pre">torch.jit.Future[T]</span></code> asynchronous task, returning the result of the task.</p></li>
<li><p>Synonymous to <code class="docutils literal notranslate"><span class="pre">torch.jit._wait()</span></code>, which is only kept for backward compatibility reasons.</p></li>
<li><p>More details about its usage and examples can be found in <a class="reference internal" href="generated/torch.jit.wait.html#torch.jit.wait" title="torch.jit.wait"><code class="xref py py-meth docutils literal notranslate"><span class="pre">wait()</span></code></a>.</p></li>
</ul>
</dd>
</dl>
</li>
</ul>
</section>
<section id="type-annotations">
<span id="torch-apis-in-torchscript-annotation"></span><h3>Type Annotations<a class="headerlink" href="#type-annotations" title="Link to this heading">#</a></h3>
<p>TorchScript is statically-typed. It provides and supports a set of utilities to help annotate variables and attributes:</p>
<ul class="simple">
<li><dl class="simple">
<dt><code class="docutils literal notranslate"><span class="pre">torch.jit.annotate()</span></code></dt><dd><ul>
<li><p>Provides a type hint to TorchScript where Python 3 style type hints do not work well.</p></li>
<li><p>One common example is to annotate type for expressions like <code class="docutils literal notranslate"><span class="pre">[]</span></code>. <code class="docutils literal notranslate"><span class="pre">[]</span></code> is treated as <code class="docutils literal notranslate"><span class="pre">List[torch.Tensor]</span></code> by default. When a different type is needed, you can use this code to hint TorchScript: <code class="docutils literal notranslate"><span class="pre">torch.jit.annotate(List[int],</span> <span class="pre">[])</span></code>.</p></li>
<li><p>More details can be found in <a class="reference internal" href="generated/torch.jit.annotate.html#torch.jit.annotate" title="torch.jit.annotate"><code class="xref py py-meth docutils literal notranslate"><span class="pre">annotate()</span></code></a></p></li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt><code class="docutils literal notranslate"><span class="pre">torch.jit.Attribute</span></code></dt><dd><ul>
<li><p>Common use cases include providing type hint for <code class="docutils literal notranslate"><span class="pre">torch.nn.Module</span></code> attributes. Because their <code class="docutils literal notranslate"><span class="pre">__init__</span></code> methods are not parsed by TorchScript, <code class="docutils literal notranslate"><span class="pre">torch.jit.Attribute</span></code> should be used instead of <code class="docutils literal notranslate"><span class="pre">torch.jit.annotate</span></code> in the module’s <code class="docutils literal notranslate"><span class="pre">__init__</span></code> methods.</p></li>
<li><p>More details can be found in <a class="reference internal" href="generated/torch.jit.Attribute.html#torch.jit.Attribute" title="torch.jit.Attribute"><code class="xref py py-meth docutils literal notranslate"><span class="pre">Attribute()</span></code></a></p></li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt><code class="docutils literal notranslate"><span class="pre">torch.jit.Final</span></code></dt><dd><ul>
<li><p>An alias for Python’s <code class="docutils literal notranslate"><span class="pre">typing.Final</span></code>. <code class="docutils literal notranslate"><span class="pre">torch.jit.Final</span></code> is kept only for backward compatibility reasons.</p></li>
</ul>
</dd>
</dl>
</li>
</ul>
</section>
<section id="meta-programming">
<span id="torch-apis-in-torchscript-meta-programming"></span><h3>Meta Programming<a class="headerlink" href="#meta-programming" title="Link to this heading">#</a></h3>
<p>TorchScript provides a set of utilities to facilitate meta programming:</p>
<ul class="simple">
<li><dl class="simple">
<dt><code class="docutils literal notranslate"><span class="pre">torch.jit.is_scripting()</span></code></dt><dd><ul>
<li><p>Returns a boolean value indicating whether the current program is compiled by <code class="docutils literal notranslate"><span class="pre">torch.jit.script</span></code> or not.</p></li>
<li><p>When used in an <code class="docutils literal notranslate"><span class="pre">assert</span></code> or an <code class="docutils literal notranslate"><span class="pre">if</span></code> statement, the scope or branch where <code class="docutils literal notranslate"><span class="pre">torch.jit.is_scripting()</span></code> evaluates to <code class="docutils literal notranslate"><span class="pre">False</span></code> is not compiled.</p></li>
<li><p>Its value can be evaluated statically at compile time, thus commonly used in <code class="docutils literal notranslate"><span class="pre">if</span></code> statements to stop TorchScript from compiling one of the branches.</p></li>
<li><p>More details and examples can be found in <a class="reference internal" href="jit_language_reference.html#torch.jit.is_scripting" title="torch.jit.is_scripting"><code class="xref py py-meth docutils literal notranslate"><span class="pre">is_scripting()</span></code></a></p></li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt><code class="docutils literal notranslate"><span class="pre">torch.jit.is_tracing()</span></code></dt><dd><ul>
<li><p>Returns a boolean value indicating whether the current program is traced by <code class="docutils literal notranslate"><span class="pre">torch.jit.trace</span></code> / <code class="docutils literal notranslate"><span class="pre">torch.jit.trace_module</span></code> or not.</p></li>
<li><p>More details can be found in <a class="reference internal" href="jit_language_reference.html#torch.jit.is_tracing" title="torch.jit.is_tracing"><code class="xref py py-meth docutils literal notranslate"><span class="pre">is_tracing()</span></code></a></p></li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt><code class="docutils literal notranslate"><span class="pre">&#64;torch.jit.ignore</span></code></dt><dd><ul>
<li><p>This decorator indicates to the compiler that a function or method should be ignored and left as a Python function.</p></li>
<li><p>This allows you to leave code in your model that is not yet TorchScript compatible.</p></li>
<li><p>If a function decorated by <code class="docutils literal notranslate"><span class="pre">&#64;torch.jit.ignore</span></code> is called from TorchScript, ignored functions will dispatch the call to the Python interpreter.</p></li>
<li><p>Models with ignored functions cannot be exported.</p></li>
<li><p>More details and examples can be found in <a class="reference internal" href="generated/torch.jit.ignore.html#torch.jit.ignore" title="torch.jit.ignore"><code class="xref py py-meth docutils literal notranslate"><span class="pre">ignore()</span></code></a></p></li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt><code class="docutils literal notranslate"><span class="pre">&#64;torch.jit.unused</span></code></dt><dd><ul>
<li><p>This decorator indicates to the compiler that a function or method should be ignored and replaced with the raising of an exception.</p></li>
<li><p>This allows you to leave code in your model that is not yet TorchScript compatible and still export your model.</p></li>
<li><p>If a function decorated by <code class="docutils literal notranslate"><span class="pre">&#64;torch.jit.unused</span></code> is called from TorchScript, a runtime error will be raised.</p></li>
<li><p>More details and examples can be found in <a class="reference internal" href="generated/torch.jit.unused.html#torch.jit.unused" title="torch.jit.unused"><code class="xref py py-meth docutils literal notranslate"><span class="pre">unused()</span></code></a></p></li>
</ul>
</dd>
</dl>
</li>
</ul>
</section>
<section id="type-refinement">
<span id="torch-apis-in-torchscript-type-refinement"></span><h3>Type Refinement<a class="headerlink" href="#type-refinement" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><dl class="simple">
<dt><code class="docutils literal notranslate"><span class="pre">torch.jit.isinstance()</span></code></dt><dd><ul>
<li><p>Returns a boolean indicating whether a variable is of the specified type.</p></li>
<li><p>More details about its usage and examples can be found in <a class="reference internal" href="generated/torch.jit.isinstance.html#torch.jit.isinstance" title="torch.jit.isinstance"><code class="xref py py-meth docutils literal notranslate"><span class="pre">isinstance()</span></code></a>.</p></li>
</ul>
</dd>
</dl>
</li>
</ul>
</section>
</section>
</section>


                </article>
              
              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="jit_language_reference.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">TorchScript Language Reference</p>
      </div>
    </a>
    <a class="right-next"
       href="generated/torch.jit.script.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">torch.jit.script</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <dialog id="pst-secondary-sidebar-modal"></dialog>
                <div id="pst-secondary-sidebar" class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
<div
    id="pst-page-navigation-heading-2"
    class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> On this page
  </div>
  <nav class="bd-toc-nav page-toc" aria-labelledby="pst-page-navigation-heading-2">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#terminology">Terminology</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">Type System</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torchscript-types">TorchScript Types</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#meta-types">Meta Types</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#any-type"><code class="docutils literal notranslate"><span class="pre">Any</span></code> Type</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#operators-supported-for-any-type">Operators Supported for <code class="docutils literal notranslate"><span class="pre">Any</span></code> Type</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#design-notes">Design Notes</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#primitive-types">Primitive Types</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#structural-types">Structural Types</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#nominal-types">Nominal Types</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#built-in-class">Built-in Class</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#special-note-on-torch-nn-modulelist-and-torch-nn-moduledict">Special Note on torch.nn.ModuleList and torch.nn.ModuleDict</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#custom-class">Custom Class</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#enum-type">Enum Type</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torchscript-module-class">TorchScript Module Class</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#module-instance-class">Module Instance Class</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#type-annotation">Type Annotation</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#when-to-annotate-types">When to Annotate Types</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#annotate-function-signature">Annotate Function Signature</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#annotate-variables-and-data-attributes">Annotate Variables and Data Attributes</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#local-variables">Local Variables</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#instance-data-attributes">Instance Data Attributes</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#type-annotation-apis">Type Annotation APIs</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch-jit-annotate-t-expr"><code class="docutils literal notranslate"><span class="pre">torch.jit.annotate(T,</span> <span class="pre">expr)</span></code></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#type-annotation-appendix">Type Annotation Appendix</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torchscript-type-system-definition">TorchScript Type System Definition</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#unsupported-typing-constructs">Unsupported Typing Constructs</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#expressions">Expressions</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#arithmetic-conversions">Arithmetic Conversions</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#atoms">Atoms</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#identifiers">Identifiers</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#literals">Literals</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#parenthesized-forms">Parenthesized Forms</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#list-and-dictionary-displays">List and Dictionary Displays</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#primaries">Primaries</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#attribute-references">Attribute References</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#subscriptions">Subscriptions</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#slicings">Slicings</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#calls">Calls</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#power-operator">Power Operator</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#unary-and-arithmetic-bitwise-operations">Unary and Arithmetic Bitwise Operations</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#binary-arithmetic-operations">Binary Arithmetic Operations</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#shifting-operations">Shifting Operations</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#binary-bitwise-operations">Binary Bitwise Operations</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#comparisons">Comparisons</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#value-comparisons">Value Comparisons</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#membership-test-operations">Membership Test Operations</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#identity-comparisons">Identity Comparisons</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#boolean-operations">Boolean Operations</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#conditional-expressions">Conditional Expressions</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#expression-lists">Expression Lists</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#simple-statements">Simple Statements</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#expression-statements">Expression Statements</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#assignment-statements">Assignment Statements</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#augmented-assignment-statements">Augmented Assignment Statements</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#annotated-assignment-statements">Annotated Assignment Statements</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-raise-statement">The <code class="docutils literal notranslate"><span class="pre">raise</span></code> Statement</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-assert-statement">The <code class="docutils literal notranslate"><span class="pre">assert</span></code> Statement</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-return-statement">The <code class="docutils literal notranslate"><span class="pre">return</span></code> Statement</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-del-statement">The <code class="docutils literal notranslate"><span class="pre">del</span></code> Statement</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-pass-statement">The <code class="docutils literal notranslate"><span class="pre">pass</span></code> Statement</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-print-statement">The <code class="docutils literal notranslate"><span class="pre">print</span></code> Statement</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-break-statement">The <code class="docutils literal notranslate"><span class="pre">break</span></code> Statement</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-continue-statement">The <code class="docutils literal notranslate"><span class="pre">continue</span></code> Statement:</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#compound-statements">Compound Statements</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-if-statement">The <code class="docutils literal notranslate"><span class="pre">if</span></code> Statement</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#basic-if-else-statement">Basic <code class="docutils literal notranslate"><span class="pre">if/else</span></code> Statement</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#ternary-if-else-statement">Ternary <code class="docutils literal notranslate"><span class="pre">if/else</span></code> Statement</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-while-statement">The <code class="docutils literal notranslate"><span class="pre">while</span></code> Statement</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-for-in-statement">The <code class="docutils literal notranslate"><span class="pre">for-in</span></code> Statement</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-with-statement">The <code class="docutils literal notranslate"><span class="pre">with</span></code> Statement</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-tuple-statement">The <code class="docutils literal notranslate"><span class="pre">tuple</span></code> Statement</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-getattr-statement">The <code class="docutils literal notranslate"><span class="pre">getattr</span></code> Statement</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-hasattr-statement">The <code class="docutils literal notranslate"><span class="pre">hasattr</span></code> Statement</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-zip-statement">The <code class="docutils literal notranslate"><span class="pre">zip</span></code> Statement</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-enumerate-statement">The <code class="docutils literal notranslate"><span class="pre">enumerate</span></code> Statement</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#python-values">Python Values</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#resolution-rules">Resolution Rules</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#python-built-in-functions-support">Python Built-in Functions Support</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#python-built-in-values-support">Python Built-in Values Support</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#torch-apis">torch.* APIs</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#remote-procedure-calls">Remote Procedure Calls</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#asynchronous-execution">Asynchronous Execution</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#type-annotations">Type Annotations</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#meta-programming">Meta Programming</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#type-refinement">Type Refinement</a></li>
</ul>
</li>
</ul>
  </nav></div>

  <div class="sidebar-secondary-item">

  
  <div class="tocsection editthispage">
    <a href="https://github.com/pytorch/pytorch/edit/main/docs/source/python-api/jit_language_reference_v2.rst">
      <i class="fa-solid fa-pencil"></i>
      
      
        
          Edit on GitHub
        
      
    </a>
  </div>
</div>

  <div class="sidebar-secondary-item">
    <div class="tocsection sourcelink">
      <a href="../_sources/python-api/jit_language_reference_v2.rst.txt">
        <i class="fa-solid fa-file-lines"></i> Show Source
      </a>
    </div>
</div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
          </footer>
        
      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script defer src="../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf"></script>
<script defer src="../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf"></script>


  <footer class="bd-footer">
<div class="bd-footer__inner bd-page-width">
  
    <div class="footer-items__start">
      
        <div class="footer-item">

  <p class="copyright">
    
      © Copyright PyTorch Contributors.
      <br/>
    
  </p>
</div>
      
        <div class="footer-item">

  <p class="sphinx-version">
    Created using <a href="https://www.sphinx-doc.org/">Sphinx</a> 8.1.0.
    <br/>
  </p>
</div>
      
    </div>
  
  
  
    <div class="footer-items__end">
      
        <div class="footer-item">
<p class="theme-version">
  <!-- # L10n: Setting the PST URL as an argument as this does not need to be localized -->
  Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.16.1.
</p></div>
      
    </div>
  
</div>

  </footer>
  </body>
</html>