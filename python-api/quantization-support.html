
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Quantization API Reference &#8212; PyTorch main documentation</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  <!--
    this give us a css class that will be invisible only if js is disabled
  -->
  <noscript>
    <style>
      .pst-js-only { display: none !important; }

    </style>
  </noscript>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=949a1ff5" />
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/katex.min.css" />
    <link rel="stylesheet" type="text/css" href="../_static/katex-math.css?v=91adb8b6" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="../_static/css/jit.css?v=8de1ea5d" />
    <link rel="stylesheet" type="text/css" href="../_static/css/custom.css?v=3ccf5357" />
    <link rel="stylesheet" type="text/css" href="../_static/css/custom2.css?v=189c4a6a" />
  
  <!-- So that users can add custom icons -->
  <script src="../_static/scripts/fontawesome.js?digest=8878045cc6db502f8baf"></script>
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf" />

    <script src="../_static/documentation_options.js?v=53c08c8d"></script>
    <script src="../_static/doctools.js?v=9bcbadda"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=940804e7"></script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'python-api/quantization-support';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  <meta name="docsearch:version" content="main (2.7.0a0+git74cfb4f )" />
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>

          <li class="main-menu-item">
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Learn
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="">
                  <span class=dropdown-title>Get Started</span>
                  <p>Run PyTorch locally or get started quickly with one of the supported cloud platforms</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Tutorials</span>
                  <p>Whats new in PyTorch tutorials</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Learn the Basics</span>
                  <p>Familiarize yourself with PyTorch concepts and modules</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">PyTorch Recipes</span>
                  <p>Bite-size, ready-to-deploy PyTorch code examples</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Intro to PyTorch - YouTube Series</span>
                  <p>Master PyTorch basics with our engaging YouTube tutorial series</p>
                </a>
              </div>
            </div>
          </li>

          <li>
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Ecosystem
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Tools</span>
                  <p>Learn about the tools and frameworks in the PyTorch Ecosystem</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class=dropdown-title>Community</span>
                  <p>Join the PyTorch developer community to contribute, learn, and get your questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="" target="_blank">
                  <span class=dropdown-title>Forums</span>
                  <p>A place to discuss PyTorch code, issues, install, research</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class=dropdown-title>Developer Resources</span>
                  <p>Find resources and get questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Contributor Awards - 2024</span>
                  <p>Award winners announced at this year's PyTorch Conference</p>
                </a>
              </div>
            </div>
          </li>

          <li>
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Edge
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">About PyTorch Edge</span>
                  <p>Build innovative and privacy-aware AI experiences for edge devices</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">ExecuTorch</span>
                  <p>End-to-end solution for enabling on-device inference capabilities across mobile and edge devices</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">ExecuTorch Docs</span>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Docs
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">PyTorch</span>
                  <p>Explore the documentation for comprehensive guidance on how to use PyTorch</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">PyTorch Domains</span>
                  <p>Read the PyTorch Domains documentation to learn more about domain-specific libraries</p>
                </a>
              </div>
            </div>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Blogs & News
              lnhetrlnle</a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">PyTorch Blog</span>
                  <p>Catch up on the latest technical news and happenings</p>
                </a>
                 <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Community Blog</span>
                  <p>Stories from the PyTorch ecosystem</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Videos</span>
                  <p>Learn about the latest PyTorch tutorials, new, and more </p>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Community Stories</span>
                  <p>Learn how our community solves real, everyday machine learning problems with PyTorch</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Events</span>
                  <p>Find events, webinars, and podcasts</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Newsletter</span>
                  <p>Stay up-to-date with the latest updates</p>
                </a>
            </div>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                About
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">PyTorch Foundation</span>
                  <p>Learn more about the PyTorch Foundation</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Governing Board</span>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Cloud Credit Program</span>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Technical Advisory Council</span>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Staff</span>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Contact Us</span>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
            <div class="no-dropdown">
              <a href="" data-cta="join">
                Become a Member
              </a>
            </div>
          </li>
          <li>
           <div class="main-menu-item">
             <a href="" class="github-icon">
             </a>
           </div>
          </li>
          <!--- TODO: This block adds the search icon to the nav bar. We will enable it later.
          <li>
            <div class="main-menu-item">
             <a href="" class="search-icon">
             </a>
            </div>
          </li>
          --->
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>
  </div>
</div>


  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <dialog id="pst-search-dialog">
    
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </dialog>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
<div class="bd-header__inner bd-page-width">
  <button class="pst-navbar-icon sidebar-toggle primary-toggle" aria-label="Site navigation">
    <span class="fa-solid fa-bars"></span>
  </button>
  
  
    <div class="navbar-header-items__start" style="display: flex; align-items: center; justify-content: flex-start;">
    <div class="navbar-item">
      <a class="nav-link nav-internal" href="/index.html">Home</a></div>
    
      <div class="navbar-item">

  
    
  

<a class="navbar-brand logo" href="../index.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/pytorch-logo-dark-unstable.png" class="logo__image only-light" alt="PyTorch main documentation - Home"/>
    <img src="../_static/pytorch-logo-dark-unstable.png" class="logo__image only-dark pst-js-only" alt="PyTorch main documentation - Home"/>
  
  
</a></div>
    
  </div>
  
  <div class="col-lg-9 navbar-header-items">
    
    <div class="me-auto navbar-header-items__center">
      
        <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item ">
  <a class="nav-link nav-internal" href="index.html">
    Python API
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../notes/index.html">
    Developer Notes
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../community/index.html">
    Community
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../lang-bindings/index.html">
    Language Bindings
  </a>
</li>

  </ul>
</nav></div>
      
    </div>
    
    
    <div class="navbar-header-items__end">
      
        <div class="navbar-item navbar-persistent--container">
          

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button>
        </div>
      
      
        <div class="navbar-item">

<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button></div>
      
    </div>
    
  </div>
  
  
    <div class="navbar-persistent--mobile">

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button>
    </div>
  

  
    <button class="pst-navbar-icon sidebar-toggle secondary-toggle" aria-label="On this page">
      <span class="fa-solid fa-outdent"></span>
    </button>
  
</div>

    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <dialog id="pst-primary-sidebar-modal"></dialog>
      <div id="pst-primary-sidebar" class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
      <div class="sidebar-header-items__center">
        
          
          
            <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item ">
  <a class="nav-link nav-internal" href="index.html">
    Python API
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../notes/index.html">
    Developer Notes
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../community/index.html">
    Community
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../lang-bindings/index.html">
    Language Bindings
  </a>
</li>

  </ul>
</nav></div>
          
        
      </div>
    
    
    
      <div class="sidebar-header-items__end">
        
          <div class="navbar-item">

<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button></div>
        
      </div>
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
<nav class="bd-docs-nav bd-links"
     aria-label="Section Navigation">
  <p class="bd-links__title" role="heading" aria-level="1">Section Navigation</p>
  <div class="bd-toc-item navbar-nav"><ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="generated/torch.ao.quantization.quantize.html">quantize</a></li>
<li class="toctree-l1"><a class="reference internal" href="generated/torch.ao.quantization.quantize_dynamic.html">quantize_dynamic</a></li>
<li class="toctree-l1"><a class="reference internal" href="generated/torch.ao.quantization.quantize_qat.html">quantize_qat</a></li>
<li class="toctree-l1"><a class="reference internal" href="generated/torch.ao.quantization.prepare.html">prepare</a></li>
<li class="toctree-l1"><a class="reference internal" href="generated/torch.ao.quantization.prepare_qat.html">prepare_qat</a></li>
<li class="toctree-l1"><a class="reference internal" href="generated/torch.ao.quantization.convert.html">convert</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="generated/torch.ao.quantization.fuse_modules.fuse_modules.html">fuse_modules</a></li>
<li class="toctree-l1"><a class="reference internal" href="generated/torch.ao.quantization.QuantStub.html">QuantStub</a></li>
<li class="toctree-l1"><a class="reference internal" href="generated/torch.ao.quantization.DeQuantStub.html">DeQuantStub</a></li>
<li class="toctree-l1"><a class="reference internal" href="generated/torch.ao.quantization.QuantWrapper.html">QuantWrapper</a></li>
<li class="toctree-l1"><a class="reference internal" href="generated/torch.ao.quantization.add_quant_dequant.html">add_quant_dequant</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="generated/torch.ao.quantization.swap_module.html">swap_module</a></li>
<li class="toctree-l1"><a class="reference internal" href="generated/torch.ao.quantization.propagate_qconfig_.html">propagate_qconfig</a></li>
<li class="toctree-l1"><a class="reference internal" href="generated/torch.ao.quantization.default_eval_fn.html">default_eval_fn</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="generated/torch.ao.quantization.quantize_fx.prepare_fx.html">prepare_fx</a></li>
<li class="toctree-l1"><a class="reference internal" href="generated/torch.ao.quantization.quantize_fx.prepare_qat_fx.html">prepare_qat_fx</a></li>
<li class="toctree-l1"><a class="reference internal" href="generated/torch.ao.quantization.quantize_fx.convert_fx.html">convert_fx</a></li>
<li class="toctree-l1"><a class="reference internal" href="generated/torch.ao.quantization.quantize_fx.fuse_fx.html">fuse_fx</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="generated/torch.ao.quantization.qconfig_mapping.QConfigMapping.html">QConfigMapping</a></li>
<li class="toctree-l1"><a class="reference internal" href="generated/torch.ao.quantization.qconfig_mapping.get_default_qconfig_mapping.html">get_default_qconfig_mapping</a></li>
<li class="toctree-l1"><a class="reference internal" href="generated/torch.ao.quantization.qconfig_mapping.get_default_qat_qconfig_mapping.html">get_default_qat_qconfig_mapping</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="generated/torch.ao.quantization.backend_config.BackendConfig.html">BackendConfig</a></li>
<li class="toctree-l1"><a class="reference internal" href="generated/torch.ao.quantization.backend_config.BackendPatternConfig.html">BackendPatternConfig</a></li>
<li class="toctree-l1"><a class="reference internal" href="generated/torch.ao.quantization.backend_config.DTypeConfig.html">DTypeConfig</a></li>
<li class="toctree-l1"><a class="reference internal" href="generated/torch.ao.quantization.backend_config.DTypeWithConstraints.html">DTypeWithConstraints</a></li>
<li class="toctree-l1"><a class="reference internal" href="generated/torch.ao.quantization.backend_config.ObservationType.html">ObservationType</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="generated/torch.ao.quantization.fx.custom_config.FuseCustomConfig.html">FuseCustomConfig</a></li>
<li class="toctree-l1"><a class="reference internal" href="generated/torch.ao.quantization.fx.custom_config.PrepareCustomConfig.html">PrepareCustomConfig</a></li>
<li class="toctree-l1"><a class="reference internal" href="generated/torch.ao.quantization.fx.custom_config.ConvertCustomConfig.html">ConvertCustomConfig</a></li>
<li class="toctree-l1"><a class="reference internal" href="generated/torch.ao.quantization.fx.custom_config.StandaloneModuleConfigEntry.html">StandaloneModuleConfigEntry</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="generated/torch.ao.quantization.pt2e.export_utils.model_is_exported.html">model_is_exported</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="generated/torch.ao.quantization.generate_numeric_debug_handle.html">generate_numeric_debug_handle</a></li>
<li class="toctree-l1"><a class="reference internal" href="generated/torch.ao.quantization.CUSTOM_KEY.html">CUSTOM_KEY</a></li>
<li class="toctree-l1"><a class="reference internal" href="generated/torch.ao.quantization.NUMERIC_DEBUG_HANDLE_KEY.html">NUMERIC_DEBUG_HANDLE_KEY</a></li>
<li class="toctree-l1"><a class="reference internal" href="generated/torch.ao.quantization.prepare_for_propagation_comparison.html">prepare_for_propagation_comparison</a></li>
<li class="toctree-l1"><a class="reference internal" href="generated/torch.ao.quantization.extract_results_from_loggers.html">extract_results_from_loggers</a></li>
<li class="toctree-l1"><a class="reference internal" href="generated/torch.ao.quantization.compare_results.html">compare_results</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="generated/torch.quantize_per_tensor.html">quantize_per_tensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="generated/torch.quantize_per_channel.html">quantize_per_channel</a></li>
<li class="toctree-l1"><a class="reference internal" href="generated/torch.dequantize.html">dequantize</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="generated/torch.Tensor.view.html">view</a></li>
<li class="toctree-l1"><a class="reference internal" href="generated/torch.Tensor.as_strided.html">as_strided</a></li>
<li class="toctree-l1"><a class="reference internal" href="generated/torch.Tensor.expand.html">expand</a></li>
<li class="toctree-l1"><a class="reference internal" href="generated/torch.Tensor.flatten.html">flatten</a></li>
<li class="toctree-l1"><a class="reference internal" href="generated/torch.Tensor.select.html">select</a></li>
<li class="toctree-l1"><a class="reference internal" href="generated/torch.Tensor.ne.html">ne</a></li>
<li class="toctree-l1"><a class="reference internal" href="generated/torch.Tensor.eq.html">eq</a></li>
<li class="toctree-l1"><a class="reference internal" href="generated/torch.Tensor.ge.html">ge</a></li>
<li class="toctree-l1"><a class="reference internal" href="generated/torch.Tensor.le.html">le</a></li>
<li class="toctree-l1"><a class="reference internal" href="generated/torch.Tensor.gt.html">gt</a></li>
<li class="toctree-l1"><a class="reference internal" href="generated/torch.Tensor.lt.html">lt</a></li>
<li class="toctree-l1"><a class="reference internal" href="generated/torch.Tensor.copy_.html">copy</a></li>
<li class="toctree-l1"><a class="reference internal" href="generated/torch.Tensor.clone.html">clone</a></li>
<li class="toctree-l1"><a class="reference internal" href="generated/torch.Tensor.dequantize.html">dequantize</a></li>
<li class="toctree-l1"><a class="reference internal" href="generated/torch.Tensor.equal.html">equal</a></li>
<li class="toctree-l1"><a class="reference internal" href="generated/torch.Tensor.int_repr.html">int_repr</a></li>
<li class="toctree-l1"><a class="reference internal" href="generated/torch.Tensor.max.html">max</a></li>
<li class="toctree-l1"><a class="reference internal" href="generated/torch.Tensor.mean.html">mean</a></li>
<li class="toctree-l1"><a class="reference internal" href="generated/torch.Tensor.min.html">min</a></li>
<li class="toctree-l1"><a class="reference internal" href="generated/torch.Tensor.q_scale.html">q_scale</a></li>
<li class="toctree-l1"><a class="reference internal" href="generated/torch.Tensor.q_zero_point.html">q_zero_point</a></li>
<li class="toctree-l1"><a class="reference internal" href="generated/torch.Tensor.q_per_channel_scales.html">q_per_channel_scales</a></li>
<li class="toctree-l1"><a class="reference internal" href="generated/torch.Tensor.q_per_channel_zero_points.html">q_per_channel_zero_points</a></li>
<li class="toctree-l1"><a class="reference internal" href="generated/torch.Tensor.q_per_channel_axis.html">q_per_channel_axis</a></li>
<li class="toctree-l1"><a class="reference internal" href="generated/torch.Tensor.resize_.html">resize</a></li>
<li class="toctree-l1"><a class="reference internal" href="generated/torch.Tensor.sort.html">sort</a></li>
<li class="toctree-l1"><a class="reference internal" href="generated/torch.Tensor.topk.html">topk</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="generated/torch.ao.quantization.observer.ObserverBase.html">ObserverBase</a></li>
<li class="toctree-l1"><a class="reference internal" href="generated/torch.ao.quantization.observer.MinMaxObserver.html">MinMaxObserver</a></li>
<li class="toctree-l1"><a class="reference internal" href="generated/torch.ao.quantization.observer.MovingAverageMinMaxObserver.html">MovingAverageMinMaxObserver</a></li>
<li class="toctree-l1"><a class="reference internal" href="generated/torch.ao.quantization.observer.PerChannelMinMaxObserver.html">PerChannelMinMaxObserver</a></li>
<li class="toctree-l1"><a class="reference internal" href="generated/torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver.html">MovingAveragePerChannelMinMaxObserver</a></li>
<li class="toctree-l1"><a class="reference internal" href="generated/torch.ao.quantization.observer.HistogramObserver.html">HistogramObserver</a></li>
<li class="toctree-l1"><a class="reference internal" href="generated/torch.ao.quantization.observer.PlaceholderObserver.html">PlaceholderObserver</a></li>
<li class="toctree-l1"><a class="reference internal" href="generated/torch.ao.quantization.observer.RecordingObserver.html">RecordingObserver</a></li>
<li class="toctree-l1"><a class="reference internal" href="generated/torch.ao.quantization.observer.NoopObserver.html">NoopObserver</a></li>
<li class="toctree-l1"><a class="reference internal" href="generated/torch.ao.quantization.observer.get_observer_state_dict.html">get_observer_state_dict</a></li>
<li class="toctree-l1"><a class="reference internal" href="generated/torch.ao.quantization.observer.load_observer_state_dict.html">load_observer_state_dict</a></li>
<li class="toctree-l1"><a class="reference internal" href="generated/torch.ao.quantization.observer.default_observer.html">default_observer</a></li>
<li class="toctree-l1"><a class="reference internal" href="generated/torch.ao.quantization.observer.default_placeholder_observer.html">default_placeholder_observer</a></li>
<li class="toctree-l1"><a class="reference internal" href="generated/torch.ao.quantization.observer.default_debug_observer.html">default_debug_observer</a></li>
<li class="toctree-l1"><a class="reference internal" href="generated/torch.ao.quantization.observer.default_weight_observer.html">default_weight_observer</a></li>
<li class="toctree-l1"><a class="reference internal" href="generated/torch.ao.quantization.observer.default_histogram_observer.html">default_histogram_observer</a></li>
<li class="toctree-l1"><a class="reference internal" href="generated/torch.ao.quantization.observer.default_per_channel_weight_observer.html">default_per_channel_weight_observer</a></li>
<li class="toctree-l1"><a class="reference internal" href="generated/torch.ao.quantization.observer.default_dynamic_quant_observer.html">default_dynamic_quant_observer</a></li>
<li class="toctree-l1"><a class="reference internal" href="generated/torch.ao.quantization.observer.default_float_qparams_observer.html">default_float_qparams_observer</a></li>
<li class="toctree-l1"><a class="reference internal" href="generated/torch.ao.quantization.observer.AffineQuantizedObserverBase.html">AffineQuantizedObserverBase</a></li>
<li class="toctree-l1"><a class="reference internal" href="generated/torch.ao.quantization.observer.Granularity.html">Granularity</a></li>
<li class="toctree-l1"><a class="reference internal" href="generated/torch.ao.quantization.observer.MappingType.html">MappingType</a></li>
<li class="toctree-l1"><a class="reference internal" href="generated/torch.ao.quantization.observer.PerAxis.html">PerAxis</a></li>
<li class="toctree-l1"><a class="reference internal" href="generated/torch.ao.quantization.observer.PerBlock.html">PerBlock</a></li>
<li class="toctree-l1"><a class="reference internal" href="generated/torch.ao.quantization.observer.PerGroup.html">PerGroup</a></li>
<li class="toctree-l1"><a class="reference internal" href="generated/torch.ao.quantization.observer.PerRow.html">PerRow</a></li>
<li class="toctree-l1"><a class="reference internal" href="generated/torch.ao.quantization.observer.PerTensor.html">PerTensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="generated/torch.ao.quantization.observer.PerToken.html">PerToken</a></li>
<li class="toctree-l1"><a class="reference internal" href="generated/torch.ao.quantization.observer.TorchAODType.html">TorchAODType</a></li>
<li class="toctree-l1"><a class="reference internal" href="generated/torch.ao.quantization.observer.ZeroPointDomain.html">ZeroPointDomain</a></li>
<li class="toctree-l1"><a class="reference internal" href="generated/torch.ao.quantization.observer.get_block_size.html">get_block_size</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="generated/torch.ao.quantization.fake_quantize.FakeQuantizeBase.html">FakeQuantizeBase</a></li>
<li class="toctree-l1"><a class="reference internal" href="generated/torch.ao.quantization.fake_quantize.FakeQuantize.html">FakeQuantize</a></li>
<li class="toctree-l1"><a class="reference internal" href="generated/torch.ao.quantization.fake_quantize.FixedQParamsFakeQuantize.html">FixedQParamsFakeQuantize</a></li>
<li class="toctree-l1"><a class="reference internal" href="generated/torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize.html">FusedMovingAvgObsFakeQuantize</a></li>
<li class="toctree-l1"><a class="reference internal" href="generated/torch.ao.quantization.fake_quantize.default_fake_quant.html">default_fake_quant</a></li>
<li class="toctree-l1"><a class="reference internal" href="generated/torch.ao.quantization.fake_quantize.default_weight_fake_quant.html">default_weight_fake_quant</a></li>
<li class="toctree-l1"><a class="reference internal" href="generated/torch.ao.quantization.fake_quantize.default_per_channel_weight_fake_quant.html">default_per_channel_weight_fake_quant</a></li>
<li class="toctree-l1"><a class="reference internal" href="generated/torch.ao.quantization.fake_quantize.default_histogram_fake_quant.html">default_histogram_fake_quant</a></li>
<li class="toctree-l1"><a class="reference internal" href="generated/torch.ao.quantization.fake_quantize.default_fused_act_fake_quant.html">default_fused_act_fake_quant</a></li>
<li class="toctree-l1"><a class="reference internal" href="generated/torch.ao.quantization.fake_quantize.default_fused_wt_fake_quant.html">default_fused_wt_fake_quant</a></li>
<li class="toctree-l1"><a class="reference internal" href="generated/torch.ao.quantization.fake_quantize.default_fused_per_channel_wt_fake_quant.html">default_fused_per_channel_wt_fake_quant</a></li>
<li class="toctree-l1"><a class="reference internal" href="generated/torch.ao.quantization.fake_quantize.disable_fake_quant.html">disable_fake_quant</a></li>
<li class="toctree-l1"><a class="reference internal" href="generated/torch.ao.quantization.fake_quantize.enable_fake_quant.html">enable_fake_quant</a></li>
<li class="toctree-l1"><a class="reference internal" href="generated/torch.ao.quantization.fake_quantize.disable_observer.html">disable_observer</a></li>
<li class="toctree-l1"><a class="reference internal" href="generated/torch.ao.quantization.fake_quantize.enable_observer.html">enable_observer</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="generated/torch.ao.quantization.qconfig.QConfig.html">QConfig</a></li>
<li class="toctree-l1"><a class="reference internal" href="generated/torch.ao.quantization.qconfig.default_qconfig.html">default_qconfig</a></li>
<li class="toctree-l1"><a class="reference internal" href="generated/torch.ao.quantization.qconfig.default_debug_qconfig.html">default_debug_qconfig</a></li>
<li class="toctree-l1"><a class="reference internal" href="generated/torch.ao.quantization.qconfig.default_per_channel_qconfig.html">default_per_channel_qconfig</a></li>
<li class="toctree-l1"><a class="reference internal" href="generated/torch.ao.quantization.qconfig.default_dynamic_qconfig.html">default_dynamic_qconfig</a></li>
<li class="toctree-l1"><a class="reference internal" href="generated/torch.ao.quantization.qconfig.float16_dynamic_qconfig.html">float16_dynamic_qconfig</a></li>
<li class="toctree-l1"><a class="reference internal" href="generated/torch.ao.quantization.qconfig.float16_static_qconfig.html">float16_static_qconfig</a></li>
<li class="toctree-l1"><a class="reference internal" href="generated/torch.ao.quantization.qconfig.per_channel_dynamic_qconfig.html">per_channel_dynamic_qconfig</a></li>
<li class="toctree-l1"><a class="reference internal" href="generated/torch.ao.quantization.qconfig.float_qparams_weight_only_qconfig.html">float_qparams_weight_only_qconfig</a></li>
<li class="toctree-l1"><a class="reference internal" href="generated/torch.ao.quantization.qconfig.default_qat_qconfig.html">default_qat_qconfig</a></li>
<li class="toctree-l1"><a class="reference internal" href="generated/torch.ao.quantization.qconfig.default_weight_only_qconfig.html">default_weight_only_qconfig</a></li>
<li class="toctree-l1"><a class="reference internal" href="generated/torch.ao.quantization.qconfig.default_activation_only_qconfig.html">default_activation_only_qconfig</a></li>
<li class="toctree-l1"><a class="reference internal" href="generated/torch.ao.quantization.qconfig.default_qat_qconfig_v2.html">default_qat_qconfig_v2</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="generated/torch.ao.nn.intrinsic.ConvReLU1d.html">ConvReLU1d</a></li>
<li class="toctree-l1"><a class="reference internal" href="generated/torch.ao.nn.intrinsic.ConvReLU2d.html">ConvReLU2d</a></li>
<li class="toctree-l1"><a class="reference internal" href="generated/torch.ao.nn.intrinsic.ConvReLU3d.html">ConvReLU3d</a></li>
<li class="toctree-l1"><a class="reference internal" href="generated/torch.ao.nn.intrinsic.LinearReLU.html">LinearReLU</a></li>
<li class="toctree-l1"><a class="reference internal" href="generated/torch.ao.nn.intrinsic.ConvBn1d.html">ConvBn1d</a></li>
<li class="toctree-l1"><a class="reference internal" href="generated/torch.ao.nn.intrinsic.ConvBn2d.html">ConvBn2d</a></li>
<li class="toctree-l1"><a class="reference internal" href="generated/torch.ao.nn.intrinsic.ConvBn3d.html">ConvBn3d</a></li>
<li class="toctree-l1"><a class="reference internal" href="generated/torch.ao.nn.intrinsic.ConvBnReLU1d.html">ConvBnReLU1d</a></li>
<li class="toctree-l1"><a class="reference internal" href="generated/torch.ao.nn.intrinsic.ConvBnReLU2d.html">ConvBnReLU2d</a></li>
<li class="toctree-l1"><a class="reference internal" href="generated/torch.ao.nn.intrinsic.ConvBnReLU3d.html">ConvBnReLU3d</a></li>
<li class="toctree-l1"><a class="reference internal" href="generated/torch.ao.nn.intrinsic.BNReLU2d.html">BNReLU2d</a></li>
<li class="toctree-l1"><a class="reference internal" href="generated/torch.ao.nn.intrinsic.BNReLU3d.html">BNReLU3d</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="generated/torch.ao.nn.intrinsic.qat.LinearReLU.html">LinearReLU</a></li>
<li class="toctree-l1"><a class="reference internal" href="generated/torch.ao.nn.intrinsic.qat.ConvBn1d.html">ConvBn1d</a></li>
<li class="toctree-l1"><a class="reference internal" href="generated/torch.ao.nn.intrinsic.qat.ConvBnReLU1d.html">ConvBnReLU1d</a></li>
<li class="toctree-l1"><a class="reference internal" href="generated/torch.ao.nn.intrinsic.qat.ConvBn2d.html">ConvBn2d</a></li>
<li class="toctree-l1"><a class="reference internal" href="generated/torch.ao.nn.intrinsic.qat.ConvBnReLU2d.html">ConvBnReLU2d</a></li>
<li class="toctree-l1"><a class="reference internal" href="generated/torch.ao.nn.intrinsic.qat.ConvReLU2d.html">ConvReLU2d</a></li>
<li class="toctree-l1"><a class="reference internal" href="generated/torch.ao.nn.intrinsic.qat.ConvBn3d.html">ConvBn3d</a></li>
<li class="toctree-l1"><a class="reference internal" href="generated/torch.ao.nn.intrinsic.qat.ConvBnReLU3d.html">ConvBnReLU3d</a></li>
<li class="toctree-l1"><a class="reference internal" href="generated/torch.ao.nn.intrinsic.qat.ConvReLU3d.html">ConvReLU3d</a></li>
<li class="toctree-l1"><a class="reference internal" href="generated/torch.ao.nn.intrinsic.qat.update_bn_stats.html">update_bn_stats</a></li>
<li class="toctree-l1"><a class="reference internal" href="generated/torch.ao.nn.intrinsic.qat.freeze_bn_stats.html">freeze_bn_stats</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="generated/torch.ao.nn.intrinsic.quantized.BNReLU2d.html">BNReLU2d</a></li>
<li class="toctree-l1"><a class="reference internal" href="generated/torch.ao.nn.intrinsic.quantized.BNReLU3d.html">BNReLU3d</a></li>
<li class="toctree-l1"><a class="reference internal" href="generated/torch.ao.nn.intrinsic.quantized.ConvReLU1d.html">ConvReLU1d</a></li>
<li class="toctree-l1"><a class="reference internal" href="generated/torch.ao.nn.intrinsic.quantized.ConvReLU2d.html">ConvReLU2d</a></li>
<li class="toctree-l1"><a class="reference internal" href="generated/torch.ao.nn.intrinsic.quantized.ConvReLU3d.html">ConvReLU3d</a></li>
<li class="toctree-l1"><a class="reference internal" href="generated/torch.ao.nn.intrinsic.quantized.LinearReLU.html">LinearReLU</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="generated/torch.ao.nn.intrinsic.quantized.dynamic.LinearReLU.html">LinearReLU</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="generated/torch.ao.nn.qat.Conv2d.html">Conv2d</a></li>
<li class="toctree-l1"><a class="reference internal" href="generated/torch.ao.nn.qat.Conv3d.html">Conv3d</a></li>
<li class="toctree-l1"><a class="reference internal" href="generated/torch.ao.nn.qat.Linear.html">Linear</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="generated/torch.ao.nn.qat.dynamic.Linear.html">Linear</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="generated/torch.ao.nn.quantized.ReLU6.html">ReLU6</a></li>
<li class="toctree-l1"><a class="reference internal" href="generated/torch.ao.nn.quantized.Hardswish.html">Hardswish</a></li>
<li class="toctree-l1"><a class="reference internal" href="generated/torch.ao.nn.quantized.ELU.html">ELU</a></li>
<li class="toctree-l1"><a class="reference internal" href="generated/torch.ao.nn.quantized.LeakyReLU.html">LeakyReLU</a></li>
<li class="toctree-l1"><a class="reference internal" href="generated/torch.ao.nn.quantized.Sigmoid.html">Sigmoid</a></li>
<li class="toctree-l1"><a class="reference internal" href="generated/torch.ao.nn.quantized.BatchNorm2d.html">BatchNorm2d</a></li>
<li class="toctree-l1"><a class="reference internal" href="generated/torch.ao.nn.quantized.BatchNorm3d.html">BatchNorm3d</a></li>
<li class="toctree-l1"><a class="reference internal" href="generated/torch.ao.nn.quantized.Conv1d.html">Conv1d</a></li>
<li class="toctree-l1"><a class="reference internal" href="generated/torch.ao.nn.quantized.Conv2d.html">Conv2d</a></li>
<li class="toctree-l1"><a class="reference internal" href="generated/torch.ao.nn.quantized.Conv3d.html">Conv3d</a></li>
<li class="toctree-l1"><a class="reference internal" href="generated/torch.ao.nn.quantized.ConvTranspose1d.html">ConvTranspose1d</a></li>
<li class="toctree-l1"><a class="reference internal" href="generated/torch.ao.nn.quantized.ConvTranspose2d.html">ConvTranspose2d</a></li>
<li class="toctree-l1"><a class="reference internal" href="generated/torch.ao.nn.quantized.ConvTranspose3d.html">ConvTranspose3d</a></li>
<li class="toctree-l1"><a class="reference internal" href="generated/torch.ao.nn.quantized.Embedding.html">Embedding</a></li>
<li class="toctree-l1"><a class="reference internal" href="generated/torch.ao.nn.quantized.EmbeddingBag.html">EmbeddingBag</a></li>
<li class="toctree-l1"><a class="reference internal" href="generated/torch.ao.nn.quantized.FloatFunctional.html">FloatFunctional</a></li>
<li class="toctree-l1"><a class="reference internal" href="generated/torch.ao.nn.quantized.FXFloatFunctional.html">FXFloatFunctional</a></li>
<li class="toctree-l1"><a class="reference internal" href="generated/torch.ao.nn.quantized.QFunctional.html">QFunctional</a></li>
<li class="toctree-l1"><a class="reference internal" href="generated/torch.ao.nn.quantized.Linear.html">Linear</a></li>
<li class="toctree-l1"><a class="reference internal" href="generated/torch.ao.nn.quantized.LayerNorm.html">LayerNorm</a></li>
<li class="toctree-l1"><a class="reference internal" href="generated/torch.ao.nn.quantized.GroupNorm.html">GroupNorm</a></li>
<li class="toctree-l1"><a class="reference internal" href="generated/torch.ao.nn.quantized.InstanceNorm1d.html">InstanceNorm1d</a></li>
<li class="toctree-l1"><a class="reference internal" href="generated/torch.ao.nn.quantized.InstanceNorm2d.html">InstanceNorm2d</a></li>
<li class="toctree-l1"><a class="reference internal" href="generated/torch.ao.nn.quantized.InstanceNorm3d.html">InstanceNorm3d</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="generated/torch.ao.nn.quantized.functional.avg_pool2d.html">avg_pool2d</a></li>
<li class="toctree-l1"><a class="reference internal" href="generated/torch.ao.nn.quantized.functional.avg_pool3d.html">avg_pool3d</a></li>
<li class="toctree-l1"><a class="reference internal" href="generated/torch.ao.nn.quantized.functional.adaptive_avg_pool2d.html">adaptive_avg_pool2d</a></li>
<li class="toctree-l1"><a class="reference internal" href="generated/torch.ao.nn.quantized.functional.adaptive_avg_pool3d.html">adaptive_avg_pool3d</a></li>
<li class="toctree-l1"><a class="reference internal" href="generated/torch.ao.nn.quantized.functional.conv1d.html">conv1d</a></li>
<li class="toctree-l1"><a class="reference internal" href="generated/torch.ao.nn.quantized.functional.conv2d.html">conv2d</a></li>
<li class="toctree-l1"><a class="reference internal" href="generated/torch.ao.nn.quantized.functional.conv3d.html">conv3d</a></li>
<li class="toctree-l1"><a class="reference internal" href="generated/torch.ao.nn.quantized.functional.interpolate.html">interpolate</a></li>
<li class="toctree-l1"><a class="reference internal" href="generated/torch.ao.nn.quantized.functional.linear.html">linear</a></li>
<li class="toctree-l1"><a class="reference internal" href="generated/torch.ao.nn.quantized.functional.max_pool1d.html">max_pool1d</a></li>
<li class="toctree-l1"><a class="reference internal" href="generated/torch.ao.nn.quantized.functional.max_pool2d.html">max_pool2d</a></li>
<li class="toctree-l1"><a class="reference internal" href="generated/torch.ao.nn.quantized.functional.celu.html">celu</a></li>
<li class="toctree-l1"><a class="reference internal" href="generated/torch.ao.nn.quantized.functional.leaky_relu.html">leaky_relu</a></li>
<li class="toctree-l1"><a class="reference internal" href="generated/torch.ao.nn.quantized.functional.hardtanh.html">hardtanh</a></li>
<li class="toctree-l1"><a class="reference internal" href="generated/torch.ao.nn.quantized.functional.hardswish.html">hardswish</a></li>
<li class="toctree-l1"><a class="reference internal" href="generated/torch.ao.nn.quantized.functional.threshold.html">threshold</a></li>
<li class="toctree-l1"><a class="reference internal" href="generated/torch.ao.nn.quantized.functional.elu.html">elu</a></li>
<li class="toctree-l1"><a class="reference internal" href="generated/torch.ao.nn.quantized.functional.hardsigmoid.html">hardsigmoid</a></li>
<li class="toctree-l1"><a class="reference internal" href="generated/torch.ao.nn.quantized.functional.clamp.html">clamp</a></li>
<li class="toctree-l1"><a class="reference internal" href="generated/torch.ao.nn.quantized.functional.upsample.html">upsample</a></li>
<li class="toctree-l1"><a class="reference internal" href="generated/torch.ao.nn.quantized.functional.upsample_bilinear.html">upsample_bilinear</a></li>
<li class="toctree-l1"><a class="reference internal" href="generated/torch.ao.nn.quantized.functional.upsample_nearest.html">upsample_nearest</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="generated/torch.ao.nn.quantizable.LSTM.html">LSTM</a></li>
<li class="toctree-l1"><a class="reference internal" href="generated/torch.ao.nn.quantizable.MultiheadAttention.html">MultiheadAttention</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="generated/torch.ao.nn.quantized.dynamic.Linear.html">Linear</a></li>
<li class="toctree-l1"><a class="reference internal" href="generated/torch.ao.nn.quantized.dynamic.LSTM.html">LSTM</a></li>
<li class="toctree-l1"><a class="reference internal" href="generated/torch.ao.nn.quantized.dynamic.GRU.html">GRU</a></li>
<li class="toctree-l1"><a class="reference internal" href="generated/torch.ao.nn.quantized.dynamic.RNNCell.html">RNNCell</a></li>
<li class="toctree-l1"><a class="reference internal" href="generated/torch.ao.nn.quantized.dynamic.LSTMCell.html">LSTMCell</a></li>
<li class="toctree-l1"><a class="reference internal" href="generated/torch.ao.nn.quantized.dynamic.GRUCell.html">GRUCell</a></li>
</ul>
</div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
      <div class="sidebar-primary-item">
<div id="ethical-ad-placement"
      class="flat"
      data-ea-publisher="readthedocs"
      data-ea-type="readthedocs-sidebar"
      data-ea-manual="true">
</div></div>
  </div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        
          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item">

<nav aria-label="Breadcrumb" class="d-print-none">
  <ul class="bd-breadcrumbs">
    
    <li class="breadcrumb-item breadcrumb-home">
      <a href="../index.html" class="nav-link" aria-label="Home">
        <i class="fa-solid fa-home"></i>
      </a>
    </li>
    <li class="breadcrumb-item active" aria-current="page"><span class="ellipsis">Quantization API Reference</span></li>
  </ul>
</nav>
</div>
      
    </div>
  
  
</div>
</div>
              
              
              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section id="quantization-api-reference">
<h1>Quantization API Reference<a class="headerlink" href="#quantization-api-reference" title="Link to this heading">#</a></h1>
<section id="torch-ao-quantization">
<h2>torch.ao.quantization<a class="headerlink" href="#torch-ao-quantization" title="Link to this heading">#</a></h2>
<p>This module contains Eager mode quantization APIs.</p>
<section id="top-level-apis">
<h3>Top level APIs<a class="headerlink" href="#top-level-apis" title="Link to this heading">#</a></h3>
<div class="pst-scrollable-table-container"><table class="autosummary longtable table autosummary">
<tbody>
<tr class="row-odd"><td><p><p id="torch.ao.quantization.quantize"/><p id="torch.ao.quantization.quantize"/><a class="reference internal" href="generated/torch.ao.quantization.quantize.html#torch.ao.quantization.quantize" title="torch.ao.quantization.quantize"><code class="xref py py-obj docutils literal notranslate"><span class="pre">quantize</span></code></a></p></td>
<td><p>Quantize the input float model with post training static quantization.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.ao.quantization.quantize_dynamic"/><p id="torch.ao.quantization.quantize_dynamic"/><a class="reference internal" href="generated/torch.ao.quantization.quantize_dynamic.html#torch.ao.quantization.quantize_dynamic" title="torch.ao.quantization.quantize_dynamic"><code class="xref py py-obj docutils literal notranslate"><span class="pre">quantize_dynamic</span></code></a></p></td>
<td><p>Converts a float model to dynamic (i.e. weights-only) quantized model.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.ao.quantization.quantize_qat"/><p id="torch.ao.quantization.quantize_qat"/><a class="reference internal" href="generated/torch.ao.quantization.quantize_qat.html#torch.ao.quantization.quantize_qat" title="torch.ao.quantization.quantize_qat"><code class="xref py py-obj docutils literal notranslate"><span class="pre">quantize_qat</span></code></a></p></td>
<td><p>Do quantization aware training and output a quantized model</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.ao.quantization.prepare"/><p id="torch.ao.quantization.prepare"/><a class="reference internal" href="generated/torch.ao.quantization.prepare.html#torch.ao.quantization.prepare" title="torch.ao.quantization.prepare"><code class="xref py py-obj docutils literal notranslate"><span class="pre">prepare</span></code></a></p></td>
<td><p>Prepares a copy of the model for quantization calibration or quantization-aware training.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.ao.quantization.prepare_qat"/><p id="torch.ao.quantization.prepare_qat"/><a class="reference internal" href="generated/torch.ao.quantization.prepare_qat.html#torch.ao.quantization.prepare_qat" title="torch.ao.quantization.prepare_qat"><code class="xref py py-obj docutils literal notranslate"><span class="pre">prepare_qat</span></code></a></p></td>
<td><p>Prepares a copy of the model for quantization calibration or quantization-aware training and converts it to quantized version.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.ao.quantization.convert"/><p id="torch.ao.quantization.convert"/><a class="reference internal" href="generated/torch.ao.quantization.convert.html#torch.ao.quantization.convert" title="torch.ao.quantization.convert"><code class="xref py py-obj docutils literal notranslate"><span class="pre">convert</span></code></a></p></td>
<td><p>Converts submodules in input module to a different module according to <cite>mapping</cite> by calling <cite>from_float</cite> method on the target module class.</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="preparing-model-for-quantization">
<h3>Preparing model for quantization<a class="headerlink" href="#preparing-model-for-quantization" title="Link to this heading">#</a></h3>
<div class="pst-scrollable-table-container"><table class="autosummary longtable table autosummary">
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.ao.quantization.fuse_modules.fuse_modules.html#torch.ao.quantization.fuse_modules.fuse_modules" title="torch.ao.quantization.fuse_modules.fuse_modules"><code class="xref py py-obj docutils literal notranslate"><span class="pre">fuse_modules.fuse_modules</span></code></a></p></td>
<td><p>Fuse a list of modules into a single module.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.ao.quantization.QuantStub"/><p id="torch.ao.quantization.QuantStub"/><a class="reference internal" href="generated/torch.ao.quantization.QuantStub.html#torch.ao.quantization.QuantStub" title="torch.ao.quantization.QuantStub"><code class="xref py py-obj docutils literal notranslate"><span class="pre">QuantStub</span></code></a></p></td>
<td><p>Quantize stub module, before calibration, this is same as an observer, it will be swapped as <cite>nnq.Quantize</cite> in <cite>convert</cite>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.ao.quantization.DeQuantStub"/><p id="torch.ao.quantization.DeQuantStub"/><a class="reference internal" href="generated/torch.ao.quantization.DeQuantStub.html#torch.ao.quantization.DeQuantStub" title="torch.ao.quantization.DeQuantStub"><code class="xref py py-obj docutils literal notranslate"><span class="pre">DeQuantStub</span></code></a></p></td>
<td><p>Dequantize stub module, before calibration, this is same as identity, this will be swapped as <cite>nnq.DeQuantize</cite> in <cite>convert</cite>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.ao.quantization.QuantWrapper"/><p id="torch.ao.quantization.QuantWrapper"/><a class="reference internal" href="generated/torch.ao.quantization.QuantWrapper.html#torch.ao.quantization.QuantWrapper" title="torch.ao.quantization.QuantWrapper"><code class="xref py py-obj docutils literal notranslate"><span class="pre">QuantWrapper</span></code></a></p></td>
<td><p>A wrapper class that wraps the input module, adds QuantStub and DeQuantStub and surround the call to module with call to quant and dequant modules.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.ao.quantization.add_quant_dequant"/><p id="torch.ao.quantization.add_quant_dequant"/><a class="reference internal" href="generated/torch.ao.quantization.add_quant_dequant.html#torch.ao.quantization.add_quant_dequant" title="torch.ao.quantization.add_quant_dequant"><code class="xref py py-obj docutils literal notranslate"><span class="pre">add_quant_dequant</span></code></a></p></td>
<td><p>Wrap the leaf child module in QuantWrapper if it has a valid qconfig Note that this function will modify the children of module inplace and it can return a new module which wraps the input module as well.</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="utility-functions">
<h3>Utility functions<a class="headerlink" href="#utility-functions" title="Link to this heading">#</a></h3>
<div class="pst-scrollable-table-container"><table class="autosummary longtable table autosummary">
<tbody>
<tr class="row-odd"><td><p><p id="torch.ao.quantization.swap_module"/><p id="torch.ao.quantization.swap_module"/><a class="reference internal" href="generated/torch.ao.quantization.swap_module.html#torch.ao.quantization.swap_module" title="torch.ao.quantization.swap_module"><code class="xref py py-obj docutils literal notranslate"><span class="pre">swap_module</span></code></a></p></td>
<td><p>Swaps the module if it has a quantized counterpart and it has an <cite>observer</cite> attached.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.ao.quantization.propagate_qconfig_"/><p id="torch.ao.quantization.propagate_qconfig_"/><a class="reference internal" href="generated/torch.ao.quantization.propagate_qconfig_.html#torch.ao.quantization.propagate_qconfig_" title="torch.ao.quantization.propagate_qconfig_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">propagate_qconfig_</span></code></a></p></td>
<td><p>Propagate qconfig through the module hierarchy and assign <cite>qconfig</cite> attribute on each leaf module</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.ao.quantization.default_eval_fn"/><p id="torch.ao.quantization.default_eval_fn"/><a class="reference internal" href="generated/torch.ao.quantization.default_eval_fn.html#torch.ao.quantization.default_eval_fn" title="torch.ao.quantization.default_eval_fn"><code class="xref py py-obj docutils literal notranslate"><span class="pre">default_eval_fn</span></code></a></p></td>
<td><p>Define the default evaluation function.</p></td>
</tr>
</tbody>
</table>
</div>
</section>
</section>
<section id="torch-ao-quantization-quantize-fx">
<h2>torch.ao.quantization.quantize_fx<a class="headerlink" href="#torch-ao-quantization-quantize-fx" title="Link to this heading">#</a></h2>
<p>This module contains FX graph mode quantization APIs (prototype).</p>
<div class="pst-scrollable-table-container"><table class="autosummary longtable table autosummary">
<tbody>
<tr class="row-odd"><td><p><p id="torch.ao.quantization.quantize_fx.prepare_fx"/><p id="torch.ao.quantization.quantize_fx.prepare_fx"/><a class="reference internal" href="generated/torch.ao.quantization.quantize_fx.prepare_fx.html#torch.ao.quantization.quantize_fx.prepare_fx" title="torch.ao.quantization.quantize_fx.prepare_fx"><code class="xref py py-obj docutils literal notranslate"><span class="pre">prepare_fx</span></code></a></p></td>
<td><p>Prepare a model for post training quantization</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.ao.quantization.quantize_fx.prepare_qat_fx"/><p id="torch.ao.quantization.quantize_fx.prepare_qat_fx"/><a class="reference internal" href="generated/torch.ao.quantization.quantize_fx.prepare_qat_fx.html#torch.ao.quantization.quantize_fx.prepare_qat_fx" title="torch.ao.quantization.quantize_fx.prepare_qat_fx"><code class="xref py py-obj docutils literal notranslate"><span class="pre">prepare_qat_fx</span></code></a></p></td>
<td><p>Prepare a model for quantization aware training</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.ao.quantization.quantize_fx.convert_fx"/><p id="torch.ao.quantization.quantize_fx.convert_fx"/><a class="reference internal" href="generated/torch.ao.quantization.quantize_fx.convert_fx.html#torch.ao.quantization.quantize_fx.convert_fx" title="torch.ao.quantization.quantize_fx.convert_fx"><code class="xref py py-obj docutils literal notranslate"><span class="pre">convert_fx</span></code></a></p></td>
<td><p>Convert a calibrated or trained model to a quantized model</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.ao.quantization.quantize_fx.fuse_fx"/><p id="torch.ao.quantization.quantize_fx.fuse_fx"/><a class="reference internal" href="generated/torch.ao.quantization.quantize_fx.fuse_fx.html#torch.ao.quantization.quantize_fx.fuse_fx" title="torch.ao.quantization.quantize_fx.fuse_fx"><code class="xref py py-obj docutils literal notranslate"><span class="pre">fuse_fx</span></code></a></p></td>
<td><p>Fuse modules like conv+bn, conv+bn+relu etc, model must be in eval mode.</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="torch-ao-quantization-qconfig-mapping">
<h2>torch.ao.quantization.qconfig_mapping<a class="headerlink" href="#torch-ao-quantization-qconfig-mapping" title="Link to this heading">#</a></h2>
<p>This module contains QConfigMapping for configuring FX graph mode quantization.</p>
<div class="pst-scrollable-table-container"><table class="autosummary longtable table autosummary">
<tbody>
<tr class="row-odd"><td><p><p id="torch.ao.quantization.qconfig_mapping.QConfigMapping"/><p id="torch.ao.quantization.qconfig_mapping.QConfigMapping"/><a class="reference internal" href="generated/torch.ao.quantization.qconfig_mapping.QConfigMapping.html#torch.ao.quantization.qconfig_mapping.QConfigMapping" title="torch.ao.quantization.qconfig_mapping.QConfigMapping"><code class="xref py py-obj docutils literal notranslate"><span class="pre">QConfigMapping</span></code></a></p></td>
<td><p>Mapping from model ops to <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.ao.quantization.QConfig</span></code> s.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.ao.quantization.qconfig_mapping.get_default_qconfig_mapping"/><p id="torch.ao.quantization.qconfig_mapping.get_default_qconfig_mapping"/><a class="reference internal" href="generated/torch.ao.quantization.qconfig_mapping.get_default_qconfig_mapping.html#torch.ao.quantization.qconfig_mapping.get_default_qconfig_mapping" title="torch.ao.quantization.qconfig_mapping.get_default_qconfig_mapping"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_default_qconfig_mapping</span></code></a></p></td>
<td><p>Return the default QConfigMapping for post training quantization.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.ao.quantization.qconfig_mapping.get_default_qat_qconfig_mapping"/><p id="torch.ao.quantization.qconfig_mapping.get_default_qat_qconfig_mapping"/><a class="reference internal" href="generated/torch.ao.quantization.qconfig_mapping.get_default_qat_qconfig_mapping.html#torch.ao.quantization.qconfig_mapping.get_default_qat_qconfig_mapping" title="torch.ao.quantization.qconfig_mapping.get_default_qat_qconfig_mapping"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_default_qat_qconfig_mapping</span></code></a></p></td>
<td><p>Return the default QConfigMapping for quantization aware training.</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="torch-ao-quantization-backend-config">
<h2>torch.ao.quantization.backend_config<a class="headerlink" href="#torch-ao-quantization-backend-config" title="Link to this heading">#</a></h2>
<p>This module contains BackendConfig, a config object that defines how quantization is supported
in a backend. Currently only used by FX Graph Mode Quantization, but we may extend Eager Mode
Quantization to work with this as well.</p>
<div class="pst-scrollable-table-container"><table class="autosummary longtable table autosummary">
<tbody>
<tr class="row-odd"><td><p><p id="torch.ao.quantization.backend_config.BackendConfig"/><p id="torch.ao.quantization.backend_config.BackendConfig"/><a class="reference internal" href="generated/torch.ao.quantization.backend_config.BackendConfig.html#torch.ao.quantization.backend_config.BackendConfig" title="torch.ao.quantization.backend_config.BackendConfig"><code class="xref py py-obj docutils literal notranslate"><span class="pre">BackendConfig</span></code></a></p></td>
<td><p>Config that defines the set of patterns that can be quantized on a given backend, and how reference quantized models can be produced from these patterns.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.ao.quantization.backend_config.BackendPatternConfig"/><p id="torch.ao.quantization.backend_config.BackendPatternConfig"/><a class="reference internal" href="generated/torch.ao.quantization.backend_config.BackendPatternConfig.html#torch.ao.quantization.backend_config.BackendPatternConfig" title="torch.ao.quantization.backend_config.BackendPatternConfig"><code class="xref py py-obj docutils literal notranslate"><span class="pre">BackendPatternConfig</span></code></a></p></td>
<td><p>Config object that specifies quantization behavior for a given operator pattern.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.ao.quantization.backend_config.DTypeConfig"/><p id="torch.ao.quantization.backend_config.DTypeConfig"/><a class="reference internal" href="generated/torch.ao.quantization.backend_config.DTypeConfig.html#torch.ao.quantization.backend_config.DTypeConfig" title="torch.ao.quantization.backend_config.DTypeConfig"><code class="xref py py-obj docutils literal notranslate"><span class="pre">DTypeConfig</span></code></a></p></td>
<td><p>Config object that specifies the supported data types passed as arguments to quantize ops in the reference model spec, for input and output activations, weights, and biases.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.ao.quantization.backend_config.DTypeWithConstraints"/><p id="torch.ao.quantization.backend_config.DTypeWithConstraints"/><a class="reference internal" href="generated/torch.ao.quantization.backend_config.DTypeWithConstraints.html#torch.ao.quantization.backend_config.DTypeWithConstraints" title="torch.ao.quantization.backend_config.DTypeWithConstraints"><code class="xref py py-obj docutils literal notranslate"><span class="pre">DTypeWithConstraints</span></code></a></p></td>
<td><p>Config for specifying additional constraints for a given dtype, such as quantization value ranges, scale value ranges, and fixed quantization params, to be used in <a class="reference internal" href="generated/torch.ao.quantization.backend_config.DTypeConfig.html#torch.ao.quantization.backend_config.DTypeConfig" title="torch.ao.quantization.backend_config.DTypeConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">DTypeConfig</span></code></a>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.ao.quantization.backend_config.ObservationType"/><p id="torch.ao.quantization.backend_config.ObservationType"/><a class="reference internal" href="generated/torch.ao.quantization.backend_config.ObservationType.html#torch.ao.quantization.backend_config.ObservationType" title="torch.ao.quantization.backend_config.ObservationType"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ObservationType</span></code></a></p></td>
<td><p>An enum that represents different ways of how an operator/operator pattern should be observed</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="torch-ao-quantization-fx-custom-config">
<h2>torch.ao.quantization.fx.custom_config<a class="headerlink" href="#torch-ao-quantization-fx-custom-config" title="Link to this heading">#</a></h2>
<p>This module contains a few CustomConfig classes that’s used in both eager mode and FX graph mode quantization</p>
<div class="pst-scrollable-table-container"><table class="autosummary longtable table autosummary">
<tbody>
<tr class="row-odd"><td><p><p id="torch.ao.quantization.fx.custom_config.FuseCustomConfig"/><p id="torch.ao.quantization.fx.custom_config.FuseCustomConfig"/><a class="reference internal" href="generated/torch.ao.quantization.fx.custom_config.FuseCustomConfig.html#torch.ao.quantization.fx.custom_config.FuseCustomConfig" title="torch.ao.quantization.fx.custom_config.FuseCustomConfig"><code class="xref py py-obj docutils literal notranslate"><span class="pre">FuseCustomConfig</span></code></a></p></td>
<td><p>Custom configuration for <a class="reference internal" href="generated/torch.ao.quantization.quantize_fx.fuse_fx.html#torch.ao.quantization.quantize_fx.fuse_fx" title="torch.ao.quantization.quantize_fx.fuse_fx"><code class="xref py py-func docutils literal notranslate"><span class="pre">fuse_fx()</span></code></a>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.ao.quantization.fx.custom_config.PrepareCustomConfig"/><p id="torch.ao.quantization.fx.custom_config.PrepareCustomConfig"/><a class="reference internal" href="generated/torch.ao.quantization.fx.custom_config.PrepareCustomConfig.html#torch.ao.quantization.fx.custom_config.PrepareCustomConfig" title="torch.ao.quantization.fx.custom_config.PrepareCustomConfig"><code class="xref py py-obj docutils literal notranslate"><span class="pre">PrepareCustomConfig</span></code></a></p></td>
<td><p>Custom configuration for <a class="reference internal" href="generated/torch.ao.quantization.quantize_fx.prepare_fx.html#torch.ao.quantization.quantize_fx.prepare_fx" title="torch.ao.quantization.quantize_fx.prepare_fx"><code class="xref py py-func docutils literal notranslate"><span class="pre">prepare_fx()</span></code></a> and <a class="reference internal" href="generated/torch.ao.quantization.quantize_fx.prepare_qat_fx.html#torch.ao.quantization.quantize_fx.prepare_qat_fx" title="torch.ao.quantization.quantize_fx.prepare_qat_fx"><code class="xref py py-func docutils literal notranslate"><span class="pre">prepare_qat_fx()</span></code></a>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.ao.quantization.fx.custom_config.ConvertCustomConfig"/><p id="torch.ao.quantization.fx.custom_config.ConvertCustomConfig"/><a class="reference internal" href="generated/torch.ao.quantization.fx.custom_config.ConvertCustomConfig.html#torch.ao.quantization.fx.custom_config.ConvertCustomConfig" title="torch.ao.quantization.fx.custom_config.ConvertCustomConfig"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ConvertCustomConfig</span></code></a></p></td>
<td><p>Custom configuration for <a class="reference internal" href="generated/torch.ao.quantization.quantize_fx.convert_fx.html#torch.ao.quantization.quantize_fx.convert_fx" title="torch.ao.quantization.quantize_fx.convert_fx"><code class="xref py py-func docutils literal notranslate"><span class="pre">convert_fx()</span></code></a>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.ao.quantization.fx.custom_config.StandaloneModuleConfigEntry"/><p id="torch.ao.quantization.fx.custom_config.StandaloneModuleConfigEntry"/><a class="reference internal" href="generated/torch.ao.quantization.fx.custom_config.StandaloneModuleConfigEntry.html#torch.ao.quantization.fx.custom_config.StandaloneModuleConfigEntry" title="torch.ao.quantization.fx.custom_config.StandaloneModuleConfigEntry"><code class="xref py py-obj docutils literal notranslate"><span class="pre">StandaloneModuleConfigEntry</span></code></a></p></td>
<td><p></p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="module-torch.ao.quantization.quantizer">
<span id="torch-ao-quantization-quantizer"></span><h2>torch.ao.quantization.quantizer<a class="headerlink" href="#module-torch.ao.quantization.quantizer" title="Link to this heading">#</a></h2>
</section>
<section id="module-torch.ao.quantization.pt2e">
<span id="torch-ao-quantization-pt2e-quantization-in-pytorch-2-0-export-implementation"></span><h2>torch.ao.quantization.pt2e (quantization in pytorch 2.0 export implementation)<a class="headerlink" href="#module-torch.ao.quantization.pt2e" title="Link to this heading">#</a></h2>
</section>
<section id="torch-ao-quantization-pt2e-export-utils">
<span id="module-torch.ao.quantization.pt2e.representation"></span><h2>torch.ao.quantization.pt2e.export_utils<a class="headerlink" href="#torch-ao-quantization-pt2e-export-utils" title="Link to this heading">#</a></h2>
<div class="pst-scrollable-table-container"><table class="autosummary longtable table autosummary">
<tbody>
<tr class="row-odd"><td><p><p id="torch.ao.quantization.pt2e.export_utils.model_is_exported"/><p id="torch.ao.quantization.pt2e.export_utils.model_is_exported"/><a class="reference internal" href="generated/torch.ao.quantization.pt2e.export_utils.model_is_exported.html#torch.ao.quantization.pt2e.export_utils.model_is_exported" title="torch.ao.quantization.pt2e.export_utils.model_is_exported"><code class="xref py py-obj docutils literal notranslate"><span class="pre">model_is_exported</span></code></a></p></td>
<td><p>Return True if the <cite>torch.nn.Module</cite> was exported, False otherwise (e.g. if the model was FX symbolically traced or not traced at all).</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="pt2-export-pt2e-numeric-debugger">
<h2>PT2 Export (pt2e) Numeric Debugger<a class="headerlink" href="#pt2-export-pt2e-numeric-debugger" title="Link to this heading">#</a></h2>
<div class="pst-scrollable-table-container"><table class="autosummary longtable table autosummary">
<tbody>
<tr class="row-odd"><td><p><p id="torch.ao.quantization.generate_numeric_debug_handle"/><p id="torch.ao.quantization.generate_numeric_debug_handle"/><a class="reference internal" href="generated/torch.ao.quantization.generate_numeric_debug_handle.html#torch.ao.quantization.generate_numeric_debug_handle" title="torch.ao.quantization.generate_numeric_debug_handle"><code class="xref py py-obj docutils literal notranslate"><span class="pre">generate_numeric_debug_handle</span></code></a></p></td>
<td><p>Attach numeric_debug_handle_id for all nodes in the graph module of the given ExportedProgram, like conv2d, squeeze, conv1d, etc, except for placeholder.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.ao.quantization.CUSTOM_KEY"/><p id="torch.ao.quantization.CUSTOM_KEY"/><a class="reference internal" href="generated/torch.ao.quantization.CUSTOM_KEY.html#torch.ao.quantization.CUSTOM_KEY" title="torch.ao.quantization.CUSTOM_KEY"><code class="xref py py-obj docutils literal notranslate"><span class="pre">CUSTOM_KEY</span></code></a></p></td>
<td><p>str(object='') -&gt; str str(bytes_or_buffer[, encoding[, errors]]) -&gt; str</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.ao.quantization.NUMERIC_DEBUG_HANDLE_KEY"/><p id="torch.ao.quantization.NUMERIC_DEBUG_HANDLE_KEY"/><a class="reference internal" href="generated/torch.ao.quantization.NUMERIC_DEBUG_HANDLE_KEY.html#torch.ao.quantization.NUMERIC_DEBUG_HANDLE_KEY" title="torch.ao.quantization.NUMERIC_DEBUG_HANDLE_KEY"><code class="xref py py-obj docutils literal notranslate"><span class="pre">NUMERIC_DEBUG_HANDLE_KEY</span></code></a></p></td>
<td><p>str(object='') -&gt; str str(bytes_or_buffer[, encoding[, errors]]) -&gt; str</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.ao.quantization.prepare_for_propagation_comparison"/><p id="torch.ao.quantization.prepare_for_propagation_comparison"/><a class="reference internal" href="generated/torch.ao.quantization.prepare_for_propagation_comparison.html#torch.ao.quantization.prepare_for_propagation_comparison" title="torch.ao.quantization.prepare_for_propagation_comparison"><code class="xref py py-obj docutils literal notranslate"><span class="pre">prepare_for_propagation_comparison</span></code></a></p></td>
<td><p>Add output loggers to node that has numeric_debug_handle</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.ao.quantization.extract_results_from_loggers"/><p id="torch.ao.quantization.extract_results_from_loggers"/><a class="reference internal" href="generated/torch.ao.quantization.extract_results_from_loggers.html#torch.ao.quantization.extract_results_from_loggers" title="torch.ao.quantization.extract_results_from_loggers"><code class="xref py py-obj docutils literal notranslate"><span class="pre">extract_results_from_loggers</span></code></a></p></td>
<td><p>For a given model, extract the tensors stats and related information for each debug handle.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.ao.quantization.compare_results"/><p id="torch.ao.quantization.compare_results"/><a class="reference internal" href="generated/torch.ao.quantization.compare_results.html#torch.ao.quantization.compare_results" title="torch.ao.quantization.compare_results"><code class="xref py py-obj docutils literal notranslate"><span class="pre">compare_results</span></code></a></p></td>
<td><p>Given two dict mapping from <cite>debug_handle_id</cite> (int) to list of tensors return a map from <cite>debug_handle_id</cite> to <cite>NodeAccuracySummary</cite> that contains comparison information like SQNR, MSE etc.</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="torch-quantization-related-functions">
<h2>torch (quantization related functions)<a class="headerlink" href="#torch-quantization-related-functions" title="Link to this heading">#</a></h2>
<p>This describes the quantization related functions of the <cite>torch</cite> namespace.</p>
<div class="pst-scrollable-table-container"><table class="autosummary longtable table autosummary">
<tbody>
<tr class="row-odd"><td><p><p id="torch.quantize_per_tensor"/><p id="torch.quantize_per_tensor"/><a class="reference internal" href="generated/torch.quantize_per_tensor.html#torch.quantize_per_tensor" title="torch.quantize_per_tensor"><code class="xref py py-obj docutils literal notranslate"><span class="pre">quantize_per_tensor</span></code></a></p></td>
<td><p>Converts a float tensor to a quantized tensor with given scale and zero point.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.quantize_per_channel"/><p id="torch.quantize_per_channel"/><a class="reference internal" href="generated/torch.quantize_per_channel.html#torch.quantize_per_channel" title="torch.quantize_per_channel"><code class="xref py py-obj docutils literal notranslate"><span class="pre">quantize_per_channel</span></code></a></p></td>
<td><p>Converts a float tensor to a per-channel quantized tensor with given scales and zero points.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.dequantize"/><p id="torch.dequantize"/><a class="reference internal" href="generated/torch.Tensor.dequantize.html#torch.dequantize" title="torch.dequantize"><code class="xref py py-obj docutils literal notranslate"><span class="pre">dequantize</span></code></a></p></td>
<td><p>Returns an fp32 Tensor by dequantizing a quantized Tensor</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="torch-tensor-quantization-related-methods">
<h2>torch.Tensor (quantization related methods)<a class="headerlink" href="#torch-tensor-quantization-related-methods" title="Link to this heading">#</a></h2>
<p>Quantized Tensors support a limited subset of data manipulation methods of the
regular full-precision tensor.</p>
<div class="pst-scrollable-table-container"><table class="autosummary longtable table autosummary">
<tbody>
<tr class="row-odd"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">view</span></code></p></td>
<td><p>Returns a new tensor with the same data as the <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor but of a different <a class="reference internal" href="generated/torch.Tensor.shape.html#torch.Tensor.shape" title="torch.Tensor.shape"><code class="xref py py-attr docutils literal notranslate"><span class="pre">shape</span></code></a>.</p></td>
</tr>
<tr class="row-even"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">as_strided</span></code></p></td>
<td><p>See <a class="reference internal" href="generated/torch.Tensor.as_strided.html#torch.as_strided" title="torch.as_strided"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.as_strided()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">expand</span></code></p></td>
<td><p>Returns a new view of the <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor with singleton dimensions expanded to a larger size.</p></td>
</tr>
<tr class="row-even"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">flatten</span></code></p></td>
<td><p>See <a class="reference internal" href="generated/torch.Tensor.flatten.html#torch.flatten" title="torch.flatten"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.flatten()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">select</span></code></p></td>
<td><p>See <a class="reference internal" href="generated/torch.Tensor.select.html#torch.select" title="torch.select"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.select()</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">ne</span></code></p></td>
<td><p>See <a class="reference internal" href="generated/torch.Tensor.ne.html#torch.ne" title="torch.ne"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.ne()</span></code></a>.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">eq</span></code></p></td>
<td><p>See <a class="reference internal" href="generated/torch.Tensor.eq.html#torch.eq" title="torch.eq"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.eq()</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">ge</span></code></p></td>
<td><p>See <a class="reference internal" href="generated/torch.Tensor.ge.html#torch.ge" title="torch.ge"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.ge()</span></code></a>.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">le</span></code></p></td>
<td><p>See <a class="reference internal" href="generated/torch.Tensor.le.html#torch.le" title="torch.le"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.le()</span></code></a>.</p></td>
</tr>
<tr class="row-even"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">gt</span></code></p></td>
<td><p>See <a class="reference internal" href="generated/torch.Tensor.gt.html#torch.gt" title="torch.gt"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.gt()</span></code></a>.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">lt</span></code></p></td>
<td><p>See <a class="reference internal" href="generated/torch.Tensor.lt.html#torch.lt" title="torch.lt"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.lt()</span></code></a>.</p></td>
</tr>
<tr class="row-even"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">copy_</span></code></p></td>
<td><p>Copies the elements from <code class="xref py py-attr docutils literal notranslate"><span class="pre">src</span></code> into <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor and returns <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">clone</span></code></p></td>
<td><p>See <a class="reference internal" href="generated/torch.Tensor.clone.html#torch.clone" title="torch.clone"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.clone()</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">dequantize</span></code></p></td>
<td><p>Given a quantized Tensor, dequantize it and return the dequantized float Tensor.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">equal</span></code></p></td>
<td><p>See <a class="reference internal" href="generated/torch.Tensor.equal.html#torch.equal" title="torch.equal"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.equal()</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">int_repr</span></code></p></td>
<td><p>Given a quantized Tensor, <code class="docutils literal notranslate"><span class="pre">self.int_repr()</span></code> returns a CPU Tensor with uint8_t as data type that stores the underlying uint8_t values of the given Tensor.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">max</span></code></p></td>
<td><p>See <a class="reference internal" href="generated/torch.Tensor.max.html#torch.max" title="torch.max"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.max()</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">mean</span></code></p></td>
<td><p>See <a class="reference internal" href="generated/torch.Tensor.mean.html#torch.mean" title="torch.mean"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.mean()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">min</span></code></p></td>
<td><p>See <a class="reference internal" href="generated/torch.Tensor.min.html#torch.min" title="torch.min"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.min()</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">q_scale</span></code></p></td>
<td><p>Given a Tensor quantized by linear(affine) quantization, returns the scale of the underlying quantizer().</p></td>
</tr>
<tr class="row-odd"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">q_zero_point</span></code></p></td>
<td><p>Given a Tensor quantized by linear(affine) quantization, returns the zero_point of the underlying quantizer().</p></td>
</tr>
<tr class="row-even"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">q_per_channel_scales</span></code></p></td>
<td><p>Given a Tensor quantized by linear (affine) per-channel quantization, returns a Tensor of scales of the underlying quantizer.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">q_per_channel_zero_points</span></code></p></td>
<td><p>Given a Tensor quantized by linear (affine) per-channel quantization, returns a tensor of zero_points of the underlying quantizer.</p></td>
</tr>
<tr class="row-even"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">q_per_channel_axis</span></code></p></td>
<td><p>Given a Tensor quantized by linear (affine) per-channel quantization, returns the index of dimension on which per-channel quantization is applied.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">resize_</span></code></p></td>
<td><p>Resizes <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor to the specified size.</p></td>
</tr>
<tr class="row-even"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">sort</span></code></p></td>
<td><p>See <a class="reference internal" href="generated/torch.Tensor.sort.html#torch.sort" title="torch.sort"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.sort()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">topk</span></code></p></td>
<td><p>See <a class="reference internal" href="generated/torch.Tensor.topk.html#torch.topk" title="torch.topk"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.topk()</span></code></a></p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="torch-ao-quantization-observer">
<h2>torch.ao.quantization.observer<a class="headerlink" href="#torch-ao-quantization-observer" title="Link to this heading">#</a></h2>
<p>This module contains observers which are used to collect statistics about
the values observed during calibration (PTQ) or training (QAT).</p>
<div class="pst-scrollable-table-container"><table class="autosummary longtable table autosummary">
<tbody>
<tr class="row-odd"><td><p><p id="torch.ao.quantization.observer.ObserverBase"/><p id="torch.ao.quantization.observer.ObserverBase"/><a class="reference internal" href="generated/torch.ao.quantization.observer.ObserverBase.html#torch.ao.quantization.observer.ObserverBase" title="torch.ao.quantization.observer.ObserverBase"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ObserverBase</span></code></a></p></td>
<td><p>Base observer Module.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.ao.quantization.observer.MinMaxObserver"/><p id="torch.ao.quantization.observer.MinMaxObserver"/><a class="reference internal" href="generated/torch.ao.quantization.observer.MinMaxObserver.html#torch.ao.quantization.observer.MinMaxObserver" title="torch.ao.quantization.observer.MinMaxObserver"><code class="xref py py-obj docutils literal notranslate"><span class="pre">MinMaxObserver</span></code></a></p></td>
<td><p>Observer module for computing the quantization parameters based on the running min and max values.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.ao.quantization.observer.MovingAverageMinMaxObserver"/><p id="torch.ao.quantization.observer.MovingAverageMinMaxObserver"/><a class="reference internal" href="generated/torch.ao.quantization.observer.MovingAverageMinMaxObserver.html#torch.ao.quantization.observer.MovingAverageMinMaxObserver" title="torch.ao.quantization.observer.MovingAverageMinMaxObserver"><code class="xref py py-obj docutils literal notranslate"><span class="pre">MovingAverageMinMaxObserver</span></code></a></p></td>
<td><p>Observer module for computing the quantization parameters based on the moving average of the min and max values.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.ao.quantization.observer.PerChannelMinMaxObserver"/><p id="torch.ao.quantization.observer.PerChannelMinMaxObserver"/><a class="reference internal" href="generated/torch.ao.quantization.observer.PerChannelMinMaxObserver.html#torch.ao.quantization.observer.PerChannelMinMaxObserver" title="torch.ao.quantization.observer.PerChannelMinMaxObserver"><code class="xref py py-obj docutils literal notranslate"><span class="pre">PerChannelMinMaxObserver</span></code></a></p></td>
<td><p>Observer module for computing the quantization parameters based on the running per channel min and max values.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver"/><p id="torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver"/><a class="reference internal" href="generated/torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver.html#torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver" title="torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver"><code class="xref py py-obj docutils literal notranslate"><span class="pre">MovingAveragePerChannelMinMaxObserver</span></code></a></p></td>
<td><p>Observer module for computing the quantization parameters based on the running per channel min and max values.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.ao.quantization.observer.HistogramObserver"/><p id="torch.ao.quantization.observer.HistogramObserver"/><a class="reference internal" href="generated/torch.ao.quantization.observer.HistogramObserver.html#torch.ao.quantization.observer.HistogramObserver" title="torch.ao.quantization.observer.HistogramObserver"><code class="xref py py-obj docutils literal notranslate"><span class="pre">HistogramObserver</span></code></a></p></td>
<td><p>The module records the running histogram of tensor values along with min/max values.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.ao.quantization.observer.PlaceholderObserver"/><p id="torch.ao.quantization.observer.PlaceholderObserver"/><a class="reference internal" href="generated/torch.ao.quantization.observer.PlaceholderObserver.html#torch.ao.quantization.observer.PlaceholderObserver" title="torch.ao.quantization.observer.PlaceholderObserver"><code class="xref py py-obj docutils literal notranslate"><span class="pre">PlaceholderObserver</span></code></a></p></td>
<td><p>Observer that doesn't do anything and just passes its configuration to the quantized module's <code class="docutils literal notranslate"><span class="pre">.from_float()</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.ao.quantization.observer.RecordingObserver"/><p id="torch.ao.quantization.observer.RecordingObserver"/><a class="reference internal" href="generated/torch.ao.quantization.observer.RecordingObserver.html#torch.ao.quantization.observer.RecordingObserver" title="torch.ao.quantization.observer.RecordingObserver"><code class="xref py py-obj docutils literal notranslate"><span class="pre">RecordingObserver</span></code></a></p></td>
<td><p>The module is mainly for debug and records the tensor values during runtime.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.ao.quantization.observer.NoopObserver"/><p id="torch.ao.quantization.observer.NoopObserver"/><a class="reference internal" href="generated/torch.ao.quantization.observer.NoopObserver.html#torch.ao.quantization.observer.NoopObserver" title="torch.ao.quantization.observer.NoopObserver"><code class="xref py py-obj docutils literal notranslate"><span class="pre">NoopObserver</span></code></a></p></td>
<td><p>Observer that doesn't do anything and just passes its configuration to the quantized module's <code class="docutils literal notranslate"><span class="pre">.from_float()</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.ao.quantization.observer.get_observer_state_dict"/><p id="torch.ao.quantization.observer.get_observer_state_dict"/><a class="reference internal" href="generated/torch.ao.quantization.observer.get_observer_state_dict.html#torch.ao.quantization.observer.get_observer_state_dict" title="torch.ao.quantization.observer.get_observer_state_dict"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_observer_state_dict</span></code></a></p></td>
<td><p>Returns the state dict corresponding to the observer stats.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.ao.quantization.observer.load_observer_state_dict"/><p id="torch.ao.quantization.observer.load_observer_state_dict"/><a class="reference internal" href="generated/torch.ao.quantization.observer.load_observer_state_dict.html#torch.ao.quantization.observer.load_observer_state_dict" title="torch.ao.quantization.observer.load_observer_state_dict"><code class="xref py py-obj docutils literal notranslate"><span class="pre">load_observer_state_dict</span></code></a></p></td>
<td><p>Given input model and a state_dict containing model observer stats, load the stats back into the model.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.ao.quantization.observer.default_observer"/><p id="torch.ao.quantization.observer.default_observer"/><a class="reference internal" href="generated/torch.ao.quantization.observer.default_observer.html#torch.ao.quantization.observer.default_observer" title="torch.ao.quantization.observer.default_observer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">default_observer</span></code></a></p></td>
<td><p>Default observer for static quantization, usually used for debugging.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.ao.quantization.observer.default_placeholder_observer"/><p id="torch.ao.quantization.observer.default_placeholder_observer"/><a class="reference internal" href="generated/torch.ao.quantization.observer.default_placeholder_observer.html#torch.ao.quantization.observer.default_placeholder_observer" title="torch.ao.quantization.observer.default_placeholder_observer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">default_placeholder_observer</span></code></a></p></td>
<td><p>Default placeholder observer, usually used for quantization to torch.float16.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.ao.quantization.observer.default_debug_observer"/><p id="torch.ao.quantization.observer.default_debug_observer"/><a class="reference internal" href="generated/torch.ao.quantization.observer.default_debug_observer.html#torch.ao.quantization.observer.default_debug_observer" title="torch.ao.quantization.observer.default_debug_observer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">default_debug_observer</span></code></a></p></td>
<td><p>Default debug-only observer.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.ao.quantization.observer.default_weight_observer"/><p id="torch.ao.quantization.observer.default_weight_observer"/><a class="reference internal" href="generated/torch.ao.quantization.observer.default_weight_observer.html#torch.ao.quantization.observer.default_weight_observer" title="torch.ao.quantization.observer.default_weight_observer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">default_weight_observer</span></code></a></p></td>
<td><p>Default weight observer.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.ao.quantization.observer.default_histogram_observer"/><p id="torch.ao.quantization.observer.default_histogram_observer"/><a class="reference internal" href="generated/torch.ao.quantization.observer.default_histogram_observer.html#torch.ao.quantization.observer.default_histogram_observer" title="torch.ao.quantization.observer.default_histogram_observer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">default_histogram_observer</span></code></a></p></td>
<td><p>Default histogram observer, usually used for PTQ.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.ao.quantization.observer.default_per_channel_weight_observer"/><p id="torch.ao.quantization.observer.default_per_channel_weight_observer"/><a class="reference internal" href="generated/torch.ao.quantization.observer.default_per_channel_weight_observer.html#torch.ao.quantization.observer.default_per_channel_weight_observer" title="torch.ao.quantization.observer.default_per_channel_weight_observer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">default_per_channel_weight_observer</span></code></a></p></td>
<td><p>Default per-channel weight observer, usually used on backends where per-channel weight quantization is supported, such as <cite>fbgemm</cite>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.ao.quantization.observer.default_dynamic_quant_observer"/><p id="torch.ao.quantization.observer.default_dynamic_quant_observer"/><a class="reference internal" href="generated/torch.ao.quantization.observer.default_dynamic_quant_observer.html#torch.ao.quantization.observer.default_dynamic_quant_observer" title="torch.ao.quantization.observer.default_dynamic_quant_observer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">default_dynamic_quant_observer</span></code></a></p></td>
<td><p>Default observer for dynamic quantization.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.ao.quantization.observer.default_float_qparams_observer"/><p id="torch.ao.quantization.observer.default_float_qparams_observer"/><a class="reference internal" href="generated/torch.ao.quantization.observer.default_float_qparams_observer.html#torch.ao.quantization.observer.default_float_qparams_observer" title="torch.ao.quantization.observer.default_float_qparams_observer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">default_float_qparams_observer</span></code></a></p></td>
<td><p>Default observer for a floating point zero-point.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.ao.quantization.observer.AffineQuantizedObserverBase"/><p id="torch.ao.quantization.observer.AffineQuantizedObserverBase"/><a class="reference internal" href="generated/torch.ao.quantization.observer.AffineQuantizedObserverBase.html#torch.ao.quantization.observer.AffineQuantizedObserverBase" title="torch.ao.quantization.observer.AffineQuantizedObserverBase"><code class="xref py py-obj docutils literal notranslate"><span class="pre">AffineQuantizedObserverBase</span></code></a></p></td>
<td><p>Observer module for affine quantization (<a class="github reference external" href="https://github.com/pytorch/ao/tree/main/torchao/quantization#affine-quantization">pytorch/ao</a>)</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.ao.quantization.observer.Granularity"/><p id="torch.ao.quantization.observer.Granularity"/><a class="reference internal" href="generated/torch.ao.quantization.observer.Granularity.html#torch.ao.quantization.observer.Granularity" title="torch.ao.quantization.observer.Granularity"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Granularity</span></code></a></p></td>
<td><p>Base class for representing the granularity of quantization.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.ao.quantization.observer.MappingType"/><p id="torch.ao.quantization.observer.MappingType"/><a class="reference internal" href="generated/torch.ao.quantization.observer.MappingType.html#torch.ao.quantization.observer.MappingType" title="torch.ao.quantization.observer.MappingType"><code class="xref py py-obj docutils literal notranslate"><span class="pre">MappingType</span></code></a></p></td>
<td><p>How floating point number is mapped to integer number</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.ao.quantization.observer.PerAxis"/><p id="torch.ao.quantization.observer.PerAxis"/><a class="reference internal" href="generated/torch.ao.quantization.observer.PerAxis.html#torch.ao.quantization.observer.PerAxis" title="torch.ao.quantization.observer.PerAxis"><code class="xref py py-obj docutils literal notranslate"><span class="pre">PerAxis</span></code></a></p></td>
<td><p>Represents per-axis granularity in quantization.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.ao.quantization.observer.PerBlock"/><p id="torch.ao.quantization.observer.PerBlock"/><a class="reference internal" href="generated/torch.ao.quantization.observer.PerBlock.html#torch.ao.quantization.observer.PerBlock" title="torch.ao.quantization.observer.PerBlock"><code class="xref py py-obj docutils literal notranslate"><span class="pre">PerBlock</span></code></a></p></td>
<td><p>Represents per-block granularity in quantization.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.ao.quantization.observer.PerGroup"/><p id="torch.ao.quantization.observer.PerGroup"/><a class="reference internal" href="generated/torch.ao.quantization.observer.PerGroup.html#torch.ao.quantization.observer.PerGroup" title="torch.ao.quantization.observer.PerGroup"><code class="xref py py-obj docutils literal notranslate"><span class="pre">PerGroup</span></code></a></p></td>
<td><p>Represents per-channel group granularity in quantization.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.ao.quantization.observer.PerRow"/><p id="torch.ao.quantization.observer.PerRow"/><a class="reference internal" href="generated/torch.ao.quantization.observer.PerRow.html#torch.ao.quantization.observer.PerRow" title="torch.ao.quantization.observer.PerRow"><code class="xref py py-obj docutils literal notranslate"><span class="pre">PerRow</span></code></a></p></td>
<td><p>Represents row-wise granularity in quantization.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.ao.quantization.observer.PerTensor"/><p id="torch.ao.quantization.observer.PerTensor"/><a class="reference internal" href="generated/torch.ao.quantization.observer.PerTensor.html#torch.ao.quantization.observer.PerTensor" title="torch.ao.quantization.observer.PerTensor"><code class="xref py py-obj docutils literal notranslate"><span class="pre">PerTensor</span></code></a></p></td>
<td><p>Represents per-tensor granularity in quantization.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.ao.quantization.observer.PerToken"/><p id="torch.ao.quantization.observer.PerToken"/><a class="reference internal" href="generated/torch.ao.quantization.observer.PerToken.html#torch.ao.quantization.observer.PerToken" title="torch.ao.quantization.observer.PerToken"><code class="xref py py-obj docutils literal notranslate"><span class="pre">PerToken</span></code></a></p></td>
<td><p>Represents per-token granularity in quantization.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.ao.quantization.observer.TorchAODType"/><p id="torch.ao.quantization.observer.TorchAODType"/><a class="reference internal" href="generated/torch.ao.quantization.observer.TorchAODType.html#torch.ao.quantization.observer.TorchAODType" title="torch.ao.quantization.observer.TorchAODType"><code class="xref py py-obj docutils literal notranslate"><span class="pre">TorchAODType</span></code></a></p></td>
<td><p>Placeholder for dtypes that do not exist in PyTorch core yet.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.ao.quantization.observer.ZeroPointDomain"/><p id="torch.ao.quantization.observer.ZeroPointDomain"/><a class="reference internal" href="generated/torch.ao.quantization.observer.ZeroPointDomain.html#torch.ao.quantization.observer.ZeroPointDomain" title="torch.ao.quantization.observer.ZeroPointDomain"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ZeroPointDomain</span></code></a></p></td>
<td><p>Enum that indicate whether zero_point is in integer domain or floating point domain</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.ao.quantization.observer.get_block_size"/><p id="torch.ao.quantization.observer.get_block_size"/><a class="reference internal" href="generated/torch.ao.quantization.observer.get_block_size.html#torch.ao.quantization.observer.get_block_size" title="torch.ao.quantization.observer.get_block_size"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_block_size</span></code></a></p></td>
<td><p>Get the block size based on the input shape and granularity type.</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="torch-ao-quantization-fake-quantize">
<h2>torch.ao.quantization.fake_quantize<a class="headerlink" href="#torch-ao-quantization-fake-quantize" title="Link to this heading">#</a></h2>
<p>This module implements modules which are used to perform fake quantization
during QAT.</p>
<div class="pst-scrollable-table-container"><table class="autosummary longtable table autosummary">
<tbody>
<tr class="row-odd"><td><p><p id="torch.ao.quantization.fake_quantize.FakeQuantizeBase"/><p id="torch.ao.quantization.fake_quantize.FakeQuantizeBase"/><a class="reference internal" href="generated/torch.ao.quantization.fake_quantize.FakeQuantizeBase.html#torch.ao.quantization.fake_quantize.FakeQuantizeBase" title="torch.ao.quantization.fake_quantize.FakeQuantizeBase"><code class="xref py py-obj docutils literal notranslate"><span class="pre">FakeQuantizeBase</span></code></a></p></td>
<td><p>Base fake quantize module.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.ao.quantization.fake_quantize.FakeQuantize"/><p id="torch.ao.quantization.fake_quantize.FakeQuantize"/><a class="reference internal" href="generated/torch.ao.quantization.fake_quantize.FakeQuantize.html#torch.ao.quantization.fake_quantize.FakeQuantize" title="torch.ao.quantization.fake_quantize.FakeQuantize"><code class="xref py py-obj docutils literal notranslate"><span class="pre">FakeQuantize</span></code></a></p></td>
<td><p>Simulate the quantize and dequantize operations in training time.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.ao.quantization.fake_quantize.FixedQParamsFakeQuantize"/><p id="torch.ao.quantization.fake_quantize.FixedQParamsFakeQuantize"/><a class="reference internal" href="generated/torch.ao.quantization.fake_quantize.FixedQParamsFakeQuantize.html#torch.ao.quantization.fake_quantize.FixedQParamsFakeQuantize" title="torch.ao.quantization.fake_quantize.FixedQParamsFakeQuantize"><code class="xref py py-obj docutils literal notranslate"><span class="pre">FixedQParamsFakeQuantize</span></code></a></p></td>
<td><p>Simulate quantize and dequantize in training time.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize"/><p id="torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize"/><a class="reference internal" href="generated/torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize.html#torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize" title="torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize"><code class="xref py py-obj docutils literal notranslate"><span class="pre">FusedMovingAvgObsFakeQuantize</span></code></a></p></td>
<td><p>Define a fused module to observe the tensor.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.ao.quantization.fake_quantize.default_fake_quant"/><p id="torch.ao.quantization.fake_quantize.default_fake_quant"/><a class="reference internal" href="generated/torch.ao.quantization.fake_quantize.default_fake_quant.html#torch.ao.quantization.fake_quantize.default_fake_quant" title="torch.ao.quantization.fake_quantize.default_fake_quant"><code class="xref py py-obj docutils literal notranslate"><span class="pre">default_fake_quant</span></code></a></p></td>
<td><p>Default fake_quant for activations.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.ao.quantization.fake_quantize.default_weight_fake_quant"/><p id="torch.ao.quantization.fake_quantize.default_weight_fake_quant"/><a class="reference internal" href="generated/torch.ao.quantization.fake_quantize.default_weight_fake_quant.html#torch.ao.quantization.fake_quantize.default_weight_fake_quant" title="torch.ao.quantization.fake_quantize.default_weight_fake_quant"><code class="xref py py-obj docutils literal notranslate"><span class="pre">default_weight_fake_quant</span></code></a></p></td>
<td><p>Default fake_quant for weights.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.ao.quantization.fake_quantize.default_per_channel_weight_fake_quant"/><p id="torch.ao.quantization.fake_quantize.default_per_channel_weight_fake_quant"/><a class="reference internal" href="generated/torch.ao.quantization.fake_quantize.default_per_channel_weight_fake_quant.html#torch.ao.quantization.fake_quantize.default_per_channel_weight_fake_quant" title="torch.ao.quantization.fake_quantize.default_per_channel_weight_fake_quant"><code class="xref py py-obj docutils literal notranslate"><span class="pre">default_per_channel_weight_fake_quant</span></code></a></p></td>
<td><p>Default fake_quant for per-channel weights.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.ao.quantization.fake_quantize.default_histogram_fake_quant"/><p id="torch.ao.quantization.fake_quantize.default_histogram_fake_quant"/><a class="reference internal" href="generated/torch.ao.quantization.fake_quantize.default_histogram_fake_quant.html#torch.ao.quantization.fake_quantize.default_histogram_fake_quant" title="torch.ao.quantization.fake_quantize.default_histogram_fake_quant"><code class="xref py py-obj docutils literal notranslate"><span class="pre">default_histogram_fake_quant</span></code></a></p></td>
<td><p>Fake_quant for activations using a histogram..</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.ao.quantization.fake_quantize.default_fused_act_fake_quant"/><p id="torch.ao.quantization.fake_quantize.default_fused_act_fake_quant"/><a class="reference internal" href="generated/torch.ao.quantization.fake_quantize.default_fused_act_fake_quant.html#torch.ao.quantization.fake_quantize.default_fused_act_fake_quant" title="torch.ao.quantization.fake_quantize.default_fused_act_fake_quant"><code class="xref py py-obj docutils literal notranslate"><span class="pre">default_fused_act_fake_quant</span></code></a></p></td>
<td><p>Fused version of <cite>default_fake_quant</cite>, with improved performance.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.ao.quantization.fake_quantize.default_fused_wt_fake_quant"/><p id="torch.ao.quantization.fake_quantize.default_fused_wt_fake_quant"/><a class="reference internal" href="generated/torch.ao.quantization.fake_quantize.default_fused_wt_fake_quant.html#torch.ao.quantization.fake_quantize.default_fused_wt_fake_quant" title="torch.ao.quantization.fake_quantize.default_fused_wt_fake_quant"><code class="xref py py-obj docutils literal notranslate"><span class="pre">default_fused_wt_fake_quant</span></code></a></p></td>
<td><p>Fused version of <cite>default_weight_fake_quant</cite>, with improved performance.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.ao.quantization.fake_quantize.default_fused_per_channel_wt_fake_quant"/><p id="torch.ao.quantization.fake_quantize.default_fused_per_channel_wt_fake_quant"/><a class="reference internal" href="generated/torch.ao.quantization.fake_quantize.default_fused_per_channel_wt_fake_quant.html#torch.ao.quantization.fake_quantize.default_fused_per_channel_wt_fake_quant" title="torch.ao.quantization.fake_quantize.default_fused_per_channel_wt_fake_quant"><code class="xref py py-obj docutils literal notranslate"><span class="pre">default_fused_per_channel_wt_fake_quant</span></code></a></p></td>
<td><p>Fused version of <cite>default_per_channel_weight_fake_quant</cite>, with improved performance.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.ao.quantization.fake_quantize.disable_fake_quant"/><p id="torch.ao.quantization.fake_quantize.disable_fake_quant"/><a class="reference internal" href="generated/torch.ao.quantization.fake_quantize.disable_fake_quant.html#torch.ao.quantization.fake_quantize.disable_fake_quant" title="torch.ao.quantization.fake_quantize.disable_fake_quant"><code class="xref py py-obj docutils literal notranslate"><span class="pre">disable_fake_quant</span></code></a></p></td>
<td><p>Disable fake quantization for the module.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.ao.quantization.fake_quantize.enable_fake_quant"/><p id="torch.ao.quantization.fake_quantize.enable_fake_quant"/><a class="reference internal" href="generated/torch.ao.quantization.fake_quantize.enable_fake_quant.html#torch.ao.quantization.fake_quantize.enable_fake_quant" title="torch.ao.quantization.fake_quantize.enable_fake_quant"><code class="xref py py-obj docutils literal notranslate"><span class="pre">enable_fake_quant</span></code></a></p></td>
<td><p>Enable fake quantization for the module.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.ao.quantization.fake_quantize.disable_observer"/><p id="torch.ao.quantization.fake_quantize.disable_observer"/><a class="reference internal" href="generated/torch.ao.quantization.fake_quantize.disable_observer.html#torch.ao.quantization.fake_quantize.disable_observer" title="torch.ao.quantization.fake_quantize.disable_observer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">disable_observer</span></code></a></p></td>
<td><p>Disable observation for this module.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.ao.quantization.fake_quantize.enable_observer"/><p id="torch.ao.quantization.fake_quantize.enable_observer"/><a class="reference internal" href="generated/torch.ao.quantization.fake_quantize.enable_observer.html#torch.ao.quantization.fake_quantize.enable_observer" title="torch.ao.quantization.fake_quantize.enable_observer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">enable_observer</span></code></a></p></td>
<td><p>Enable observation for this module.</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="torch-ao-quantization-qconfig">
<h2>torch.ao.quantization.qconfig<a class="headerlink" href="#torch-ao-quantization-qconfig" title="Link to this heading">#</a></h2>
<p>This module defines <cite>QConfig</cite> objects which are used
to configure quantization settings for individual ops.</p>
<div class="pst-scrollable-table-container"><table class="autosummary longtable table autosummary">
<tbody>
<tr class="row-odd"><td><p><p id="torch.ao.quantization.qconfig.QConfig"/><p id="torch.ao.quantization.qconfig.QConfig"/><a class="reference internal" href="generated/torch.ao.quantization.qconfig.QConfig.html#torch.ao.quantization.qconfig.QConfig" title="torch.ao.quantization.qconfig.QConfig"><code class="xref py py-obj docutils literal notranslate"><span class="pre">QConfig</span></code></a></p></td>
<td><p>Describes how to quantize a layer or a part of the network by providing settings (observer classes) for activations and weights respectively.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.ao.quantization.qconfig.default_qconfig"/><p id="torch.ao.quantization.qconfig.default_qconfig"/><a class="reference internal" href="generated/torch.ao.quantization.qconfig.default_qconfig.html#torch.ao.quantization.qconfig.default_qconfig" title="torch.ao.quantization.qconfig.default_qconfig"><code class="xref py py-obj docutils literal notranslate"><span class="pre">default_qconfig</span></code></a></p></td>
<td><p>Default qconfig configuration.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.ao.quantization.qconfig.default_debug_qconfig"/><p id="torch.ao.quantization.qconfig.default_debug_qconfig"/><a class="reference internal" href="generated/torch.ao.quantization.qconfig.default_debug_qconfig.html#torch.ao.quantization.qconfig.default_debug_qconfig" title="torch.ao.quantization.qconfig.default_debug_qconfig"><code class="xref py py-obj docutils literal notranslate"><span class="pre">default_debug_qconfig</span></code></a></p></td>
<td><p>Default qconfig configuration for debugging.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.ao.quantization.qconfig.default_per_channel_qconfig"/><p id="torch.ao.quantization.qconfig.default_per_channel_qconfig"/><a class="reference internal" href="generated/torch.ao.quantization.qconfig.default_per_channel_qconfig.html#torch.ao.quantization.qconfig.default_per_channel_qconfig" title="torch.ao.quantization.qconfig.default_per_channel_qconfig"><code class="xref py py-obj docutils literal notranslate"><span class="pre">default_per_channel_qconfig</span></code></a></p></td>
<td><p>Default qconfig configuration for per channel weight quantization.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.ao.quantization.qconfig.default_dynamic_qconfig"/><p id="torch.ao.quantization.qconfig.default_dynamic_qconfig"/><a class="reference internal" href="generated/torch.ao.quantization.qconfig.default_dynamic_qconfig.html#torch.ao.quantization.qconfig.default_dynamic_qconfig" title="torch.ao.quantization.qconfig.default_dynamic_qconfig"><code class="xref py py-obj docutils literal notranslate"><span class="pre">default_dynamic_qconfig</span></code></a></p></td>
<td><p>Default dynamic qconfig.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.ao.quantization.qconfig.float16_dynamic_qconfig"/><p id="torch.ao.quantization.qconfig.float16_dynamic_qconfig"/><a class="reference internal" href="generated/torch.ao.quantization.qconfig.float16_dynamic_qconfig.html#torch.ao.quantization.qconfig.float16_dynamic_qconfig" title="torch.ao.quantization.qconfig.float16_dynamic_qconfig"><code class="xref py py-obj docutils literal notranslate"><span class="pre">float16_dynamic_qconfig</span></code></a></p></td>
<td><p>Dynamic qconfig with weights quantized to <cite>torch.float16</cite>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.ao.quantization.qconfig.float16_static_qconfig"/><p id="torch.ao.quantization.qconfig.float16_static_qconfig"/><a class="reference internal" href="generated/torch.ao.quantization.qconfig.float16_static_qconfig.html#torch.ao.quantization.qconfig.float16_static_qconfig" title="torch.ao.quantization.qconfig.float16_static_qconfig"><code class="xref py py-obj docutils literal notranslate"><span class="pre">float16_static_qconfig</span></code></a></p></td>
<td><p>Dynamic qconfig with both activations and weights quantized to <cite>torch.float16</cite>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.ao.quantization.qconfig.per_channel_dynamic_qconfig"/><p id="torch.ao.quantization.qconfig.per_channel_dynamic_qconfig"/><a class="reference internal" href="generated/torch.ao.quantization.qconfig.per_channel_dynamic_qconfig.html#torch.ao.quantization.qconfig.per_channel_dynamic_qconfig" title="torch.ao.quantization.qconfig.per_channel_dynamic_qconfig"><code class="xref py py-obj docutils literal notranslate"><span class="pre">per_channel_dynamic_qconfig</span></code></a></p></td>
<td><p>Dynamic qconfig with weights quantized per channel.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.ao.quantization.qconfig.float_qparams_weight_only_qconfig"/><p id="torch.ao.quantization.qconfig.float_qparams_weight_only_qconfig"/><a class="reference internal" href="generated/torch.ao.quantization.qconfig.float_qparams_weight_only_qconfig.html#torch.ao.quantization.qconfig.float_qparams_weight_only_qconfig" title="torch.ao.quantization.qconfig.float_qparams_weight_only_qconfig"><code class="xref py py-obj docutils literal notranslate"><span class="pre">float_qparams_weight_only_qconfig</span></code></a></p></td>
<td><p>Dynamic qconfig with weights quantized with a floating point zero_point.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.ao.quantization.qconfig.default_qat_qconfig"/><p id="torch.ao.quantization.qconfig.default_qat_qconfig"/><a class="reference internal" href="generated/torch.ao.quantization.qconfig.default_qat_qconfig.html#torch.ao.quantization.qconfig.default_qat_qconfig" title="torch.ao.quantization.qconfig.default_qat_qconfig"><code class="xref py py-obj docutils literal notranslate"><span class="pre">default_qat_qconfig</span></code></a></p></td>
<td><p>Default qconfig for QAT.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.ao.quantization.qconfig.default_weight_only_qconfig"/><p id="torch.ao.quantization.qconfig.default_weight_only_qconfig"/><a class="reference internal" href="generated/torch.ao.quantization.qconfig.default_weight_only_qconfig.html#torch.ao.quantization.qconfig.default_weight_only_qconfig" title="torch.ao.quantization.qconfig.default_weight_only_qconfig"><code class="xref py py-obj docutils literal notranslate"><span class="pre">default_weight_only_qconfig</span></code></a></p></td>
<td><p>Default qconfig for quantizing weights only.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.ao.quantization.qconfig.default_activation_only_qconfig"/><p id="torch.ao.quantization.qconfig.default_activation_only_qconfig"/><a class="reference internal" href="generated/torch.ao.quantization.qconfig.default_activation_only_qconfig.html#torch.ao.quantization.qconfig.default_activation_only_qconfig" title="torch.ao.quantization.qconfig.default_activation_only_qconfig"><code class="xref py py-obj docutils literal notranslate"><span class="pre">default_activation_only_qconfig</span></code></a></p></td>
<td><p>Default qconfig for quantizing activations only.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.ao.quantization.qconfig.default_qat_qconfig_v2"/><p id="torch.ao.quantization.qconfig.default_qat_qconfig_v2"/><a class="reference internal" href="generated/torch.ao.quantization.qconfig.default_qat_qconfig_v2.html#torch.ao.quantization.qconfig.default_qat_qconfig_v2" title="torch.ao.quantization.qconfig.default_qat_qconfig_v2"><code class="xref py py-obj docutils literal notranslate"><span class="pre">default_qat_qconfig_v2</span></code></a></p></td>
<td><p>Fused version of <cite>default_qat_config</cite>, has performance benefits.</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="module-torch.ao.nn.intrinsic">
<span id="torch-ao-nn-intrinsic"></span><h2>torch.ao.nn.intrinsic<a class="headerlink" href="#module-torch.ao.nn.intrinsic" title="Link to this heading">#</a></h2>
<p id="module-torch.ao.nn.intrinsic.modules">This module implements the combined (fused) modules conv + relu which can
then be quantized.</p>
<div class="pst-scrollable-table-container"><table class="autosummary longtable table autosummary">
<tbody>
<tr class="row-odd"><td><p><p id="torch.ao.nn.intrinsic.ConvReLU1d"/><p id="torch.ao.nn.intrinsic.ConvReLU1d"/><a class="reference internal" href="generated/torch.ao.nn.intrinsic.ConvReLU1d.html#torch.ao.nn.intrinsic.ConvReLU1d" title="torch.ao.nn.intrinsic.ConvReLU1d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ConvReLU1d</span></code></a></p></td>
<td><p>This is a sequential container which calls the Conv1d and ReLU modules.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.ao.nn.intrinsic.ConvReLU2d"/><p id="torch.ao.nn.intrinsic.ConvReLU2d"/><a class="reference internal" href="generated/torch.ao.nn.intrinsic.ConvReLU2d.html#torch.ao.nn.intrinsic.ConvReLU2d" title="torch.ao.nn.intrinsic.ConvReLU2d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ConvReLU2d</span></code></a></p></td>
<td><p>This is a sequential container which calls the Conv2d and ReLU modules.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.ao.nn.intrinsic.ConvReLU3d"/><p id="torch.ao.nn.intrinsic.ConvReLU3d"/><a class="reference internal" href="generated/torch.ao.nn.intrinsic.ConvReLU3d.html#torch.ao.nn.intrinsic.ConvReLU3d" title="torch.ao.nn.intrinsic.ConvReLU3d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ConvReLU3d</span></code></a></p></td>
<td><p>This is a sequential container which calls the Conv3d and ReLU modules.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.ao.nn.intrinsic.LinearReLU"/><p id="torch.ao.nn.intrinsic.LinearReLU"/><a class="reference internal" href="generated/torch.ao.nn.intrinsic.LinearReLU.html#torch.ao.nn.intrinsic.LinearReLU" title="torch.ao.nn.intrinsic.LinearReLU"><code class="xref py py-obj docutils literal notranslate"><span class="pre">LinearReLU</span></code></a></p></td>
<td><p>This is a sequential container which calls the Linear and ReLU modules.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.ao.nn.intrinsic.ConvBn1d"/><p id="torch.ao.nn.intrinsic.ConvBn1d"/><a class="reference internal" href="generated/torch.ao.nn.intrinsic.ConvBn1d.html#torch.ao.nn.intrinsic.ConvBn1d" title="torch.ao.nn.intrinsic.ConvBn1d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ConvBn1d</span></code></a></p></td>
<td><p>This is a sequential container which calls the Conv 1d and Batch Norm 1d modules.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.ao.nn.intrinsic.ConvBn2d"/><p id="torch.ao.nn.intrinsic.ConvBn2d"/><a class="reference internal" href="generated/torch.ao.nn.intrinsic.ConvBn2d.html#torch.ao.nn.intrinsic.ConvBn2d" title="torch.ao.nn.intrinsic.ConvBn2d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ConvBn2d</span></code></a></p></td>
<td><p>This is a sequential container which calls the Conv 2d and Batch Norm 2d modules.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.ao.nn.intrinsic.ConvBn3d"/><p id="torch.ao.nn.intrinsic.ConvBn3d"/><a class="reference internal" href="generated/torch.ao.nn.intrinsic.ConvBn3d.html#torch.ao.nn.intrinsic.ConvBn3d" title="torch.ao.nn.intrinsic.ConvBn3d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ConvBn3d</span></code></a></p></td>
<td><p>This is a sequential container which calls the Conv 3d and Batch Norm 3d modules.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.ao.nn.intrinsic.ConvBnReLU1d"/><p id="torch.ao.nn.intrinsic.ConvBnReLU1d"/><a class="reference internal" href="generated/torch.ao.nn.intrinsic.ConvBnReLU1d.html#torch.ao.nn.intrinsic.ConvBnReLU1d" title="torch.ao.nn.intrinsic.ConvBnReLU1d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ConvBnReLU1d</span></code></a></p></td>
<td><p>This is a sequential container which calls the Conv 1d, Batch Norm 1d, and ReLU modules.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.ao.nn.intrinsic.ConvBnReLU2d"/><p id="torch.ao.nn.intrinsic.ConvBnReLU2d"/><a class="reference internal" href="generated/torch.ao.nn.intrinsic.ConvBnReLU2d.html#torch.ao.nn.intrinsic.ConvBnReLU2d" title="torch.ao.nn.intrinsic.ConvBnReLU2d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ConvBnReLU2d</span></code></a></p></td>
<td><p>This is a sequential container which calls the Conv 2d, Batch Norm 2d, and ReLU modules.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.ao.nn.intrinsic.ConvBnReLU3d"/><p id="torch.ao.nn.intrinsic.ConvBnReLU3d"/><a class="reference internal" href="generated/torch.ao.nn.intrinsic.ConvBnReLU3d.html#torch.ao.nn.intrinsic.ConvBnReLU3d" title="torch.ao.nn.intrinsic.ConvBnReLU3d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ConvBnReLU3d</span></code></a></p></td>
<td><p>This is a sequential container which calls the Conv 3d, Batch Norm 3d, and ReLU modules.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.ao.nn.intrinsic.BNReLU2d"/><p id="torch.ao.nn.intrinsic.BNReLU2d"/><a class="reference internal" href="generated/torch.ao.nn.intrinsic.BNReLU2d.html#torch.ao.nn.intrinsic.BNReLU2d" title="torch.ao.nn.intrinsic.BNReLU2d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">BNReLU2d</span></code></a></p></td>
<td><p>This is a sequential container which calls the BatchNorm 2d and ReLU modules.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.ao.nn.intrinsic.BNReLU3d"/><p id="torch.ao.nn.intrinsic.BNReLU3d"/><a class="reference internal" href="generated/torch.ao.nn.intrinsic.BNReLU3d.html#torch.ao.nn.intrinsic.BNReLU3d" title="torch.ao.nn.intrinsic.BNReLU3d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">BNReLU3d</span></code></a></p></td>
<td><p>This is a sequential container which calls the BatchNorm 3d and ReLU modules.</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="module-torch.ao.nn.intrinsic.qat">
<span id="torch-ao-nn-intrinsic-qat"></span><h2>torch.ao.nn.intrinsic.qat<a class="headerlink" href="#module-torch.ao.nn.intrinsic.qat" title="Link to this heading">#</a></h2>
<p id="module-torch.ao.nn.intrinsic.qat.modules">This module implements the versions of those fused operations needed for
quantization aware training.</p>
<div class="pst-scrollable-table-container"><table class="autosummary longtable table autosummary">
<tbody>
<tr class="row-odd"><td><p><p id="torch.ao.nn.intrinsic.qat.LinearReLU"/><p id="torch.ao.nn.intrinsic.qat.LinearReLU"/><a class="reference internal" href="generated/torch.ao.nn.intrinsic.qat.LinearReLU.html#torch.ao.nn.intrinsic.qat.LinearReLU" title="torch.ao.nn.intrinsic.qat.LinearReLU"><code class="xref py py-obj docutils literal notranslate"><span class="pre">LinearReLU</span></code></a></p></td>
<td><p>A LinearReLU module fused from Linear and ReLU modules, attached with FakeQuantize modules for weight, used in quantization aware training.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.ao.nn.intrinsic.qat.ConvBn1d"/><p id="torch.ao.nn.intrinsic.qat.ConvBn1d"/><a class="reference internal" href="generated/torch.ao.nn.intrinsic.qat.ConvBn1d.html#torch.ao.nn.intrinsic.qat.ConvBn1d" title="torch.ao.nn.intrinsic.qat.ConvBn1d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ConvBn1d</span></code></a></p></td>
<td><p>A ConvBn1d module is a module fused from Conv1d and BatchNorm1d, attached with FakeQuantize modules for weight, used in quantization aware training.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.ao.nn.intrinsic.qat.ConvBnReLU1d"/><p id="torch.ao.nn.intrinsic.qat.ConvBnReLU1d"/><a class="reference internal" href="generated/torch.ao.nn.intrinsic.qat.ConvBnReLU1d.html#torch.ao.nn.intrinsic.qat.ConvBnReLU1d" title="torch.ao.nn.intrinsic.qat.ConvBnReLU1d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ConvBnReLU1d</span></code></a></p></td>
<td><p>A ConvBnReLU1d module is a module fused from Conv1d, BatchNorm1d and ReLU, attached with FakeQuantize modules for weight, used in quantization aware training.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.ao.nn.intrinsic.qat.ConvBn2d"/><p id="torch.ao.nn.intrinsic.qat.ConvBn2d"/><a class="reference internal" href="generated/torch.ao.nn.intrinsic.qat.ConvBn2d.html#torch.ao.nn.intrinsic.qat.ConvBn2d" title="torch.ao.nn.intrinsic.qat.ConvBn2d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ConvBn2d</span></code></a></p></td>
<td><p>A ConvBn2d module is a module fused from Conv2d and BatchNorm2d, attached with FakeQuantize modules for weight, used in quantization aware training.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.ao.nn.intrinsic.qat.ConvBnReLU2d"/><p id="torch.ao.nn.intrinsic.qat.ConvBnReLU2d"/><a class="reference internal" href="generated/torch.ao.nn.intrinsic.qat.ConvBnReLU2d.html#torch.ao.nn.intrinsic.qat.ConvBnReLU2d" title="torch.ao.nn.intrinsic.qat.ConvBnReLU2d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ConvBnReLU2d</span></code></a></p></td>
<td><p>A ConvBnReLU2d module is a module fused from Conv2d, BatchNorm2d and ReLU, attached with FakeQuantize modules for weight, used in quantization aware training.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.ao.nn.intrinsic.qat.ConvReLU2d"/><p id="torch.ao.nn.intrinsic.qat.ConvReLU2d"/><a class="reference internal" href="generated/torch.ao.nn.intrinsic.qat.ConvReLU2d.html#torch.ao.nn.intrinsic.qat.ConvReLU2d" title="torch.ao.nn.intrinsic.qat.ConvReLU2d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ConvReLU2d</span></code></a></p></td>
<td><p>A ConvReLU2d module is a fused module of Conv2d and ReLU, attached with FakeQuantize modules for weight for quantization aware training.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.ao.nn.intrinsic.qat.ConvBn3d"/><p id="torch.ao.nn.intrinsic.qat.ConvBn3d"/><a class="reference internal" href="generated/torch.ao.nn.intrinsic.qat.ConvBn3d.html#torch.ao.nn.intrinsic.qat.ConvBn3d" title="torch.ao.nn.intrinsic.qat.ConvBn3d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ConvBn3d</span></code></a></p></td>
<td><p>A ConvBn3d module is a module fused from Conv3d and BatchNorm3d, attached with FakeQuantize modules for weight, used in quantization aware training.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.ao.nn.intrinsic.qat.ConvBnReLU3d"/><p id="torch.ao.nn.intrinsic.qat.ConvBnReLU3d"/><a class="reference internal" href="generated/torch.ao.nn.intrinsic.qat.ConvBnReLU3d.html#torch.ao.nn.intrinsic.qat.ConvBnReLU3d" title="torch.ao.nn.intrinsic.qat.ConvBnReLU3d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ConvBnReLU3d</span></code></a></p></td>
<td><p>A ConvBnReLU3d module is a module fused from Conv3d, BatchNorm3d and ReLU, attached with FakeQuantize modules for weight, used in quantization aware training.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.ao.nn.intrinsic.qat.ConvReLU3d"/><p id="torch.ao.nn.intrinsic.qat.ConvReLU3d"/><a class="reference internal" href="generated/torch.ao.nn.intrinsic.qat.ConvReLU3d.html#torch.ao.nn.intrinsic.qat.ConvReLU3d" title="torch.ao.nn.intrinsic.qat.ConvReLU3d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ConvReLU3d</span></code></a></p></td>
<td><p>A ConvReLU3d module is a fused module of Conv3d and ReLU, attached with FakeQuantize modules for weight for quantization aware training.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.ao.nn.intrinsic.qat.update_bn_stats"/><p id="torch.ao.nn.intrinsic.qat.update_bn_stats"/><a class="reference internal" href="generated/torch.ao.nn.intrinsic.qat.update_bn_stats.html#torch.ao.nn.intrinsic.qat.update_bn_stats" title="torch.ao.nn.intrinsic.qat.update_bn_stats"><code class="xref py py-obj docutils literal notranslate"><span class="pre">update_bn_stats</span></code></a></p></td>
<td><p></p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.ao.nn.intrinsic.qat.freeze_bn_stats"/><p id="torch.ao.nn.intrinsic.qat.freeze_bn_stats"/><a class="reference internal" href="generated/torch.ao.nn.intrinsic.qat.freeze_bn_stats.html#torch.ao.nn.intrinsic.qat.freeze_bn_stats" title="torch.ao.nn.intrinsic.qat.freeze_bn_stats"><code class="xref py py-obj docutils literal notranslate"><span class="pre">freeze_bn_stats</span></code></a></p></td>
<td><p></p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="module-torch.ao.nn.intrinsic.quantized">
<span id="torch-ao-nn-intrinsic-quantized"></span><h2>torch.ao.nn.intrinsic.quantized<a class="headerlink" href="#module-torch.ao.nn.intrinsic.quantized" title="Link to this heading">#</a></h2>
<p id="module-torch.ao.nn.intrinsic.quantized.modules">This module implements the quantized implementations of fused operations
like conv + relu. No BatchNorm variants as it’s usually folded into convolution
for inference.</p>
<div class="pst-scrollable-table-container"><table class="autosummary longtable table autosummary">
<tbody>
<tr class="row-odd"><td><p><p id="torch.ao.nn.intrinsic.quantized.BNReLU2d"/><p id="torch.ao.nn.intrinsic.quantized.BNReLU2d"/><a class="reference internal" href="generated/torch.ao.nn.intrinsic.quantized.BNReLU2d.html#torch.ao.nn.intrinsic.quantized.BNReLU2d" title="torch.ao.nn.intrinsic.quantized.BNReLU2d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">BNReLU2d</span></code></a></p></td>
<td><p>A BNReLU2d module is a fused module of BatchNorm2d and ReLU</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.ao.nn.intrinsic.quantized.BNReLU3d"/><p id="torch.ao.nn.intrinsic.quantized.BNReLU3d"/><a class="reference internal" href="generated/torch.ao.nn.intrinsic.quantized.BNReLU3d.html#torch.ao.nn.intrinsic.quantized.BNReLU3d" title="torch.ao.nn.intrinsic.quantized.BNReLU3d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">BNReLU3d</span></code></a></p></td>
<td><p>A BNReLU3d module is a fused module of BatchNorm3d and ReLU</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.ao.nn.intrinsic.quantized.ConvReLU1d"/><p id="torch.ao.nn.intrinsic.quantized.ConvReLU1d"/><a class="reference internal" href="generated/torch.ao.nn.intrinsic.quantized.ConvReLU1d.html#torch.ao.nn.intrinsic.quantized.ConvReLU1d" title="torch.ao.nn.intrinsic.quantized.ConvReLU1d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ConvReLU1d</span></code></a></p></td>
<td><p>A ConvReLU1d module is a fused module of Conv1d and ReLU</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.ao.nn.intrinsic.quantized.ConvReLU2d"/><p id="torch.ao.nn.intrinsic.quantized.ConvReLU2d"/><a class="reference internal" href="generated/torch.ao.nn.intrinsic.quantized.ConvReLU2d.html#torch.ao.nn.intrinsic.quantized.ConvReLU2d" title="torch.ao.nn.intrinsic.quantized.ConvReLU2d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ConvReLU2d</span></code></a></p></td>
<td><p>A ConvReLU2d module is a fused module of Conv2d and ReLU</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.ao.nn.intrinsic.quantized.ConvReLU3d"/><p id="torch.ao.nn.intrinsic.quantized.ConvReLU3d"/><a class="reference internal" href="generated/torch.ao.nn.intrinsic.quantized.ConvReLU3d.html#torch.ao.nn.intrinsic.quantized.ConvReLU3d" title="torch.ao.nn.intrinsic.quantized.ConvReLU3d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ConvReLU3d</span></code></a></p></td>
<td><p>A ConvReLU3d module is a fused module of Conv3d and ReLU</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.ao.nn.intrinsic.quantized.LinearReLU"/><p id="torch.ao.nn.intrinsic.quantized.LinearReLU"/><a class="reference internal" href="generated/torch.ao.nn.intrinsic.quantized.LinearReLU.html#torch.ao.nn.intrinsic.quantized.LinearReLU" title="torch.ao.nn.intrinsic.quantized.LinearReLU"><code class="xref py py-obj docutils literal notranslate"><span class="pre">LinearReLU</span></code></a></p></td>
<td><p>A LinearReLU module fused from Linear and ReLU modules</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="module-torch.ao.nn.intrinsic.quantized.dynamic">
<span id="torch-ao-nn-intrinsic-quantized-dynamic"></span><h2>torch.ao.nn.intrinsic.quantized.dynamic<a class="headerlink" href="#module-torch.ao.nn.intrinsic.quantized.dynamic" title="Link to this heading">#</a></h2>
<p id="module-torch.ao.nn.intrinsic.quantized.dynamic.modules">This module implements the quantized dynamic implementations of fused operations
like linear + relu.</p>
<div class="pst-scrollable-table-container"><table class="autosummary longtable table autosummary">
<tbody>
<tr class="row-odd"><td><p><p id="torch.ao.nn.intrinsic.quantized.dynamic.LinearReLU"/><p id="torch.ao.nn.intrinsic.quantized.dynamic.LinearReLU"/><a class="reference internal" href="generated/torch.ao.nn.intrinsic.quantized.dynamic.LinearReLU.html#torch.ao.nn.intrinsic.quantized.dynamic.LinearReLU" title="torch.ao.nn.intrinsic.quantized.dynamic.LinearReLU"><code class="xref py py-obj docutils literal notranslate"><span class="pre">LinearReLU</span></code></a></p></td>
<td><p>A LinearReLU module fused from Linear and ReLU modules that can be used for dynamic quantization.</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="module-torch.ao.nn.qat">
<span id="torch-ao-nn-qat"></span><h2>torch.ao.nn.qat<a class="headerlink" href="#module-torch.ao.nn.qat" title="Link to this heading">#</a></h2>
<p id="module-torch.ao.nn.qat.modules">This module implements versions of the key nn modules <strong>Conv2d()</strong> and
<strong>Linear()</strong> which run in FP32 but with rounding applied to simulate the
effect of INT8 quantization.</p>
<div class="pst-scrollable-table-container"><table class="autosummary longtable table autosummary">
<tbody>
<tr class="row-odd"><td><p><p id="torch.ao.nn.qat.Conv2d"/><p id="torch.ao.nn.qat.Conv2d"/><a class="reference internal" href="generated/torch.ao.nn.qat.Conv2d.html#torch.ao.nn.qat.Conv2d" title="torch.ao.nn.qat.Conv2d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Conv2d</span></code></a></p></td>
<td><p>A Conv2d module attached with FakeQuantize modules for weight, used for quantization aware training.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.ao.nn.qat.Conv3d"/><p id="torch.ao.nn.qat.Conv3d"/><a class="reference internal" href="generated/torch.ao.nn.qat.Conv3d.html#torch.ao.nn.qat.Conv3d" title="torch.ao.nn.qat.Conv3d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Conv3d</span></code></a></p></td>
<td><p>A Conv3d module attached with FakeQuantize modules for weight, used for quantization aware training.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.ao.nn.qat.Linear"/><p id="torch.ao.nn.qat.Linear"/><a class="reference internal" href="generated/torch.ao.nn.qat.Linear.html#torch.ao.nn.qat.Linear" title="torch.ao.nn.qat.Linear"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Linear</span></code></a></p></td>
<td><p>A linear module attached with FakeQuantize modules for weight, used for quantization aware training.</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="module-torch.ao.nn.qat.dynamic">
<span id="torch-ao-nn-qat-dynamic"></span><h2>torch.ao.nn.qat.dynamic<a class="headerlink" href="#module-torch.ao.nn.qat.dynamic" title="Link to this heading">#</a></h2>
<p id="module-torch.ao.nn.qat.dynamic.modules">This module implements versions of the key nn modules such as <strong>Linear()</strong>
which run in FP32 but with rounding applied to simulate the effect of INT8
quantization and will be dynamically quantized during inference.</p>
<div class="pst-scrollable-table-container"><table class="autosummary longtable table autosummary">
<tbody>
<tr class="row-odd"><td><p><p id="torch.ao.nn.qat.dynamic.Linear"/><p id="torch.ao.nn.qat.dynamic.Linear"/><a class="reference internal" href="generated/torch.ao.nn.qat.dynamic.Linear.html#torch.ao.nn.qat.dynamic.Linear" title="torch.ao.nn.qat.dynamic.Linear"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Linear</span></code></a></p></td>
<td><p>A linear module attached with FakeQuantize modules for weight, used for dynamic quantization aware training.</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="module-torch.ao.nn.quantized.modules">
<span id="torch-ao-nn-quantized"></span><h2>torch.ao.nn.quantized<a class="headerlink" href="#module-torch.ao.nn.quantized.modules" title="Link to this heading">#</a></h2>
<p>This module implements the quantized versions of the nn layers such as
~`torch.nn.Conv2d` and <cite>torch.nn.ReLU</cite>.</p>
<div class="pst-scrollable-table-container"><table class="autosummary longtable table autosummary">
<tbody>
<tr class="row-odd"><td><p><p id="torch.ao.nn.quantized.ReLU6"/><p id="torch.ao.nn.quantized.ReLU6"/><a class="reference internal" href="generated/torch.ao.nn.quantized.ReLU6.html#torch.ao.nn.quantized.ReLU6" title="torch.ao.nn.quantized.ReLU6"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ReLU6</span></code></a></p></td>
<td><p>Applies the element-wise function:</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.ao.nn.quantized.Hardswish"/><p id="torch.ao.nn.quantized.Hardswish"/><a class="reference internal" href="generated/torch.ao.nn.quantized.Hardswish.html#torch.ao.nn.quantized.Hardswish" title="torch.ao.nn.quantized.Hardswish"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Hardswish</span></code></a></p></td>
<td><p>This is the quantized version of <a class="reference internal" href="generated/torch.nn.Hardswish.html#torch.nn.Hardswish" title="torch.nn.Hardswish"><code class="xref py py-class docutils literal notranslate"><span class="pre">Hardswish</span></code></a>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.ao.nn.quantized.ELU"/><p id="torch.ao.nn.quantized.ELU"/><a class="reference internal" href="generated/torch.ao.nn.quantized.ELU.html#torch.ao.nn.quantized.ELU" title="torch.ao.nn.quantized.ELU"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ELU</span></code></a></p></td>
<td><p>This is the quantized equivalent of <a class="reference internal" href="generated/torch.nn.ELU.html#torch.nn.ELU" title="torch.nn.ELU"><code class="xref py py-class docutils literal notranslate"><span class="pre">ELU</span></code></a>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.ao.nn.quantized.LeakyReLU"/><p id="torch.ao.nn.quantized.LeakyReLU"/><a class="reference internal" href="generated/torch.ao.nn.quantized.LeakyReLU.html#torch.ao.nn.quantized.LeakyReLU" title="torch.ao.nn.quantized.LeakyReLU"><code class="xref py py-obj docutils literal notranslate"><span class="pre">LeakyReLU</span></code></a></p></td>
<td><p>This is the quantized equivalent of <a class="reference internal" href="generated/torch.nn.LeakyReLU.html#torch.nn.LeakyReLU" title="torch.nn.LeakyReLU"><code class="xref py py-class docutils literal notranslate"><span class="pre">LeakyReLU</span></code></a>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.ao.nn.quantized.Sigmoid"/><p id="torch.ao.nn.quantized.Sigmoid"/><a class="reference internal" href="generated/torch.ao.nn.quantized.Sigmoid.html#torch.ao.nn.quantized.Sigmoid" title="torch.ao.nn.quantized.Sigmoid"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Sigmoid</span></code></a></p></td>
<td><p>This is the quantized equivalent of <a class="reference internal" href="generated/torch.nn.Sigmoid.html#torch.nn.Sigmoid" title="torch.nn.Sigmoid"><code class="xref py py-class docutils literal notranslate"><span class="pre">Sigmoid</span></code></a>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.ao.nn.quantized.BatchNorm2d"/><p id="torch.ao.nn.quantized.BatchNorm2d"/><a class="reference internal" href="generated/torch.ao.nn.quantized.BatchNorm2d.html#torch.ao.nn.quantized.BatchNorm2d" title="torch.ao.nn.quantized.BatchNorm2d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">BatchNorm2d</span></code></a></p></td>
<td><p>This is the quantized version of <a class="reference internal" href="generated/torch.nn.BatchNorm2d.html#torch.nn.BatchNorm2d" title="torch.nn.BatchNorm2d"><code class="xref py py-class docutils literal notranslate"><span class="pre">BatchNorm2d</span></code></a>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.ao.nn.quantized.BatchNorm3d"/><p id="torch.ao.nn.quantized.BatchNorm3d"/><a class="reference internal" href="generated/torch.ao.nn.quantized.BatchNorm3d.html#torch.ao.nn.quantized.BatchNorm3d" title="torch.ao.nn.quantized.BatchNorm3d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">BatchNorm3d</span></code></a></p></td>
<td><p>This is the quantized version of <a class="reference internal" href="generated/torch.nn.BatchNorm3d.html#torch.nn.BatchNorm3d" title="torch.nn.BatchNorm3d"><code class="xref py py-class docutils literal notranslate"><span class="pre">BatchNorm3d</span></code></a>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.ao.nn.quantized.Conv1d"/><p id="torch.ao.nn.quantized.Conv1d"/><a class="reference internal" href="generated/torch.ao.nn.quantized.Conv1d.html#torch.ao.nn.quantized.Conv1d" title="torch.ao.nn.quantized.Conv1d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Conv1d</span></code></a></p></td>
<td><p>Applies a 1D convolution over a quantized input signal composed of several quantized input planes.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.ao.nn.quantized.Conv2d"/><p id="torch.ao.nn.quantized.Conv2d"/><a class="reference internal" href="generated/torch.ao.nn.quantized.Conv2d.html#torch.ao.nn.quantized.Conv2d" title="torch.ao.nn.quantized.Conv2d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Conv2d</span></code></a></p></td>
<td><p>Applies a 2D convolution over a quantized input signal composed of several quantized input planes.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.ao.nn.quantized.Conv3d"/><p id="torch.ao.nn.quantized.Conv3d"/><a class="reference internal" href="generated/torch.ao.nn.quantized.Conv3d.html#torch.ao.nn.quantized.Conv3d" title="torch.ao.nn.quantized.Conv3d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Conv3d</span></code></a></p></td>
<td><p>Applies a 3D convolution over a quantized input signal composed of several quantized input planes.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.ao.nn.quantized.ConvTranspose1d"/><p id="torch.ao.nn.quantized.ConvTranspose1d"/><a class="reference internal" href="generated/torch.ao.nn.quantized.ConvTranspose1d.html#torch.ao.nn.quantized.ConvTranspose1d" title="torch.ao.nn.quantized.ConvTranspose1d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ConvTranspose1d</span></code></a></p></td>
<td><p>Applies a 1D transposed convolution operator over an input image composed of several input planes.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.ao.nn.quantized.ConvTranspose2d"/><p id="torch.ao.nn.quantized.ConvTranspose2d"/><a class="reference internal" href="generated/torch.ao.nn.quantized.ConvTranspose2d.html#torch.ao.nn.quantized.ConvTranspose2d" title="torch.ao.nn.quantized.ConvTranspose2d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ConvTranspose2d</span></code></a></p></td>
<td><p>Applies a 2D transposed convolution operator over an input image composed of several input planes.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.ao.nn.quantized.ConvTranspose3d"/><p id="torch.ao.nn.quantized.ConvTranspose3d"/><a class="reference internal" href="generated/torch.ao.nn.quantized.ConvTranspose3d.html#torch.ao.nn.quantized.ConvTranspose3d" title="torch.ao.nn.quantized.ConvTranspose3d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ConvTranspose3d</span></code></a></p></td>
<td><p>Applies a 3D transposed convolution operator over an input image composed of several input planes.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.ao.nn.quantized.Embedding"/><p id="torch.ao.nn.quantized.Embedding"/><a class="reference internal" href="generated/torch.ao.nn.quantized.Embedding.html#torch.ao.nn.quantized.Embedding" title="torch.ao.nn.quantized.Embedding"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Embedding</span></code></a></p></td>
<td><p>A quantized Embedding module with quantized packed weights as inputs.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.ao.nn.quantized.EmbeddingBag"/><p id="torch.ao.nn.quantized.EmbeddingBag"/><a class="reference internal" href="generated/torch.ao.nn.quantized.EmbeddingBag.html#torch.ao.nn.quantized.EmbeddingBag" title="torch.ao.nn.quantized.EmbeddingBag"><code class="xref py py-obj docutils literal notranslate"><span class="pre">EmbeddingBag</span></code></a></p></td>
<td><p>A quantized EmbeddingBag module with quantized packed weights as inputs.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.ao.nn.quantized.FloatFunctional"/><p id="torch.ao.nn.quantized.FloatFunctional"/><a class="reference internal" href="generated/torch.ao.nn.quantized.FloatFunctional.html#torch.ao.nn.quantized.FloatFunctional" title="torch.ao.nn.quantized.FloatFunctional"><code class="xref py py-obj docutils literal notranslate"><span class="pre">FloatFunctional</span></code></a></p></td>
<td><p>State collector class for float operations.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.ao.nn.quantized.FXFloatFunctional"/><p id="torch.ao.nn.quantized.FXFloatFunctional"/><a class="reference internal" href="generated/torch.ao.nn.quantized.FXFloatFunctional.html#torch.ao.nn.quantized.FXFloatFunctional" title="torch.ao.nn.quantized.FXFloatFunctional"><code class="xref py py-obj docutils literal notranslate"><span class="pre">FXFloatFunctional</span></code></a></p></td>
<td><p>module to replace FloatFunctional module before FX graph mode quantization, since activation_post_process will be inserted in top level module directly</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.ao.nn.quantized.QFunctional"/><p id="torch.ao.nn.quantized.QFunctional"/><a class="reference internal" href="generated/torch.ao.nn.quantized.QFunctional.html#torch.ao.nn.quantized.QFunctional" title="torch.ao.nn.quantized.QFunctional"><code class="xref py py-obj docutils literal notranslate"><span class="pre">QFunctional</span></code></a></p></td>
<td><p>Wrapper class for quantized operations.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.ao.nn.quantized.Linear"/><p id="torch.ao.nn.quantized.Linear"/><a class="reference internal" href="generated/torch.ao.nn.quantized.Linear.html#torch.ao.nn.quantized.Linear" title="torch.ao.nn.quantized.Linear"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Linear</span></code></a></p></td>
<td><p>A quantized linear module with quantized tensor as inputs and outputs.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.ao.nn.quantized.LayerNorm"/><p id="torch.ao.nn.quantized.LayerNorm"/><a class="reference internal" href="generated/torch.ao.nn.quantized.LayerNorm.html#torch.ao.nn.quantized.LayerNorm" title="torch.ao.nn.quantized.LayerNorm"><code class="xref py py-obj docutils literal notranslate"><span class="pre">LayerNorm</span></code></a></p></td>
<td><p>This is the quantized version of <a class="reference internal" href="generated/torch.nn.LayerNorm.html#torch.nn.LayerNorm" title="torch.nn.LayerNorm"><code class="xref py py-class docutils literal notranslate"><span class="pre">LayerNorm</span></code></a>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.ao.nn.quantized.GroupNorm"/><p id="torch.ao.nn.quantized.GroupNorm"/><a class="reference internal" href="generated/torch.ao.nn.quantized.GroupNorm.html#torch.ao.nn.quantized.GroupNorm" title="torch.ao.nn.quantized.GroupNorm"><code class="xref py py-obj docutils literal notranslate"><span class="pre">GroupNorm</span></code></a></p></td>
<td><p>This is the quantized version of <a class="reference internal" href="generated/torch.nn.GroupNorm.html#torch.nn.GroupNorm" title="torch.nn.GroupNorm"><code class="xref py py-class docutils literal notranslate"><span class="pre">GroupNorm</span></code></a>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.ao.nn.quantized.InstanceNorm1d"/><p id="torch.ao.nn.quantized.InstanceNorm1d"/><a class="reference internal" href="generated/torch.ao.nn.quantized.InstanceNorm1d.html#torch.ao.nn.quantized.InstanceNorm1d" title="torch.ao.nn.quantized.InstanceNorm1d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">InstanceNorm1d</span></code></a></p></td>
<td><p>This is the quantized version of <a class="reference internal" href="generated/torch.nn.InstanceNorm1d.html#torch.nn.InstanceNorm1d" title="torch.nn.InstanceNorm1d"><code class="xref py py-class docutils literal notranslate"><span class="pre">InstanceNorm1d</span></code></a>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.ao.nn.quantized.InstanceNorm2d"/><p id="torch.ao.nn.quantized.InstanceNorm2d"/><a class="reference internal" href="generated/torch.ao.nn.quantized.InstanceNorm2d.html#torch.ao.nn.quantized.InstanceNorm2d" title="torch.ao.nn.quantized.InstanceNorm2d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">InstanceNorm2d</span></code></a></p></td>
<td><p>This is the quantized version of <a class="reference internal" href="generated/torch.nn.InstanceNorm2d.html#torch.nn.InstanceNorm2d" title="torch.nn.InstanceNorm2d"><code class="xref py py-class docutils literal notranslate"><span class="pre">InstanceNorm2d</span></code></a>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.ao.nn.quantized.InstanceNorm3d"/><p id="torch.ao.nn.quantized.InstanceNorm3d"/><a class="reference internal" href="generated/torch.ao.nn.quantized.InstanceNorm3d.html#torch.ao.nn.quantized.InstanceNorm3d" title="torch.ao.nn.quantized.InstanceNorm3d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">InstanceNorm3d</span></code></a></p></td>
<td><p>This is the quantized version of <a class="reference internal" href="generated/torch.nn.InstanceNorm3d.html#torch.nn.InstanceNorm3d" title="torch.nn.InstanceNorm3d"><code class="xref py py-class docutils literal notranslate"><span class="pre">InstanceNorm3d</span></code></a>.</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="module-torch.ao.nn.quantized.functional">
<span id="torch-ao-nn-quantized-functional"></span><h2>torch.ao.nn.quantized.functional<a class="headerlink" href="#module-torch.ao.nn.quantized.functional" title="Link to this heading">#</a></h2>
<p>Functional interface (quantized).</p>
<p>This module implements the quantized versions of the functional layers such as
~`torch.nn.functional.conv2d` and <cite>torch.nn.functional.relu</cite>. Note:
<a class="reference internal" href="generated/torch.nn.functional.relu.html#torch.nn.functional.relu" title="torch.nn.functional.relu"><code class="xref py py-meth docutils literal notranslate"><span class="pre">relu()</span></code></a> supports quantized inputs.</p>
<div class="pst-scrollable-table-container"><table class="autosummary longtable table autosummary">
<tbody>
<tr class="row-odd"><td><p><p id="torch.ao.nn.quantized.functional.avg_pool2d"/><p id="torch.ao.nn.quantized.functional.avg_pool2d"/><a class="reference internal" href="generated/torch.ao.nn.quantized.functional.avg_pool2d.html#torch.ao.nn.quantized.functional.avg_pool2d" title="torch.ao.nn.quantized.functional.avg_pool2d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">avg_pool2d</span></code></a></p></td>
<td><p>Applies 2D average-pooling operation in <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi><mi>H</mi><mo>×</mo><mi>k</mi><mi>W</mi></mrow><annotation encoding="application/x-tex">kH \times kW</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7778em;vertical-align:-0.0833em;"></span><span class="mord mathnormal" style="margin-right:0.03148em;">k</span><span class="mord mathnormal" style="margin-right:0.08125em;">H</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">kW</span></span></span></span></span> regions by step size <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>s</mi><mi>H</mi><mo>×</mo><mi>s</mi><mi>W</mi></mrow><annotation encoding="application/x-tex">sH \times sW</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7667em;vertical-align:-0.0833em;"></span><span class="mord mathnormal" style="margin-right:0.08125em;">sH</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal">s</span><span class="mord mathnormal" style="margin-right:0.13889em;">W</span></span></span></span></span> steps.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.ao.nn.quantized.functional.avg_pool3d"/><p id="torch.ao.nn.quantized.functional.avg_pool3d"/><a class="reference internal" href="generated/torch.ao.nn.quantized.functional.avg_pool3d.html#torch.ao.nn.quantized.functional.avg_pool3d" title="torch.ao.nn.quantized.functional.avg_pool3d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">avg_pool3d</span></code></a></p></td>
<td><p>Applies 3D average-pooling operation in <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi><mi>D</mi><mtext> </mtext><mi>t</mi><mi>i</mi><mi>m</mi><mi>e</mi><mi>s</mi><mi>k</mi><mi>H</mi><mo>×</mo><mi>k</mi><mi>W</mi></mrow><annotation encoding="application/x-tex">kD \ times kH \times kW</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7778em;vertical-align:-0.0833em;"></span><span class="mord mathnormal" style="margin-right:0.03148em;">k</span><span class="mord mathnormal" style="margin-right:0.02778em;">D</span><span class="mspace"> </span><span class="mord mathnormal">t</span><span class="mord mathnormal">im</span><span class="mord mathnormal">es</span><span class="mord mathnormal" style="margin-right:0.03148em;">k</span><span class="mord mathnormal" style="margin-right:0.08125em;">H</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">kW</span></span></span></span></span> regions by step size <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>s</mi><mi>D</mi><mo>×</mo><mi>s</mi><mi>H</mi><mo>×</mo><mi>s</mi><mi>W</mi></mrow><annotation encoding="application/x-tex">sD \times sH \times sW</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7667em;vertical-align:-0.0833em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">sD</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.7667em;vertical-align:-0.0833em;"></span><span class="mord mathnormal" style="margin-right:0.08125em;">sH</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal">s</span><span class="mord mathnormal" style="margin-right:0.13889em;">W</span></span></span></span></span> steps.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.ao.nn.quantized.functional.adaptive_avg_pool2d"/><p id="torch.ao.nn.quantized.functional.adaptive_avg_pool2d"/><a class="reference internal" href="generated/torch.ao.nn.quantized.functional.adaptive_avg_pool2d.html#torch.ao.nn.quantized.functional.adaptive_avg_pool2d" title="torch.ao.nn.quantized.functional.adaptive_avg_pool2d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">adaptive_avg_pool2d</span></code></a></p></td>
<td><p>Applies a 2D adaptive average pooling over a quantized input signal composed of several quantized input planes.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.ao.nn.quantized.functional.adaptive_avg_pool3d"/><p id="torch.ao.nn.quantized.functional.adaptive_avg_pool3d"/><a class="reference internal" href="generated/torch.ao.nn.quantized.functional.adaptive_avg_pool3d.html#torch.ao.nn.quantized.functional.adaptive_avg_pool3d" title="torch.ao.nn.quantized.functional.adaptive_avg_pool3d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">adaptive_avg_pool3d</span></code></a></p></td>
<td><p>Applies a 3D adaptive average pooling over a quantized input signal composed of several quantized input planes.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.ao.nn.quantized.functional.conv1d"/><p id="torch.ao.nn.quantized.functional.conv1d"/><a class="reference internal" href="generated/torch.ao.nn.quantized.functional.conv1d.html#torch.ao.nn.quantized.functional.conv1d" title="torch.ao.nn.quantized.functional.conv1d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">conv1d</span></code></a></p></td>
<td><p>Applies a 1D convolution over a quantized 1D input composed of several input planes.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.ao.nn.quantized.functional.conv2d"/><p id="torch.ao.nn.quantized.functional.conv2d"/><a class="reference internal" href="generated/torch.ao.nn.quantized.functional.conv2d.html#torch.ao.nn.quantized.functional.conv2d" title="torch.ao.nn.quantized.functional.conv2d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">conv2d</span></code></a></p></td>
<td><p>Applies a 2D convolution over a quantized 2D input composed of several input planes.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.ao.nn.quantized.functional.conv3d"/><p id="torch.ao.nn.quantized.functional.conv3d"/><a class="reference internal" href="generated/torch.ao.nn.quantized.functional.conv3d.html#torch.ao.nn.quantized.functional.conv3d" title="torch.ao.nn.quantized.functional.conv3d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">conv3d</span></code></a></p></td>
<td><p>Applies a 3D convolution over a quantized 3D input composed of several input planes.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.ao.nn.quantized.functional.interpolate"/><p id="torch.ao.nn.quantized.functional.interpolate"/><a class="reference internal" href="generated/torch.ao.nn.quantized.functional.interpolate.html#torch.ao.nn.quantized.functional.interpolate" title="torch.ao.nn.quantized.functional.interpolate"><code class="xref py py-obj docutils literal notranslate"><span class="pre">interpolate</span></code></a></p></td>
<td><p>Down/up samples the input to either the given <code class="xref py py-attr docutils literal notranslate"><span class="pre">size</span></code> or the given <code class="xref py py-attr docutils literal notranslate"><span class="pre">scale_factor</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.ao.nn.quantized.functional.linear"/><p id="torch.ao.nn.quantized.functional.linear"/><a class="reference internal" href="generated/torch.ao.nn.quantized.functional.linear.html#torch.ao.nn.quantized.functional.linear" title="torch.ao.nn.quantized.functional.linear"><code class="xref py py-obj docutils literal notranslate"><span class="pre">linear</span></code></a></p></td>
<td><p>Applies a linear transformation to the incoming quantized data: <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi><mo>=</mo><mi>x</mi><msup><mi>A</mi><mi>T</mi></msup><mo>+</mo><mi>b</mi></mrow><annotation encoding="application/x-tex">y = xA^T + b</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.9247em;vertical-align:-0.0833em;"></span><span class="mord mathnormal">x</span><span class="mord"><span class="mord mathnormal">A</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal">b</span></span></span></span></span>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.ao.nn.quantized.functional.max_pool1d"/><p id="torch.ao.nn.quantized.functional.max_pool1d"/><a class="reference internal" href="generated/torch.ao.nn.quantized.functional.max_pool1d.html#torch.ao.nn.quantized.functional.max_pool1d" title="torch.ao.nn.quantized.functional.max_pool1d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">max_pool1d</span></code></a></p></td>
<td><p>Applies a 1D max pooling over a quantized input signal composed of several quantized input planes.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.ao.nn.quantized.functional.max_pool2d"/><p id="torch.ao.nn.quantized.functional.max_pool2d"/><a class="reference internal" href="generated/torch.ao.nn.quantized.functional.max_pool2d.html#torch.ao.nn.quantized.functional.max_pool2d" title="torch.ao.nn.quantized.functional.max_pool2d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">max_pool2d</span></code></a></p></td>
<td><p>Applies a 2D max pooling over a quantized input signal composed of several quantized input planes.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.ao.nn.quantized.functional.celu"/><p id="torch.ao.nn.quantized.functional.celu"/><a class="reference internal" href="generated/torch.ao.nn.quantized.functional.celu.html#torch.ao.nn.quantized.functional.celu" title="torch.ao.nn.quantized.functional.celu"><code class="xref py py-obj docutils literal notranslate"><span class="pre">celu</span></code></a></p></td>
<td><p>Applies the quantized CELU function element-wise.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.ao.nn.quantized.functional.leaky_relu"/><p id="torch.ao.nn.quantized.functional.leaky_relu"/><a class="reference internal" href="generated/torch.ao.nn.quantized.functional.leaky_relu.html#torch.ao.nn.quantized.functional.leaky_relu" title="torch.ao.nn.quantized.functional.leaky_relu"><code class="xref py py-obj docutils literal notranslate"><span class="pre">leaky_relu</span></code></a></p></td>
<td><p>Quantized version of the.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.ao.nn.quantized.functional.hardtanh"/><p id="torch.ao.nn.quantized.functional.hardtanh"/><a class="reference internal" href="generated/torch.ao.nn.quantized.functional.hardtanh.html#torch.ao.nn.quantized.functional.hardtanh" title="torch.ao.nn.quantized.functional.hardtanh"><code class="xref py py-obj docutils literal notranslate"><span class="pre">hardtanh</span></code></a></p></td>
<td><p>This is the quantized version of <a class="reference internal" href="generated/torch.nn.functional.hardtanh.html#torch.nn.functional.hardtanh" title="torch.nn.functional.hardtanh"><code class="xref py py-func docutils literal notranslate"><span class="pre">hardtanh()</span></code></a>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.ao.nn.quantized.functional.hardswish"/><p id="torch.ao.nn.quantized.functional.hardswish"/><a class="reference internal" href="generated/torch.ao.nn.quantized.functional.hardswish.html#torch.ao.nn.quantized.functional.hardswish" title="torch.ao.nn.quantized.functional.hardswish"><code class="xref py py-obj docutils literal notranslate"><span class="pre">hardswish</span></code></a></p></td>
<td><p>This is the quantized version of <a class="reference internal" href="generated/torch.nn.functional.hardswish.html#torch.nn.functional.hardswish" title="torch.nn.functional.hardswish"><code class="xref py py-func docutils literal notranslate"><span class="pre">hardswish()</span></code></a>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.ao.nn.quantized.functional.threshold"/><p id="torch.ao.nn.quantized.functional.threshold"/><a class="reference internal" href="generated/torch.ao.nn.quantized.functional.threshold.html#torch.ao.nn.quantized.functional.threshold" title="torch.ao.nn.quantized.functional.threshold"><code class="xref py py-obj docutils literal notranslate"><span class="pre">threshold</span></code></a></p></td>
<td><p>Applies the quantized version of the threshold function element-wise:</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.ao.nn.quantized.functional.elu"/><p id="torch.ao.nn.quantized.functional.elu"/><a class="reference internal" href="generated/torch.ao.nn.quantized.functional.elu.html#torch.ao.nn.quantized.functional.elu" title="torch.ao.nn.quantized.functional.elu"><code class="xref py py-obj docutils literal notranslate"><span class="pre">elu</span></code></a></p></td>
<td><p>This is the quantized version of <a class="reference internal" href="generated/torch.nn.functional.elu.html#torch.nn.functional.elu" title="torch.nn.functional.elu"><code class="xref py py-func docutils literal notranslate"><span class="pre">elu()</span></code></a>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.ao.nn.quantized.functional.hardsigmoid"/><p id="torch.ao.nn.quantized.functional.hardsigmoid"/><a class="reference internal" href="generated/torch.ao.nn.quantized.functional.hardsigmoid.html#torch.ao.nn.quantized.functional.hardsigmoid" title="torch.ao.nn.quantized.functional.hardsigmoid"><code class="xref py py-obj docutils literal notranslate"><span class="pre">hardsigmoid</span></code></a></p></td>
<td><p>This is the quantized version of <a class="reference internal" href="generated/torch.nn.functional.hardsigmoid.html#torch.nn.functional.hardsigmoid" title="torch.nn.functional.hardsigmoid"><code class="xref py py-func docutils literal notranslate"><span class="pre">hardsigmoid()</span></code></a>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.ao.nn.quantized.functional.clamp"/><p id="torch.ao.nn.quantized.functional.clamp"/><a class="reference internal" href="generated/torch.ao.nn.quantized.functional.clamp.html#torch.ao.nn.quantized.functional.clamp" title="torch.ao.nn.quantized.functional.clamp"><code class="xref py py-obj docutils literal notranslate"><span class="pre">clamp</span></code></a></p></td>
<td><p>float(input, min_, max_) -&gt; Tensor</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.ao.nn.quantized.functional.upsample"/><p id="torch.ao.nn.quantized.functional.upsample"/><a class="reference internal" href="generated/torch.ao.nn.quantized.functional.upsample.html#torch.ao.nn.quantized.functional.upsample" title="torch.ao.nn.quantized.functional.upsample"><code class="xref py py-obj docutils literal notranslate"><span class="pre">upsample</span></code></a></p></td>
<td><p>Upsamples the input to either the given <code class="xref py py-attr docutils literal notranslate"><span class="pre">size</span></code> or the given <code class="xref py py-attr docutils literal notranslate"><span class="pre">scale_factor</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.ao.nn.quantized.functional.upsample_bilinear"/><p id="torch.ao.nn.quantized.functional.upsample_bilinear"/><a class="reference internal" href="generated/torch.ao.nn.quantized.functional.upsample_bilinear.html#torch.ao.nn.quantized.functional.upsample_bilinear" title="torch.ao.nn.quantized.functional.upsample_bilinear"><code class="xref py py-obj docutils literal notranslate"><span class="pre">upsample_bilinear</span></code></a></p></td>
<td><p>Upsamples the input, using bilinear upsampling.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.ao.nn.quantized.functional.upsample_nearest"/><p id="torch.ao.nn.quantized.functional.upsample_nearest"/><a class="reference internal" href="generated/torch.ao.nn.quantized.functional.upsample_nearest.html#torch.ao.nn.quantized.functional.upsample_nearest" title="torch.ao.nn.quantized.functional.upsample_nearest"><code class="xref py py-obj docutils literal notranslate"><span class="pre">upsample_nearest</span></code></a></p></td>
<td><p>Upsamples the input, using nearest neighbours' pixel values.</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="torch-ao-nn-quantizable">
<h2>torch.ao.nn.quantizable<a class="headerlink" href="#torch-ao-nn-quantizable" title="Link to this heading">#</a></h2>
<p>This module implements the quantizable versions of some of the nn layers.
These modules can be used in conjunction with the custom module mechanism,
by providing the <code class="docutils literal notranslate"><span class="pre">custom_module_config</span></code> argument to both prepare and convert.</p>
<div class="pst-scrollable-table-container"><table class="autosummary longtable table autosummary">
<tbody>
<tr class="row-odd"><td><p><p id="torch.ao.nn.quantizable.LSTM"/><p id="torch.ao.nn.quantizable.LSTM"/><a class="reference internal" href="generated/torch.ao.nn.quantizable.LSTM.html#torch.ao.nn.quantizable.LSTM" title="torch.ao.nn.quantizable.LSTM"><code class="xref py py-obj docutils literal notranslate"><span class="pre">LSTM</span></code></a></p></td>
<td><p>A quantizable long short-term memory (LSTM).</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.ao.nn.quantizable.MultiheadAttention"/><p id="torch.ao.nn.quantizable.MultiheadAttention"/><a class="reference internal" href="generated/torch.ao.nn.quantizable.MultiheadAttention.html#torch.ao.nn.quantizable.MultiheadAttention" title="torch.ao.nn.quantizable.MultiheadAttention"><code class="xref py py-obj docutils literal notranslate"><span class="pre">MultiheadAttention</span></code></a></p></td>
<td><p></p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="module-torch.ao.nn.quantized.dynamic">
<span id="torch-ao-nn-quantized-dynamic"></span><h2>torch.ao.nn.quantized.dynamic<a class="headerlink" href="#module-torch.ao.nn.quantized.dynamic" title="Link to this heading">#</a></h2>
<p id="module-torch.ao.nn.quantized.dynamic.modules">Dynamically quantized <a class="reference internal" href="generated/torch.nn.Linear.html#torch.nn.Linear" title="torch.nn.Linear"><code class="xref py py-class docutils literal notranslate"><span class="pre">Linear</span></code></a>, <a class="reference internal" href="generated/torch.nn.LSTM.html#torch.nn.LSTM" title="torch.nn.LSTM"><code class="xref py py-class docutils literal notranslate"><span class="pre">LSTM</span></code></a>,
<a class="reference internal" href="generated/torch.nn.LSTMCell.html#torch.nn.LSTMCell" title="torch.nn.LSTMCell"><code class="xref py py-class docutils literal notranslate"><span class="pre">LSTMCell</span></code></a>, <a class="reference internal" href="generated/torch.nn.GRUCell.html#torch.nn.GRUCell" title="torch.nn.GRUCell"><code class="xref py py-class docutils literal notranslate"><span class="pre">GRUCell</span></code></a>, and
<a class="reference internal" href="generated/torch.nn.RNNCell.html#torch.nn.RNNCell" title="torch.nn.RNNCell"><code class="xref py py-class docutils literal notranslate"><span class="pre">RNNCell</span></code></a>.</p>
<div class="pst-scrollable-table-container"><table class="autosummary longtable table autosummary">
<tbody>
<tr class="row-odd"><td><p><p id="torch.ao.nn.quantized.dynamic.Linear"/><p id="torch.ao.nn.quantized.dynamic.Linear"/><a class="reference internal" href="generated/torch.ao.nn.quantized.dynamic.Linear.html#torch.ao.nn.quantized.dynamic.Linear" title="torch.ao.nn.quantized.dynamic.Linear"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Linear</span></code></a></p></td>
<td><p>A dynamic quantized linear module with floating point tensor as inputs and outputs.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.ao.nn.quantized.dynamic.LSTM"/><p id="torch.ao.nn.quantized.dynamic.LSTM"/><a class="reference internal" href="generated/torch.ao.nn.quantized.dynamic.LSTM.html#torch.ao.nn.quantized.dynamic.LSTM" title="torch.ao.nn.quantized.dynamic.LSTM"><code class="xref py py-obj docutils literal notranslate"><span class="pre">LSTM</span></code></a></p></td>
<td><p>A dynamic quantized LSTM module with floating point tensor as inputs and outputs.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.ao.nn.quantized.dynamic.GRU"/><p id="torch.ao.nn.quantized.dynamic.GRU"/><a class="reference internal" href="generated/torch.ao.nn.quantized.dynamic.GRU.html#torch.ao.nn.quantized.dynamic.GRU" title="torch.ao.nn.quantized.dynamic.GRU"><code class="xref py py-obj docutils literal notranslate"><span class="pre">GRU</span></code></a></p></td>
<td><p>Applies a multi-layer gated recurrent unit (GRU) RNN to an input sequence.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.ao.nn.quantized.dynamic.RNNCell"/><p id="torch.ao.nn.quantized.dynamic.RNNCell"/><a class="reference internal" href="generated/torch.ao.nn.quantized.dynamic.RNNCell.html#torch.ao.nn.quantized.dynamic.RNNCell" title="torch.ao.nn.quantized.dynamic.RNNCell"><code class="xref py py-obj docutils literal notranslate"><span class="pre">RNNCell</span></code></a></p></td>
<td><p>An Elman RNN cell with tanh or ReLU non-linearity.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.ao.nn.quantized.dynamic.LSTMCell"/><p id="torch.ao.nn.quantized.dynamic.LSTMCell"/><a class="reference internal" href="generated/torch.ao.nn.quantized.dynamic.LSTMCell.html#torch.ao.nn.quantized.dynamic.LSTMCell" title="torch.ao.nn.quantized.dynamic.LSTMCell"><code class="xref py py-obj docutils literal notranslate"><span class="pre">LSTMCell</span></code></a></p></td>
<td><p>A long short-term memory (LSTM) cell.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.ao.nn.quantized.dynamic.GRUCell"/><p id="torch.ao.nn.quantized.dynamic.GRUCell"/><a class="reference internal" href="generated/torch.ao.nn.quantized.dynamic.GRUCell.html#torch.ao.nn.quantized.dynamic.GRUCell" title="torch.ao.nn.quantized.dynamic.GRUCell"><code class="xref py py-obj docutils literal notranslate"><span class="pre">GRUCell</span></code></a></p></td>
<td><p>A gated recurrent unit (GRU) cell</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="quantized-dtypes-and-quantization-schemes">
<h2>Quantized dtypes and quantization schemes<a class="headerlink" href="#quantized-dtypes-and-quantization-schemes" title="Link to this heading">#</a></h2>
<p>Note that operator implementations currently only
support per channel quantization for weights of the <strong>conv</strong> and <strong>linear</strong>
operators. Furthermore, the input data is
mapped linearly to the quantized data and vice versa
as follows:</p>
<blockquote>
<div><div class="math">
<span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mtable rowspacing="0.25em" columnalign="right left" columnspacing="0em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mtext>Quantization:</mtext></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><msub><mi>Q</mi><mtext>out</mtext></msub><mo>=</mo><mtext>clamp</mtext><mo stretchy="false">(</mo><msub><mi>x</mi><mtext>input</mtext></msub><mi mathvariant="normal">/</mi><mi>s</mi><mo>+</mo><mi>z</mi><mo separator="true">,</mo><msub><mi>Q</mi><mtext>min</mtext></msub><mo separator="true">,</mo><msub><mi>Q</mi><mtext>max</mtext></msub><mo stretchy="false">)</mo></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mtext>Dequantization:</mtext></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><msub><mi>x</mi><mtext>out</mtext></msub><mo>=</mo><mo stretchy="false">(</mo><msub><mi>Q</mi><mtext>input</mtext></msub><mo>−</mo><mi>z</mi><mo stretchy="false">)</mo><mo>∗</mo><mi>s</mi></mrow></mstyle></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{aligned}
    \text{Quantization:}&amp;\\
    &amp;Q_\text{out} = \text{clamp}(x_\text{input}/s+z, Q_\text{min}, Q_\text{max})\\
    \text{Dequantization:}&amp;\\
    &amp;x_\text{out} = (Q_\text{input}-z)*s
\end{aligned}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:6em;vertical-align:-2.75em;"></span><span class="mord"><span class="mtable"><span class="col-align-r"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:3.25em;"><span style="top:-5.41em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord text"><span class="mord">Quantization:</span></span></span></span><span style="top:-3.91em;"><span class="pstrut" style="height:3em;"></span><span class="mord"></span></span><span style="top:-2.41em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord text"><span class="mord">Dequantization:</span></span></span></span><span style="top:-0.91em;"><span class="pstrut" style="height:3em;"></span><span class="mord"></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:2.75em;"><span></span></span></span></span></span><span class="col-align-l"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:3.25em;"><span style="top:-5.41em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"></span></span></span><span style="top:-3.91em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"></span><span class="mord"><span class="mord mathnormal">Q</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">out</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mord text"><span class="mord">clamp</span></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3175em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">input</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span><span class="mord">/</span><span class="mord mathnormal">s</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mord mathnormal" style="margin-right:0.04398em;">z</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal">Q</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3175em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">min</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal">Q</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">max</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span><span style="top:-2.41em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"></span></span></span><span style="top:-0.91em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">out</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">Q</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3175em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">input</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mord mathnormal" style="margin-right:0.04398em;">z</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mord mathnormal">s</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:2.75em;"><span></span></span></span></span></span></span></span></span></span></span></span></div></div></blockquote>
<p>where <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>clamp</mtext><mo stretchy="false">(</mo><mi mathvariant="normal">.</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\text{clamp}(.)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord text"><span class="mord">clamp</span></span><span class="mopen">(</span><span class="mord">.</span><span class="mclose">)</span></span></span></span></span> is the same as <a class="reference internal" href="generated/torch.clamp.html#torch.clamp" title="torch.clamp"><code class="xref py py-func docutils literal notranslate"><span class="pre">clamp()</span></code></a> while the
scale <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>s</mi></mrow><annotation encoding="application/x-tex">s</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">s</span></span></span></span></span> and zero point <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>z</mi></mrow><annotation encoding="application/x-tex">z</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.04398em;">z</span></span></span></span></span> are then computed
as described in <a class="reference internal" href="generated/torch.ao.quantization.observer.MinMaxObserver.html#torch.ao.quantization.observer.MinMaxObserver" title="torch.ao.quantization.observer.MinMaxObserver"><code class="xref py py-class docutils literal notranslate"><span class="pre">MinMaxObserver</span></code></a>, specifically:</p>
<blockquote>
<div><div class="math">
<span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mtable rowspacing="0.25em" columnalign="right left" columnspacing="0em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mtext>if Symmetric:</mtext></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mi>s</mi><mo>=</mo><mn>2</mn><mi>max</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi mathvariant="normal">∣</mi><msub><mi>x</mi><mtext>min</mtext></msub><mi mathvariant="normal">∣</mi><mo separator="true">,</mo><msub><mi>x</mi><mtext>max</mtext></msub><mo stretchy="false">)</mo><mi mathvariant="normal">/</mi><mrow><mo fence="true">(</mo><msub><mi>Q</mi><mtext>max</mtext></msub><mo>−</mo><msub><mi>Q</mi><mtext>min</mtext></msub><mo fence="true">)</mo></mrow></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mi>z</mi><mo>=</mo><mrow><mo fence="true">{</mo><mtable rowspacing="0.36em" columnalign="left left" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mtext>if dtype is qint8</mtext></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>128</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mtext>otherwise</mtext></mstyle></mtd></mtr></mtable></mrow></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mtext>Otherwise:</mtext></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mi>s</mi><mo>=</mo><mrow><mo fence="true">(</mo><msub><mi>x</mi><mtext>max</mtext></msub><mo>−</mo><msub><mi>x</mi><mtext>min</mtext></msub><mo fence="true">)</mo></mrow><mi mathvariant="normal">/</mi><mrow><mo fence="true">(</mo><msub><mi>Q</mi><mtext>max</mtext></msub><mo>−</mo><msub><mi>Q</mi><mtext>min</mtext></msub><mo fence="true">)</mo></mrow></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mi>z</mi><mo>=</mo><msub><mi>Q</mi><mtext>min</mtext></msub><mo>−</mo><mtext>round</mtext><mo stretchy="false">(</mo><msub><mi>x</mi><mtext>min</mtext></msub><mi mathvariant="normal">/</mi><mi>s</mi><mo stretchy="false">)</mo></mrow></mstyle></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{aligned}
    \text{if Symmetric:}&amp;\\
    &amp;s = 2 \max(|x_\text{min}|, x_\text{max}) /
        \left( Q_\text{max} - Q_\text{min} \right) \\
    &amp;z = \begin{cases}
        0 &amp; \text{if dtype is qint8} \\
        128 &amp; \text{otherwise}
    \end{cases}\\
    \text{Otherwise:}&amp;\\
        &amp;s = \left( x_\text{max} - x_\text{min}  \right ) /
            \left( Q_\text{max} - Q_\text{min} \right ) \\
        &amp;z = Q_\text{min} - \text{round}(x_\text{min} / s)
\end{aligned}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:10.8em;vertical-align:-5.15em;"></span><span class="mord"><span class="mtable"><span class="col-align-r"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:5.65em;"><span style="top:-8.56em;"><span class="pstrut" style="height:3.75em;"></span><span class="mord"><span class="mord text"><span class="mord">if Symmetric:</span></span></span></span><span style="top:-7.06em;"><span class="pstrut" style="height:3.75em;"></span><span class="mord"></span></span><span style="top:-4.65em;"><span class="pstrut" style="height:3.75em;"></span><span class="mord"></span></span><span style="top:-2.26em;"><span class="pstrut" style="height:3.75em;"></span><span class="mord"><span class="mord text"><span class="mord">Otherwise:</span></span></span></span><span style="top:-0.76em;"><span class="pstrut" style="height:3.75em;"></span><span class="mord"></span></span><span style="top:0.74em;"><span class="pstrut" style="height:3.75em;"></span><span class="mord"></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:5.15em;"><span></span></span></span></span></span><span class="col-align-l"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:5.65em;"><span style="top:-8.56em;"><span class="pstrut" style="height:3.75em;"></span><span class="mord"><span class="mord"></span></span></span><span style="top:-7.06em;"><span class="pstrut" style="height:3.75em;"></span><span class="mord"><span class="mord"></span><span class="mord mathnormal">s</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mord">2</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mop">max</span><span class="mopen">(</span><span class="mord">∣</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3175em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">min</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord">∣</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">max</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mord">/</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;">(</span><span class="mord"><span class="mord mathnormal">Q</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">max</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mord"><span class="mord mathnormal">Q</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3175em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">min</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose delimcenter" style="top:0em;">)</span></span></span></span><span style="top:-4.65em;"><span class="pstrut" style="height:3.75em;"></span><span class="mord"><span class="mord"></span><span class="mord mathnormal" style="margin-right:0.04398em;">z</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;"><span class="delimsizing size4">{</span></span><span class="mord"><span class="mtable"><span class="col-align-l"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.69em;"><span style="top:-3.69em;"><span class="pstrut" style="height:3.008em;"></span><span class="mord"><span class="mord">0</span></span></span><span style="top:-2.25em;"><span class="pstrut" style="height:3.008em;"></span><span class="mord"><span class="mord">128</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.19em;"><span></span></span></span></span></span><span class="arraycolsep" style="width:1em;"></span><span class="col-align-l"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.69em;"><span style="top:-3.69em;"><span class="pstrut" style="height:3.008em;"></span><span class="mord"><span class="mord text"><span class="mord">if dtype is qint8</span></span></span></span><span style="top:-2.25em;"><span class="pstrut" style="height:3.008em;"></span><span class="mord"><span class="mord text"><span class="mord">otherwise</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.19em;"><span></span></span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span><span style="top:-2.26em;"><span class="pstrut" style="height:3.75em;"></span><span class="mord"><span class="mord"></span></span></span><span style="top:-0.76em;"><span class="pstrut" style="height:3.75em;"></span><span class="mord"><span class="mord"></span><span class="mord mathnormal">s</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;">(</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">max</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3175em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">min</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose delimcenter" style="top:0em;">)</span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord">/</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;">(</span><span class="mord"><span class="mord mathnormal">Q</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">max</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mord"><span class="mord mathnormal">Q</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3175em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">min</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose delimcenter" style="top:0em;">)</span></span></span></span><span style="top:0.74em;"><span class="pstrut" style="height:3.75em;"></span><span class="mord"><span class="mord"></span><span class="mord mathnormal" style="margin-right:0.04398em;">z</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mord"><span class="mord mathnormal">Q</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3175em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">min</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mord text"><span class="mord">round</span></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3175em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">min</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord">/</span><span class="mord mathnormal">s</span><span class="mclose">)</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:5.15em;"><span></span></span></span></span></span></span></span></span></span></span></span></div></div></blockquote>
<p>where <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">[</mo><msub><mi>x</mi><mtext>min</mtext></msub><mo separator="true">,</mo><msub><mi>x</mi><mtext>max</mtext></msub><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">[x_\text{min}, x_\text{max}]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">[</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3175em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">min</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">max</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">]</span></span></span></span></span> denotes the range of the input data while
<span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>Q</mi><mtext>min</mtext></msub></mrow><annotation encoding="application/x-tex">Q_\text{min}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8778em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathnormal">Q</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3175em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">min</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></span> and <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>Q</mi><mtext>max</mtext></msub></mrow><annotation encoding="application/x-tex">Q_\text{max}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8778em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathnormal">Q</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">max</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></span> are respectively the minimum and maximum values of the quantized dtype.</p>
<p>Note that the choice of <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>s</mi></mrow><annotation encoding="application/x-tex">s</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">s</span></span></span></span></span> and <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>z</mi></mrow><annotation encoding="application/x-tex">z</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.04398em;">z</span></span></span></span></span> implies that zero is represented with no quantization error whenever zero is within
the range of the input data or symmetric quantization is being used.</p>
<p>Additional data types and quantization schemes can be implemented through
the <a class="reference external" href="https://pytorch.org/tutorials/advanced/torch_script_custom_ops.html">custom operator mechanism</a>.</p>
<ul class="simple">
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">torch.qscheme</span></code> — Type to describe the quantization scheme of a tensor.
Supported types:</p>
<ul>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">torch.per_tensor_affine</span></code> — per tensor, asymmetric</p></li>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">torch.per_channel_affine</span></code> — per channel, asymmetric</p></li>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">torch.per_tensor_symmetric</span></code> — per tensor, symmetric</p></li>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">torch.per_channel_symmetric</span></code> — per channel, symmetric</p></li>
</ul>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">torch.dtype</span></code> — Type to describe the data. Supported types:</p>
<ul>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">torch.quint8</span></code> — 8-bit unsigned integer</p></li>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">torch.qint8</span></code> — 8-bit signed integer</p></li>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">torch.qint32</span></code> — 32-bit signed integer</p></li>
</ul>
</li>
</ul>
<p id="module-torch.nn.qat.modules"><span id="module-torch.nn.qat.dynamic.modules"></span><span id="module-torch.nn.quantizable"></span>QAT Modules.</p>
<p>This package is in the process of being deprecated.
Please, use <cite>torch.ao.nn.qat.modules</cite> instead.</p>
<p id="module-torch.nn.qat">QAT Dynamic Modules.</p>
<p>This package is in the process of being deprecated.
Please, use <cite>torch.ao.nn.qat.dynamic</cite> instead.</p>
<p id="module-torch.quantization.fx"><span id="module-torch.nn.intrinsic.quantized.modules"></span><span id="module-torch.nn.intrinsic"></span><span id="module-torch.nn.quantized.dynamic"></span><span id="module-torch.nn.intrinsic.qat.modules"></span>This file is in the process of migration to <cite>torch/ao/quantization</cite>, and
is kept here for compatibility while the migration process is ongoing.
If you are adding a new entry/functionality, please, add it to the
appropriate files under <cite>torch/ao/quantization/fx/</cite>, while adding an import statement
here.</p>
<p id="module-torch.nn.qat.dynamic"><span id="module-torch.nn.intrinsic.quantized.dynamic"></span>QAT Dynamic Modules.</p>
<p>This package is in the process of being deprecated.
Please, use <cite>torch.ao.nn.qat.dynamic</cite> instead.</p>
<p id="module-torch.nn.quantized.modules"><span id="module-torch.nn.intrinsic.qat"></span>Quantized Modules.</p>
<dl class="simple">
<dt>Note::</dt><dd><p>The <cite>torch.nn.quantized</cite> namespace is in the process of being deprecated.
Please, use <cite>torch.ao.nn.quantized</cite> instead.</p>
</dd>
</dl>
<p id="module-torch.nn.quantized.dynamic.modules"><span id="module-torch.nn.intrinsic.quantized.dynamic.modules"></span><span id="module-torch.nn.quantized"></span><span id="module-torch.nn.quantizable.modules"></span><span id="module-torch.nn.intrinsic.quantized"></span>Quantized Dynamic Modules.</p>
<p>This file is in the process of migration to <cite>torch/ao/nn/quantized/dynamic</cite>,
and is kept here for compatibility while the migration process is ongoing.
If you are adding a new entry/functionality, please, add it to the
appropriate file under the <cite>torch/ao/nn/quantized/dynamic</cite>,
while adding an import statement here.</p>
<span class="target" id="module-torch.nn.intrinsic.modules"><span id="module-torch.quantization"></span></span></section>
</section>


                </article>
              
              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
</div>
                </footer>
              
            </div>
            
            
              
                <dialog id="pst-secondary-sidebar-modal"></dialog>
                <div id="pst-secondary-sidebar" class="bd-sidebar-secondary bd-toc">
<div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
<div
    id="pst-page-navigation-heading-2"
    class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> On this page
  </div>
  <nav class="bd-toc-nav page-toc" aria-labelledby="pst-page-navigation-heading-2">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#torch-ao-quantization">torch.ao.quantization</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#top-level-apis">Top level APIs</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#preparing-model-for-quantization">Preparing model for quantization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#utility-functions">Utility functions</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#torch-ao-quantization-quantize-fx">torch.ao.quantization.quantize_fx</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#torch-ao-quantization-qconfig-mapping">torch.ao.quantization.qconfig_mapping</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#torch-ao-quantization-backend-config">torch.ao.quantization.backend_config</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#torch-ao-quantization-fx-custom-config">torch.ao.quantization.fx.custom_config</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#module-torch.ao.quantization.quantizer">torch.ao.quantization.quantizer</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#module-torch.ao.quantization.pt2e">torch.ao.quantization.pt2e (quantization in pytorch 2.0 export implementation)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#torch-ao-quantization-pt2e-export-utils">torch.ao.quantization.pt2e.export_utils</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#pt2-export-pt2e-numeric-debugger">PT2 Export (pt2e) Numeric Debugger</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#torch-quantization-related-functions">torch (quantization related functions)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#torch-tensor-quantization-related-methods">torch.Tensor (quantization related methods)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#torch-ao-quantization-observer">torch.ao.quantization.observer</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#torch-ao-quantization-fake-quantize">torch.ao.quantization.fake_quantize</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#torch-ao-quantization-qconfig">torch.ao.quantization.qconfig</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#module-torch.ao.nn.intrinsic">torch.ao.nn.intrinsic</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#module-torch.ao.nn.intrinsic.qat">torch.ao.nn.intrinsic.qat</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#module-torch.ao.nn.intrinsic.quantized">torch.ao.nn.intrinsic.quantized</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#module-torch.ao.nn.intrinsic.quantized.dynamic">torch.ao.nn.intrinsic.quantized.dynamic</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#module-torch.ao.nn.qat">torch.ao.nn.qat</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#module-torch.ao.nn.qat.dynamic">torch.ao.nn.qat.dynamic</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#module-torch.ao.nn.quantized.modules">torch.ao.nn.quantized</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#module-torch.ao.nn.quantized.functional">torch.ao.nn.quantized.functional</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#torch-ao-nn-quantizable">torch.ao.nn.quantizable</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#module-torch.ao.nn.quantized.dynamic">torch.ao.nn.quantized.dynamic</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#quantized-dtypes-and-quantization-schemes">Quantized dtypes and quantization schemes</a></li>
</ul>
  </nav></div>

  <div class="sidebar-secondary-item">

  <div class="tocsection sourcelink">
    <a href="../_sources/python-api/quantization-support.rst">
      <i class="fa-solid fa-file-lines"></i> Show Source
    </a>
  </div>
</div>

</div>
<div class="sidebar-secondary-item"> 
</div>
<div class="sidebar-secondary-item">
 <p>PyTorch Libraries</p>
 <ul>
 
  <li><a href="https://pytorch.org/vision">torchvision</a></li>
 
  <li><a href="https://pytorch.org/executorch">ExecuTorch</a></li>
 
  <li><a href="https://https://pytorch.org/ao">torchao</a></li>
 
  <li><a href="https://https://pytorch.org/audio">torchaudio</a></li>
 
  <li><a href="https://https://pytorch.org/torchrec">torchrec</a></li>
 
  <li><a href="https://https://pytorch.org/serve">torchserve</a></li>
 
  <li><a href="https://https://pytorch.org/data">torchdata</a></li>
 
  <li><a href="https://https://pytorch.org/xla">PyTorch on XLA devices</a></li>
 
 </ul>
</div>
</div>

</div>
              
            
          </div>
          <footer class="bd-footer-content">
            
          </footer>
        
      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script defer src="../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf"></script>
<script defer src="../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf"></script>



  <footer class="bd-footer">
<div class="bd-footer__inner bd-page-width">
  
    <div class="footer-items__start">
      
        <div class="footer-item">

  <p class="copyright">
    
      © Copyright PyTorch Contributors.
      <br/>
    
  </p>
</div>
      
        <div class="footer-item">

  <p class="sphinx-version">
    Created using <a href="https://www.sphinx-doc.org/">Sphinx</a> 8.1.0.
    <br/>
  </p>
</div>
      
    </div>
  
  
  
    <div class="footer-items__end">
      
        <div class="footer-item">
<p class="theme-version">
  <!-- # L10n: Setting the PST URL as an argument as this does not need to be localized -->
  Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.16.1.
</p></div>
      
    </div>
  
</div>

  </footer>
  </body>
</html>