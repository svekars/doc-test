
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>torch.distributed.tensor &#8212; PyTorch main documentation</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  <!--
    this give us a css class that will be invisible only if js is disabled
  -->
  <noscript>
    <style>
      .pst-js-only { display: none !important; }

    </style>
  </noscript>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=949a1ff5" />
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/katex.min.css" />
    <link rel="stylesheet" type="text/css" href="../_static/katex-math.css?v=91adb8b6" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="../_static/css/jit.css?v=8de1ea5d" />
    <link rel="stylesheet" type="text/css" href="../_static/css/custom.css?v=3ccf5357" />
    <link rel="stylesheet" type="text/css" href="../_static/css/custom2.css?v=a9f46c5e" />
  
  <!-- So that users can add custom icons -->
  <script src="../_static/scripts/fontawesome.js?digest=8878045cc6db502f8baf"></script>
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf" />

    <script src="../_static/documentation_options.js?v=a8da1a53"></script>
    <script src="../_static/doctools.js?v=9bcbadda"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=940804e7"></script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'python-api/distributed.tensor';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  <meta name="docsearch:version" content="main (2.6.0 )" />
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>

          <li class="main-menu-item">
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Learn
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="">
                  <span class=dropdown-title>Get Started</span>
                  <p>Run PyTorch locally or get started quickly with one of the supported cloud platforms</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Tutorials</span>
                  <p>Whats new in PyTorch tutorials</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Learn the Basics</span>
                  <p>Familiarize yourself with PyTorch concepts and modules</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">PyTorch Recipes</span>
                  <p>Bite-size, ready-to-deploy PyTorch code examples</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Intro to PyTorch - YouTube Series</span>
                  <p>Master PyTorch basics with our engaging YouTube tutorial series</p>
                </a>
              </div>
            </div>
          </li>

          <li>
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Ecosystem
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Tools</span>
                  <p>Learn about the tools and frameworks in the PyTorch Ecosystem</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class=dropdown-title>Community</span>
                  <p>Join the PyTorch developer community to contribute, learn, and get your questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="" target="_blank">
                  <span class=dropdown-title>Forums</span>
                  <p>A place to discuss PyTorch code, issues, install, research</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class=dropdown-title>Developer Resources</span>
                  <p>Find resources and get questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Contributor Awards - 2024</span>
                  <p>Award winners announced at this year's PyTorch Conference</p>
                </a>
              </div>
            </div>
          </li>

          <li>
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Edge
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">About PyTorch Edge</span>
                  <p>Build innovative and privacy-aware AI experiences for edge devices</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">ExecuTorch</span>
                  <p>End-to-end solution for enabling on-device inference capabilities across mobile and edge devices</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">ExecuTorch Docs</span>
                </a>
              </div>
            </div>  
          </li>

          <li class="main-menu-item">
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Docs
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">PyTorch</span>
                  <p>Explore the documentation for comprehensive guidance on how to use PyTorch</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">PyTorch Domains</span>
                  <p>Read the PyTorch Domains documentation to learn more about domain-specific libraries</p>
                </a>
              </div>
            </div>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Blogs & News 
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">PyTorch Blog</span>
                  <p>Catch up on the latest technical news and happenings</p>
                </a>
                 <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Community Blog</span>
                  <p>Stories from the PyTorch ecosystem</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Videos</span>
                  <p>Learn about the latest PyTorch tutorials, new, and more </p>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Community Stories</span>
                  <p>Learn how our community solves real, everyday machine learning problems with PyTorch</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Events</span>
                  <p>Find events, webinars, and podcasts</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Newsletter</span>
                  <p>Stay up-to-date with the latest updates</p>
                </a>
            </div>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                About
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">PyTorch Foundation</span>
                  <p>Learn more about the PyTorch Foundation</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Governing Board</span>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Cloud Credit Program</span>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Technical Advisory Council</span>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Staff</span>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Contact Us</span>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
            <div class="no-dropdown">
              <a href="" data-cta="join">
                Become a Member
              </a>
            </div>
          </li>
          <li>
           <div class="main-menu-item">
             <a href="" class="github-icon">
             </a>
           </div>
          </li>
          <!--- TODO: This block adds the search icon to the nav bar. We will enable it later. 
          <li>
            <div class="main-menu-item">
             <a href="" class="search-icon">
             </a>
            </div>
          </li>
          --->
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>
  </div>
</div>

  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <dialog id="pst-search-dialog">
    
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </dialog>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
<div class="bd-header__inner bd-page-width">
  <button class="pst-navbar-icon sidebar-toggle primary-toggle" aria-label="Site navigation">
    <span class="fa-solid fa-bars"></span>
  </button>
  
  
  <div class=" navbar-header-items__start">
    
      <div class="navbar-item">
<div class="version-switcher__container dropdown pst-js-only">
  <button id="pst-version-switcher-button-2"
    type="button"
    class="version-switcher__button btn btn-sm dropdown-toggle"
    data-bs-toggle="dropdown"
    aria-haspopup="listbox"
    aria-controls="pst-version-switcher-list-2"
    aria-label="Version switcher list"
  >
    Choose version  <!-- this text may get changed later by javascript -->
    <span class="caret"></span>
  </button>
  <div id="pst-version-switcher-list-2"
    class="version-switcher__menu dropdown-menu list-group-flush py-0"
    role="listbox" aria-labelledby="pst-version-switcher-button-2">
    <!-- dropdown will be populated by javascript on page load -->
  </div>
</div></div>
    
      <div class="navbar-item">

  
    
  

<a class="navbar-brand logo" href="../index.html">
  
  
  
  
  
  
    <p class="title logo__title">Home</p>
  
</a></div>
    
  </div>
  
  <div class=" navbar-header-items">
    
    <div class="me-auto navbar-header-items__center">
      
        <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item ">
  <a class="nav-link nav-internal" href="index.html">
    Python API
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../notes/index.html">
    Developer Notes
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../community/index.html">
    Community
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../lang-bindings/index.html">
    Language Bindings
  </a>
</li>

  </ul>
</nav></div>
      
    </div>
    
    
    <div class="navbar-header-items__end">
      
        <div class="navbar-item navbar-persistent--container">
          

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button>
        </div>
      
      
        <div class="navbar-item">

<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button></div>
      
        <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://x.com/PyTorch" title="X" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-x-twitter fa-lg" aria-hidden="true"></i>
            <span class="sr-only">X</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/pytorch/pytorch" title="GitHub" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">GitHub</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://pypi.org/project/torch" title="PyPI" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-custom fa-pypi fa-lg" aria-hidden="true"></i>
            <span class="sr-only">PyPI</span></a>
        </li>
</ul></div>
      
    </div>
    
  </div>
  
  
    <div class="navbar-persistent--mobile">

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button>
    </div>
  

  
    <button class="pst-navbar-icon sidebar-toggle secondary-toggle" aria-label="On this page">
      <span class="fa-solid fa-outdent"></span>
    </button>
  
</div>

    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
        
      
      <dialog id="pst-primary-sidebar-modal"></dialog>
      <div id="pst-primary-sidebar" class="bd-sidebar-primary bd-sidebar hide-on-wide">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
      <div class="sidebar-header-items__center">
        
          
          
            <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item ">
  <a class="nav-link nav-internal" href="index.html">
    Python API
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../notes/index.html">
    Developer Notes
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../community/index.html">
    Community
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../lang-bindings/index.html">
    Language Bindings
  </a>
</li>

  </ul>
</nav></div>
          
        
      </div>
    
    
    
      <div class="sidebar-header-items__end">
        
          <div class="navbar-item">

<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button></div>
        
          <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://x.com/PyTorch" title="X" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-x-twitter fa-lg" aria-hidden="true"></i>
            <span class="sr-only">X</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/pytorch/pytorch" title="GitHub" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">GitHub</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://pypi.org/project/torch" title="PyPI" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-custom fa-pypi fa-lg" aria-hidden="true"></i>
            <span class="sr-only">PyPI</span></a>
        </li>
</ul></div>
        
      </div>
    
  </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
      <div class="sidebar-primary-item">
<div id="ethical-ad-placement"
      class="flat"
      data-ea-publisher="readthedocs"
      data-ea-type="readthedocs-sidebar"
      data-ea-manual="true">
</div></div>
  </div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        
          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item">

<nav aria-label="Breadcrumb" class="d-print-none">
  <ul class="bd-breadcrumbs">
    
    <li class="breadcrumb-item breadcrumb-home">
      <a href="../index.html" class="nav-link" aria-label="Home">
        <i class="fa-solid fa-home"></i>
      </a>
    </li>
    <li class="breadcrumb-item active" aria-current="page"><span class="ellipsis">torch.distributed.tensor</span></li>
  </ul>
</nav>
</div>
      
    </div>
  
  
</div>
</div>
              
              
              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section id="torch-distributed-tensor">
<h1>torch.distributed.tensor<a class="headerlink" href="#torch-distributed-tensor" title="Link to this heading">#</a></h1>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><code class="docutils literal notranslate"><span class="pre">torch.distributed.tensor</span></code> is currently in alpha state and under
development, we are committing backward compatibility for the most APIs listed
in the doc, but there might be API changes if necessary.</p>
</div>
<section id="pytorch-dtensor-distributed-tensor">
<h2>PyTorch DTensor (Distributed Tensor)<a class="headerlink" href="#pytorch-dtensor-distributed-tensor" title="Link to this heading">#</a></h2>
<p>PyTorch DTensor offers simple and flexible tensor sharding primitives that transparently handles distributed
logic, including sharded storage, operator computation and collective communications across devices/hosts.
<code class="docutils literal notranslate"><span class="pre">DTensor</span></code> could be used to build different paralleism solutions and support sharded state_dict representation
when working with multi-dimensional sharding.</p>
<p>Please see examples from the PyTorch native parallelism solutions that are built on top of <code class="docutils literal notranslate"><span class="pre">DTensor</span></code>:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://pytorch.org/docs/main/distributed.tensor.parallel.html">Tensor Parallel</a></p></li>
<li><p><a class="reference external" href="https://github.com/pytorch/torchtitan/blob/main/docs/fsdp.md">FSDP2</a></p></li>
</ul>
<p id="module-torch.distributed.tensor"><a class="reference internal" href="#torch.distributed.tensor.DTensor" title="torch.distributed.tensor.DTensor"><code class="xref py py-class docutils literal notranslate"><span class="pre">DTensor</span></code></a> follows the SPMD (single program, multiple data) programming model to empower users to
write distributed program as if it’s a <strong>single-device program with the same convergence property</strong>. It
provides a uniform tensor sharding layout (DTensor Layout) through specifying the <code class="xref py py-class docutils literal notranslate"><span class="pre">DeviceMesh</span></code>
and <code class="xref py py-class docutils literal notranslate"><span class="pre">Placement</span></code>:</p>
<ul class="simple">
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">DeviceMesh</span></code> represents the device topology and the communicators of the cluster using
an n-dimensional array.</p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Placement</span></code> describes the sharding layout of the logical tensor on the <code class="xref py py-class docutils literal notranslate"><span class="pre">DeviceMesh</span></code>.
DTensor supports three types of placements: <code class="xref py py-class docutils literal notranslate"><span class="pre">Shard</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Replicate</span></code> and <code class="xref py py-class docutils literal notranslate"><span class="pre">Partial</span></code>.</p></li>
</ul>
<section id="dtensor-class-apis">
<h3>DTensor Class APIs<a class="headerlink" href="#dtensor-class-apis" title="Link to this heading">#</a></h3>
<p><a class="reference internal" href="#torch.distributed.tensor.DTensor" title="torch.distributed.tensor.DTensor"><code class="xref py py-class docutils literal notranslate"><span class="pre">DTensor</span></code></a> is a <code class="docutils literal notranslate"><span class="pre">torch.Tensor</span></code> subclass. This means once a <a class="reference internal" href="#torch.distributed.tensor.DTensor" title="torch.distributed.tensor.DTensor"><code class="xref py py-class docutils literal notranslate"><span class="pre">DTensor</span></code></a> is created, it could be
used in very similar way to <code class="docutils literal notranslate"><span class="pre">torch.Tensor</span></code>, including running different types of PyTorch operators as if
running them in a single device, allowing proper distributed computation for PyTorch operators.</p>
<p>In addition to existing <code class="docutils literal notranslate"><span class="pre">torch.Tensor</span></code> methods, it also offers a set of additional methods to interact with
<code class="docutils literal notranslate"><span class="pre">torch.Tensor</span></code>, <code class="docutils literal notranslate"><span class="pre">redistribute</span></code> the DTensor Layout to a new DTensor, get the full tensor content
on all devices, etc.</p>
<dl class="py class">
<dt class="sig sig-object py" id="torch.distributed.tensor.DTensor">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torch.distributed.tensor.</span></span><span class="sig-name descname"><span class="pre">DTensor</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">local_tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">spec</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">requires_grad</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributed.tensor.DTensor" title="Link to this definition">#</a></dt>
<dd><p><code class="docutils literal notranslate"><span class="pre">DTensor</span></code> (Distributed Tensor) is a subclass of <code class="docutils literal notranslate"><span class="pre">torch.Tensor</span></code> that provides single-device like
abstraction to program with multi-device <code class="docutils literal notranslate"><span class="pre">torch.Tensor</span></code>. It describes the distributed tensor sharding
layout (DTensor Layout) through the <code class="xref py py-class docutils literal notranslate"><span class="pre">DeviceMesh</span></code> and following types of <code class="xref py py-class docutils literal notranslate"><span class="pre">Placement</span></code>:</p>
<ul class="simple">
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Shard</span></code>: Tensor sharded on the tensor dimension <code class="docutils literal notranslate"><span class="pre">dim</span></code> on the devices of the <code class="docutils literal notranslate"><span class="pre">DeviceMesh</span></code> dimension</p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Replicate</span></code>: Tensor replicated on the devices of the <code class="docutils literal notranslate"><span class="pre">DeviceMesh</span></code> dimension</p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Partial</span></code>: Tensor is pending reduction on the devices of the <code class="docutils literal notranslate"><span class="pre">DeviceMesh</span></code> dimension</p></li>
</ul>
<p>When calling PyTorch operators, <code class="docutils literal notranslate"><span class="pre">DTensor</span></code> overrides the PyTorch operators to perform sharded computation and issue
communications whenever necessary. Along with the operator computation, <code class="docutils literal notranslate"><span class="pre">DTensor</span></code> will transform or propagate the
placements (DTensor Layout) properly (based on the operator semantic itself) and generate new <code class="docutils literal notranslate"><span class="pre">DTensor</span></code> outputs.</p>
<p>To ensure numerical correctness of the <code class="docutils literal notranslate"><span class="pre">DTensor</span></code> sharded computation when calling PyTorch operators, <code class="docutils literal notranslate"><span class="pre">DTensor</span></code>
requires every Tensor argument of the operator be DTensor.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Directly using the Tensor subclass constructor here is not the recommended way to create a <code class="docutils literal notranslate"><span class="pre">DTensor</span></code>
(i.e. it does not handle autograd correctly hence is not the public API). Please refer to the <a class="reference internal" href="#create-dtensor">create_dtensor</a>
section to see how to create a <code class="docutils literal notranslate"><span class="pre">DTensor</span></code>.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference internal" href="#torch.distributed.tensor.DTensor" title="torch.distributed.tensor.DTensor">DTensor</a></p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="torch.distributed.tensor.DTensor.from_local">
<em class="property"><span class="pre">static</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">from_local</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">local_tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device_mesh</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">placements</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">run_check</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">shape</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stride</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torch/distributed/tensor/_api.html#DTensor.from_local"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributed/tensor/_api.py#L352"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.tensor.DTensor.from_local" title="Link to this definition">#</a></dt>
<dd><p>Create a <a class="reference internal" href="#torch.distributed.tensor.DTensor" title="torch.distributed.tensor.DTensor"><code class="xref py py-class docutils literal notranslate"><span class="pre">DTensor</span></code></a> from a local torch.Tensor on each rank
according to the <code class="docutils literal notranslate"><span class="pre">device_mesh</span></code> and <code class="docutils literal notranslate"><span class="pre">placements</span></code> specified.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>local_tensor</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>torch.Tensor</em></a>) – local torch.Tensor on each rank.</p></li>
<li><p><strong>device_mesh</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">DeviceMesh</span></code>, optional) – DeviceMesh to place the
tensor, if not specified, must be called under a DeviceMesh
context manager, default: None</p></li>
<li><p><strong>placements</strong> (List[<code class="xref py py-class docutils literal notranslate"><span class="pre">Placement</span></code>], optional) – the placements that
describes how to place the local torch.Tensor on DeviceMesh, must
have the same number of elements as <code class="docutils literal notranslate"><span class="pre">device_mesh.ndim</span></code>.</p></li>
</ul>
</dd>
<dt class="field-even">Keyword Arguments<span class="colon">:</span></dt>
<dd class="field-even"><ul class="simple">
<li><p><strong>run_check</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><em>bool</em></a><em>, </em><em>optional</em>) – at a cost of extra communications, perform
sanity check across ranks to check each local tensor’s meta information
to ensure correctness. If have <code class="xref py py-class docutils literal notranslate"><span class="pre">Replicate</span></code> in <code class="docutils literal notranslate"><span class="pre">placements</span></code>, the
data on first rank of the device mesh dimension will be broadcasted
to other ranks. default: False</p></li>
<li><p><strong>shape</strong> (<a class="reference internal" href="size.html#torch.Size" title="torch.Size"><em>torch.Size</em></a><em>, </em><em>optional</em>) – A List of int which specifies the size of
DTensor which build on top of <cite>local_tensor</cite>. Note this needs to be
provided if the shape of <code class="docutils literal notranslate"><span class="pre">local_tensor</span></code> are different across the ranks.
If not provided, <code class="docutils literal notranslate"><span class="pre">shape</span></code> will be computed assuming the given distributed
tensor is evenly sharded across ranks. default: None</p></li>
<li><p><strong>stride</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.13)"><em>tuple</em></a><em>, </em><em>optional</em>) – A List of int which specifies the stride of DTensor.
If not provided, <code class="docutils literal notranslate"><span class="pre">stride</span></code> will be computed assuming the given distributed
tensor is evenly sharded across ranks. default: None</p></li>
</ul>
</dd>
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>A <a class="reference internal" href="#torch.distributed.tensor.DTensor" title="torch.distributed.tensor.DTensor"><code class="xref py py-class docutils literal notranslate"><span class="pre">DTensor</span></code></a> object</p>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p><a class="reference internal" href="#torch.distributed.tensor.DTensor" title="torch.distributed.tensor.DTensor"><em>DTensor</em></a></p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>When <code class="docutils literal notranslate"><span class="pre">run_check=False</span></code>, it is the user’s responsibility to ensure the
local tensor passed in is correct across ranks (i.e. the tensor is sharded for
the <code class="docutils literal notranslate"><span class="pre">Shard(dim)</span></code> placement or replicated for the <code class="docutils literal notranslate"><span class="pre">Replicate()</span></code> placement).
If not, the behavior of the created DTensor is undefined.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><code class="docutils literal notranslate"><span class="pre">from_local</span></code> is differentiable, the <cite>requires_grad</cite> of the created
<cite>DTensor</cite> object will depend on if <cite>local_tensor</cite> requires_grad or not.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributed.tensor.DTensor.full_tensor">
<span class="sig-name descname"><span class="pre">full_tensor</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">grad_placements</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torch/distributed/tensor/_api.html#DTensor.full_tensor"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributed/tensor/_api.py#L546"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.tensor.DTensor.full_tensor" title="Link to this definition">#</a></dt>
<dd><p>Return the full tensor of this DTensor. It will perform necessary collectives
to gather the local tensors from other ranks in its DeviceMesh and concatenate
them together. It’s a syntatic sugar of the following code:</p>
<p><code class="docutils literal notranslate"><span class="pre">dtensor.redistribute(placements=[Replicate()]</span> <span class="pre">*</span> <span class="pre">mesh.ndim).to_local()</span></code></p>
<dl class="field-list simple">
<dt class="field-odd">Keyword Arguments<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>grad_placements</strong> (List[<code class="xref py py-class docutils literal notranslate"><span class="pre">Placement</span></code>], optional) – the placements describes
the future layout of any gradient layout of the full Tensor returned from this
function.
<cite>full_tensor</cite> converts DTensor to a full torch.Tensor and the returned torch.tensor
might not be used as the original replicated DTensor layout later in the code. This
argument is the hint that user can give to autograd in case the gradient
layout of the returned tensor does not match the original replicated DTensor layout.
If not specified, we will assume the gradient layout of the full tensor be replicated.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A <a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Tensor</span></code></a> object that represents the full tensor of this DTensor.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a></p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><code class="docutils literal notranslate"><span class="pre">full_tensor</span></code> is differentiable.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributed.tensor.DTensor.redistribute">
<span class="sig-name descname"><span class="pre">redistribute</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">device_mesh</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">placements</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">async_op</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torch/distributed/tensor/_api.html#DTensor.redistribute"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributed/tensor/_api.py#L474"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.tensor.DTensor.redistribute" title="Link to this definition">#</a></dt>
<dd><p><code class="docutils literal notranslate"><span class="pre">redistribute</span></code> performs necessary collective operations that redistribute the current
DTensor from its current placements to a new placements, or from is current DeviceMesh
to a new DeviceMesh. i.e. we can turn a Sharded DTensor to a Replicated DTensor by
specifying a Replicate placement for each dimension of the DeviceMesh.</p>
<p>When redistributing from current to the new placements on one device mesh dimension, we
will perform the following operations including communication collective or local operation:</p>
<ol class="arabic simple">
<li><p><code class="docutils literal notranslate"><span class="pre">Shard(dim)</span></code> -&gt; <code class="docutils literal notranslate"><span class="pre">Replicate()</span></code>: <code class="docutils literal notranslate"><span class="pre">all_gather</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Shard(src_dim)</span></code> -&gt; <code class="docutils literal notranslate"><span class="pre">Shard(dst_dim)</span></code>: <code class="docutils literal notranslate"><span class="pre">all_to_all</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Replicate()</span></code> -&gt; <code class="docutils literal notranslate"><span class="pre">Shard(dim)</span></code>: local chunking (i.e. <code class="docutils literal notranslate"><span class="pre">torch.chunk</span></code>)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Partial()</span></code> -&gt; <code class="docutils literal notranslate"><span class="pre">Replicate()</span></code>: <code class="docutils literal notranslate"><span class="pre">all_reduce</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Partial()</span></code> -&gt; <code class="docutils literal notranslate"><span class="pre">Shard(dim)</span></code>: <code class="docutils literal notranslate"><span class="pre">reduce_scatter</span></code></p></li>
</ol>
<p><code class="docutils literal notranslate"><span class="pre">redistribute</span></code> would correctly figure out the necessary redistribute steps for DTensors
that are created either on 1-D or N-D DeviceMesh.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>device_mesh</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">DeviceMesh</span></code>, optional) – DeviceMesh to place the
DTensor. If not specified, it would use the current DTensor’s DeviceMesh.
default: None</p></li>
<li><p><strong>placements</strong> (List[<code class="xref py py-class docutils literal notranslate"><span class="pre">Placement</span></code>], optional) – the new placements that
describes how to place the DTensor into the DeviceMesh, must
have the same number of elements as <code class="docutils literal notranslate"><span class="pre">device_mesh.ndim</span></code>.
default: replicate on all mesh dimensions</p></li>
</ul>
</dd>
<dt class="field-even">Keyword Arguments<span class="colon">:</span></dt>
<dd class="field-even"><p><strong>async_op</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><em>bool</em></a><em>, </em><em>optional</em>) – whether to perform the DTensor redistribute operation
asynchronously or not. Default: False</p>
</dd>
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>A <a class="reference internal" href="#torch.distributed.tensor.DTensor" title="torch.distributed.tensor.DTensor"><code class="xref py py-class docutils literal notranslate"><span class="pre">DTensor</span></code></a> object</p>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p><a class="reference internal" href="#torch.distributed.tensor.DTensor" title="torch.distributed.tensor.DTensor"><em>DTensor</em></a></p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><code class="docutils literal notranslate"><span class="pre">redistribute</span></code> is differentiable, which means user do not need to worry about
the backward formula of the redistribute operation.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><code class="docutils literal notranslate"><span class="pre">redistribute</span></code> currently only supports redistributing DTensor on the same DeviceMesh,
Please file an issue if you need to redistribute DTensor to different DeviceMesh.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributed.tensor.DTensor.to_local">
<span class="sig-name descname"><span class="pre">to_local</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">grad_placements</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torch/distributed/tensor/_api.html#DTensor.to_local"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributed/tensor/_api.py#L437"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.tensor.DTensor.to_local" title="Link to this definition">#</a></dt>
<dd><p>Get the local tensor of this DTensor on its current rank. For sharding it returns
a local shard of the logical tensor view, for replication it returns the replica on
its current rank.</p>
<dl class="field-list simple">
<dt class="field-odd">Keyword Arguments<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>grad_placements</strong> (List[<code class="xref py py-class docutils literal notranslate"><span class="pre">Placement</span></code>], optional) – the placements describes
the future layout of any gradient layout of the Tensor returned from this
function.
<cite>to_local</cite> converts DTensor to local tensor and the returned local tensor
might not be used as the original DTensor layout later in the code. This
argument is the hint that user can give to autograd in case the gradient
layout of the returned tensor does not match the original DTensor layout.
If not specified, we will assume the gradient layout remains the same
as the original DTensor and use that for gradient computation.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A <a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Tensor</span></code></a> or <code class="docutils literal notranslate"><span class="pre">AsyncCollectiveTensor</span></code> object. it represents the
local tensor on its current rank. When an <code class="docutils literal notranslate"><span class="pre">AsyncCollectiveTensor</span></code> object is returned,
it means the local tensor is not ready yet (i.e. communication is not finished). In this
case, user needs to call <code class="docutils literal notranslate"><span class="pre">wait</span></code> to wait the local tensor to be ready.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a></p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><code class="docutils literal notranslate"><span class="pre">to_local</span></code> is differentiable, the <code class="docutils literal notranslate"><span class="pre">requires_grad</span></code> of the local tensor returned
will depend on if the <cite>DTensor</cite> requires_grad or not.</p>
</div>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="torch.distributed.tensor.DTensor.device_mesh">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">device_mesh</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><a class="reference internal" href="distributed.html#torch.distributed.device_mesh.DeviceMesh" title="torch.distributed.device_mesh.DeviceMesh"><span class="pre">DeviceMesh</span></a></em><a class="headerlink" href="#torch.distributed.tensor.DTensor.device_mesh" title="Link to this definition">#</a></dt>
<dd><p>The <code class="xref py py-class docutils literal notranslate"><span class="pre">DeviceMesh</span></code> attribute that associates with this DTensor object.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><code class="docutils literal notranslate"><span class="pre">device_mesh</span></code> is a read-only property, it can not be set.</p>
</div>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="torch.distributed.tensor.DTensor.placements">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">placements</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Tuple" title="(in Python v3.13)"><span class="pre">Tuple</span></a><span class="p"><span class="pre">[</span></span><a class="reference internal" href="#torch.distributed.tensor.placement_types.Placement" title="torch.distributed.tensor.placement_types.Placement"><span class="pre">Placement</span></a><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="p"><span class="pre">...</span></span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#torch.distributed.tensor.DTensor.placements" title="Link to this definition">#</a></dt>
<dd><p>The placements attribute of this DTensor that describes the layout of this
DTensor on the its DeviceMesh.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><code class="docutils literal notranslate"><span class="pre">placements</span></code> is a read-only property, it can not be set.</p>
</div>
</dd></dl>

</dd></dl>

</section>
<section id="devicemesh-as-the-distributed-communicator">
<h3>DeviceMesh as the distributed communicator<a class="headerlink" href="#devicemesh-as-the-distributed-communicator" title="Link to this heading">#</a></h3>
<p><a class="reference internal" href="distributed.html#torch.distributed.device_mesh.DeviceMesh" title="torch.distributed.device_mesh.DeviceMesh"><code class="xref py py-class docutils literal notranslate"><span class="pre">DeviceMesh</span></code></a> was built from DTensor as the abstraction to describe cluster’s device topology and represent
multi-dimensional communicators (on top of <code class="docutils literal notranslate"><span class="pre">ProcessGroup</span></code>). To see the details of how to create/use a DeviceMesh,
please refer to the <a class="reference external" href="https://pytorch.org/tutorials/recipes/distributed_device_mesh.html">DeviceMesh recipe</a>.</p>
</section>
<section id="module-torch.distributed.tensor.placement_types">
<span id="dtensor-placement-types"></span><h3>DTensor Placement Types<a class="headerlink" href="#module-torch.distributed.tensor.placement_types" title="Link to this heading">#</a></h3>
<p>DTensor supports the following types of <a class="reference internal" href="#torch.distributed.tensor.placement_types.Placement" title="torch.distributed.tensor.placement_types.Placement"><code class="xref py py-class docutils literal notranslate"><span class="pre">Placement</span></code></a> on each <code class="xref py py-class docutils literal notranslate"><span class="pre">DeviceMesh</span></code> dimension:</p>
<dl class="py class">
<dt class="sig sig-object py" id="torch.distributed.tensor.placement_types.Shard">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torch.distributed.tensor.placement_types.</span></span><span class="sig-name descname"><span class="pre">Shard</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dim</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torch/distributed/tensor/placement_types.html#Shard"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributed/tensor/placement_types.py#L48"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.tensor.placement_types.Shard" title="Link to this definition">#</a></dt>
<dd><p>The <code class="docutils literal notranslate"><span class="pre">Shard(dim)</span></code> placement describes the DTensor sharding on tensor dimension
<code class="docutils literal notranslate"><span class="pre">dim</span></code> over a corresponding <code class="docutils literal notranslate"><span class="pre">DeviceMesh</span></code> dimension, where each rank on the
DeviceMesh dimension only holds a shard/piece of the global Tensor. The
<code class="docutils literal notranslate"><span class="pre">Shard(dim)</span></code> placement follows the <code class="docutils literal notranslate"><span class="pre">torch.chunk(dim)</span></code> semantic, where the
last few shards on the DeviceMesh dimension might be empty when the tensor dimension
is not evenly divisible on the DeviceMesh dimension. The <code class="docutils literal notranslate"><span class="pre">Shard</span></code> placement can be
used by all DTensor APIs (i.e. distribute_tensor, from_local, etc.)</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>dim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – The tensor dimension that describes the DTensor is sharded over its
corresponding DeviceMesh dimension.</p>
</dd>
</dl>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>sharding on a tensor dimension where the tensor dimension size is not
evenly divisible on a DeviceMesh dimension is currently experimental and subject to change.</p>
</div>
<dl class="py attribute">
<dt class="sig sig-object py" id="torch.distributed.tensor.placement_types.Shard.dim">
<span class="sig-name descname"><span class="pre">dim</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><span class="pre">int</span></a></em><a class="headerlink" href="#torch.distributed.tensor.placement_types.Shard.dim" title="Link to this definition">#</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="torch.distributed.tensor.placement_types.Replicate">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torch.distributed.tensor.placement_types.</span></span><span class="sig-name descname"><span class="pre">Replicate</span></span><a class="reference internal" href="../_modules/torch/distributed/tensor/placement_types.html#Replicate"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributed/tensor/placement_types.py#L527"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.tensor.placement_types.Replicate" title="Link to this definition">#</a></dt>
<dd><p>The <code class="docutils literal notranslate"><span class="pre">Replicate()</span></code> placement describes the DTensor replicating on a corresponding
<code class="docutils literal notranslate"><span class="pre">DeviceMesh</span></code> dimension, where each rank on the DeviceMesh dimension holds a
replica of the global Tensor. The <code class="docutils literal notranslate"><span class="pre">Replicate</span></code> placement can be used by all
DTensor APIs (i.e. <code class="docutils literal notranslate"><span class="pre">distribute_tensor</span></code>, <code class="docutils literal notranslate"><span class="pre">DTensor.from_local</span></code>, etc.)</p>
<dl class="field-list simple">
</dl>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="torch.distributed.tensor.placement_types.Partial">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torch.distributed.tensor.placement_types.</span></span><span class="sig-name descname"><span class="pre">Partial</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">reduce_op</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'sum'</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torch/distributed/tensor/placement_types.html#Partial"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributed/tensor/placement_types.py#L572"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.tensor.placement_types.Partial" title="Link to this definition">#</a></dt>
<dd><p>The <code class="docutils literal notranslate"><span class="pre">Partial(reduce_op)</span></code> placement describes the DTensor that is pending
reduction on a specified <code class="docutils literal notranslate"><span class="pre">DeviceMesh</span></code> dimension, where each rank on the
DeviceMesh dimension holds the partial value of the global Tensor. User can
redistribute the <code class="docutils literal notranslate"><span class="pre">Partial</span></code> DTensor to a <code class="docutils literal notranslate"><span class="pre">Replicate</span></code> or <code class="docutils literal notranslate"><span class="pre">Shard(dim)</span></code>
placement on the specified <code class="docutils literal notranslate"><span class="pre">DeviceMesh</span></code> dimension using <code class="docutils literal notranslate"><span class="pre">redistribute</span></code>,
which would trigger necessary communication operations under the hood (i.e.
<code class="docutils literal notranslate"><span class="pre">allreduce</span></code>, <code class="docutils literal notranslate"><span class="pre">reduce_scatter</span></code>).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>reduce_op</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><em>str</em></a><em>, </em><em>optional</em>) – The reduction op to be used for the partial DTensor
to produce Replicated/Sharded DTensor. Only element-wise reduction operations
are supported, including: “sum”, “avg”, “product”, “max”, “min”, default: “sum”.</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The <code class="docutils literal notranslate"><span class="pre">Partial</span></code> placement can be generated as a result of the DTensor operators,
and can only be used by the <code class="docutils literal notranslate"><span class="pre">DTensor.from_local</span></code> API.</p>
</div>
<dl class="py attribute">
<dt class="sig sig-object py" id="torch.distributed.tensor.placement_types.Partial.reduce_op">
<span class="sig-name descname"><span class="pre">reduce_op</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">'sum'</span></em><a class="headerlink" href="#torch.distributed.tensor.placement_types.Partial.reduce_op" title="Link to this definition">#</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="torch.distributed.tensor.placement_types.Placement">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torch.distributed.tensor.placement_types.</span></span><span class="sig-name descname"><span class="pre">Placement</span></span><a class="reference internal" href="../_modules/torch/distributed/tensor/placement_types.html#Placement"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributed/tensor/placement_types.py#L23"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.tensor.placement_types.Placement" title="Link to this definition">#</a></dt>
<dd><p>The base class for the Placement type, where it describes how a DTensor is placed onto the
<code class="docutils literal notranslate"><span class="pre">DeviceMesh</span></code>. <code class="docutils literal notranslate"><span class="pre">Placement</span></code> and <code class="docutils literal notranslate"><span class="pre">DeviceMesh</span></code> together could describe the DTensor Layout.
It is the base class of the three main DTensor Placement types: <code class="docutils literal notranslate"><span class="pre">Shard</span></code>, <code class="docutils literal notranslate"><span class="pre">Replicate</span></code>,
and <code class="docutils literal notranslate"><span class="pre">Partial</span></code>.</p>
<p>This class is not meant to be used directly, mainly served as a typing stub.</p>
<dl class="py method">
<dt class="sig sig-object py" id="torch.distributed.tensor.placement_types.Placement.is_partial">
<span class="sig-name descname"><span class="pre">is_partial</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torch/distributed/tensor/placement_types.html#Placement.is_partial"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributed/tensor/placement_types.py#L44"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.tensor.placement_types.Placement.is_partial" title="Link to this definition">#</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)">bool</a></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributed.tensor.placement_types.Placement.is_replicate">
<span class="sig-name descname"><span class="pre">is_replicate</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torch/distributed/tensor/placement_types.html#Placement.is_replicate"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributed/tensor/placement_types.py#L41"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.tensor.placement_types.Placement.is_replicate" title="Link to this definition">#</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)">bool</a></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributed.tensor.placement_types.Placement.is_shard">
<span class="sig-name descname"><span class="pre">is_shard</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torch/distributed/tensor/placement_types.html#Placement.is_shard"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributed/tensor/placement_types.py#L34"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.tensor.placement_types.Placement.is_shard" title="Link to this definition">#</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)">bool</a></p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
</section>
<section id="different-ways-to-create-a-dtensor">
<span id="create-dtensor"></span><h2>Different ways to create a DTensor<a class="headerlink" href="#different-ways-to-create-a-dtensor" title="Link to this heading">#</a></h2>
<dl class="simple">
<dt>There’re three ways to construct a <a class="reference internal" href="#torch.distributed.tensor.DTensor" title="torch.distributed.tensor.DTensor"><code class="xref py py-class docutils literal notranslate"><span class="pre">DTensor</span></code></a>:</dt><dd><ul class="simple">
<li><p><a class="reference internal" href="#torch.distributed.tensor.distribute_tensor" title="torch.distributed.tensor.distribute_tensor"><code class="xref py py-meth docutils literal notranslate"><span class="pre">distribute_tensor()</span></code></a> creates a <a class="reference internal" href="#torch.distributed.tensor.DTensor" title="torch.distributed.tensor.DTensor"><code class="xref py py-class docutils literal notranslate"><span class="pre">DTensor</span></code></a> from a logical or “global” <code class="docutils literal notranslate"><span class="pre">torch.Tensor</span></code> on
each rank. This could be used to shard the leaf <code class="docutils literal notranslate"><span class="pre">torch.Tensor</span></code> s (i.e. model parameters/buffers
and inputs).</p></li>
<li><p><a class="reference internal" href="#torch.distributed.tensor.DTensor.from_local" title="torch.distributed.tensor.DTensor.from_local"><code class="xref py py-meth docutils literal notranslate"><span class="pre">DTensor.from_local()</span></code></a> creates a <a class="reference internal" href="#torch.distributed.tensor.DTensor" title="torch.distributed.tensor.DTensor"><code class="xref py py-class docutils literal notranslate"><span class="pre">DTensor</span></code></a> from a local <code class="docutils literal notranslate"><span class="pre">torch.Tensor</span></code> on each rank, which can
be used to create <a class="reference internal" href="#torch.distributed.tensor.DTensor" title="torch.distributed.tensor.DTensor"><code class="xref py py-class docutils literal notranslate"><span class="pre">DTensor</span></code></a> from a non-leaf <code class="docutils literal notranslate"><span class="pre">torch.Tensor</span></code> s (i.e. intermediate activation
tensors during forward/backward).</p></li>
<li><p>DTensor provides dedicated tensor factory functions (e.g. <a class="reference internal" href="#torch.distributed.tensor.empty" title="torch.distributed.tensor.empty"><code class="xref py py-meth docutils literal notranslate"><span class="pre">empty()</span></code></a>, <a class="reference internal" href="#torch.distributed.tensor.ones" title="torch.distributed.tensor.ones"><code class="xref py py-meth docutils literal notranslate"><span class="pre">ones()</span></code></a>, <a class="reference internal" href="#torch.distributed.tensor.randn" title="torch.distributed.tensor.randn"><code class="xref py py-meth docutils literal notranslate"><span class="pre">randn()</span></code></a>, etc.)
to allow different <a class="reference internal" href="#torch.distributed.tensor.DTensor" title="torch.distributed.tensor.DTensor"><code class="xref py py-class docutils literal notranslate"><span class="pre">DTensor</span></code></a> creations by directly specifying the <code class="xref py py-class docutils literal notranslate"><span class="pre">DeviceMesh</span></code> and
<code class="xref py py-class docutils literal notranslate"><span class="pre">Placement</span></code>. Compare to <a class="reference internal" href="#torch.distributed.tensor.distribute_tensor" title="torch.distributed.tensor.distribute_tensor"><code class="xref py py-meth docutils literal notranslate"><span class="pre">distribute_tensor()</span></code></a>, this could directly materializing the sharded memory
on device, instead of performing sharding after initializing the logical Tensor memory.</p></li>
</ul>
</dd>
</dl>
<section id="create-dtensor-from-a-logical-torch-tensor">
<h3>Create DTensor from a logical torch.Tensor<a class="headerlink" href="#create-dtensor-from-a-logical-torch-tensor" title="Link to this heading">#</a></h3>
<p>The SPMD (single program, multiple data) programming model in <code class="docutils literal notranslate"><span class="pre">torch.distributed</span></code> launches multiple processes
(i.e. via <code class="docutils literal notranslate"><span class="pre">torchrun</span></code>) to execute the same program, this means that the model inside the program would be
initialized on different processes first (i.e. the model might be initialized on CPU, or meta device, or directly
on GPU if enough memory).</p>
<p><code class="docutils literal notranslate"><span class="pre">DTensor</span></code> offers a <a class="reference internal" href="#torch.distributed.tensor.distribute_tensor" title="torch.distributed.tensor.distribute_tensor"><code class="xref py py-meth docutils literal notranslate"><span class="pre">distribute_tensor()</span></code></a> API that could shard the model weights or Tensors to <code class="docutils literal notranslate"><span class="pre">DTensor</span></code> s,
where it would create a DTensor from the “logical” Tensor on each process. This would empower the created
<code class="docutils literal notranslate"><span class="pre">DTensor</span></code> s to comply with the single device semantic, which is critical for <strong>numerical correctness</strong>.</p>
<dl class="py function">
<dt class="sig sig-object py" id="torch.distributed.tensor.distribute_tensor">
<span class="sig-prename descclassname"><span class="pre">torch.distributed.tensor.</span></span><span class="sig-name descname"><span class="pre">distribute_tensor</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device_mesh</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">placements</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributed/tensor/_api.py#L629"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.tensor.distribute_tensor" title="Link to this definition">#</a></dt>
<dd><p>Distribute a leaf <code class="docutils literal notranslate"><span class="pre">torch.Tensor</span></code> (i.e. nn.Parameter/buffers) to the <code class="docutils literal notranslate"><span class="pre">device_mesh</span></code> according
to the <code class="docutils literal notranslate"><span class="pre">placements</span></code> specified. The rank of <code class="docutils literal notranslate"><span class="pre">device_mesh</span></code> and <code class="docutils literal notranslate"><span class="pre">placements</span></code> must be the
same. The <code class="docutils literal notranslate"><span class="pre">tensor</span></code> to distribute is the logical or “global” tensor, and the API would use
the <code class="docutils literal notranslate"><span class="pre">tensor</span></code> from first rank of the DeviceMesh dimension as the source of truth to preserve
the single-device semantic. If you want to construct a DTensor in the middle of the Autograd
computation, please use <a class="reference internal" href="#torch.distributed.tensor.DTensor.from_local" title="torch.distributed.tensor.DTensor.from_local"><code class="xref py py-meth docutils literal notranslate"><span class="pre">DTensor.from_local()</span></code></a> instead.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>tensor</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>torch.Tensor</em></a>) – torch.Tensor to be distributed. Note that if you
want to shard a tensor on a dimension that is not evenly divisible by
the number of devices in that mesh dimension, we use <code class="docutils literal notranslate"><span class="pre">torch.chunk</span></code>
semantic to shard the tensor and scatter the shards. The uneven sharding
behavior is experimental and subject to change.</p></li>
<li><p><strong>device_mesh</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">DeviceMesh</span></code>, optional) – DeviceMesh to distribute the
tensor, if not specified, must be called under a DeviceMesh context
manager, default: None</p></li>
<li><p><strong>placements</strong> (List[<code class="xref py py-class docutils literal notranslate"><span class="pre">Placement</span></code>], optional) – the placements that
describes how to place the tensor on DeviceMesh, must have the same
number of elements as <code class="docutils literal notranslate"><span class="pre">device_mesh.ndim</span></code>. If not specified, we will
by default replicate the tensor across the <code class="docutils literal notranslate"><span class="pre">device_mesh</span></code> from the
first rank of each dimension of the <cite>device_mesh</cite>.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A <a class="reference internal" href="#torch.distributed.tensor.DTensor" title="torch.distributed.tensor.DTensor"><code class="xref py py-class docutils literal notranslate"><span class="pre">DTensor</span></code></a> or <code class="docutils literal notranslate"><span class="pre">XLAShardedTensor</span></code> object.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference internal" href="#torch.distributed.tensor.DTensor" title="torch.distributed.tensor.DTensor"><em>DTensor</em></a></p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>When initialize the DeviceMesh with the <code class="docutils literal notranslate"><span class="pre">xla</span></code> device_type, <code class="docutils literal notranslate"><span class="pre">distribute_tensor</span></code>
return <cite>XLAShardedTensor</cite> instead. see <a class="reference external" href="https://github.com/pytorch/pytorch/issues/92909">this issue</a>
for more details. The XLA integration is experimental and subject to change.</p>
</div>
</dd></dl>

<p>Along with <a class="reference internal" href="#torch.distributed.tensor.distribute_tensor" title="torch.distributed.tensor.distribute_tensor"><code class="xref py py-meth docutils literal notranslate"><span class="pre">distribute_tensor()</span></code></a>, DTensor also offers a <a class="reference internal" href="#torch.distributed.tensor.distribute_module" title="torch.distributed.tensor.distribute_module"><code class="xref py py-meth docutils literal notranslate"><span class="pre">distribute_module()</span></code></a> API to allow easier
sharding on the <code class="xref py py-class docutils literal notranslate"><span class="pre">nn.Module</span></code> level</p>
<dl class="py function">
<dt class="sig sig-object py" id="torch.distributed.tensor.distribute_module">
<span class="sig-prename descclassname"><span class="pre">torch.distributed.tensor.</span></span><span class="sig-name descname"><span class="pre">distribute_module</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">module</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device_mesh</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">partition_fn</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">input_fn</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_fn</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributed/tensor/_api.py#L817"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.tensor.distribute_module" title="Link to this definition">#</a></dt>
<dd><p>This function expose three functions to control the parameters/inputs/outputs of the module:</p>
<p>1. To perform sharding on the module before runtime execution by specifying the
<code class="docutils literal notranslate"><span class="pre">partition_fn</span></code> (i.e. allow user to convert Module parameters to <a class="reference internal" href="#torch.distributed.tensor.DTensor" title="torch.distributed.tensor.DTensor"><code class="xref py py-class docutils literal notranslate"><span class="pre">DTensor</span></code></a>
parameters according to the <cite>partition_fn</cite> specified).
2. To control the inputs or outputs of the module during runtime execution by
specifying the <code class="docutils literal notranslate"><span class="pre">input_fn</span></code> and <code class="docutils literal notranslate"><span class="pre">output_fn</span></code>. (i.e. convert the input to
<a class="reference internal" href="#torch.distributed.tensor.DTensor" title="torch.distributed.tensor.DTensor"><code class="xref py py-class docutils literal notranslate"><span class="pre">DTensor</span></code></a>, convert the output back to <code class="docutils literal notranslate"><span class="pre">torch.Tensor</span></code>)</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>module</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">nn.Module</span></code>) – user module to be partitioned.</p></li>
<li><p><strong>device_mesh</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">DeviceMesh</span></code>) – the device mesh to place the module.</p></li>
<li><p><strong>partition_fn</strong> (<em>Callable</em>) – the function to partition parameters (i.e. shard certain
parameters across the <code class="docutils literal notranslate"><span class="pre">device_mesh</span></code>). If <code class="docutils literal notranslate"><span class="pre">partition_fn</span></code> is not specified,
by default we replicate all module parameters of <code class="docutils literal notranslate"><span class="pre">module</span></code> across the mesh.</p></li>
<li><p><strong>input_fn</strong> (<em>Callable</em>) – specify the input distribution, i.e. could control how the
input of the module is sharded. <code class="docutils literal notranslate"><span class="pre">input_fn</span></code> will be installed as a module
<code class="docutils literal notranslate"><span class="pre">forward_pre_hook</span></code> (pre forward hook).</p></li>
<li><p><strong>output_fn</strong> (<em>Callable</em>) – specify the output distribution, i.e. could control how the
output is sharded, or convert it back to torch.Tensor. <code class="docutils literal notranslate"><span class="pre">output_fn</span></code> will be
installed as a module <code class="docutils literal notranslate"><span class="pre">forward_hook</span></code> (post forward hook).</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A module that contains parameters/buffers that are all <code class="docutils literal notranslate"><span class="pre">DTensor</span></code> s.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><p id="torch.nn.Module"/><p id="torch.nn.Module"/><a class="reference internal" href="generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.modules.module.Module"><em>Module</em></a></p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>When initialize the DeviceMesh with the <code class="docutils literal notranslate"><span class="pre">xla</span></code> device_type, <code class="docutils literal notranslate"><span class="pre">distribute_module</span></code>
return nn.Module with PyTorch/XLA SPMD annotated parameters. See
<a class="reference external" href="https://github.com/pytorch/pytorch/issues/92909">this issue</a>
for more details. The XLA integration is experimental and subject to change.</p>
</div>
</dd></dl>

</section>
<section id="dtensor-factory-functions">
<h3>DTensor Factory Functions<a class="headerlink" href="#dtensor-factory-functions" title="Link to this heading">#</a></h3>
<p>DTensor also provides dedicated tensor factory functions to allow creating <a class="reference internal" href="#torch.distributed.tensor.DTensor" title="torch.distributed.tensor.DTensor"><code class="xref py py-class docutils literal notranslate"><span class="pre">DTensor</span></code></a> directly
using torch.Tensor like factory function APIs (i.e. torch.ones, torch.empty, etc), by additionally
specifying the <code class="xref py py-class docutils literal notranslate"><span class="pre">DeviceMesh</span></code> and <code class="xref py py-class docutils literal notranslate"><span class="pre">Placement</span></code> for the <a class="reference internal" href="#torch.distributed.tensor.DTensor" title="torch.distributed.tensor.DTensor"><code class="xref py py-class docutils literal notranslate"><span class="pre">DTensor</span></code></a> created:</p>
<dl class="py function">
<dt class="sig sig-object py" id="torch.distributed.tensor.zeros">
<span class="sig-prename descclassname"><span class="pre">torch.distributed.tensor.</span></span><span class="sig-name descname"><span class="pre">zeros</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">requires_grad</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dtype</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">layout</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">torch.strided</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device_mesh</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">placements</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributed/tensor/_api.py#L1249"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.tensor.zeros" title="Link to this definition">#</a></dt>
<dd><p>Returns a <a class="reference internal" href="#torch.distributed.tensor.DTensor" title="torch.distributed.tensor.DTensor"><code class="xref py py-class docutils literal notranslate"><span class="pre">DTensor</span></code></a> filled with the scalar value 0.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>size</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a><em>...</em>) – a sequence of integers defining the shape of the output <a class="reference internal" href="#torch.distributed.tensor.DTensor" title="torch.distributed.tensor.DTensor"><code class="xref py py-class docutils literal notranslate"><span class="pre">DTensor</span></code></a>.
Can be a variable number of arguments or a collection like a list or tuple.
E.g.: zeros(1,2,3..) or zeros([1,2,3..]) or zeros((1,2,3..))</p>
</dd>
<dt class="field-even">Keyword Arguments<span class="colon">:</span></dt>
<dd class="field-even"><ul class="simple">
<li><p><strong>requires_grad</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><em>bool</em></a><em>, </em><em>optional</em>) – If autograd should record operations on the
returned <a class="reference internal" href="#torch.distributed.tensor.DTensor" title="torch.distributed.tensor.DTensor"><code class="xref py py-class docutils literal notranslate"><span class="pre">DTensor</span></code></a>. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p></li>
<li><p><strong>dtype</strong> (<a class="reference internal" href="tensor_attributes.html#torch.dtype" title="torch.dtype"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.dtype</span></code></a>, optional) – the desired data type of returned <a class="reference internal" href="#torch.distributed.tensor.DTensor" title="torch.distributed.tensor.DTensor"><code class="xref py py-class docutils literal notranslate"><span class="pre">DTensor</span></code></a>.
Default: if <code class="docutils literal notranslate"><span class="pre">None</span></code>, uses a global default (see <a class="reference internal" href="generated/torch.set_default_dtype.html#torch.set_default_dtype" title="torch.set_default_dtype"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.set_default_dtype()</span></code></a>).</p></li>
<li><p><strong>layout</strong> (<a class="reference internal" href="tensor_attributes.html#torch.layout" title="torch.layout"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.layout</span></code></a>, optional) – the desired layout of returned <a class="reference internal" href="#torch.distributed.tensor.DTensor" title="torch.distributed.tensor.DTensor"><code class="xref py py-class docutils literal notranslate"><span class="pre">DTensor</span></code></a>.
Default: <code class="docutils literal notranslate"><span class="pre">torch.strided</span></code>.</p></li>
<li><p><strong>device_mesh</strong> – <code class="xref py py-class docutils literal notranslate"><span class="pre">DeviceMesh</span></code> type, contains the mesh info of ranks</p></li>
<li><p><strong>placements</strong> – a sequence of <code class="xref py py-class docutils literal notranslate"><span class="pre">Placement</span></code> type: <code class="docutils literal notranslate"><span class="pre">Shard</span></code>, <code class="docutils literal notranslate"><span class="pre">Replicate</span></code></p></li>
</ul>
</dd>
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>A <a class="reference internal" href="#torch.distributed.tensor.DTensor" title="torch.distributed.tensor.DTensor"><code class="xref py py-class docutils literal notranslate"><span class="pre">DTensor</span></code></a> object on each rank</p>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p><a class="reference internal" href="#torch.distributed.tensor.DTensor" title="torch.distributed.tensor.DTensor"><em>DTensor</em></a></p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="torch.distributed.tensor.ones">
<span class="sig-prename descclassname"><span class="pre">torch.distributed.tensor.</span></span><span class="sig-name descname"><span class="pre">ones</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dtype</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">layout</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">torch.strided</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">requires_grad</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device_mesh</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">placements</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributed/tensor/_api.py#L1028"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.tensor.ones" title="Link to this definition">#</a></dt>
<dd><p>Returns a <a class="reference internal" href="#torch.distributed.tensor.DTensor" title="torch.distributed.tensor.DTensor"><code class="xref py py-class docutils literal notranslate"><span class="pre">DTensor</span></code></a> filled with the scalar value 1, with the shape defined
by the variable argument <code class="docutils literal notranslate"><span class="pre">size</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>size</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a><em>...</em>) – a sequence of integers defining the shape of the output <a class="reference internal" href="#torch.distributed.tensor.DTensor" title="torch.distributed.tensor.DTensor"><code class="xref py py-class docutils literal notranslate"><span class="pre">DTensor</span></code></a>.
Can be a variable number of arguments or a collection like a list or tuple.
E.g.: ones(1,2,3..) or ones([1,2,3..]) or ones((1,2,3..))</p>
</dd>
<dt class="field-even">Keyword Arguments<span class="colon">:</span></dt>
<dd class="field-even"><ul class="simple">
<li><p><strong>dtype</strong> (<a class="reference internal" href="tensor_attributes.html#torch.dtype" title="torch.dtype"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.dtype</span></code></a>, optional) – the desired data type of returned <a class="reference internal" href="#torch.distributed.tensor.DTensor" title="torch.distributed.tensor.DTensor"><code class="xref py py-class docutils literal notranslate"><span class="pre">DTensor</span></code></a>.
Default: if <code class="docutils literal notranslate"><span class="pre">None</span></code>, uses a global default (see <a class="reference internal" href="generated/torch.set_default_dtype.html#torch.set_default_dtype" title="torch.set_default_dtype"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.set_default_dtype()</span></code></a>).</p></li>
<li><p><strong>layout</strong> (<a class="reference internal" href="tensor_attributes.html#torch.layout" title="torch.layout"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.layout</span></code></a>, optional) – the desired layout of returned DTensor.
Default: <code class="docutils literal notranslate"><span class="pre">torch.strided</span></code>.</p></li>
<li><p><strong>requires_grad</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><em>bool</em></a><em>, </em><em>optional</em>) – If autograd should record operations on the
returned <a class="reference internal" href="#torch.distributed.tensor.DTensor" title="torch.distributed.tensor.DTensor"><code class="xref py py-class docutils literal notranslate"><span class="pre">DTensor</span></code></a>. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p></li>
<li><p><strong>device_mesh</strong> – <code class="xref py py-class docutils literal notranslate"><span class="pre">DeviceMesh</span></code> type, contains the mesh info of ranks</p></li>
<li><p><strong>placements</strong> – a sequence of <code class="xref py py-class docutils literal notranslate"><span class="pre">Placement</span></code> type: <code class="docutils literal notranslate"><span class="pre">Shard</span></code>, <code class="docutils literal notranslate"><span class="pre">Replicate</span></code></p></li>
</ul>
</dd>
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>A <a class="reference internal" href="#torch.distributed.tensor.DTensor" title="torch.distributed.tensor.DTensor"><code class="xref py py-class docutils literal notranslate"><span class="pre">DTensor</span></code></a> object on each rank</p>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p><a class="reference internal" href="#torch.distributed.tensor.DTensor" title="torch.distributed.tensor.DTensor"><em>DTensor</em></a></p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="torch.distributed.tensor.empty">
<span class="sig-prename descclassname"><span class="pre">torch.distributed.tensor.</span></span><span class="sig-name descname"><span class="pre">empty</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dtype</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">layout</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">torch.strided</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">requires_grad</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device_mesh</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">placements</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributed/tensor/_api.py#L1071"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.tensor.empty" title="Link to this definition">#</a></dt>
<dd><p>Returns a <a class="reference internal" href="#torch.distributed.tensor.DTensor" title="torch.distributed.tensor.DTensor"><code class="xref py py-class docutils literal notranslate"><span class="pre">DTensor</span></code></a> filled with uninitialized data. The shape of the <a class="reference internal" href="#torch.distributed.tensor.DTensor" title="torch.distributed.tensor.DTensor"><code class="xref py py-class docutils literal notranslate"><span class="pre">DTensor</span></code></a>
is defined by the variable argument <code class="docutils literal notranslate"><span class="pre">size</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>size</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a><em>...</em>) – a sequence of integers defining the shape of the output <a class="reference internal" href="#torch.distributed.tensor.DTensor" title="torch.distributed.tensor.DTensor"><code class="xref py py-class docutils literal notranslate"><span class="pre">DTensor</span></code></a>.
Can be a variable number of arguments or a collection like a list or tuple.
E.g.: empty(1,2,3..) or empty([1,2,3..]) or empty((1,2,3..))</p>
</dd>
<dt class="field-even">Keyword Arguments<span class="colon">:</span></dt>
<dd class="field-even"><ul class="simple">
<li><p><strong>dtype</strong> (<a class="reference internal" href="tensor_attributes.html#torch.dtype" title="torch.dtype"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.dtype</span></code></a>, optional) – the desired data type of returned <a class="reference internal" href="#torch.distributed.tensor.DTensor" title="torch.distributed.tensor.DTensor"><code class="xref py py-class docutils literal notranslate"><span class="pre">DTensor</span></code></a>.
Default: if <code class="docutils literal notranslate"><span class="pre">None</span></code>, uses a global default (see <a class="reference internal" href="generated/torch.set_default_dtype.html#torch.set_default_dtype" title="torch.set_default_dtype"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.set_default_dtype()</span></code></a>).        layout (<a class="reference internal" href="tensor_attributes.html#torch.layout" title="torch.layout"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.layout</span></code></a>, optional): the desired layout of returned <a class="reference internal" href="#torch.distributed.tensor.DTensor" title="torch.distributed.tensor.DTensor"><code class="xref py py-class docutils literal notranslate"><span class="pre">DTensor</span></code></a>.
Default: <code class="docutils literal notranslate"><span class="pre">torch.strided</span></code>.</p></li>
<li><p><strong>requires_grad</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><em>bool</em></a><em>, </em><em>optional</em>) – If autograd should record operations on the
returned <a class="reference internal" href="#torch.distributed.tensor.DTensor" title="torch.distributed.tensor.DTensor"><code class="xref py py-class docutils literal notranslate"><span class="pre">DTensor</span></code></a>. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p></li>
<li><p><strong>device_mesh</strong> – <code class="xref py py-class docutils literal notranslate"><span class="pre">DeviceMesh</span></code> type, contains the mesh info of ranks</p></li>
<li><p><strong>placements</strong> – a sequence of <code class="xref py py-class docutils literal notranslate"><span class="pre">Placement</span></code> type: <code class="docutils literal notranslate"><span class="pre">Shard</span></code>, <code class="docutils literal notranslate"><span class="pre">Replicate</span></code></p></li>
</ul>
</dd>
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>A <a class="reference internal" href="#torch.distributed.tensor.DTensor" title="torch.distributed.tensor.DTensor"><code class="xref py py-class docutils literal notranslate"><span class="pre">DTensor</span></code></a> object on each rank</p>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p><a class="reference internal" href="#torch.distributed.tensor.DTensor" title="torch.distributed.tensor.DTensor"><em>DTensor</em></a></p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="torch.distributed.tensor.full">
<span class="sig-prename descclassname"><span class="pre">torch.distributed.tensor.</span></span><span class="sig-name descname"><span class="pre">full</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">fill_value</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dtype</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">layout</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">torch.strided</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">requires_grad</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device_mesh</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">placements</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributed/tensor/_api.py#L1114"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.tensor.full" title="Link to this definition">#</a></dt>
<dd><p>Returns a <a class="reference internal" href="#torch.distributed.tensor.DTensor" title="torch.distributed.tensor.DTensor"><code class="xref py py-class docutils literal notranslate"><span class="pre">DTensor</span></code></a> filled with <code class="docutils literal notranslate"><span class="pre">fill_value</span></code> according to <code class="docutils literal notranslate"><span class="pre">device_mesh</span></code> and
<code class="docutils literal notranslate"><span class="pre">placements</span></code>, with the shape defined by the argument <code class="docutils literal notranslate"><span class="pre">size</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>size</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a><em>...</em>) – a sequence of integers defining the shape of the output <a class="reference internal" href="#torch.distributed.tensor.DTensor" title="torch.distributed.tensor.DTensor"><code class="xref py py-class docutils literal notranslate"><span class="pre">DTensor</span></code></a>.
Can be a variable number of arguments or a collection like a list or tuple.
E.g.: ones(1,2,3..) or ones([1,2,3..]) or ones((1,2,3..))</p></li>
<li><p><strong>fill_value</strong> (<em>Scalar</em>) – the value to fill the output tensor with.</p></li>
</ul>
</dd>
<dt class="field-even">Keyword Arguments<span class="colon">:</span></dt>
<dd class="field-even"><ul class="simple">
<li><p><strong>dtype</strong> (<a class="reference internal" href="tensor_attributes.html#torch.dtype" title="torch.dtype"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.dtype</span></code></a>, optional) – the desired data type of returned <a class="reference internal" href="#torch.distributed.tensor.DTensor" title="torch.distributed.tensor.DTensor"><code class="xref py py-class docutils literal notranslate"><span class="pre">DTensor</span></code></a>.
Default: if <code class="docutils literal notranslate"><span class="pre">None</span></code>, uses a global default (see <a class="reference internal" href="generated/torch.set_default_dtype.html#torch.set_default_dtype" title="torch.set_default_dtype"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.set_default_dtype()</span></code></a>).</p></li>
<li><p><strong>layout</strong> (<a class="reference internal" href="tensor_attributes.html#torch.layout" title="torch.layout"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.layout</span></code></a>, optional) – the desired layout of returned DTensor.
Default: <code class="docutils literal notranslate"><span class="pre">torch.strided</span></code>.</p></li>
<li><p><strong>requires_grad</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><em>bool</em></a><em>, </em><em>optional</em>) – If autograd should record operations on the
returned <a class="reference internal" href="#torch.distributed.tensor.DTensor" title="torch.distributed.tensor.DTensor"><code class="xref py py-class docutils literal notranslate"><span class="pre">DTensor</span></code></a>. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p></li>
<li><p><strong>device_mesh</strong> – <code class="xref py py-class docutils literal notranslate"><span class="pre">DeviceMesh</span></code> type, contains the mesh info of ranks.</p></li>
<li><p><strong>placements</strong> – a sequence of <code class="xref py py-class docutils literal notranslate"><span class="pre">Placement</span></code> type: <code class="docutils literal notranslate"><span class="pre">Shard</span></code>, <code class="docutils literal notranslate"><span class="pre">Replicate</span></code></p></li>
</ul>
</dd>
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>A <a class="reference internal" href="#torch.distributed.tensor.DTensor" title="torch.distributed.tensor.DTensor"><code class="xref py py-class docutils literal notranslate"><span class="pre">DTensor</span></code></a> object on each rank</p>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p><a class="reference internal" href="#torch.distributed.tensor.DTensor" title="torch.distributed.tensor.DTensor"><em>DTensor</em></a></p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="torch.distributed.tensor.rand">
<span class="sig-prename descclassname"><span class="pre">torch.distributed.tensor.</span></span><span class="sig-name descname"><span class="pre">rand</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">requires_grad</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dtype</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">layout</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">torch.strided</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device_mesh</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">placements</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributed/tensor/_api.py#L1161"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.tensor.rand" title="Link to this definition">#</a></dt>
<dd><p>Returns a <a class="reference internal" href="#torch.distributed.tensor.DTensor" title="torch.distributed.tensor.DTensor"><code class="xref py py-class docutils literal notranslate"><span class="pre">DTensor</span></code></a> filled with random numbers from a uniform distribution
on the interval <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">1)</span></code>. The shape of the tensor is defined by the variable
argument <code class="docutils literal notranslate"><span class="pre">size</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>size</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a><em>...</em>) – a sequence of integers defining the shape of the output <a class="reference internal" href="#torch.distributed.tensor.DTensor" title="torch.distributed.tensor.DTensor"><code class="xref py py-class docutils literal notranslate"><span class="pre">DTensor</span></code></a>.
Can be a variable number of arguments or a collection like a list or tuple.
E.g.: ones(1,2,3..) or ones([1,2,3..]) or ones((1,2,3..))</p>
</dd>
<dt class="field-even">Keyword Arguments<span class="colon">:</span></dt>
<dd class="field-even"><ul class="simple">
<li><p><strong>dtype</strong> (<a class="reference internal" href="tensor_attributes.html#torch.dtype" title="torch.dtype"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.dtype</span></code></a>, optional) – the desired data type of returned <a class="reference internal" href="#torch.distributed.tensor.DTensor" title="torch.distributed.tensor.DTensor"><code class="xref py py-class docutils literal notranslate"><span class="pre">DTensor</span></code></a>.
Default: if <code class="docutils literal notranslate"><span class="pre">None</span></code>, uses a global default (see <a class="reference internal" href="generated/torch.set_default_dtype.html#torch.set_default_dtype" title="torch.set_default_dtype"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.set_default_dtype()</span></code></a>).</p></li>
<li><p><strong>layout</strong> (<a class="reference internal" href="tensor_attributes.html#torch.layout" title="torch.layout"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.layout</span></code></a>, optional) – the desired layout of returned DTensor.
Default: <code class="docutils literal notranslate"><span class="pre">torch.strided</span></code>.</p></li>
<li><p><strong>requires_grad</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><em>bool</em></a><em>, </em><em>optional</em>) – If autograd should record operations on the
returned <a class="reference internal" href="#torch.distributed.tensor.DTensor" title="torch.distributed.tensor.DTensor"><code class="xref py py-class docutils literal notranslate"><span class="pre">DTensor</span></code></a>. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p></li>
<li><p><strong>device_mesh</strong> – <code class="xref py py-class docutils literal notranslate"><span class="pre">DeviceMesh</span></code> type, contains the mesh info of ranks.</p></li>
<li><p><strong>placements</strong> – a sequence of <code class="xref py py-class docutils literal notranslate"><span class="pre">Placement</span></code> type: <code class="docutils literal notranslate"><span class="pre">Shard</span></code>, <code class="docutils literal notranslate"><span class="pre">Replicate</span></code></p></li>
</ul>
</dd>
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>A <a class="reference internal" href="#torch.distributed.tensor.DTensor" title="torch.distributed.tensor.DTensor"><code class="xref py py-class docutils literal notranslate"><span class="pre">DTensor</span></code></a> object on each rank</p>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p><a class="reference internal" href="#torch.distributed.tensor.DTensor" title="torch.distributed.tensor.DTensor"><em>DTensor</em></a></p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="torch.distributed.tensor.randn">
<span class="sig-prename descclassname"><span class="pre">torch.distributed.tensor.</span></span><span class="sig-name descname"><span class="pre">randn</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">requires_grad</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dtype</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">layout</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">torch.strided</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device_mesh</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">placements</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributed/tensor/_api.py#L1205"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.tensor.randn" title="Link to this definition">#</a></dt>
<dd><p>Returns a <a class="reference internal" href="#torch.distributed.tensor.DTensor" title="torch.distributed.tensor.DTensor"><code class="xref py py-class docutils literal notranslate"><span class="pre">DTensor</span></code></a> filled with random numbers from a normal distribution
with mean 0 and variance 1. The shape of the tensor is defined by the variable
argument <code class="docutils literal notranslate"><span class="pre">size</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>size</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a><em>...</em>) – a sequence of integers defining the shape of the output <a class="reference internal" href="#torch.distributed.tensor.DTensor" title="torch.distributed.tensor.DTensor"><code class="xref py py-class docutils literal notranslate"><span class="pre">DTensor</span></code></a>.
Can be a variable number of arguments or a collection like a list or tuple.
E.g.: ones(1,2,3..) or ones([1,2,3..]) or ones((1,2,3..))</p>
</dd>
<dt class="field-even">Keyword Arguments<span class="colon">:</span></dt>
<dd class="field-even"><ul class="simple">
<li><p><strong>dtype</strong> (<a class="reference internal" href="tensor_attributes.html#torch.dtype" title="torch.dtype"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.dtype</span></code></a>, optional) – the desired data type of returned <a class="reference internal" href="#torch.distributed.tensor.DTensor" title="torch.distributed.tensor.DTensor"><code class="xref py py-class docutils literal notranslate"><span class="pre">DTensor</span></code></a>.
Default: if <code class="docutils literal notranslate"><span class="pre">None</span></code>, uses a global default (see <a class="reference internal" href="generated/torch.set_default_dtype.html#torch.set_default_dtype" title="torch.set_default_dtype"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.set_default_dtype()</span></code></a>).</p></li>
<li><p><strong>layout</strong> (<a class="reference internal" href="tensor_attributes.html#torch.layout" title="torch.layout"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.layout</span></code></a>, optional) – the desired layout of returned DTensor.
Default: <code class="docutils literal notranslate"><span class="pre">torch.strided</span></code>.</p></li>
<li><p><strong>requires_grad</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><em>bool</em></a><em>, </em><em>optional</em>) – If autograd should record operations on the
returned <a class="reference internal" href="#torch.distributed.tensor.DTensor" title="torch.distributed.tensor.DTensor"><code class="xref py py-class docutils literal notranslate"><span class="pre">DTensor</span></code></a>. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p></li>
<li><p><strong>device_mesh</strong> – <code class="xref py py-class docutils literal notranslate"><span class="pre">DeviceMesh</span></code> type, contains the mesh info of ranks.</p></li>
<li><p><strong>placements</strong> – a sequence of <code class="xref py py-class docutils literal notranslate"><span class="pre">Placement</span></code> type: <code class="docutils literal notranslate"><span class="pre">Shard</span></code>, <code class="docutils literal notranslate"><span class="pre">Replicate</span></code></p></li>
</ul>
</dd>
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>A <a class="reference internal" href="#torch.distributed.tensor.DTensor" title="torch.distributed.tensor.DTensor"><code class="xref py py-class docutils literal notranslate"><span class="pre">DTensor</span></code></a> object on each rank</p>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p><a class="reference internal" href="#torch.distributed.tensor.DTensor" title="torch.distributed.tensor.DTensor"><em>DTensor</em></a></p>
</dd>
</dl>
</dd></dl>

</section>
</section>
<section id="module-torch.distributed.tensor.debug">
<span id="debugging"></span><h2>Debugging<a class="headerlink" href="#module-torch.distributed.tensor.debug" title="Link to this heading">#</a></h2>
<section id="logging">
<h3>Logging<a class="headerlink" href="#logging" title="Link to this heading">#</a></h3>
<p>When launching the program, you can turn on additional logging using the <cite>TORCH_LOGS</cite> environment variable from
<a class="reference external" href="https://pytorch.org/docs/main/logging.html#module-torch._logging">torch._logging</a> :</p>
<ul class="simple">
<li><p><cite>TORCH_LOGS=+dtensor</cite> will display <cite>logging.DEBUG</cite> messages and all levels above it.</p></li>
<li><p><cite>TORCH_LOGS=dtensor</cite> will display <cite>logging.INFO</cite> messages and above.</p></li>
<li><p><cite>TORCH_LOGS=-dtensor</cite> will display <cite>logging.WARNING</cite> messages and above.</p></li>
</ul>
</section>
<section id="debugging-tools">
<h3>Debugging Tools<a class="headerlink" href="#debugging-tools" title="Link to this heading">#</a></h3>
<p>To debug the program that applied DTensor, and understand more details about what collectives happened under the
hood, DTensor provides a <a class="reference internal" href="#torch.distributed.tensor.debug.CommDebugMode" title="torch.distributed.tensor.debug.CommDebugMode"><code class="xref py py-class docutils literal notranslate"><span class="pre">CommDebugMode</span></code></a>:</p>
<dl class="py class">
<dt class="sig sig-object py" id="torch.distributed.tensor.debug.CommDebugMode">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torch.distributed.tensor.debug.</span></span><span class="sig-name descname"><span class="pre">CommDebugMode</span></span><a class="headerlink" href="#torch.distributed.tensor.debug.CommDebugMode" title="Link to this definition">#</a></dt>
<dd><p><a class="reference internal" href="#torch.distributed.tensor.debug.CommDebugMode" title="torch.distributed.tensor.debug.CommDebugMode"><code class="xref py py-class docutils literal notranslate"><span class="pre">CommDebugMode</span></code></a> is a context manager that counts the number of
functional collectives within its context. It does this using a
<code class="docutils literal notranslate"><span class="pre">TorchDispatchMode</span></code>.</p>
<p>Example usage</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">mod</span> <span class="o">=</span> <span class="o">...</span>
<span class="n">comm_mode</span> <span class="o">=</span> <span class="n">CommDebugMode</span><span class="p">()</span>
<span class="k">with</span> <span class="n">comm_mode</span><span class="p">:</span>
    <span class="n">mod</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">comm_mode</span><span class="o">.</span><span class="n">get_comm_counts</span><span class="p">())</span>
</pre></div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="torch.distributed.tensor.debug.CommDebugMode.generate_comm_debug_tracing_table">
<span class="sig-name descname"><span class="pre">generate_comm_debug_tracing_table</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">noise_level</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">3</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torch/distributed/tensor/debug/_comm_mode.html#CommDebugMode.generate_comm_debug_tracing_table"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributed/tensor/debug/_comm_mode.py#L402"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.tensor.debug.CommDebugMode.generate_comm_debug_tracing_table" title="Link to this definition">#</a></dt>
<dd><p>Generates detailed table displaying operations and collective tracing information
on a module level. Amount of information is dependent on noise_level</p>
<ol class="arabic simple" start="0">
<li><p>prints module-level collective counts</p></li>
<li><p>prints dTensor operations not included in trivial operations, module information</p></li>
<li><p>prints operations not included in trivial operations</p></li>
<li><p>prints all operations</p></li>
</ol>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributed.tensor.debug.CommDebugMode.generate_json_dump">
<span class="sig-name descname"><span class="pre">generate_json_dump</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">file_name</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'comm_mode_log.json'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">noise_level</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">3</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torch/distributed/tensor/debug/_comm_mode.html#CommDebugMode.generate_json_dump"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributed/tensor/debug/_comm_mode.py#L254"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.tensor.debug.CommDebugMode.generate_json_dump" title="Link to this definition">#</a></dt>
<dd><p>Creates json file used to build browser visual
0. prints module-level collective counts
1. prints dTensor operations not included in trivial operations
2. prints operations not included in trivial operations
3. prints all operations</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributed.tensor.debug.CommDebugMode.get_comm_counts">
<span class="sig-name descname"><span class="pre">get_comm_counts</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torch/distributed/tensor/debug/_comm_mode.html#CommDebugMode.get_comm_counts"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributed/tensor/debug/_comm_mode.py#L570"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.tensor.debug.CommDebugMode.get_comm_counts" title="Link to this definition">#</a></dt>
<dd><p>Returns the communication counts as a dictionary.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>The communication counts as a dictionary.</p>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p>Dict[Any, <a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)">int</a>]</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributed.tensor.debug.CommDebugMode.get_parameter_info">
<span class="sig-name descname"><span class="pre">get_parameter_info</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torch/distributed/tensor/debug/_comm_mode.html#CommDebugMode.get_parameter_info"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributed/tensor/debug/_comm_mode.py#L578"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.tensor.debug.CommDebugMode.get_parameter_info" title="Link to this definition">#</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Dict" title="(in Python v3.13)"><em>Dict</em></a>[<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)">str</a>, <a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Dict" title="(in Python v3.13)"><em>Dict</em></a>[<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)">str</a>, <a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.13)"><em>Any</em></a>]]</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributed.tensor.debug.CommDebugMode.get_sharding_info">
<span class="sig-name descname"><span class="pre">get_sharding_info</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torch/distributed/tensor/debug/_comm_mode.html#CommDebugMode.get_sharding_info"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributed/tensor/debug/_comm_mode.py#L581"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.tensor.debug.CommDebugMode.get_sharding_info" title="Link to this definition">#</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Dict" title="(in Python v3.13)"><em>Dict</em></a>[<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)">str</a>, <a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Dict" title="(in Python v3.13)"><em>Dict</em></a>[<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)">str</a>, <a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.13)"><em>Any</em></a>]]</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributed.tensor.debug.CommDebugMode.get_total_counts">
<span class="sig-name descname"><span class="pre">get_total_counts</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torch/distributed/tensor/debug/_comm_mode.html#CommDebugMode.get_total_counts"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributed/tensor/debug/_comm_mode.py#L567"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.tensor.debug.CommDebugMode.get_total_counts" title="Link to this definition">#</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)">int</a></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributed.tensor.debug.CommDebugMode.log_comm_debug_tracing_table_to_file">
<span class="sig-name descname"><span class="pre">log_comm_debug_tracing_table_to_file</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">file_name</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'comm_mode_log.txt'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">noise_level</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">3</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torch/distributed/tensor/debug/_comm_mode.html#CommDebugMode.log_comm_debug_tracing_table_to_file"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributed/tensor/debug/_comm_mode.py#L601"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.tensor.debug.CommDebugMode.log_comm_debug_tracing_table_to_file" title="Link to this definition">#</a></dt>
<dd><p>Alternative to console CommDebugMode output, writes to file specified by the user</p>
</dd></dl>

</dd></dl>

<p>To visualize the sharding of a DTensor that have less than 3 dimensions, DTensor provides <a class="reference internal" href="#torch.distributed.tensor.debug.visualize_sharding" title="torch.distributed.tensor.debug.visualize_sharding"><code class="xref py py-meth docutils literal notranslate"><span class="pre">visualize_sharding()</span></code></a>:</p>
<dl class="py function">
<dt class="sig sig-object py" id="torch.distributed.tensor.debug.visualize_sharding">
<span class="sig-prename descclassname"><span class="pre">torch.distributed.tensor.debug.</span></span><span class="sig-name descname"><span class="pre">visualize_sharding</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dtensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">header</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">''</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributed/tensor/debug/_visualize_sharding.py#L136"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.tensor.debug.visualize_sharding" title="Link to this definition">#</a></dt>
<dd><p>Visualizes sharding in the terminal for <code class="xref py py-class docutils literal notranslate"><span class="pre">DTensor</span></code> that are 1D or 2D.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This requires the <code class="docutils literal notranslate"><span class="pre">tabulate</span></code> package. No sharding info will be printed for empty tensors</p>
</div>
</dd></dl>

</section>
</section>
<section id="experimental-features">
<h2>Experimental Features<a class="headerlink" href="#experimental-features" title="Link to this heading">#</a></h2>
<p><code class="docutils literal notranslate"><span class="pre">DTensor</span></code> also provides a set of experimental features. These features are either in prototyping stage, or the basic
functionality is done and but looking for user feedbacks. Please submit a issue to PyTorch if you have feedbacks to
these features.</p>
<dl class="py function" id="module-torch.distributed.tensor.experimental">
<dt class="sig sig-object py" id="torch.distributed.tensor.experimental.context_parallel">
<span class="sig-prename descclassname"><span class="pre">torch.distributed.tensor.experimental.</span></span><span class="sig-name descname"><span class="pre">context_parallel</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">mesh</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">buffers</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">buffer_seq_dims</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">no_restore_buffers</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributed/tensor/experimental/_attention.py#L1211"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.tensor.experimental.context_parallel" title="Link to this definition">#</a></dt>
<dd><p><code class="docutils literal notranslate"><span class="pre">context_parallel</span></code> is an experimental API to enable context
parallelism (CP). This API performs two actions: 1) patch the SDPA
(<code class="docutils literal notranslate"><span class="pre">torch.nn.functional.scaled_dot_product_attention</span></code>) with the CP-enabled
one, 2) shard <code class="docutils literal notranslate"><span class="pre">buffers</span></code> along the sequence dimension and each rank will
preserve the corresponding shard according <code class="docutils literal notranslate"><span class="pre">mesh</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>mesh</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">DeviceMesh</span></code>) – the device mesh for the context parallelism.</p></li>
<li><p><strong>buffers</strong> (<em>Optional</em><em>[</em><em>List</em><em>[</em><a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>torch.Tensor</em></a><em>]</em><em>]</em>) – buffers that the usage depend
on the sequence dimension. Examples are input batch, labels and
positional embedding buffers. These buffers must be sharded along
the sequence dimension to ensure the accuracy. The sharding will
happen in-place, the buffer’s shape will change within the context.
The buffers will be restored after the context finishes.
<code class="docutils literal notranslate"><span class="pre">no_restore_buffers</span></code> can be used to specify which buffers don’t
need to be restored. Note that <code class="docutils literal notranslate"><span class="pre">buffers</span></code> should not contain any
nn.Parameter.</p></li>
<li><p><strong>buffer_seq_dims</strong> (<em>Optional</em><em>[</em><em>List</em><em>[</em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a><em>]</em><em>]</em>) – the sequence dimensions of <code class="docutils literal notranslate"><span class="pre">buffers</span></code>.</p></li>
<li><p><strong>no_restore_buffers</strong> (<em>Optional</em><em>[</em><em>Set</em><em>[</em><a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>torch.Tensor</em></a><em>]</em><em>]</em>) – buffers in these set
won’t be restored after the context exits. This set must be a subset
of <code class="docutils literal notranslate"><span class="pre">buffers</span></code>. If the buffers won’t be used after the context exits,
these buffers can be put in this list to avoid extra restore time.</p></li>
</ul>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Generator" title="(in Python v3.13)"><em>Generator</em></a>[None, None, None]</p>
</dd>
</dl>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p><cite>torch.distributed._tensor.experimental.attention.context_parallel</cite> is a
prototype feature in PyTorch. The API is subject to change.</p>
</div>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="torch.distributed.tensor.experimental.local_map">
<span class="sig-prename descclassname"><span class="pre">torch.distributed.tensor.experimental.</span></span><span class="sig-name descname"><span class="pre">local_map</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">func</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">out_placements</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">in_placements</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device_mesh</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">redistribute_inputs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributed/tensor/experimental/_func_map.py#L24"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.tensor.experimental.local_map" title="Link to this definition">#</a></dt>
<dd><p><a class="reference internal" href="#torch.distributed.tensor.experimental.local_map" title="torch.distributed.tensor.experimental.local_map"><code class="xref py py-meth docutils literal notranslate"><span class="pre">local_map()</span></code></a> is an experimental API that allows users to pass <code class="xref py py-class docutils literal notranslate"><span class="pre">DTensor</span></code> s
to a function that is written to be applied on <code class="docutils literal notranslate"><span class="pre">torch.Tensor</span></code> s. It is done by extracting
the local components of <code class="xref py py-class docutils literal notranslate"><span class="pre">DTensor</span></code>, call the function, and wrap the outputs to
<code class="xref py py-class docutils literal notranslate"><span class="pre">DTensor</span></code> according to the <code class="docutils literal notranslate"><span class="pre">out_placements</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>func</strong> (<em>Callable</em>) – the function to be applied on each local shard of
<code class="xref py py-class docutils literal notranslate"><span class="pre">DTensor</span></code> s.</p></li>
<li><p><strong>out_placements</strong> (Union[<cite>PlacementType</cite>, Tuple[<cite>PlacementType</cite>, …]]) – the desired placements of the <code class="xref py py-class docutils literal notranslate"><span class="pre">DTensor</span></code> s in <code class="docutils literal notranslate"><span class="pre">func</span></code>’s flattened output.
If the flattened <code class="docutils literal notranslate"><span class="pre">output</span></code> is a single value, the <code class="docutils literal notranslate"><span class="pre">out_placements</span></code> should be
of type <cite>PlacementType</cite>. Otherwise if the flattened <code class="docutils literal notranslate"><span class="pre">output</span></code> has multiple
values, the <code class="docutils literal notranslate"><span class="pre">out_placements</span></code> should be a tuple of <cite>PlacementType</cite> values 1:1
mapping to the flattened <code class="docutils literal notranslate"><span class="pre">output</span></code>.
Besides, for <code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code> output, we use <cite>PlacementType</cite> as its
placements (a <cite>Tuple[Placement]</cite> value). For non-Tensor output, the <cite>PlacementType</cite>
should be <cite>None</cite>.
Note that the only exception is when no <code class="xref py py-class docutils literal notranslate"><span class="pre">DTensor</span></code> argument is passed
in. In this case, even if <cite>out_placements</cite> is not <cite>None</cite>, the result function
should ignore the desired placements because the function is not running with
<code class="xref py py-class docutils literal notranslate"><span class="pre">DTensor</span></code> s.</p></li>
<li><p><strong>in_placements</strong> (Tuple[<cite>PlacementType</cite>, …], optional) – the required placements of the <code class="xref py py-class docutils literal notranslate"><span class="pre">DTensor</span></code> s in the flattened inputs of <code class="docutils literal notranslate"><span class="pre">func</span></code>.
If <code class="docutils literal notranslate"><span class="pre">in_placements</span></code> is specified, <a class="reference internal" href="#torch.distributed.tensor.experimental.local_map" title="torch.distributed.tensor.experimental.local_map"><code class="xref py py-meth docutils literal notranslate"><span class="pre">local_map()</span></code></a> would examine whether the
placements of each <code class="xref py py-class docutils literal notranslate"><span class="pre">DTensor</span></code> argument is the same as the required
placements or not. If the placements are not the same and
<code class="docutils literal notranslate"><span class="pre">redistribute_inputs</span></code> is <code class="docutils literal notranslate"><span class="pre">False</span></code>, an exception will be raised. Otherwise if
<code class="docutils literal notranslate"><span class="pre">redistribute_inputs</span></code> is <code class="docutils literal notranslate"><span class="pre">True</span></code>, the argument will be first redistributed to
the required sharding placements before passing its local tensor to <code class="docutils literal notranslate"><span class="pre">func</span></code>.
The only exception is when required placements are not <code class="docutils literal notranslate"><span class="pre">None</span></code> and the
argument is a <a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Tensor</span></code></a>. In this case, the placements examination
will be skipped and the argument will be directly passed to <code class="docutils literal notranslate"><span class="pre">func</span></code>.
If <code class="docutils literal notranslate"><span class="pre">in_placements</span></code> is <code class="docutils literal notranslate"><span class="pre">None</span></code>, no placements examination will be performed.
Default: None</p></li>
<li><p><strong>device_mesh</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">DeviceMesh</span></code>, optional) – the device mesh that all the <code class="xref py py-class docutils literal notranslate"><span class="pre">DTensor</span></code> s are placed on. If not
specified, this will be inferred from the input <code class="xref py py-class docutils literal notranslate"><span class="pre">DTensor</span></code> s’ device
mesh. <cite>local_map</cite> requires every <code class="xref py py-class docutils literal notranslate"><span class="pre">DTensor</span></code> s to be placed on the same
device mesh. Default: None.</p></li>
<li><p><strong>redistribute_inputs</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><em>bool</em></a><em>, </em><em>optional</em>) – the bool value indicating whether to reshard the input <code class="xref py py-class docutils literal notranslate"><span class="pre">DTensor</span></code> s when
their placements are different from the required input placements. If this
value is <code class="docutils literal notranslate"><span class="pre">False</span></code> and some <code class="xref py py-class docutils literal notranslate"><span class="pre">DTensor</span></code> input has a different placement,
an exception will be raised. Default: False.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A <code class="docutils literal notranslate"><span class="pre">Callable</span></code> that applies <code class="docutils literal notranslate"><span class="pre">func</span></code> to each local shard of the input <code class="xref py py-class docutils literal notranslate"><span class="pre">DTensor</span></code>
and returns a <code class="xref py py-class docutils literal notranslate"><span class="pre">DTensor</span></code> constructed from the return value of <code class="docutils literal notranslate"><span class="pre">func</span></code>.</p>
</dd>
<dt class="field-odd">Raises<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><a class="reference external" href="https://docs.python.org/3/library/exceptions.html#AssertionError" title="(in Python v3.13)"><strong>AssertionError</strong></a> – If the input <code class="xref py py-class docutils literal notranslate"><span class="pre">DTensor</span></code> is not placed on the same device
    mesh, or if they are placed on a different device mesh than the <code class="docutils literal notranslate"><span class="pre">device_mesh</span></code>
    argument passed in.</p></li>
<li><p><a class="reference external" href="https://docs.python.org/3/library/exceptions.html#AssertionError" title="(in Python v3.13)"><strong>AssertionError</strong></a> – For any non-DTensor output, we require its corresponding
    output placement in <code class="docutils literal notranslate"><span class="pre">out_placements</span></code> be None. An AssertionError will be raised
    if this is not the case.</p></li>
<li><p><a class="reference external" href="https://docs.python.org/3/library/exceptions.html#ValueError" title="(in Python v3.13)"><strong>ValueError</strong></a> – If <code class="docutils literal notranslate"><span class="pre">redistribute_inputs=False</span></code> but the input <code class="xref py py-class docutils literal notranslate"><span class="pre">DTensor</span></code> needs
    a redistribution according to <code class="docutils literal notranslate"><span class="pre">in_placements</span></code>.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Example</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">def</span><span class="w"> </span><span class="nf">mm_allreduce_forward</span><span class="p">(</span><span class="n">device_mesh</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">partial_sum_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mm</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">reduced_tensor</span> <span class="o">=</span> <span class="n">funcol</span><span class="o">.</span><span class="n">all_reduce</span><span class="p">(</span><span class="n">partial_sum_tensor</span><span class="p">,</span> <span class="s2">&quot;sum&quot;</span><span class="p">,</span> <span class="n">device_mesh</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">return</span> <span class="n">reduced_tensor</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">W</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">Y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mm</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">row_wise</span> <span class="o">=</span> <span class="p">[</span><span class="n">Shard</span><span class="p">(</span><span class="mi">0</span><span class="p">)]</span>  <span class="c1"># row-wise sharding placements on 1-d mesh</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">col_wise</span> <span class="o">=</span> <span class="p">[</span><span class="n">Shard</span><span class="p">(</span><span class="mi">1</span><span class="p">)]</span>  <span class="c1"># col-wise sharding placements on 1-d mesh</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># local_mm_allreduce_forward is the function wrapped with DTensor/Tensor convertion</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">local_mm_allreduce_forward</span> <span class="o">=</span> <span class="n">local_map</span><span class="p">(</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">mm_allreduce_forward</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">out_placements</span><span class="o">=</span><span class="p">[</span><span class="n">Replicate</span><span class="p">()],</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">in_placements</span><span class="o">=</span><span class="p">[</span><span class="n">col_wise</span><span class="p">,</span> <span class="n">row_wise</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">device_mesh</span><span class="o">=</span><span class="n">device_mesh</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">W_dt</span> <span class="o">=</span> <span class="n">distribute_tensor</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">device_mesh</span><span class="p">,</span> <span class="p">(</span><span class="n">col_wise</span><span class="p">))</span>  <span class="c1"># col-wisely sharded W tensor</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X_dt</span> <span class="o">=</span> <span class="n">distribute_tensor</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">device_mesh</span><span class="p">,</span> <span class="p">(</span><span class="n">row_wise</span><span class="p">))</span>  <span class="c1"># row-wisely sharded X tensor</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">Y_dt</span> <span class="o">=</span> <span class="n">local_mm_allreduce_forward</span><span class="p">(</span><span class="n">device_mesh</span><span class="p">,</span> <span class="n">W_dt</span><span class="p">,</span> <span class="n">X_dt</span><span class="p">)</span>  <span class="c1"># apply local_mm_allreduce_forward to DTensors</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This API is currently experimental and subject to change</p>
</div>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="torch.distributed.tensor.experimental.register_sharding">
<span class="sig-prename descclassname"><span class="pre">torch.distributed.tensor.experimental.</span></span><span class="sig-name descname"><span class="pre">register_sharding</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">op</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributed/tensor/experimental/_register_sharding.py#L24"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.tensor.experimental.register_sharding" title="Link to this definition">#</a></dt>
<dd><p><a class="reference internal" href="#torch.distributed.tensor.experimental.register_sharding" title="torch.distributed.tensor.experimental.register_sharding"><code class="xref py py-meth docutils literal notranslate"><span class="pre">register_sharding()</span></code></a> is an experimental API that allows users to register sharding
strategies for an operator when the tensor inputs and outputs are DTensor.
It can be useful when: (1) there doesn’t exist a default sharding strategy for <code class="docutils literal notranslate"><span class="pre">op</span></code>,
e.g. when <code class="docutils literal notranslate"><span class="pre">op</span></code> is a custom operator that is not supported by <code class="xref py py-class docutils literal notranslate"><span class="pre">DTensor</span></code>; (2)
when users would like to overwrite default sharding strategies of existing operators.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>op</strong> (<em>Union</em><em>[</em><em>OpOverload</em><em>, </em><em>List</em><em>[</em><em>OpOverload</em><em>]</em><em>]</em>) – An op or a list of ops to register the customized sharding function.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A function decorator which can be used to wrap a function that defines the sharding
strategy for the operator specified in <code class="docutils literal notranslate"><span class="pre">op</span></code>. The defined sharding strategy will be
registered to DTensor and will override the default sharding strategy if DTensor has
already implemented the operator. The customized sharding function takes the same inputs
as the original op (except that if an arg is a <a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Tensor</span></code></a>, it will be
replaced by a tensor-like object that DTensor uses internally). The function should
return a sequence of 2-tuples, each specifying acceptable output placements and its
corresponding intput placements.</p>
</dd>
</dl>
<p class="rubric">Example</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nd">@register_sharding</span><span class="p">(</span><span class="n">aten</span><span class="o">.</span><span class="n">_softmax</span><span class="o">.</span><span class="n">default</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">def</span><span class="w"> </span><span class="nf">custom_softmax_sharding</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">half_to_float</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">softmax_dim</span> <span class="o">=</span> <span class="n">dim</span> <span class="k">if</span> <span class="n">dim</span> <span class="o">&gt;=</span> <span class="mi">0</span> <span class="k">else</span> <span class="n">dim</span> <span class="o">+</span> <span class="n">x</span><span class="o">.</span><span class="n">ndim</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">acceptable_shardings</span> <span class="o">=</span> <span class="p">[]</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">all_replicate</span> <span class="o">=</span> <span class="p">([</span><span class="n">Replicate</span><span class="p">()],</span> <span class="p">[</span><span class="n">Replicate</span><span class="p">(),</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">acceptable_shardings</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">all_replicate</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">for</span> <span class="n">sharding_dim</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">ndim</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">if</span> <span class="n">sharding_dim</span> <span class="o">!=</span> <span class="n">softmax_dim</span><span class="p">:</span>
<span class="gp">&gt;&gt;&gt; </span>            <span class="n">all_sharded</span> <span class="o">=</span> <span class="p">(</span>
<span class="gp">&gt;&gt;&gt; </span>                <span class="p">[</span><span class="n">Shard</span><span class="p">(</span><span class="n">sharding_dim</span><span class="p">)],</span>
<span class="gp">&gt;&gt;&gt; </span>                <span class="p">[</span><span class="n">Shard</span><span class="p">(</span><span class="n">sharding_dim</span><span class="p">),</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span>            <span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>            <span class="n">acceptable_shardings</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">all_sharded</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">return</span> <span class="n">acceptable_shardings</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This API is currently experimental and subject to change</p>
</div>
</dd></dl>

<span class="target" id="module-torch.distributed.tensor.device_mesh"></span></section>
</section>


                </article>
              
              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
</div>
                </footer>
              
            </div>
            
            
              
                <dialog id="pst-secondary-sidebar-modal"></dialog>
                <div id="pst-secondary-sidebar" class="bd-sidebar-secondary bd-toc">
<div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
<div
    id="pst-page-navigation-heading-2"
    class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> On this page
  </div>
  <nav class="bd-toc-nav page-toc" aria-labelledby="pst-page-navigation-heading-2">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#pytorch-dtensor-distributed-tensor">PyTorch DTensor (Distributed Tensor)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#dtensor-class-apis">DTensor Class APIs</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributed.tensor.DTensor"><code class="docutils literal notranslate"><span class="pre">DTensor</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributed.tensor.DTensor.from_local"><code class="docutils literal notranslate"><span class="pre">DTensor.from_local()</span></code></a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributed.tensor.DTensor.full_tensor"><code class="docutils literal notranslate"><span class="pre">DTensor.full_tensor()</span></code></a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributed.tensor.DTensor.redistribute"><code class="docutils literal notranslate"><span class="pre">DTensor.redistribute()</span></code></a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributed.tensor.DTensor.to_local"><code class="docutils literal notranslate"><span class="pre">DTensor.to_local()</span></code></a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributed.tensor.DTensor.device_mesh"><code class="docutils literal notranslate"><span class="pre">DTensor.device_mesh</span></code></a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributed.tensor.DTensor.placements"><code class="docutils literal notranslate"><span class="pre">DTensor.placements</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#devicemesh-as-the-distributed-communicator">DeviceMesh as the distributed communicator</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#module-torch.distributed.tensor.placement_types">DTensor Placement Types</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributed.tensor.placement_types.Shard"><code class="docutils literal notranslate"><span class="pre">Shard</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributed.tensor.placement_types.Shard.dim"><code class="docutils literal notranslate"><span class="pre">Shard.dim</span></code></a></li>
</ul>
</li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributed.tensor.placement_types.Replicate"><code class="docutils literal notranslate"><span class="pre">Replicate</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributed.tensor.placement_types.Partial"><code class="docutils literal notranslate"><span class="pre">Partial</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributed.tensor.placement_types.Partial.reduce_op"><code class="docutils literal notranslate"><span class="pre">Partial.reduce_op</span></code></a></li>
</ul>
</li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributed.tensor.placement_types.Placement"><code class="docutils literal notranslate"><span class="pre">Placement</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributed.tensor.placement_types.Placement.is_partial"><code class="docutils literal notranslate"><span class="pre">Placement.is_partial()</span></code></a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributed.tensor.placement_types.Placement.is_replicate"><code class="docutils literal notranslate"><span class="pre">Placement.is_replicate()</span></code></a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributed.tensor.placement_types.Placement.is_shard"><code class="docutils literal notranslate"><span class="pre">Placement.is_shard()</span></code></a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#different-ways-to-create-a-dtensor">Different ways to create a DTensor</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#create-dtensor-from-a-logical-torch-tensor">Create DTensor from a logical torch.Tensor</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributed.tensor.distribute_tensor"><code class="docutils literal notranslate"><span class="pre">distribute_tensor()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributed.tensor.distribute_module"><code class="docutils literal notranslate"><span class="pre">distribute_module()</span></code></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#dtensor-factory-functions">DTensor Factory Functions</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributed.tensor.zeros"><code class="docutils literal notranslate"><span class="pre">zeros()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributed.tensor.ones"><code class="docutils literal notranslate"><span class="pre">ones()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributed.tensor.empty"><code class="docutils literal notranslate"><span class="pre">empty()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributed.tensor.full"><code class="docutils literal notranslate"><span class="pre">full()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributed.tensor.rand"><code class="docutils literal notranslate"><span class="pre">rand()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributed.tensor.randn"><code class="docutils literal notranslate"><span class="pre">randn()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#module-torch.distributed.tensor.debug">Debugging</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#logging">Logging</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#debugging-tools">Debugging Tools</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributed.tensor.debug.CommDebugMode"><code class="docutils literal notranslate"><span class="pre">CommDebugMode</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributed.tensor.debug.CommDebugMode.generate_comm_debug_tracing_table"><code class="docutils literal notranslate"><span class="pre">CommDebugMode.generate_comm_debug_tracing_table()</span></code></a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributed.tensor.debug.CommDebugMode.generate_json_dump"><code class="docutils literal notranslate"><span class="pre">CommDebugMode.generate_json_dump()</span></code></a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributed.tensor.debug.CommDebugMode.get_comm_counts"><code class="docutils literal notranslate"><span class="pre">CommDebugMode.get_comm_counts()</span></code></a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributed.tensor.debug.CommDebugMode.get_parameter_info"><code class="docutils literal notranslate"><span class="pre">CommDebugMode.get_parameter_info()</span></code></a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributed.tensor.debug.CommDebugMode.get_sharding_info"><code class="docutils literal notranslate"><span class="pre">CommDebugMode.get_sharding_info()</span></code></a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributed.tensor.debug.CommDebugMode.get_total_counts"><code class="docutils literal notranslate"><span class="pre">CommDebugMode.get_total_counts()</span></code></a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributed.tensor.debug.CommDebugMode.log_comm_debug_tracing_table_to_file"><code class="docutils literal notranslate"><span class="pre">CommDebugMode.log_comm_debug_tracing_table_to_file()</span></code></a></li>
</ul>
</li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributed.tensor.debug.visualize_sharding"><code class="docutils literal notranslate"><span class="pre">visualize_sharding()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#experimental-features">Experimental Features</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributed.tensor.experimental.context_parallel"><code class="docutils literal notranslate"><span class="pre">context_parallel()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributed.tensor.experimental.local_map"><code class="docutils literal notranslate"><span class="pre">local_map()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributed.tensor.experimental.register_sharding"><code class="docutils literal notranslate"><span class="pre">register_sharding()</span></code></a></li>
</ul>
</li>
</ul>
  </nav></div>

  <div class="sidebar-secondary-item">

  
  <div class="tocsection editthispage">
    <a href="https://github.com/pytorch/pytorch/edit/main/docs/source/python-api/distributed.tensor.rst">
      <i class="fa-solid fa-pencil"></i>
      
      
        
          Edit on GitHub
        
      
    </a>
  </div>
</div>

  <div class="sidebar-secondary-item">
    <div class="tocsection sourcelink">
      <a href="../_sources/python-api/distributed.tensor.rst.txt">
        <i class="fa-solid fa-file-lines"></i> Show Source
      </a>
    </div>
</div>

</div>
<div class="sidebar-secondary-item">
 <h6>PyTorch Libraries</h6>
 <ul>
 
  <li><a class="nav-link nav-external" href="https://pytorch.org/audio/stable/">torchaudio</a></li>
 
  <li><a class="nav-link nav-external" href="https://pytorch.org/ao">torchao</a></li>
 
  <li><a class="nav-link nav-external" href="https://pytorch.org/executorch">ExecuTorch</a></li>
 
  <li><a class="nav-link nav-external" href="https://pytorch.org/torchrec">torchrec</a></li>
 
  <li><a class="nav-link nav-external" href="https://pytorch.org/serve/">torchserve</a></li>
 
  <li><a class="nav-link nav-external" href="https://pytorch.org/data">torchdata</a></li>
 
  <li><a class="nav-link nav-external" href="https://pytorch.org/data">torchvision</a></li>
 
  <li><a class="nav-link nav-external" href="https://pytorch.org/xla">PyTorch on XLA Devices</a></li>
 
 </ul>
</div>
</div>
</div>
              
            
          </div>
          <footer class="bd-footer-content">
            
          </footer>
        
      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script defer src="../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf"></script>
<script defer src="../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf"></script>


  <footer class="bd-footer">
<div class="bd-footer__inner bd-page-width">
  
    <div class="footer-items__start">
      
        <div class="footer-item">

  <p class="copyright">
    
      © Copyright PyTorch Contributors.
      <br/>
    
  </p>
</div>
      
        <div class="footer-item">

  <p class="sphinx-version">
    Created using <a href="https://www.sphinx-doc.org/">Sphinx</a> 8.1.0.
    <br/>
  </p>
</div>
      
    </div>
  
  
  
    <div class="footer-items__end">
      
        <div class="footer-item">
<p class="theme-version">
  <!-- # L10n: Setting the PST URL as an argument as this does not need to be localized -->
  Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.16.1.
</p></div>
      
    </div>
  
</div>

  </footer>
  </body>
</html>