

<!DOCTYPE html>


<html lang="en" data-content_root="" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.19: https://docutils.sourceforge.io/" />
<meta property="og:title" content="Probability distributions - torch.distributions" />
<meta property="og:type" content="website" />
<meta property="og:url" content="http://pytorch.org/distributions.html" />
<meta property="og:site_name" content="PyTorch" />
<meta property="og:description" content="The distributions package contains parameterizable probability distributions and sampling functions. This allows the construction of stochastic computation graphs and stochastic gradient estimators..." />
<meta property="og:image" content="https://pytorch.org/docs/stable/_static/img/pytorch-logo-dark.svg" />
<meta property="og:image:alt" content="PyTorch" />
<meta name="description" content="The distributions package contains parameterizable probability distributions and sampling functions. This allows the construction of stochastic computation graphs and stochastic gradient estimators..." />

    <title>Probability distributions - torch.distributions &#8212; PyTorch main documentation</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="_static/css/theme.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/katex.min.css" />
    <link rel="stylesheet" type="text/css" href="_static/katex-math.css" />
    <link rel="stylesheet" type="text/css" href="_static/design-style.1e8bd061cd6da7fc9cf755528e8ffc24.min.css" />
    <link rel="stylesheet" type="text/css" href="_static/css/jit.css" />
    <link rel="stylesheet" type="text/css" href="_static/css/custom.css" />
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/sphinx_highlight.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script src="_static/design-tabs.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'distributions';</script>
    <script>
        DOCUMENTATION_OPTIONS.theme_version = '0.15.4';
        DOCUMENTATION_OPTIONS.theme_switcher_json_url = 'https://pytorch.org/docs/pytorch-versions.json';
        DOCUMENTATION_OPTIONS.theme_switcher_version_match = 'main';
        DOCUMENTATION_OPTIONS.show_version_warning_banner = true;
        </script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="torch.compiler" href="torch.compiler.html" />
    <link rel="prev" title="Distributed Checkpoint - torch.distributed.checkpoint" href="distributed.checkpoint.html" />

  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
<script src="https://code.jquery.com/jquery-3.7.1.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/2.3.1/list.min.js"></script>
<link rel="stylesheet" type="text/css" href="_static/css/theme.css">
<script type="text/javascript" src="_static/js/theme.js"></script>
<link rel="stylesheet" href="_static/webfonts/all.min.css">

<!-- Google Tag Manager (noscript) -->
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-T8XT4PS"
   height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
   <!-- End Google Tag Manager (noscript) -->
   <!-- Google Tag Manager -->
<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
   new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
   j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
   'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
   j.onload = function() {
     window.dispatchEvent(new Event('gtm_loaded'));
     console.log('GTM loaded successfully');
   };
   })(window,document,'script','dataLayer','GTM-T8XT4PS');
</script>
 <!-- End Google Tag Manager -->
 <!-- Facebook Pixel Code -->
<script>
   !function(f,b,e,v,n,t,s)
   {if(f.fbq)return;n=f.fbq=function(){n.callMethod?
   n.callMethod.apply(n,arguments):n.queue.push(arguments)};
   if(!f._fbq)f._fbq=n;n.push=n;n.loaded=!0;n.version='2.0';
   n.queue=[];t=b.createElement(e);t.async=!0;
   t.src=v;s=b.getElementsByTagName(e)[0];
   s.parentNode.insertBefore(t,s)}(window,document,'script',
   'https://connect.facebook.net/en_US/fbevents.js');
   fbq('init', '243028289693773');
   fbq('track', 'PageView');
</script>
<noscript>
   <img height="1" width="1" src="https://www.facebook.com/tr?id=243028289693773&ev=PageView&noscript=1"/>
</noscript>
<!-- End Facebook Pixel Code -->

<script async src="https://cse.google.com/cse.js?cx=e65585f8c3ea1440e"></script>

   <!--
   Search engines should not index the main version of documentation.
   Stable documentation are built without release == 'main'.
   -->
   <meta name="robots" content="noindex">


  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>

  </head>
<body data-feedback-url="https://github.com/pytorch/pytorch" class="pytorch-body">
     <div class="container-fluid header-holder tutorials-header" id="header-holder">
   <div class="header-container-wrapper">
    <div class="header-container">
      <a class="header-logo" href="" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>

          <li class="main-menu-item">
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Learn
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="">
                  <span class=dropdown-title>Get Started</span>
                  <p>Run PyTorch locally or get started quickly with one of the supported cloud platforms</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Tutorials</span>
                  <p>Whats new in PyTorch tutorials</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Learn the Basics</span>
                  <p>Familiarize yourself with PyTorch concepts and modules</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">PyTorch Recipes</span>
                  <p>Bite-size, ready-to-deploy PyTorch code examples</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Intro to PyTorch - YouTube Series</span>
                  <p>Master PyTorch basics with our engaging YouTube tutorial series</p>
                </a>
              </div>
            </div>
          </li>

          <li>
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Ecosystem
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Tools</span>
                  <p>Learn about the tools and frameworks in the PyTorch Ecosystem</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class=dropdown-title>Community</span>
                  <p>Join the PyTorch developer community to contribute, learn, and get your questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="" target="_blank">
                  <span class=dropdown-title>Forums</span>
                  <p>A place to discuss PyTorch code, issues, install, research</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class=dropdown-title>Developer Resources</span>
                  <p>Find resources and get questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Contributor Awards - 2024</span>
                  <p>Award winners announced at this year's PyTorch Conference</p>
                </a>
              </div>
            </div>
          </li>

          <li>
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Edge
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">About PyTorch Edge</span>
                  <p>Build innovative and privacy-aware AI experiences for edge devices</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">ExecuTorch</span>
                  <p>End-to-end solution for enabling on-device inference capabilities across mobile and edge devices</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">ExecuTorch Docs</span>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Docs
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">PyTorch</span>
                  <p>Explore the documentation for comprehensive guidance on how to use PyTorch</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">PyTorch Domains</span>
                  <p>Read the PyTorch Domains documentation to learn more about domain-specific libraries</p>
                </a>
              </div>
            </div>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Blogs & News
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">PyTorch Blog</span>
                  <p>Catch up on the latest technical news and happenings</p>
                </a>
                 <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Community Blog</span>
                  <p>Stories from the PyTorch ecosystem</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Videos</span>
                  <p>Learn about the latest PyTorch tutorials, new, and more </p>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Community Stories</span>
                  <p>Learn how our community solves real, everyday machine learning problems with PyTorch</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Events</span>
                  <p>Find events, webinars, and podcasts</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Newsletter</span>
                  <p>Stay up-to-date with the latest updates</p>
                </a>
            </div>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                About
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">PyTorch Foundation</span>
                  <p>Learn more about the PyTorch Foundation</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Governing Board</span>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Cloud Credit Program</span>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Technical Advisory Council</span>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Staff</span>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Contact Us</span>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
            <div class="no-dropdown">
              <a href="" data-cta="join">
                Become a Member
              </a>
            </div>
          </li>
          <li>
           <div class="main-menu-item">
             <a href="" class="github-icon">
             </a>
           </div>
          </li>
          <!--- TODO: This block adds the search icon to the nav bar. We will enable it later.
          <li>
            <div class="main-menu-item">
             <a href="" class="search-icon">
             </a>
            </div>
          </li>
          --->
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu">
        <i class="fa-solid fa-ellipsis"></i>
      </a>
    </div>
  </div>
 </div>

 <!-- Begin Mobile Menu -->

<div class="mobile-main-menu">
  <div class="container-fluid">
    <div class="header-container-wrapper">
      <div class="mobile-main-menu-header-container">
        <a class="header-logo" href="" aria-label="PyTorch"></a>
        <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu">
          <i class="fa-solid fa-xmark"></i>
        </a>
      </div>
    </div>
  </div>

  <div class="mobile-main-menu-links-container">
    <div class="main-menu">
      <ul>
         <li class="resources-mobile-menu-title">
           <a>Learn</a>
         </li>
         <ul class="resources-mobile-menu-items">
           <li>
             <a href="">Get Started</a>
           </li>
           <li>
             <a href="">Tutorials</a>
           </li>
           <li>
             <a href="">Learn the Basics</a>
           </li>
           <li>
             <a href="">PyTorch Recipes</a>
           </li>
           <li>
             <a href="">Introduction to PyTorch - YouTube Series</a>
           </li>
         </ul>
         <li class="resources-mobile-menu-title">
           <a>Ecosystem</a>
         </li>
         <ul class="resources-mobile-menu-items">
           <li>
             <a href="">Tools</a>
           </li>
           <li>
             <a href="">Community</a>
           </li>
           <li>
             <a href="">Forums</a>
           </li>
           <li>
             <a href="">Developer Resources</a>
           </li>
           <li>
             <a href="">Contributor Awards - 2024</a>
           </li>
         </ul>

         <li class="resources-mobile-menu-title">
           <a>Edge</a>
         </li>

         <ul class="resources-mobile-menu-items">
           <li>
             <a href="">About PyTorch Edge</a>
           </li>

           <li>
             <a href="">ExecuTorch</a>
           </li>
           <li>
             <a href="">ExecuTorch Documentation</a>
           </li>
         </ul>

         <li class="resources-mobile-menu-title">
           <a>Docs</a>
         </li>

         <ul class="resources-mobile-menu-items">
          <li>
            <a href="">PyTorch</a>
          </li>

          <li>
            <a href="">PyTorch Domains</a>
          </li>
        </ul>

        <li class="resources-mobile-menu-title">
          <a>Blog & News</a>
        </li>

         <ul class="resources-mobile-menu-items">
          <li>
            <a href="">PyTorch Blog</a>
          </li>
          <li>
            <a href="">Community Blog</a>
          </li>

          <li>
            <a href="">Videos</a>
          </li>

          <li>
            <a href="">Community Stories</a>
          </li>
          <li>
            <a href="">Events</a>
          </li>
          <li>
             <a href="">Newsletter</a>
           </li>
        </ul>

        <li class="resources-mobile-menu-title">
          <a>About</a>
        </li>

        <ul class="resources-mobile-menu-items">
          <li>
            <a href="">PyTorch Foundation</a>
          </li>
          <li>
            <a href="">Governing Board</a>
          </li>
          <li>
             <a href="">Cloud Credit Program</a>
          </li>
          <li>
             <a href="">Technical Advisory Council</a>
          </li>
          <li>
             <a href="">Staff</a>
          </li>
          <li>
             <a href="">Contact Us</a>
          </li>
        </ul>
      </ul>
    </div>
  </div>
</div>

<!-- End Mobile Menu -->
   
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>

  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
<div class="bd-header__inner bd-page-width">
  <button class="pst-navbar-icon sidebar-toggle primary-toggle" aria-label="Site navigation">
    <span class="fa-solid fa-bars"></span>
  </button>
  
  
  <div class=" navbar-header-items__start">
    
      <div class="navbar-item">
<script>
document.write(`
  <div class="version-switcher__container dropdown">
    <button id="pst-version-switcher-button-2"
      type="button"
      class="version-switcher__button btn btn-sm dropdown-toggle"
      data-bs-toggle="dropdown"
      aria-haspopup="listbox"
      aria-controls="pst-version-switcher-list-2"
      aria-label="Version switcher list"
    >
      Choose version  <!-- this text may get changed later by javascript -->
      <span class="caret"></span>
    </button>
    <div id="pst-version-switcher-list-2"
      class="version-switcher__menu dropdown-menu list-group-flush py-0"
      role="listbox" aria-labelledby="pst-version-switcher-button-2">
      <!-- dropdown will be populated by javascript on page load -->
    </div>
  </div>
`);
</script></div>
    
      <div class="navbar-item">

  
    
  

<a class="navbar-brand logo" href="index.html">
  
  
  
  
  
  
    <p class="title logo__title">Home</p>
  
</a></div>
    
  </div>
  
  <div class=" navbar-header-items">
    
    <div class="me-auto navbar-header-items__center">
      
        <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item current active">
  <a class="nav-link nav-internal" href="pytorch-api.html">
    Python API
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="notes.html">
    Developer Notes
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-external" href="https://pytorch.org/tutorials/">
    Tutorials
  </a>
</li>

  </ul>
</nav></div>
      
    </div>
    
    
    <div class="navbar-header-items__end">
      
      
        <div class="navbar-item">
<div class="search-container-wrapper">
  <div id="sphinx-search" class="search-container">
    
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </div>

  <div id="google-search" class="search-container" style="display:none;">
    <div class="gcse-search-wrapper">
      <i class="fa-solid fa-magnifying-glass" aria-hidden="true"></i>
      <div class="gcse-search"></div>
    </div>
  </div>

  <div class="search-toggle-container" data-bs-title="Google Search Off" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <div class="search-toggle-inner">
      <label class="switch">
        <input type="checkbox" id="search-toggle">
        <span class="slider round"></span>
      </label>
    </div>
  </div>
</div>

<script>
  document.addEventListener('DOMContentLoaded', function() {
    // Define the search callback
    const myWebSearchStartingCallback = (gname, query) => {
      if (typeof dataLayer !== 'undefined' && query) {
        dataLayer.push({
          'event': 'google_search',
          'search_term': query,
          'event_category': 'Search',
          'event_label': 'Google Search'
        });
        console.log('GA event sent via callback: google_search - ' + query);
      }
      return '';
    };

    // Set up the GCSE search callbacks
    window.__gcse || (window.__gcse = {});
    window.__gcse.searchCallbacks = {
      web: {
        starting: myWebSearchStartingCallback,
      },
    };
    if (window.location.pathname.includes('/search.html')) {
      document.body.classList.add('search-page');
    }

    // Function to reinitialize Google CSE
    function reinitializeGoogleSearch() {
      if (window.__gcse) {
        // Force Google CSE to reinitialize
        if (window.__gcse.initializationCallback) {
          window.__gcse.initializationCallback();
        }
      }
    }

    // Function to handle search toggle
    function handleSearchToggle(toggle, sphinxSearch, googleSearch) {
      if (!toggle || !sphinxSearch || !googleSearch) return;

      // Check if the URL contains /stable/ or /tutorials/
      const currentUrl = window.location.href;
      const shouldDefaultToGoogle = currentUrl.includes('/stable/') || currentUrl.includes('/tutorials/');

      // Check if there's a saved preference, otherwise use the URL-based default
      const savedPreference = localStorage.getItem('searchPreference');
      if (savedPreference === 'google' || (savedPreference === null && shouldDefaultToGoogle)) {
        toggle.checked = true;
        sphinxSearch.style.display = 'none';
        googleSearch.style.display = 'block';
        // Save the preference if it wasn't already saved
        if (savedPreference === null) {
          localStorage.setItem('searchPreference', 'google');
        }
        // Ensure Google search is properly initialized
        reinitializeGoogleSearch();
      } else {
        toggle.checked = false;
        sphinxSearch.style.display = 'block';
        googleSearch.style.display = 'none';
      }

      // Update tooltip based on initial state
      const tooltipElement = document.querySelector('.search-toggle-container');
      if (tooltipElement) {
        tooltipElement.setAttribute('data-bs-title', toggle.checked ? 'Google Search On' : 'Google Search Off');
        // Reinitialize tooltip if Bootstrap's tooltip is already initialized
        if (bootstrap && bootstrap.Tooltip) {
          const tooltipInstance = bootstrap.Tooltip.getInstance(tooltipElement);
          if (tooltipInstance) tooltipInstance.dispose();
          new bootstrap.Tooltip(tooltipElement);
        }
      }

      // Add a data attribute to track if this toggle has been initialized
      if (toggle.hasAttribute('data-initialized')) {
        return; // Skip adding another event listener if already initialized
      }
      toggle.setAttribute('data-initialized', 'true');

      toggle.addEventListener('change', function() {
        if (this.checked) {
          sphinxSearch.style.display = 'none';
          googleSearch.style.display = 'block';
          localStorage.setItem('searchPreference', 'google');
          // Reinitialize Google search when switching to it
          reinitializeGoogleSearch();
          const tooltipElement = document.querySelector('.search-toggle-container');
          if (tooltipElement) {
            tooltipElement.setAttribute('data-bs-title', 'Google Search On');
            // Refresh tooltip
            if (bootstrap && bootstrap.Tooltip) {
              const tooltipInstance = bootstrap.Tooltip.getInstance(tooltipElement);
              if (tooltipInstance) tooltipInstance.dispose();
              new bootstrap.Tooltip(tooltipElement);
            }
          }
          if (typeof dataLayer !== 'undefined') {
            dataLayer.push({
              'event': 'search_engine_switch',
              'event_category': 'Search',
              'event_label': 'Google'
            });
            console.log('GA event sent via dataLayer: search_engine_switch - Google');
          } else {
            console.log('GA not available: Cannot track Google search switch');
          }
        } else {
          sphinxSearch.style.display = 'block';
          googleSearch.style.display = 'none';
          localStorage.setItem('searchPreference', 'sphinx');
          const tooltipElement = document.querySelector('.search-toggle-container');
          if (tooltipElement) {
            tooltipElement.setAttribute('data-bs-title', 'Google Search Off');
            // Refresh tooltip
            if (bootstrap && bootstrap.Tooltip) {
              const tooltipInstance = bootstrap.Tooltip.getInstance(tooltipElement);
              if (tooltipInstance) tooltipInstance.dispose();
              new bootstrap.Tooltip(tooltipElement);
            }
          }
          if (typeof dataLayer !== 'undefined') {
            dataLayer.push({
              'event': 'search_engine_switch',
              'event_category': 'Search',
              'event_label': 'Sphinx'
            });
            console.log('GA event sent via dataLayer: search_engine_switch - Sphinx');
          } else {
            console.log('GA not available: Cannot track Sphinx search switch');
          }
        }

        // Also update mobile search if it exists
        updateMobileSearch(false); // Pass false to prevent triggering another event
      });
    }


    // Function to update mobile search based on current toggle state
    function updateMobileSearch() {
      const toggle = document.getElementById('search-toggle');
      if (!toggle) return;

      // Find mobile search container
      const mobileSearchContainer = document.querySelector('.sidebar-header-items__end .navbar-item .search-container-wrapper');
      if (mobileSearchContainer) {
        const mobileSphinxSearch = mobileSearchContainer.querySelector('#sphinx-search');
        const mobileGoogleSearch = mobileSearchContainer.querySelector('#google-search');

        if (mobileSphinxSearch && mobileGoogleSearch) {
          if (toggle.checked) {
            mobileSphinxSearch.style.display = 'none';
            mobileGoogleSearch.style.display = 'block';
            // Reinitialize Google search in mobile view
            reinitializeGoogleSearch();
          } else {
            mobileSphinxSearch.style.display = 'block';
            mobileGoogleSearch.style.display = 'none';
          }
        }
      }
    }

    // Initialize desktop search toggle
    const toggle = document.getElementById('search-toggle');
    const sphinxSearch = document.getElementById('sphinx-search');
    const googleSearch = document.getElementById('google-search');
    handleSearchToggle(toggle, sphinxSearch, googleSearch);

    // Set placeholder text for Google search input
    const observer = new MutationObserver(function(mutations, obs) {
      const searchInputs = document.querySelectorAll('.gsc-input input');
      searchInputs.forEach(input => {
      if (input) {
        input.setAttribute('placeholder', 'Search the docs ...');

        if (!input.hasAttribute('data-tracking-added')) {
          input.setAttribute('data-tracking-added', 'true');
        }
      }
      });
    });

    observer.observe(document.body, { childList: true, subtree: true });

    // Watch for mobile menu creation
    const mobileMenuObserver = new MutationObserver(function(mutations) {
      for (const mutation of mutations) {
        const mobileSearchInputs = document.querySelectorAll('.sidebar-header-items__end .navbar-item .search-container-wrapper .gsc-input input');
      mobileSearchInputs.forEach(input => {
        if (input) {
          input.setAttribute('placeholder', 'Search the docs ...');
          input.style.paddingLeft = '36px';
          }
        });
        if (mutation.addedNodes.length) {
          // Check if the mobile search container was added
          const mobileSearchContainer = document.querySelector('.sidebar-header-items__end .navbar-item .search-container-wrapper');
          if (mobileSearchContainer) {
            // Clone the toggle for mobile if needed
            const mobileToggle = mobileSearchContainer.querySelector('#search-toggle');
            if (mobileToggle) {
              // Sync mobile toggle with desktop toggle
              mobileToggle.checked = toggle.checked;

              // Update mobile search display
              updateMobileSearch();

              // Add event listener to mobile toggle
              mobileToggle.addEventListener('change', function() {
                // Sync desktop toggle with mobile toggle
                toggle.checked = this.checked;
                // Trigger change event on desktop toggle to update both
                toggle.dispatchEvent(new Event('change'));
              });
            }
          }
        }
      }
    });

    mobileMenuObserver.observe(document.body, { childList: true, subtree: true });

    // Ensure Google CSE is properly loaded
    if (window.__gcse) {
      window.__gcse.callback = function() {
        // This will run after Google CSE is fully loaded
        if (toggle && toggle.checked) {
          // If Google search is active, make sure it's properly initialized
          reinitializeGoogleSearch();
        }
      };
    } else {
      // If Google CSE hasn't loaded yet, set up a callback
      window.__gcse = {
        callback: function() {
          if (toggle && toggle.checked) {
            reinitializeGoogleSearch();
          }
        }
      };
    }
  });
</script>
</div>
      
        <div class="navbar-item">

<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script></div>
      
        <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://x.com/PyTorch" title="X" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-x-twitter fa-lg" aria-hidden="true"></i>
            <span class="sr-only">X</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/pytorch/pytorch" title="GitHub" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">GitHub</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://discuss.pytorch.org/" title="PyTorch Forum" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-discourse fa-lg" aria-hidden="true"></i>
            <span class="sr-only">PyTorch Forum</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://pypi.org/project/torch/" title="PyPi" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-python fa-lg" aria-hidden="true"></i>
            <span class="sr-only">PyPi</span></a>
        </li>
</ul></div>
      
    </div>
    
  </div>
  
  

  
    <button class="pst-navbar-icon sidebar-toggle secondary-toggle" aria-label="On this page">
      <span class="fa-solid fa-outdent"></span>
    </button>
  
</div>

    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
      <div class="sidebar-header-items__center">
        
          
          
            <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item current active">
  <a class="nav-link nav-internal" href="pytorch-api.html">
    Python API
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="notes.html">
    Developer Notes
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-external" href="https://pytorch.org/tutorials/">
    Tutorials
  </a>
</li>

  </ul>
</nav></div>
          
        
      </div>
    
    
    
      <div class="sidebar-header-items__end">
        
          <div class="navbar-item">
<div class="search-container-wrapper">
  <div id="sphinx-search" class="search-container">
    
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </div>

  <div id="google-search" class="search-container" style="display:none;">
    <div class="gcse-search-wrapper">
      <i class="fa-solid fa-magnifying-glass" aria-hidden="true"></i>
      <div class="gcse-search"></div>
    </div>
  </div>

  <div class="search-toggle-container" data-bs-title="Google Search Off" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <div class="search-toggle-inner">
      <label class="switch">
        <input type="checkbox" id="search-toggle">
        <span class="slider round"></span>
      </label>
    </div>
  </div>
</div>

<script>
  document.addEventListener('DOMContentLoaded', function() {
    // Define the search callback
    const myWebSearchStartingCallback = (gname, query) => {
      if (typeof dataLayer !== 'undefined' && query) {
        dataLayer.push({
          'event': 'google_search',
          'search_term': query,
          'event_category': 'Search',
          'event_label': 'Google Search'
        });
        console.log('GA event sent via callback: google_search - ' + query);
      }
      return '';
    };

    // Set up the GCSE search callbacks
    window.__gcse || (window.__gcse = {});
    window.__gcse.searchCallbacks = {
      web: {
        starting: myWebSearchStartingCallback,
      },
    };
    if (window.location.pathname.includes('/search.html')) {
      document.body.classList.add('search-page');
    }

    // Function to reinitialize Google CSE
    function reinitializeGoogleSearch() {
      if (window.__gcse) {
        // Force Google CSE to reinitialize
        if (window.__gcse.initializationCallback) {
          window.__gcse.initializationCallback();
        }
      }
    }

    // Function to handle search toggle
    function handleSearchToggle(toggle, sphinxSearch, googleSearch) {
      if (!toggle || !sphinxSearch || !googleSearch) return;

      // Check if the URL contains /stable/ or /tutorials/
      const currentUrl = window.location.href;
      const shouldDefaultToGoogle = currentUrl.includes('/stable/') || currentUrl.includes('/tutorials/');

      // Check if there's a saved preference, otherwise use the URL-based default
      const savedPreference = localStorage.getItem('searchPreference');
      if (savedPreference === 'google' || (savedPreference === null && shouldDefaultToGoogle)) {
        toggle.checked = true;
        sphinxSearch.style.display = 'none';
        googleSearch.style.display = 'block';
        // Save the preference if it wasn't already saved
        if (savedPreference === null) {
          localStorage.setItem('searchPreference', 'google');
        }
        // Ensure Google search is properly initialized
        reinitializeGoogleSearch();
      } else {
        toggle.checked = false;
        sphinxSearch.style.display = 'block';
        googleSearch.style.display = 'none';
      }

      // Update tooltip based on initial state
      const tooltipElement = document.querySelector('.search-toggle-container');
      if (tooltipElement) {
        tooltipElement.setAttribute('data-bs-title', toggle.checked ? 'Google Search On' : 'Google Search Off');
        // Reinitialize tooltip if Bootstrap's tooltip is already initialized
        if (bootstrap && bootstrap.Tooltip) {
          const tooltipInstance = bootstrap.Tooltip.getInstance(tooltipElement);
          if (tooltipInstance) tooltipInstance.dispose();
          new bootstrap.Tooltip(tooltipElement);
        }
      }

      // Add a data attribute to track if this toggle has been initialized
      if (toggle.hasAttribute('data-initialized')) {
        return; // Skip adding another event listener if already initialized
      }
      toggle.setAttribute('data-initialized', 'true');

      toggle.addEventListener('change', function() {
        if (this.checked) {
          sphinxSearch.style.display = 'none';
          googleSearch.style.display = 'block';
          localStorage.setItem('searchPreference', 'google');
          // Reinitialize Google search when switching to it
          reinitializeGoogleSearch();
          const tooltipElement = document.querySelector('.search-toggle-container');
          if (tooltipElement) {
            tooltipElement.setAttribute('data-bs-title', 'Google Search On');
            // Refresh tooltip
            if (bootstrap && bootstrap.Tooltip) {
              const tooltipInstance = bootstrap.Tooltip.getInstance(tooltipElement);
              if (tooltipInstance) tooltipInstance.dispose();
              new bootstrap.Tooltip(tooltipElement);
            }
          }
          if (typeof dataLayer !== 'undefined') {
            dataLayer.push({
              'event': 'search_engine_switch',
              'event_category': 'Search',
              'event_label': 'Google'
            });
            console.log('GA event sent via dataLayer: search_engine_switch - Google');
          } else {
            console.log('GA not available: Cannot track Google search switch');
          }
        } else {
          sphinxSearch.style.display = 'block';
          googleSearch.style.display = 'none';
          localStorage.setItem('searchPreference', 'sphinx');
          const tooltipElement = document.querySelector('.search-toggle-container');
          if (tooltipElement) {
            tooltipElement.setAttribute('data-bs-title', 'Google Search Off');
            // Refresh tooltip
            if (bootstrap && bootstrap.Tooltip) {
              const tooltipInstance = bootstrap.Tooltip.getInstance(tooltipElement);
              if (tooltipInstance) tooltipInstance.dispose();
              new bootstrap.Tooltip(tooltipElement);
            }
          }
          if (typeof dataLayer !== 'undefined') {
            dataLayer.push({
              'event': 'search_engine_switch',
              'event_category': 'Search',
              'event_label': 'Sphinx'
            });
            console.log('GA event sent via dataLayer: search_engine_switch - Sphinx');
          } else {
            console.log('GA not available: Cannot track Sphinx search switch');
          }
        }

        // Also update mobile search if it exists
        updateMobileSearch(false); // Pass false to prevent triggering another event
      });
    }


    // Function to update mobile search based on current toggle state
    function updateMobileSearch() {
      const toggle = document.getElementById('search-toggle');
      if (!toggle) return;

      // Find mobile search container
      const mobileSearchContainer = document.querySelector('.sidebar-header-items__end .navbar-item .search-container-wrapper');
      if (mobileSearchContainer) {
        const mobileSphinxSearch = mobileSearchContainer.querySelector('#sphinx-search');
        const mobileGoogleSearch = mobileSearchContainer.querySelector('#google-search');

        if (mobileSphinxSearch && mobileGoogleSearch) {
          if (toggle.checked) {
            mobileSphinxSearch.style.display = 'none';
            mobileGoogleSearch.style.display = 'block';
            // Reinitialize Google search in mobile view
            reinitializeGoogleSearch();
          } else {
            mobileSphinxSearch.style.display = 'block';
            mobileGoogleSearch.style.display = 'none';
          }
        }
      }
    }

    // Initialize desktop search toggle
    const toggle = document.getElementById('search-toggle');
    const sphinxSearch = document.getElementById('sphinx-search');
    const googleSearch = document.getElementById('google-search');
    handleSearchToggle(toggle, sphinxSearch, googleSearch);

    // Set placeholder text for Google search input
    const observer = new MutationObserver(function(mutations, obs) {
      const searchInputs = document.querySelectorAll('.gsc-input input');
      searchInputs.forEach(input => {
      if (input) {
        input.setAttribute('placeholder', 'Search the docs ...');

        if (!input.hasAttribute('data-tracking-added')) {
          input.setAttribute('data-tracking-added', 'true');
        }
      }
      });
    });

    observer.observe(document.body, { childList: true, subtree: true });

    // Watch for mobile menu creation
    const mobileMenuObserver = new MutationObserver(function(mutations) {
      for (const mutation of mutations) {
        const mobileSearchInputs = document.querySelectorAll('.sidebar-header-items__end .navbar-item .search-container-wrapper .gsc-input input');
      mobileSearchInputs.forEach(input => {
        if (input) {
          input.setAttribute('placeholder', 'Search the docs ...');
          input.style.paddingLeft = '36px';
          }
        });
        if (mutation.addedNodes.length) {
          // Check if the mobile search container was added
          const mobileSearchContainer = document.querySelector('.sidebar-header-items__end .navbar-item .search-container-wrapper');
          if (mobileSearchContainer) {
            // Clone the toggle for mobile if needed
            const mobileToggle = mobileSearchContainer.querySelector('#search-toggle');
            if (mobileToggle) {
              // Sync mobile toggle with desktop toggle
              mobileToggle.checked = toggle.checked;

              // Update mobile search display
              updateMobileSearch();

              // Add event listener to mobile toggle
              mobileToggle.addEventListener('change', function() {
                // Sync desktop toggle with mobile toggle
                toggle.checked = this.checked;
                // Trigger change event on desktop toggle to update both
                toggle.dispatchEvent(new Event('change'));
              });
            }
          }
        }
      }
    });

    mobileMenuObserver.observe(document.body, { childList: true, subtree: true });

    // Ensure Google CSE is properly loaded
    if (window.__gcse) {
      window.__gcse.callback = function() {
        // This will run after Google CSE is fully loaded
        if (toggle && toggle.checked) {
          // If Google search is active, make sure it's properly initialized
          reinitializeGoogleSearch();
        }
      };
    } else {
      // If Google CSE hasn't loaded yet, set up a callback
      window.__gcse = {
        callback: function() {
          if (toggle && toggle.checked) {
            reinitializeGoogleSearch();
          }
        }
      };
    }
  });
</script>
</div>
        
          <div class="navbar-item">

<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script></div>
        
          <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://x.com/PyTorch" title="X" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-x-twitter fa-lg" aria-hidden="true"></i>
            <span class="sr-only">X</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/pytorch/pytorch" title="GitHub" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">GitHub</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://discuss.pytorch.org/" title="PyTorch Forum" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-discourse fa-lg" aria-hidden="true"></i>
            <span class="sr-only">PyTorch Forum</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://pypi.org/project/torch/" title="PyPi" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-python fa-lg" aria-hidden="true"></i>
            <span class="sr-only">PyPi</span></a>
        </li>
</ul></div>
        
      </div>
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
<nav class="bd-docs-nav bd-links"
     aria-label="Section Navigation">
  <p class="bd-links__title" role="heading" aria-level="1">Section Navigation</p>
  <div class="bd-toc-item navbar-nav"><ul class="current nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="torch.html">torch</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.is_tensor.html">torch.is_tensor</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.is_storage.html">torch.is_storage</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.is_complex.html">torch.is_complex</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.is_conj.html">torch.is_conj</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.is_floating_point.html">torch.is_floating_point</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.is_nonzero.html">torch.is_nonzero</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.set_default_dtype.html">torch.set_default_dtype</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.get_default_dtype.html">torch.get_default_dtype</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.set_default_device.html">torch.set_default_device</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.get_default_device.html">torch.get_default_device</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.set_default_tensor_type.html">torch.set_default_tensor_type</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.numel.html">torch.numel</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.set_printoptions.html">torch.set_printoptions</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.set_flush_denormal.html">torch.set_flush_denormal</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.tensor.html">torch.tensor</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.sparse_coo_tensor.html">torch.sparse_coo_tensor</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.sparse_csr_tensor.html">torch.sparse_csr_tensor</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.sparse_csc_tensor.html">torch.sparse_csc_tensor</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.sparse_bsr_tensor.html">torch.sparse_bsr_tensor</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.sparse_bsc_tensor.html">torch.sparse_bsc_tensor</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.asarray.html">torch.asarray</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.as_tensor.html">torch.as_tensor</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.as_strided.html">torch.as_strided</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.from_file.html">torch.from_file</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.from_numpy.html">torch.from_numpy</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.from_dlpack.html">torch.from_dlpack</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.frombuffer.html">torch.frombuffer</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.zeros.html">torch.zeros</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.zeros_like.html">torch.zeros_like</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.ones.html">torch.ones</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.ones_like.html">torch.ones_like</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.arange.html">torch.arange</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.range.html">torch.range</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.linspace.html">torch.linspace</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.logspace.html">torch.logspace</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.eye.html">torch.eye</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.empty.html">torch.empty</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.empty_like.html">torch.empty_like</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.empty_strided.html">torch.empty_strided</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.full.html">torch.full</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.full_like.html">torch.full_like</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.quantize_per_tensor.html">torch.quantize_per_tensor</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.quantize_per_channel.html">torch.quantize_per_channel</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.dequantize.html">torch.dequantize</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.complex.html">torch.complex</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.polar.html">torch.polar</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.heaviside.html">torch.heaviside</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.adjoint.html">torch.adjoint</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.argwhere.html">torch.argwhere</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cat.html">torch.cat</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.concat.html">torch.concat</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.concatenate.html">torch.concatenate</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.conj.html">torch.conj</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.chunk.html">torch.chunk</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.dsplit.html">torch.dsplit</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.column_stack.html">torch.column_stack</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.dstack.html">torch.dstack</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.gather.html">torch.gather</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.hsplit.html">torch.hsplit</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.hstack.html">torch.hstack</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.index_add.html">torch.index_add</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.index_copy.html">torch.index_copy</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.index_reduce.html">torch.index_reduce</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.index_select.html">torch.index_select</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.masked_select.html">torch.masked_select</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.movedim.html">torch.movedim</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.moveaxis.html">torch.moveaxis</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.narrow.html">torch.narrow</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.narrow_copy.html">torch.narrow_copy</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nonzero.html">torch.nonzero</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.permute.html">torch.permute</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.reshape.html">torch.reshape</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.row_stack.html">torch.row_stack</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.select.html">torch.select</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.scatter.html">torch.scatter</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.diagonal_scatter.html">torch.diagonal_scatter</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.select_scatter.html">torch.select_scatter</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.slice_scatter.html">torch.slice_scatter</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.scatter_add.html">torch.scatter_add</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.scatter_reduce.html">torch.scatter_reduce</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.split.html">torch.split</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.squeeze.html">torch.squeeze</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.stack.html">torch.stack</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.swapaxes.html">torch.swapaxes</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.swapdims.html">torch.swapdims</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.t.html">torch.t</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.take.html">torch.take</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.take_along_dim.html">torch.take_along_dim</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.tensor_split.html">torch.tensor_split</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.tile.html">torch.tile</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.transpose.html">torch.transpose</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.unbind.html">torch.unbind</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.unravel_index.html">torch.unravel_index</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.unsqueeze.html">torch.unsqueeze</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.vsplit.html">torch.vsplit</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.vstack.html">torch.vstack</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.where.html">torch.where</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Stream.html">Stream</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Event.html">Event</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Generator.html">Generator</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.seed.html">torch.seed</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.manual_seed.html">torch.manual_seed</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.initial_seed.html">torch.initial_seed</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.get_rng_state.html">torch.get_rng_state</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.set_rng_state.html">torch.set_rng_state</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.bernoulli.html">torch.bernoulli</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.multinomial.html">torch.multinomial</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.normal.html">torch.normal</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.poisson.html">torch.poisson</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.rand.html">torch.rand</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.rand_like.html">torch.rand_like</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.randint.html">torch.randint</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.randint_like.html">torch.randint_like</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.randn.html">torch.randn</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.randn_like.html">torch.randn_like</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.randperm.html">torch.randperm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.quasirandom.SobolEngine.html">SobolEngine</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.save.html">torch.save</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.load.html">torch.load</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.get_num_threads.html">torch.get_num_threads</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.set_num_threads.html">torch.set_num_threads</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.get_num_interop_threads.html">torch.get_num_interop_threads</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.set_num_interop_threads.html">torch.set_num_interop_threads</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.no_grad.html">no_grad</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.enable_grad.html">enable_grad</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.grad_mode.set_grad_enabled.html">set_grad_enabled</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.is_grad_enabled.html">torch.is_grad_enabled</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.grad_mode.inference_mode.html">inference_mode</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.is_inference_mode_enabled.html">torch.is_inference_mode_enabled</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.abs.html">torch.abs</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.absolute.html">torch.absolute</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.acos.html">torch.acos</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.arccos.html">torch.arccos</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.acosh.html">torch.acosh</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.arccosh.html">torch.arccosh</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.add.html">torch.add</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.addcdiv.html">torch.addcdiv</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.addcmul.html">torch.addcmul</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.angle.html">torch.angle</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.asin.html">torch.asin</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.arcsin.html">torch.arcsin</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.asinh.html">torch.asinh</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.arcsinh.html">torch.arcsinh</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.atan.html">torch.atan</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.arctan.html">torch.arctan</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.atanh.html">torch.atanh</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.arctanh.html">torch.arctanh</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.atan2.html">torch.atan2</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.arctan2.html">torch.arctan2</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.bitwise_not.html">torch.bitwise_not</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.bitwise_and.html">torch.bitwise_and</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.bitwise_or.html">torch.bitwise_or</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.bitwise_xor.html">torch.bitwise_xor</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.bitwise_left_shift.html">torch.bitwise_left_shift</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.bitwise_right_shift.html">torch.bitwise_right_shift</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.ceil.html">torch.ceil</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.clamp.html">torch.clamp</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.clip.html">torch.clip</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.conj_physical.html">torch.conj_physical</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.copysign.html">torch.copysign</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cos.html">torch.cos</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cosh.html">torch.cosh</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.deg2rad.html">torch.deg2rad</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.div.html">torch.div</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.divide.html">torch.divide</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.digamma.html">torch.digamma</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.erf.html">torch.erf</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.erfc.html">torch.erfc</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.erfinv.html">torch.erfinv</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.exp.html">torch.exp</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.exp2.html">torch.exp2</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.expm1.html">torch.expm1</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fake_quantize_per_channel_affine.html">torch.fake_quantize_per_channel_affine</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fake_quantize_per_tensor_affine.html">torch.fake_quantize_per_tensor_affine</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fix.html">torch.fix</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.float_power.html">torch.float_power</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.floor.html">torch.floor</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.floor_divide.html">torch.floor_divide</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fmod.html">torch.fmod</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.frac.html">torch.frac</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.frexp.html">torch.frexp</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.gradient.html">torch.gradient</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.imag.html">torch.imag</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.ldexp.html">torch.ldexp</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.lerp.html">torch.lerp</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.lgamma.html">torch.lgamma</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.log.html">torch.log</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.log10.html">torch.log10</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.log1p.html">torch.log1p</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.log2.html">torch.log2</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.logaddexp.html">torch.logaddexp</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.logaddexp2.html">torch.logaddexp2</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.logical_and.html">torch.logical_and</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.logical_not.html">torch.logical_not</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.logical_or.html">torch.logical_or</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.logical_xor.html">torch.logical_xor</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.logit.html">torch.logit</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.hypot.html">torch.hypot</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.i0.html">torch.i0</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.igamma.html">torch.igamma</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.igammac.html">torch.igammac</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mul.html">torch.mul</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.multiply.html">torch.multiply</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mvlgamma.html">torch.mvlgamma</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nan_to_num.html">torch.nan_to_num</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.neg.html">torch.neg</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.negative.html">torch.negative</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nextafter.html">torch.nextafter</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.polygamma.html">torch.polygamma</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.positive.html">torch.positive</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.pow.html">torch.pow</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.quantized_batch_norm.html">torch.quantized_batch_norm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.quantized_max_pool1d.html">torch.quantized_max_pool1d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.quantized_max_pool2d.html">torch.quantized_max_pool2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.rad2deg.html">torch.rad2deg</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.real.html">torch.real</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.reciprocal.html">torch.reciprocal</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.remainder.html">torch.remainder</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.round.html">torch.round</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.rsqrt.html">torch.rsqrt</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.sigmoid.html">torch.sigmoid</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.sign.html">torch.sign</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.sgn.html">torch.sgn</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.signbit.html">torch.signbit</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.sin.html">torch.sin</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.sinc.html">torch.sinc</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.sinh.html">torch.sinh</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.softmax.html">torch.softmax</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.sqrt.html">torch.sqrt</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.square.html">torch.square</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.sub.html">torch.sub</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.subtract.html">torch.subtract</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.tan.html">torch.tan</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.tanh.html">torch.tanh</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.true_divide.html">torch.true_divide</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.trunc.html">torch.trunc</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xlogy.html">torch.xlogy</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.argmax.html">torch.argmax</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.argmin.html">torch.argmin</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.amax.html">torch.amax</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.amin.html">torch.amin</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.aminmax.html">torch.aminmax</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.all.html">torch.all</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.any.html">torch.any</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.max.html">torch.max</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.min.html">torch.min</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.dist.html">torch.dist</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.logsumexp.html">torch.logsumexp</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mean.html">torch.mean</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nanmean.html">torch.nanmean</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.median.html">torch.median</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nanmedian.html">torch.nanmedian</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mode.html">torch.mode</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.norm.html">torch.norm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nansum.html">torch.nansum</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.prod.html">torch.prod</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.quantile.html">torch.quantile</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nanquantile.html">torch.nanquantile</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.std.html">torch.std</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.std_mean.html">torch.std_mean</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.sum.html">torch.sum</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.unique.html">torch.unique</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.unique_consecutive.html">torch.unique_consecutive</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.var.html">torch.var</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.var_mean.html">torch.var_mean</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.count_nonzero.html">torch.count_nonzero</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.allclose.html">torch.allclose</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.argsort.html">torch.argsort</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.eq.html">torch.eq</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.equal.html">torch.equal</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.ge.html">torch.ge</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.greater_equal.html">torch.greater_equal</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.gt.html">torch.gt</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.greater.html">torch.greater</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.isclose.html">torch.isclose</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.isfinite.html">torch.isfinite</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.isin.html">torch.isin</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.isinf.html">torch.isinf</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.isposinf.html">torch.isposinf</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.isneginf.html">torch.isneginf</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.isnan.html">torch.isnan</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.isreal.html">torch.isreal</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.kthvalue.html">torch.kthvalue</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.le.html">torch.le</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.less_equal.html">torch.less_equal</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.lt.html">torch.lt</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.less.html">torch.less</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.maximum.html">torch.maximum</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.minimum.html">torch.minimum</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fmax.html">torch.fmax</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fmin.html">torch.fmin</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.ne.html">torch.ne</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.not_equal.html">torch.not_equal</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.sort.html">torch.sort</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.topk.html">torch.topk</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.msort.html">torch.msort</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.stft.html">torch.stft</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.istft.html">torch.istft</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.bartlett_window.html">torch.bartlett_window</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.blackman_window.html">torch.blackman_window</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.hamming_window.html">torch.hamming_window</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.hann_window.html">torch.hann_window</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.kaiser_window.html">torch.kaiser_window</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.atleast_1d.html">torch.atleast_1d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.atleast_2d.html">torch.atleast_2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.atleast_3d.html">torch.atleast_3d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.bincount.html">torch.bincount</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.block_diag.html">torch.block_diag</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.broadcast_tensors.html">torch.broadcast_tensors</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.broadcast_to.html">torch.broadcast_to</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.broadcast_shapes.html">torch.broadcast_shapes</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.bucketize.html">torch.bucketize</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cartesian_prod.html">torch.cartesian_prod</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cdist.html">torch.cdist</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.clone.html">torch.clone</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.combinations.html">torch.combinations</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.corrcoef.html">torch.corrcoef</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cov.html">torch.cov</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cross.html">torch.cross</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cummax.html">torch.cummax</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cummin.html">torch.cummin</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cumprod.html">torch.cumprod</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cumsum.html">torch.cumsum</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.diag.html">torch.diag</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.diag_embed.html">torch.diag_embed</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.diagflat.html">torch.diagflat</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.diagonal.html">torch.diagonal</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.diff.html">torch.diff</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.einsum.html">torch.einsum</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.flatten.html">torch.flatten</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.flip.html">torch.flip</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fliplr.html">torch.fliplr</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.flipud.html">torch.flipud</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.kron.html">torch.kron</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.rot90.html">torch.rot90</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.gcd.html">torch.gcd</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.histc.html">torch.histc</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.histogram.html">torch.histogram</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.histogramdd.html">torch.histogramdd</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.meshgrid.html">torch.meshgrid</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.lcm.html">torch.lcm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.logcumsumexp.html">torch.logcumsumexp</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.ravel.html">torch.ravel</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.renorm.html">torch.renorm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.repeat_interleave.html">torch.repeat_interleave</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.roll.html">torch.roll</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.searchsorted.html">torch.searchsorted</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.tensordot.html">torch.tensordot</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.trace.html">torch.trace</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.tril.html">torch.tril</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.tril_indices.html">torch.tril_indices</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.triu.html">torch.triu</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.triu_indices.html">torch.triu_indices</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.unflatten.html">torch.unflatten</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.vander.html">torch.vander</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.view_as_real.html">torch.view_as_real</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.view_as_complex.html">torch.view_as_complex</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.resolve_conj.html">torch.resolve_conj</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.resolve_neg.html">torch.resolve_neg</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.addbmm.html">torch.addbmm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.addmm.html">torch.addmm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.addmv.html">torch.addmv</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.addr.html">torch.addr</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.baddbmm.html">torch.baddbmm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.bmm.html">torch.bmm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.chain_matmul.html">torch.chain_matmul</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cholesky.html">torch.cholesky</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cholesky_inverse.html">torch.cholesky_inverse</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cholesky_solve.html">torch.cholesky_solve</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.dot.html">torch.dot</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.geqrf.html">torch.geqrf</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.ger.html">torch.ger</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.inner.html">torch.inner</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.inverse.html">torch.inverse</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.det.html">torch.det</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.logdet.html">torch.logdet</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.slogdet.html">torch.slogdet</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.lu.html">torch.lu</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.lu_solve.html">torch.lu_solve</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.lu_unpack.html">torch.lu_unpack</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.matmul.html">torch.matmul</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.matrix_power.html">torch.matrix_power</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.matrix_exp.html">torch.matrix_exp</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mm.html">torch.mm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mv.html">torch.mv</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.orgqr.html">torch.orgqr</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.ormqr.html">torch.ormqr</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.outer.html">torch.outer</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.pinverse.html">torch.pinverse</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.qr.html">torch.qr</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.svd.html">torch.svd</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.svd_lowrank.html">torch.svd_lowrank</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.pca_lowrank.html">torch.pca_lowrank</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.lobpcg.html">torch.lobpcg</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.trapz.html">torch.trapz</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.trapezoid.html">torch.trapezoid</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cumulative_trapezoid.html">torch.cumulative_trapezoid</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.triangular_solve.html">torch.triangular_solve</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.vdot.html">torch.vdot</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_abs.html">torch._foreach_abs</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_abs_.html">torch._foreach_abs_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_acos.html">torch._foreach_acos</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_acos_.html">torch._foreach_acos_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_asin.html">torch._foreach_asin</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_asin_.html">torch._foreach_asin_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_atan.html">torch._foreach_atan</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_atan_.html">torch._foreach_atan_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_ceil.html">torch._foreach_ceil</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_ceil_.html">torch._foreach_ceil_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_cos.html">torch._foreach_cos</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_cos_.html">torch._foreach_cos_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_cosh.html">torch._foreach_cosh</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_cosh_.html">torch._foreach_cosh_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_erf.html">torch._foreach_erf</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_erf_.html">torch._foreach_erf_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_erfc.html">torch._foreach_erfc</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_erfc_.html">torch._foreach_erfc_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_exp.html">torch._foreach_exp</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_exp_.html">torch._foreach_exp_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_expm1.html">torch._foreach_expm1</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_expm1_.html">torch._foreach_expm1_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_floor.html">torch._foreach_floor</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_floor_.html">torch._foreach_floor_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_log.html">torch._foreach_log</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_log_.html">torch._foreach_log_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_log10.html">torch._foreach_log10</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_log10_.html">torch._foreach_log10_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_log1p.html">torch._foreach_log1p</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_log1p_.html">torch._foreach_log1p_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_log2.html">torch._foreach_log2</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_log2_.html">torch._foreach_log2_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_neg.html">torch._foreach_neg</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_neg_.html">torch._foreach_neg_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_tan.html">torch._foreach_tan</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_tan_.html">torch._foreach_tan_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_sin.html">torch._foreach_sin</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_sin_.html">torch._foreach_sin_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_sinh.html">torch._foreach_sinh</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_sinh_.html">torch._foreach_sinh_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_round.html">torch._foreach_round</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_round_.html">torch._foreach_round_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_sqrt.html">torch._foreach_sqrt</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_sqrt_.html">torch._foreach_sqrt_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_lgamma.html">torch._foreach_lgamma</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_lgamma_.html">torch._foreach_lgamma_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_frac.html">torch._foreach_frac</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_frac_.html">torch._foreach_frac_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_reciprocal.html">torch._foreach_reciprocal</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_reciprocal_.html">torch._foreach_reciprocal_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_sigmoid.html">torch._foreach_sigmoid</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_sigmoid_.html">torch._foreach_sigmoid_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_trunc.html">torch._foreach_trunc</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_trunc_.html">torch._foreach_trunc_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_zero_.html">torch._foreach_zero_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.compiled_with_cxx11_abi.html">torch.compiled_with_cxx11_abi</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.result_type.html">torch.result_type</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.can_cast.html">torch.can_cast</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.promote_types.html">torch.promote_types</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.use_deterministic_algorithms.html">torch.use_deterministic_algorithms</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.are_deterministic_algorithms_enabled.html">torch.are_deterministic_algorithms_enabled</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.is_deterministic_algorithms_warn_only_enabled.html">torch.is_deterministic_algorithms_warn_only_enabled</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.set_deterministic_debug_mode.html">torch.set_deterministic_debug_mode</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.get_deterministic_debug_mode.html">torch.get_deterministic_debug_mode</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.set_float32_matmul_precision.html">torch.set_float32_matmul_precision</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.get_float32_matmul_precision.html">torch.get_float32_matmul_precision</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.set_warn_always.html">torch.set_warn_always</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.get_device_module.html">torch.get_device_module</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.is_warn_always_enabled.html">torch.is_warn_always_enabled</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.vmap.html">torch.vmap</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._assert.html">torch._assert</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.sym_float.html">torch.sym_float</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.sym_fresh_size.html">torch.sym_fresh_size</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.sym_int.html">torch.sym_int</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.sym_max.html">torch.sym_max</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.sym_min.html">torch.sym_min</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.sym_not.html">torch.sym_not</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.sym_ite.html">torch.sym_ite</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.sym_sum.html">torch.sym_sum</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cond.html">torch.cond</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.compile.html">torch.compile</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="nn.html">torch.nn</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.parameter.Buffer.html">Buffer</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.parameter.Parameter.html">Parameter</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.parameter.UninitializedParameter.html">UninitializedParameter</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.parameter.UninitializedBuffer.html">UninitializedBuffer</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.Module.html">Module</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.Sequential.html">Sequential</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.ModuleList.html">ModuleList</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.ModuleDict.html">ModuleDict</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.ParameterList.html">ParameterList</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.ParameterDict.html">ParameterDict</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.module.register_module_forward_pre_hook.html">torch.nn.modules.module.register_module_forward_pre_hook</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.module.register_module_forward_hook.html">torch.nn.modules.module.register_module_forward_hook</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.module.register_module_backward_hook.html">torch.nn.modules.module.register_module_backward_hook</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.module.register_module_full_backward_pre_hook.html">torch.nn.modules.module.register_module_full_backward_pre_hook</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.module.register_module_full_backward_hook.html">torch.nn.modules.module.register_module_full_backward_hook</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.module.register_module_buffer_registration_hook.html">torch.nn.modules.module.register_module_buffer_registration_hook</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.module.register_module_module_registration_hook.html">torch.nn.modules.module.register_module_module_registration_hook</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.module.register_module_parameter_registration_hook.html">torch.nn.modules.module.register_module_parameter_registration_hook</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.Conv1d.html">Conv1d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.Conv2d.html">Conv2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.Conv3d.html">Conv3d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.ConvTranspose1d.html">ConvTranspose1d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.ConvTranspose2d.html">ConvTranspose2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.ConvTranspose3d.html">ConvTranspose3d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.LazyConv1d.html">LazyConv1d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.LazyConv2d.html">LazyConv2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.LazyConv3d.html">LazyConv3d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.LazyConvTranspose1d.html">LazyConvTranspose1d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.LazyConvTranspose2d.html">LazyConvTranspose2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.LazyConvTranspose3d.html">LazyConvTranspose3d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.Unfold.html">Unfold</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.Fold.html">Fold</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.MaxPool1d.html">MaxPool1d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.MaxPool2d.html">MaxPool2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.MaxPool3d.html">MaxPool3d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.MaxUnpool1d.html">MaxUnpool1d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.MaxUnpool2d.html">MaxUnpool2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.MaxUnpool3d.html">MaxUnpool3d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.AvgPool1d.html">AvgPool1d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.AvgPool2d.html">AvgPool2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.AvgPool3d.html">AvgPool3d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.FractionalMaxPool2d.html">FractionalMaxPool2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.FractionalMaxPool3d.html">FractionalMaxPool3d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.LPPool1d.html">LPPool1d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.LPPool2d.html">LPPool2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.LPPool3d.html">LPPool3d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.AdaptiveMaxPool1d.html">AdaptiveMaxPool1d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.AdaptiveMaxPool2d.html">AdaptiveMaxPool2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.AdaptiveMaxPool3d.html">AdaptiveMaxPool3d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.AdaptiveAvgPool1d.html">AdaptiveAvgPool1d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.AdaptiveAvgPool2d.html">AdaptiveAvgPool2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.AdaptiveAvgPool3d.html">AdaptiveAvgPool3d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.ReflectionPad1d.html">ReflectionPad1d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.ReflectionPad2d.html">ReflectionPad2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.ReflectionPad3d.html">ReflectionPad3d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.ReplicationPad1d.html">ReplicationPad1d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.ReplicationPad2d.html">ReplicationPad2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.ReplicationPad3d.html">ReplicationPad3d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.ZeroPad1d.html">ZeroPad1d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.ZeroPad2d.html">ZeroPad2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.ZeroPad3d.html">ZeroPad3d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.ConstantPad1d.html">ConstantPad1d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.ConstantPad2d.html">ConstantPad2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.ConstantPad3d.html">ConstantPad3d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.CircularPad1d.html">CircularPad1d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.CircularPad2d.html">CircularPad2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.CircularPad3d.html">CircularPad3d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.ELU.html">ELU</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.Hardshrink.html">Hardshrink</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.Hardsigmoid.html">Hardsigmoid</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.Hardtanh.html">Hardtanh</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.Hardswish.html">Hardswish</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.LeakyReLU.html">LeakyReLU</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.LogSigmoid.html">LogSigmoid</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.MultiheadAttention.html">MultiheadAttention</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.PReLU.html">PReLU</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.ReLU.html">ReLU</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.ReLU6.html">ReLU6</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.RReLU.html">RReLU</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.SELU.html">SELU</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.CELU.html">CELU</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.GELU.html">GELU</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.Sigmoid.html">Sigmoid</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.SiLU.html">SiLU</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.Mish.html">Mish</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.Softplus.html">Softplus</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.Softshrink.html">Softshrink</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.Softsign.html">Softsign</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.Tanh.html">Tanh</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.Tanhshrink.html">Tanhshrink</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.Threshold.html">Threshold</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.GLU.html">GLU</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.Softmin.html">Softmin</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.Softmax.html">Softmax</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.Softmax2d.html">Softmax2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.LogSoftmax.html">LogSoftmax</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.AdaptiveLogSoftmaxWithLoss.html">AdaptiveLogSoftmaxWithLoss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.BatchNorm1d.html">BatchNorm1d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.BatchNorm2d.html">BatchNorm2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.BatchNorm3d.html">BatchNorm3d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.LazyBatchNorm1d.html">LazyBatchNorm1d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.LazyBatchNorm2d.html">LazyBatchNorm2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.LazyBatchNorm3d.html">LazyBatchNorm3d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.GroupNorm.html">GroupNorm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.SyncBatchNorm.html">SyncBatchNorm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.InstanceNorm1d.html">InstanceNorm1d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.InstanceNorm2d.html">InstanceNorm2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.InstanceNorm3d.html">InstanceNorm3d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.LazyInstanceNorm1d.html">LazyInstanceNorm1d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.LazyInstanceNorm2d.html">LazyInstanceNorm2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.LazyInstanceNorm3d.html">LazyInstanceNorm3d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.LayerNorm.html">LayerNorm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.LocalResponseNorm.html">LocalResponseNorm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.RMSNorm.html">RMSNorm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.RNNBase.html">RNNBase</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.RNN.html">RNN</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.LSTM.html">LSTM</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.GRU.html">GRU</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.RNNCell.html">RNNCell</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.LSTMCell.html">LSTMCell</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.GRUCell.html">GRUCell</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.Transformer.html">Transformer</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.TransformerEncoder.html">TransformerEncoder</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.TransformerDecoder.html">TransformerDecoder</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.TransformerEncoderLayer.html">TransformerEncoderLayer</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.TransformerDecoderLayer.html">TransformerDecoderLayer</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.Identity.html">Identity</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.Linear.html">Linear</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.Bilinear.html">Bilinear</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.LazyLinear.html">LazyLinear</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.Dropout.html">Dropout</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.Dropout1d.html">Dropout1d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.Dropout2d.html">Dropout2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.Dropout3d.html">Dropout3d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.AlphaDropout.html">AlphaDropout</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.FeatureAlphaDropout.html">FeatureAlphaDropout</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.Embedding.html">Embedding</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.EmbeddingBag.html">EmbeddingBag</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.CosineSimilarity.html">CosineSimilarity</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.PairwiseDistance.html">PairwiseDistance</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.L1Loss.html">L1Loss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.MSELoss.html">MSELoss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.CrossEntropyLoss.html">CrossEntropyLoss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.CTCLoss.html">CTCLoss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.NLLLoss.html">NLLLoss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.PoissonNLLLoss.html">PoissonNLLLoss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.GaussianNLLLoss.html">GaussianNLLLoss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.KLDivLoss.html">KLDivLoss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.BCELoss.html">BCELoss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.BCEWithLogitsLoss.html">BCEWithLogitsLoss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.MarginRankingLoss.html">MarginRankingLoss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.HingeEmbeddingLoss.html">HingeEmbeddingLoss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.MultiLabelMarginLoss.html">MultiLabelMarginLoss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.HuberLoss.html">HuberLoss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.SmoothL1Loss.html">SmoothL1Loss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.SoftMarginLoss.html">SoftMarginLoss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.MultiLabelSoftMarginLoss.html">MultiLabelSoftMarginLoss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.CosineEmbeddingLoss.html">CosineEmbeddingLoss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.MultiMarginLoss.html">MultiMarginLoss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.TripletMarginLoss.html">TripletMarginLoss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.TripletMarginWithDistanceLoss.html">TripletMarginWithDistanceLoss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.PixelShuffle.html">PixelShuffle</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.PixelUnshuffle.html">PixelUnshuffle</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.Upsample.html">Upsample</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.UpsamplingNearest2d.html">UpsamplingNearest2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.UpsamplingBilinear2d.html">UpsamplingBilinear2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.ChannelShuffle.html">ChannelShuffle</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.DataParallel.html">DataParallel</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.parallel.DistributedDataParallel.html">DistributedDataParallel</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.clip_grad_norm_.html">torch.nn.utils.clip_grad_norm_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.clip_grad_norm.html">torch.nn.utils.clip_grad_norm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.clip_grad_value_.html">torch.nn.utils.clip_grad_value_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.get_total_norm.html">torch.nn.utils.get_total_norm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.clip_grads_with_norm_.html">torch.nn.utils.clip_grads_with_norm_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.parameters_to_vector.html">torch.nn.utils.parameters_to_vector</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.vector_to_parameters.html">torch.nn.utils.vector_to_parameters</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.fuse_conv_bn_eval.html">torch.nn.utils.fuse_conv_bn_eval</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.fuse_conv_bn_weights.html">torch.nn.utils.fuse_conv_bn_weights</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.fuse_linear_bn_eval.html">torch.nn.utils.fuse_linear_bn_eval</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.fuse_linear_bn_weights.html">torch.nn.utils.fuse_linear_bn_weights</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.convert_conv2d_weight_memory_format.html">torch.nn.utils.convert_conv2d_weight_memory_format</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.convert_conv3d_weight_memory_format.html">torch.nn.utils.convert_conv3d_weight_memory_format</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.weight_norm.html">torch.nn.utils.weight_norm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.remove_weight_norm.html">torch.nn.utils.remove_weight_norm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.spectral_norm.html">torch.nn.utils.spectral_norm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.remove_spectral_norm.html">torch.nn.utils.remove_spectral_norm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.skip_init.html">torch.nn.utils.skip_init</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.prune.BasePruningMethod.html">BasePruningMethod</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.prune.PruningContainer.html">PruningContainer</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.prune.Identity.html">torch.nn.utils.prune.identity</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.prune.RandomUnstructured.html">RandomUnstructured</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.prune.L1Unstructured.html">L1Unstructured</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.prune.RandomStructured.html">RandomStructured</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.prune.LnStructured.html">LnStructured</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.prune.CustomFromMask.html">CustomFromMask</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.prune.random_unstructured.html">torch.nn.utils.prune.random_unstructured</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.prune.l1_unstructured.html">torch.nn.utils.prune.l1_unstructured</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.prune.random_structured.html">torch.nn.utils.prune.random_structured</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.prune.ln_structured.html">torch.nn.utils.prune.ln_structured</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.prune.global_unstructured.html">torch.nn.utils.prune.global_unstructured</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.prune.custom_from_mask.html">torch.nn.utils.prune.custom_from_mask</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.prune.remove.html">torch.nn.utils.prune.remove</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.prune.is_pruned.html">torch.nn.utils.prune.is_pruned</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.parametrizations.orthogonal.html">torch.nn.utils.parametrizations.orthogonal</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.parametrizations.weight_norm.html">torch.nn.utils.parametrizations.weight_norm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.parametrizations.spectral_norm.html">torch.nn.utils.parametrizations.spectral_norm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.parametrize.register_parametrization.html">torch.nn.utils.parametrize.register_parametrization</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.parametrize.remove_parametrizations.html">torch.nn.utils.parametrize.remove_parametrizations</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.parametrize.cached.html">torch.nn.utils.parametrize.cached</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.parametrize.is_parametrized.html">torch.nn.utils.parametrize.is_parametrized</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.parametrize.ParametrizationList.html">ParametrizationList</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.stateless.functional_call.html">torch.nn.utils.stateless.functional_call</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.rnn.PackedSequence.html">PackedSequence</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.rnn.pack_padded_sequence.html">torch.nn.utils.rnn.pack_padded_sequence</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.rnn.pad_packed_sequence.html">torch.nn.utils.rnn.pad_packed_sequence</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.rnn.pad_sequence.html">torch.nn.utils.rnn.pad_sequence</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.rnn.pack_sequence.html">torch.nn.utils.rnn.pack_sequence</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.rnn.unpack_sequence.html">torch.nn.utils.rnn.unpack_sequence</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.rnn.unpad_sequence.html">torch.nn.utils.rnn.unpad_sequence</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.Flatten.html">Flatten</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.Unflatten.html">Unflatten</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.lazy.LazyModuleMixin.html">LazyModuleMixin</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.normalization.RMSNorm.html">RMSNorm</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="nn.functional.html">torch.nn.functional</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.conv1d.html">torch.nn.functional.conv1d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.conv2d.html">torch.nn.functional.conv2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.conv3d.html">torch.nn.functional.conv3d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.conv_transpose1d.html">torch.nn.functional.conv_transpose1d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.conv_transpose2d.html">torch.nn.functional.conv_transpose2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.conv_transpose3d.html">torch.nn.functional.conv_transpose3d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.unfold.html">torch.nn.functional.unfold</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.fold.html">torch.nn.functional.fold</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.avg_pool1d.html">torch.nn.functional.avg_pool1d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.avg_pool2d.html">torch.nn.functional.avg_pool2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.avg_pool3d.html">torch.nn.functional.avg_pool3d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.max_pool1d.html">torch.nn.functional.max_pool1d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.max_pool2d.html">torch.nn.functional.max_pool2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.max_pool3d.html">torch.nn.functional.max_pool3d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.max_unpool1d.html">torch.nn.functional.max_unpool1d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.max_unpool2d.html">torch.nn.functional.max_unpool2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.max_unpool3d.html">torch.nn.functional.max_unpool3d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.lp_pool1d.html">torch.nn.functional.lp_pool1d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.lp_pool2d.html">torch.nn.functional.lp_pool2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.lp_pool3d.html">torch.nn.functional.lp_pool3d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.adaptive_max_pool1d.html">torch.nn.functional.adaptive_max_pool1d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.adaptive_max_pool2d.html">torch.nn.functional.adaptive_max_pool2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.adaptive_max_pool3d.html">torch.nn.functional.adaptive_max_pool3d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.adaptive_avg_pool1d.html">torch.nn.functional.adaptive_avg_pool1d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.adaptive_avg_pool2d.html">torch.nn.functional.adaptive_avg_pool2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.adaptive_avg_pool3d.html">torch.nn.functional.adaptive_avg_pool3d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.fractional_max_pool2d.html">torch.nn.functional.fractional_max_pool2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.fractional_max_pool3d.html">torch.nn.functional.fractional_max_pool3d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.scaled_dot_product_attention.html">torch.nn.functional.scaled_dot_product_attention</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.threshold.html">torch.nn.functional.threshold</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.threshold_.html">torch.nn.functional.threshold_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.relu.html">torch.nn.functional.relu</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.relu_.html">torch.nn.functional.relu_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.hardtanh.html">torch.nn.functional.hardtanh</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.hardtanh_.html">torch.nn.functional.hardtanh_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.hardswish.html">torch.nn.functional.hardswish</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.relu6.html">torch.nn.functional.relu6</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.elu.html">torch.nn.functional.elu</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.elu_.html">torch.nn.functional.elu_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.selu.html">torch.nn.functional.selu</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.celu.html">torch.nn.functional.celu</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.leaky_relu.html">torch.nn.functional.leaky_relu</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.leaky_relu_.html">torch.nn.functional.leaky_relu_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.prelu.html">torch.nn.functional.prelu</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.rrelu.html">torch.nn.functional.rrelu</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.rrelu_.html">torch.nn.functional.rrelu_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.glu.html">torch.nn.functional.glu</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.gelu.html">torch.nn.functional.gelu</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.logsigmoid.html">torch.nn.functional.logsigmoid</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.hardshrink.html">torch.nn.functional.hardshrink</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.tanhshrink.html">torch.nn.functional.tanhshrink</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.softsign.html">torch.nn.functional.softsign</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.softplus.html">torch.nn.functional.softplus</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.softmin.html">torch.nn.functional.softmin</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.softmax.html">torch.nn.functional.softmax</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.softshrink.html">torch.nn.functional.softshrink</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.gumbel_softmax.html">torch.nn.functional.gumbel_softmax</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.log_softmax.html">torch.nn.functional.log_softmax</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.tanh.html">torch.nn.functional.tanh</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.sigmoid.html">torch.nn.functional.sigmoid</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.hardsigmoid.html">torch.nn.functional.hardsigmoid</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.silu.html">torch.nn.functional.silu</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.mish.html">torch.nn.functional.mish</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.batch_norm.html">torch.nn.functional.batch_norm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.group_norm.html">torch.nn.functional.group_norm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.instance_norm.html">torch.nn.functional.instance_norm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.layer_norm.html">torch.nn.functional.layer_norm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.local_response_norm.html">torch.nn.functional.local_response_norm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.rms_norm.html">torch.nn.functional.rms_norm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.normalize.html">torch.nn.functional.normalize</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.linear.html">torch.nn.functional.linear</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.bilinear.html">torch.nn.functional.bilinear</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.dropout.html">torch.nn.functional.dropout</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.alpha_dropout.html">torch.nn.functional.alpha_dropout</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.feature_alpha_dropout.html">torch.nn.functional.feature_alpha_dropout</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.dropout1d.html">torch.nn.functional.dropout1d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.dropout2d.html">torch.nn.functional.dropout2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.dropout3d.html">torch.nn.functional.dropout3d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.embedding.html">torch.nn.functional.embedding</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.embedding_bag.html">torch.nn.functional.embedding_bag</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.one_hot.html">torch.nn.functional.one_hot</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.pairwise_distance.html">torch.nn.functional.pairwise_distance</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.cosine_similarity.html">torch.nn.functional.cosine_similarity</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.pdist.html">torch.nn.functional.pdist</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.binary_cross_entropy.html">torch.nn.functional.binary_cross_entropy</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.binary_cross_entropy_with_logits.html">torch.nn.functional.binary_cross_entropy_with_logits</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.poisson_nll_loss.html">torch.nn.functional.poisson_nll_loss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.cosine_embedding_loss.html">torch.nn.functional.cosine_embedding_loss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.cross_entropy.html">torch.nn.functional.cross_entropy</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.ctc_loss.html">torch.nn.functional.ctc_loss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.gaussian_nll_loss.html">torch.nn.functional.gaussian_nll_loss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.hinge_embedding_loss.html">torch.nn.functional.hinge_embedding_loss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.kl_div.html">torch.nn.functional.kl_div</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.l1_loss.html">torch.nn.functional.l1_loss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.mse_loss.html">torch.nn.functional.mse_loss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.margin_ranking_loss.html">torch.nn.functional.margin_ranking_loss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.multilabel_margin_loss.html">torch.nn.functional.multilabel_margin_loss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.multilabel_soft_margin_loss.html">torch.nn.functional.multilabel_soft_margin_loss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.multi_margin_loss.html">torch.nn.functional.multi_margin_loss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.nll_loss.html">torch.nn.functional.nll_loss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.huber_loss.html">torch.nn.functional.huber_loss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.smooth_l1_loss.html">torch.nn.functional.smooth_l1_loss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.soft_margin_loss.html">torch.nn.functional.soft_margin_loss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.triplet_margin_loss.html">torch.nn.functional.triplet_margin_loss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.triplet_margin_with_distance_loss.html">torch.nn.functional.triplet_margin_with_distance_loss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.pixel_shuffle.html">torch.nn.functional.pixel_shuffle</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.pixel_unshuffle.html">torch.nn.functional.pixel_unshuffle</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.pad.html">torch.nn.functional.pad</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.interpolate.html">torch.nn.functional.interpolate</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.upsample.html">torch.nn.functional.upsample</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.upsample_nearest.html">torch.nn.functional.upsample_nearest</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.upsample_bilinear.html">torch.nn.functional.upsample_bilinear</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.grid_sample.html">torch.nn.functional.grid_sample</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.affine_grid.html">torch.nn.functional.affine_grid</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.torch.nn.parallel.data_parallel.html">torch.nn.functional.torch.nn.parallel.data_parallel</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="tensors.html">torch.Tensor</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.new_tensor.html">torch.Tensor.new_tensor</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.new_full.html">torch.Tensor.new_full</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.new_empty.html">torch.Tensor.new_empty</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.new_ones.html">torch.Tensor.new_ones</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.new_zeros.html">torch.Tensor.new_zeros</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.is_cuda.html">torch.Tensor.is_cuda</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.is_quantized.html">torch.Tensor.is_quantized</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.is_meta.html">torch.Tensor.is_meta</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.device.html">torch.Tensor.device</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.grad.html">torch.Tensor.grad</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.ndim.html">torch.Tensor.ndim</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.real.html">torch.Tensor.real</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.imag.html">torch.Tensor.imag</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.nbytes.html">torch.Tensor.nbytes</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.itemsize.html">torch.Tensor.itemsize</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.abs.html">torch.Tensor.abs</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.abs_.html">torch.Tensor.abs_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.absolute.html">torch.Tensor.absolute</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.absolute_.html">torch.Tensor.absolute_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.acos.html">torch.Tensor.acos</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.acos_.html">torch.Tensor.acos_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.arccos.html">torch.Tensor.arccos</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.arccos_.html">torch.Tensor.arccos_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.add.html">torch.Tensor.add</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.add_.html">torch.Tensor.add_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.addbmm.html">torch.Tensor.addbmm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.addbmm_.html">torch.Tensor.addbmm_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.addcdiv.html">torch.Tensor.addcdiv</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.addcdiv_.html">torch.Tensor.addcdiv_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.addcmul.html">torch.Tensor.addcmul</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.addcmul_.html">torch.Tensor.addcmul_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.addmm.html">torch.Tensor.addmm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.addmm_.html">torch.Tensor.addmm_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.sspaddmm.html">torch.Tensor.sspaddmm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.addmv.html">torch.Tensor.addmv</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.addmv_.html">torch.Tensor.addmv_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.addr.html">torch.Tensor.addr</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.addr_.html">torch.Tensor.addr_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.adjoint.html">torch.Tensor.adjoint</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.allclose.html">torch.Tensor.allclose</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.amax.html">torch.Tensor.amax</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.amin.html">torch.Tensor.amin</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.aminmax.html">torch.Tensor.aminmax</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.angle.html">torch.Tensor.angle</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.apply_.html">torch.Tensor.apply_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.argmax.html">torch.Tensor.argmax</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.argmin.html">torch.Tensor.argmin</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.argsort.html">torch.Tensor.argsort</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.argwhere.html">torch.Tensor.argwhere</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.asin.html">torch.Tensor.asin</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.asin_.html">torch.Tensor.asin_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.arcsin.html">torch.Tensor.arcsin</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.arcsin_.html">torch.Tensor.arcsin_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.as_strided.html">torch.Tensor.as_strided</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.atan.html">torch.Tensor.atan</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.atan_.html">torch.Tensor.atan_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.arctan.html">torch.Tensor.arctan</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.arctan_.html">torch.Tensor.arctan_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.atan2.html">torch.Tensor.atan2</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.atan2_.html">torch.Tensor.atan2_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.arctan2.html">torch.Tensor.arctan2</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.arctan2_.html">torch.Tensor.arctan2_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.all.html">torch.Tensor.all</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.any.html">torch.Tensor.any</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.backward.html">torch.Tensor.backward</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.baddbmm.html">torch.Tensor.baddbmm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.baddbmm_.html">torch.Tensor.baddbmm_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.bernoulli.html">torch.Tensor.bernoulli</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.bernoulli_.html">torch.Tensor.bernoulli_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.bfloat16.html">torch.Tensor.bfloat16</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.bincount.html">torch.Tensor.bincount</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.bitwise_not.html">torch.Tensor.bitwise_not</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.bitwise_not_.html">torch.Tensor.bitwise_not_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.bitwise_and.html">torch.Tensor.bitwise_and</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.bitwise_and_.html">torch.Tensor.bitwise_and_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.bitwise_or.html">torch.Tensor.bitwise_or</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.bitwise_or_.html">torch.Tensor.bitwise_or_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.bitwise_xor.html">torch.Tensor.bitwise_xor</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.bitwise_xor_.html">torch.Tensor.bitwise_xor_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.bitwise_left_shift.html">torch.Tensor.bitwise_left_shift</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.bitwise_left_shift_.html">torch.Tensor.bitwise_left_shift_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.bitwise_right_shift.html">torch.Tensor.bitwise_right_shift</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.bitwise_right_shift_.html">torch.Tensor.bitwise_right_shift_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.bmm.html">torch.Tensor.bmm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.bool.html">torch.Tensor.bool</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.byte.html">torch.Tensor.byte</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.broadcast_to.html">torch.Tensor.broadcast_to</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.cauchy_.html">torch.Tensor.cauchy_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.ceil.html">torch.Tensor.ceil</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.ceil_.html">torch.Tensor.ceil_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.char.html">torch.Tensor.char</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.cholesky.html">torch.Tensor.cholesky</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.cholesky_inverse.html">torch.Tensor.cholesky_inverse</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.cholesky_solve.html">torch.Tensor.cholesky_solve</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.chunk.html">torch.Tensor.chunk</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.clamp.html">torch.Tensor.clamp</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.clamp_.html">torch.Tensor.clamp_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.clip.html">torch.Tensor.clip</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.clip_.html">torch.Tensor.clip_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.clone.html">torch.Tensor.clone</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.contiguous.html">torch.Tensor.contiguous</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.copy_.html">torch.Tensor.copy_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.conj.html">torch.Tensor.conj</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.conj_physical.html">torch.Tensor.conj_physical</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.conj_physical_.html">torch.Tensor.conj_physical_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.resolve_conj.html">torch.Tensor.resolve_conj</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.resolve_neg.html">torch.Tensor.resolve_neg</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.copysign.html">torch.Tensor.copysign</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.copysign_.html">torch.Tensor.copysign_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.cos.html">torch.Tensor.cos</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.cos_.html">torch.Tensor.cos_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.cosh.html">torch.Tensor.cosh</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.cosh_.html">torch.Tensor.cosh_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.corrcoef.html">torch.Tensor.corrcoef</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.count_nonzero.html">torch.Tensor.count_nonzero</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.cov.html">torch.Tensor.cov</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.acosh.html">torch.Tensor.acosh</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.acosh_.html">torch.Tensor.acosh_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.arccosh.html">torch.Tensor.arccosh</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.arccosh_.html">torch.Tensor.arccosh_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.cpu.html">torch.Tensor.cpu</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.cross.html">torch.Tensor.cross</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.cuda.html">torch.Tensor.cuda</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.logcumsumexp.html">torch.Tensor.logcumsumexp</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.cummax.html">torch.Tensor.cummax</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.cummin.html">torch.Tensor.cummin</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.cumprod.html">torch.Tensor.cumprod</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.cumprod_.html">torch.Tensor.cumprod_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.cumsum.html">torch.Tensor.cumsum</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.cumsum_.html">torch.Tensor.cumsum_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.chalf.html">torch.Tensor.chalf</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.cfloat.html">torch.Tensor.cfloat</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.cdouble.html">torch.Tensor.cdouble</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.data_ptr.html">torch.Tensor.data_ptr</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.deg2rad.html">torch.Tensor.deg2rad</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.dequantize.html">torch.Tensor.dequantize</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.det.html">torch.Tensor.det</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.dense_dim.html">torch.Tensor.dense_dim</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.detach.html">torch.Tensor.detach</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.detach_.html">torch.Tensor.detach_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.diag.html">torch.Tensor.diag</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.diag_embed.html">torch.Tensor.diag_embed</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.diagflat.html">torch.Tensor.diagflat</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.diagonal.html">torch.Tensor.diagonal</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.diagonal_scatter.html">torch.Tensor.diagonal_scatter</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.fill_diagonal_.html">torch.Tensor.fill_diagonal_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.fmax.html">torch.Tensor.fmax</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.fmin.html">torch.Tensor.fmin</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.diff.html">torch.Tensor.diff</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.digamma.html">torch.Tensor.digamma</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.digamma_.html">torch.Tensor.digamma_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.dim.html">torch.Tensor.dim</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.dim_order.html">torch.Tensor.dim_order</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.dist.html">torch.Tensor.dist</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.div.html">torch.Tensor.div</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.div_.html">torch.Tensor.div_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.divide.html">torch.Tensor.divide</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.divide_.html">torch.Tensor.divide_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.dot.html">torch.Tensor.dot</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.double.html">torch.Tensor.double</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.dsplit.html">torch.Tensor.dsplit</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.element_size.html">torch.Tensor.element_size</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.eq.html">torch.Tensor.eq</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.eq_.html">torch.Tensor.eq_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.equal.html">torch.Tensor.equal</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.erf.html">torch.Tensor.erf</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.erf_.html">torch.Tensor.erf_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.erfc.html">torch.Tensor.erfc</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.erfc_.html">torch.Tensor.erfc_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.erfinv.html">torch.Tensor.erfinv</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.erfinv_.html">torch.Tensor.erfinv_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.exp.html">torch.Tensor.exp</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.exp_.html">torch.Tensor.exp_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.expm1.html">torch.Tensor.expm1</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.expm1_.html">torch.Tensor.expm1_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.expand.html">torch.Tensor.expand</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.expand_as.html">torch.Tensor.expand_as</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.exponential_.html">torch.Tensor.exponential_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.fix.html">torch.Tensor.fix</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.fix_.html">torch.Tensor.fix_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.fill_.html">torch.Tensor.fill_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.flatten.html">torch.Tensor.flatten</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.flip.html">torch.Tensor.flip</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.fliplr.html">torch.Tensor.fliplr</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.flipud.html">torch.Tensor.flipud</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.float.html">torch.Tensor.float</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.float_power.html">torch.Tensor.float_power</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.float_power_.html">torch.Tensor.float_power_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.floor.html">torch.Tensor.floor</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.floor_.html">torch.Tensor.floor_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.floor_divide.html">torch.Tensor.floor_divide</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.floor_divide_.html">torch.Tensor.floor_divide_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.fmod.html">torch.Tensor.fmod</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.fmod_.html">torch.Tensor.fmod_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.frac.html">torch.Tensor.frac</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.frac_.html">torch.Tensor.frac_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.frexp.html">torch.Tensor.frexp</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.gather.html">torch.Tensor.gather</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.gcd.html">torch.Tensor.gcd</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.gcd_.html">torch.Tensor.gcd_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.ge.html">torch.Tensor.ge</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.ge_.html">torch.Tensor.ge_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.greater_equal.html">torch.Tensor.greater_equal</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.greater_equal_.html">torch.Tensor.greater_equal_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.geometric_.html">torch.Tensor.geometric_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.geqrf.html">torch.Tensor.geqrf</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.ger.html">torch.Tensor.ger</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.get_device.html">torch.Tensor.get_device</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.gt.html">torch.Tensor.gt</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.gt_.html">torch.Tensor.gt_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.greater.html">torch.Tensor.greater</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.greater_.html">torch.Tensor.greater_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.half.html">torch.Tensor.half</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.hardshrink.html">torch.Tensor.hardshrink</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.heaviside.html">torch.Tensor.heaviside</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.histc.html">torch.Tensor.histc</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.histogram.html">torch.Tensor.histogram</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.hsplit.html">torch.Tensor.hsplit</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.hypot.html">torch.Tensor.hypot</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.hypot_.html">torch.Tensor.hypot_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.i0.html">torch.Tensor.i0</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.i0_.html">torch.Tensor.i0_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.igamma.html">torch.Tensor.igamma</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.igamma_.html">torch.Tensor.igamma_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.igammac.html">torch.Tensor.igammac</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.igammac_.html">torch.Tensor.igammac_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.index_add_.html">torch.Tensor.index_add_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.index_add.html">torch.Tensor.index_add</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.index_copy_.html">torch.Tensor.index_copy_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.index_copy.html">torch.Tensor.index_copy</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.index_fill_.html">torch.Tensor.index_fill_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.index_fill.html">torch.Tensor.index_fill</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.index_put_.html">torch.Tensor.index_put_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.index_put.html">torch.Tensor.index_put</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.index_reduce_.html">torch.Tensor.index_reduce_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.index_reduce.html">torch.Tensor.index_reduce</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.index_select.html">torch.Tensor.index_select</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.indices.html">torch.Tensor.indices</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.inner.html">torch.Tensor.inner</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.int.html">torch.Tensor.int</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.int_repr.html">torch.Tensor.int_repr</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.inverse.html">torch.Tensor.inverse</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.isclose.html">torch.Tensor.isclose</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.isfinite.html">torch.Tensor.isfinite</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.isinf.html">torch.Tensor.isinf</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.isposinf.html">torch.Tensor.isposinf</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.isneginf.html">torch.Tensor.isneginf</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.isnan.html">torch.Tensor.isnan</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.is_contiguous.html">torch.Tensor.is_contiguous</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.is_complex.html">torch.Tensor.is_complex</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.is_conj.html">torch.Tensor.is_conj</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.is_floating_point.html">torch.Tensor.is_floating_point</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.is_inference.html">torch.Tensor.is_inference</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.is_leaf.html">torch.Tensor.is_leaf</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.is_pinned.html">torch.Tensor.is_pinned</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.is_set_to.html">torch.Tensor.is_set_to</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.is_shared.html">torch.Tensor.is_shared</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.is_signed.html">torch.Tensor.is_signed</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.is_sparse.html">torch.Tensor.is_sparse</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.istft.html">torch.Tensor.istft</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.isreal.html">torch.Tensor.isreal</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.item.html">torch.Tensor.item</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.kthvalue.html">torch.Tensor.kthvalue</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.lcm.html">torch.Tensor.lcm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.lcm_.html">torch.Tensor.lcm_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.ldexp.html">torch.Tensor.ldexp</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.ldexp_.html">torch.Tensor.ldexp_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.le.html">torch.Tensor.le</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.le_.html">torch.Tensor.le_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.less_equal.html">torch.Tensor.less_equal</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.less_equal_.html">torch.Tensor.less_equal_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.lerp.html">torch.Tensor.lerp</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.lerp_.html">torch.Tensor.lerp_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.lgamma.html">torch.Tensor.lgamma</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.lgamma_.html">torch.Tensor.lgamma_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.log.html">torch.Tensor.log</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.log_.html">torch.Tensor.log_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.logdet.html">torch.Tensor.logdet</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.log10.html">torch.Tensor.log10</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.log10_.html">torch.Tensor.log10_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.log1p.html">torch.Tensor.log1p</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.log1p_.html">torch.Tensor.log1p_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.log2.html">torch.Tensor.log2</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.log2_.html">torch.Tensor.log2_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.log_normal_.html">torch.Tensor.log_normal_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.logaddexp.html">torch.Tensor.logaddexp</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.logaddexp2.html">torch.Tensor.logaddexp2</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.logsumexp.html">torch.Tensor.logsumexp</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.logical_and.html">torch.Tensor.logical_and</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.logical_and_.html">torch.Tensor.logical_and_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.logical_not.html">torch.Tensor.logical_not</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.logical_not_.html">torch.Tensor.logical_not_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.logical_or.html">torch.Tensor.logical_or</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.logical_or_.html">torch.Tensor.logical_or_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.logical_xor.html">torch.Tensor.logical_xor</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.logical_xor_.html">torch.Tensor.logical_xor_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.logit.html">torch.Tensor.logit</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.logit_.html">torch.Tensor.logit_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.long.html">torch.Tensor.long</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.lt.html">torch.Tensor.lt</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.lt_.html">torch.Tensor.lt_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.less.html">torch.Tensor.less</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.less_.html">torch.Tensor.less_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.lu.html">torch.Tensor.lu</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.lu_solve.html">torch.Tensor.lu_solve</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.as_subclass.html">torch.Tensor.as_subclass</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.map_.html">torch.Tensor.map_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.masked_scatter_.html">torch.Tensor.masked_scatter_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.masked_scatter.html">torch.Tensor.masked_scatter</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.masked_fill_.html">torch.Tensor.masked_fill_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.masked_fill.html">torch.Tensor.masked_fill</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.masked_select.html">torch.Tensor.masked_select</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.matmul.html">torch.Tensor.matmul</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.matrix_power.html">torch.Tensor.matrix_power</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.matrix_exp.html">torch.Tensor.matrix_exp</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.max.html">torch.Tensor.max</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.maximum.html">torch.Tensor.maximum</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.mean.html">torch.Tensor.mean</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.module_load.html">torch.Tensor.module_load</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.nanmean.html">torch.Tensor.nanmean</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.median.html">torch.Tensor.median</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.nanmedian.html">torch.Tensor.nanmedian</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.min.html">torch.Tensor.min</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.minimum.html">torch.Tensor.minimum</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.mm.html">torch.Tensor.mm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.smm.html">torch.Tensor.smm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.mode.html">torch.Tensor.mode</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.movedim.html">torch.Tensor.movedim</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.moveaxis.html">torch.Tensor.moveaxis</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.msort.html">torch.Tensor.msort</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.mul.html">torch.Tensor.mul</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.mul_.html">torch.Tensor.mul_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.multiply.html">torch.Tensor.multiply</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.multiply_.html">torch.Tensor.multiply_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.multinomial.html">torch.Tensor.multinomial</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.mv.html">torch.Tensor.mv</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.mvlgamma.html">torch.Tensor.mvlgamma</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.mvlgamma_.html">torch.Tensor.mvlgamma_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.nansum.html">torch.Tensor.nansum</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.narrow.html">torch.Tensor.narrow</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.narrow_copy.html">torch.Tensor.narrow_copy</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.ndimension.html">torch.Tensor.ndimension</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.nan_to_num.html">torch.Tensor.nan_to_num</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.nan_to_num_.html">torch.Tensor.nan_to_num_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.ne.html">torch.Tensor.ne</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.ne_.html">torch.Tensor.ne_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.not_equal.html">torch.Tensor.not_equal</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.not_equal_.html">torch.Tensor.not_equal_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.neg.html">torch.Tensor.neg</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.neg_.html">torch.Tensor.neg_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.negative.html">torch.Tensor.negative</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.negative_.html">torch.Tensor.negative_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.nelement.html">torch.Tensor.nelement</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.nextafter.html">torch.Tensor.nextafter</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.nextafter_.html">torch.Tensor.nextafter_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.nonzero.html">torch.Tensor.nonzero</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.norm.html">torch.Tensor.norm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.normal_.html">torch.Tensor.normal_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.numel.html">torch.Tensor.numel</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.numpy.html">torch.Tensor.numpy</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.orgqr.html">torch.Tensor.orgqr</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.ormqr.html">torch.Tensor.ormqr</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.outer.html">torch.Tensor.outer</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.permute.html">torch.Tensor.permute</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.pin_memory.html">torch.Tensor.pin_memory</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.pinverse.html">torch.Tensor.pinverse</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.polygamma.html">torch.Tensor.polygamma</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.polygamma_.html">torch.Tensor.polygamma_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.positive.html">torch.Tensor.positive</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.pow.html">torch.Tensor.pow</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.pow_.html">torch.Tensor.pow_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.prod.html">torch.Tensor.prod</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.put_.html">torch.Tensor.put_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.qr.html">torch.Tensor.qr</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.qscheme.html">torch.Tensor.qscheme</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.quantile.html">torch.Tensor.quantile</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.nanquantile.html">torch.Tensor.nanquantile</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.q_scale.html">torch.Tensor.q_scale</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.q_zero_point.html">torch.Tensor.q_zero_point</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.q_per_channel_scales.html">torch.Tensor.q_per_channel_scales</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.q_per_channel_zero_points.html">torch.Tensor.q_per_channel_zero_points</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.q_per_channel_axis.html">torch.Tensor.q_per_channel_axis</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.rad2deg.html">torch.Tensor.rad2deg</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.random_.html">torch.Tensor.random_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.ravel.html">torch.Tensor.ravel</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.reciprocal.html">torch.Tensor.reciprocal</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.reciprocal_.html">torch.Tensor.reciprocal_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.record_stream.html">torch.Tensor.record_stream</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.register_hook.html">torch.Tensor.register_hook</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.register_post_accumulate_grad_hook.html">torch.Tensor.register_post_accumulate_grad_hook</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.remainder.html">torch.Tensor.remainder</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.remainder_.html">torch.Tensor.remainder_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.renorm.html">torch.Tensor.renorm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.renorm_.html">torch.Tensor.renorm_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.repeat.html">torch.Tensor.repeat</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.repeat_interleave.html">torch.Tensor.repeat_interleave</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.requires_grad.html">torch.Tensor.requires_grad</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.requires_grad_.html">torch.Tensor.requires_grad_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.reshape.html">torch.Tensor.reshape</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.reshape_as.html">torch.Tensor.reshape_as</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.resize_.html">torch.Tensor.resize_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.resize_as_.html">torch.Tensor.resize_as_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.retain_grad.html">torch.Tensor.retain_grad</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.retains_grad.html">torch.Tensor.retains_grad</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.roll.html">torch.Tensor.roll</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.rot90.html">torch.Tensor.rot90</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.round.html">torch.Tensor.round</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.round_.html">torch.Tensor.round_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.rsqrt.html">torch.Tensor.rsqrt</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.rsqrt_.html">torch.Tensor.rsqrt_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.scatter.html">torch.Tensor.scatter</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.scatter_.html">torch.Tensor.scatter_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.scatter_add_.html">torch.Tensor.scatter_add_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.scatter_add.html">torch.Tensor.scatter_add</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.scatter_reduce_.html">torch.Tensor.scatter_reduce_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.scatter_reduce.html">torch.Tensor.scatter_reduce</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.select.html">torch.Tensor.select</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.select_scatter.html">torch.Tensor.select_scatter</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.set_.html">torch.Tensor.set_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.share_memory_.html">torch.Tensor.share_memory_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.short.html">torch.Tensor.short</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.sigmoid.html">torch.Tensor.sigmoid</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.sigmoid_.html">torch.Tensor.sigmoid_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.sign.html">torch.Tensor.sign</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.sign_.html">torch.Tensor.sign_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.signbit.html">torch.Tensor.signbit</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.sgn.html">torch.Tensor.sgn</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.sgn_.html">torch.Tensor.sgn_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.sin.html">torch.Tensor.sin</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.sin_.html">torch.Tensor.sin_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.sinc.html">torch.Tensor.sinc</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.sinc_.html">torch.Tensor.sinc_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.sinh.html">torch.Tensor.sinh</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.sinh_.html">torch.Tensor.sinh_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.asinh.html">torch.Tensor.asinh</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.asinh_.html">torch.Tensor.asinh_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.arcsinh.html">torch.Tensor.arcsinh</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.arcsinh_.html">torch.Tensor.arcsinh_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.shape.html">torch.Tensor.shape</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.size.html">torch.Tensor.size</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.slogdet.html">torch.Tensor.slogdet</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.slice_scatter.html">torch.Tensor.slice_scatter</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.softmax.html">torch.Tensor.softmax</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.sort.html">torch.Tensor.sort</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.split.html">torch.Tensor.split</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.sparse_mask.html">torch.Tensor.sparse_mask</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.sparse_dim.html">torch.Tensor.sparse_dim</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.sqrt.html">torch.Tensor.sqrt</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.sqrt_.html">torch.Tensor.sqrt_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.square.html">torch.Tensor.square</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.square_.html">torch.Tensor.square_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.squeeze.html">torch.Tensor.squeeze</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.squeeze_.html">torch.Tensor.squeeze_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.std.html">torch.Tensor.std</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.stft.html">torch.Tensor.stft</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.storage.html">torch.Tensor.storage</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.untyped_storage.html">torch.Tensor.untyped_storage</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.storage_offset.html">torch.Tensor.storage_offset</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.storage_type.html">torch.Tensor.storage_type</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.stride.html">torch.Tensor.stride</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.sub.html">torch.Tensor.sub</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.sub_.html">torch.Tensor.sub_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.subtract.html">torch.Tensor.subtract</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.subtract_.html">torch.Tensor.subtract_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.sum.html">torch.Tensor.sum</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.sum_to_size.html">torch.Tensor.sum_to_size</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.svd.html">torch.Tensor.svd</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.swapaxes.html">torch.Tensor.swapaxes</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.swapdims.html">torch.Tensor.swapdims</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.t.html">torch.Tensor.t</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.t_.html">torch.Tensor.t_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.tensor_split.html">torch.Tensor.tensor_split</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.tile.html">torch.Tensor.tile</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.to.html">torch.Tensor.to</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.to_mkldnn.html">torch.Tensor.to_mkldnn</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.take.html">torch.Tensor.take</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.take_along_dim.html">torch.Tensor.take_along_dim</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.tan.html">torch.Tensor.tan</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.tan_.html">torch.Tensor.tan_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.tanh.html">torch.Tensor.tanh</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.tanh_.html">torch.Tensor.tanh_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.atanh.html">torch.Tensor.atanh</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.atanh_.html">torch.Tensor.atanh_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.arctanh.html">torch.Tensor.arctanh</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.arctanh_.html">torch.Tensor.arctanh_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.tolist.html">torch.Tensor.tolist</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.topk.html">torch.Tensor.topk</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.to_dense.html">torch.Tensor.to_dense</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.to_sparse.html">torch.Tensor.to_sparse</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.to_sparse_csr.html">torch.Tensor.to_sparse_csr</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.to_sparse_csc.html">torch.Tensor.to_sparse_csc</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.to_sparse_bsr.html">torch.Tensor.to_sparse_bsr</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.to_sparse_bsc.html">torch.Tensor.to_sparse_bsc</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.trace.html">torch.Tensor.trace</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.transpose.html">torch.Tensor.transpose</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.transpose_.html">torch.Tensor.transpose_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.triangular_solve.html">torch.Tensor.triangular_solve</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.tril.html">torch.Tensor.tril</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.tril_.html">torch.Tensor.tril_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.triu.html">torch.Tensor.triu</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.triu_.html">torch.Tensor.triu_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.true_divide.html">torch.Tensor.true_divide</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.true_divide_.html">torch.Tensor.true_divide_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.trunc.html">torch.Tensor.trunc</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.trunc_.html">torch.Tensor.trunc_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.type.html">torch.Tensor.type</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.type_as.html">torch.Tensor.type_as</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.unbind.html">torch.Tensor.unbind</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.unflatten.html">torch.Tensor.unflatten</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.unfold.html">torch.Tensor.unfold</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.uniform_.html">torch.Tensor.uniform_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.unique.html">torch.Tensor.unique</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.unique_consecutive.html">torch.Tensor.unique_consecutive</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.unsqueeze.html">torch.Tensor.unsqueeze</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.unsqueeze_.html">torch.Tensor.unsqueeze_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.values.html">torch.Tensor.values</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.var.html">torch.Tensor.var</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.vdot.html">torch.Tensor.vdot</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.view.html">torch.Tensor.view</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.view_as.html">torch.Tensor.view_as</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.vsplit.html">torch.Tensor.vsplit</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.where.html">torch.Tensor.where</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.xlogy.html">torch.Tensor.xlogy</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.xlogy_.html">torch.Tensor.xlogy_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.xpu.html">torch.Tensor.xpu</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.zero_.html">torch.Tensor.zero_</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="tensor_attributes.html">Tensor Attributes</a></li>
<li class="toctree-l1"><a class="reference internal" href="tensor_view.html">Tensor Views</a></li>
<li class="toctree-l1"><a class="reference internal" href="amp.html">torch.amp</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="autograd.html">torch.autograd</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.backward.html">torch.autograd.backward</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.grad.html">torch.autograd.grad</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.forward_ad.dual_level.html">dual_level</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.forward_ad.make_dual.html">torch.autograd.forward_ad.make_dual</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.forward_ad.unpack_dual.html">torch.autograd.forward_ad.unpack_dual</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.forward_ad.enter_dual_level.html">torch.autograd.forward_ad.enter_dual_level</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.forward_ad.exit_dual_level.html">torch.autograd.forward_ad.exit_dual_level</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.forward_ad.UnpackedDualTensor.html">UnpackedDualTensor</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.functional.jacobian.html">torch.autograd.functional.jacobian</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.functional.hessian.html">torch.autograd.functional.hessian</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.functional.vjp.html">torch.autograd.functional.vjp</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.functional.jvp.html">torch.autograd.functional.jvp</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.functional.vhp.html">torch.autograd.functional.vhp</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.functional.hvp.html">torch.autograd.functional.hvp</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.Function.forward.html">torch.autograd.Function.forward</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.Function.backward.html">torch.autograd.Function.backward</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.Function.jvp.html">torch.autograd.Function.jvp</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.Function.vmap.html">torch.autograd.Function.vmap</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.function.FunctionCtx.mark_dirty.html">torch.autograd.function.FunctionCtx.mark_dirty</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.function.FunctionCtx.mark_non_differentiable.html">torch.autograd.function.FunctionCtx.mark_non_differentiable</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.function.FunctionCtx.save_for_backward.html">torch.autograd.function.FunctionCtx.save_for_backward</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.function.FunctionCtx.set_materialize_grads.html">torch.autograd.function.FunctionCtx.set_materialize_grads</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.function.once_differentiable.html">torch.autograd.function.once_differentiable</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.function.BackwardCFunction.html">BackwardCFunction</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.function.InplaceFunction.html">InplaceFunction</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.function.NestedIOFunction.html">NestedIOFunction</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.gradcheck.gradcheck.html">torch.autograd.gradcheck.gradcheck</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.gradcheck.gradgradcheck.html">torch.autograd.gradcheck.gradgradcheck</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.gradcheck.GradcheckError.html">torch.autograd.gradcheck.GradcheckError</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.profiler.profile.export_chrome_trace.html">torch.autograd.profiler.profile.export_chrome_trace</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.profiler.profile.key_averages.html">torch.autograd.profiler.profile.key_averages</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.profiler.profile.self_cpu_time_total.html">torch.autograd.profiler.profile.self_cpu_time_total</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.profiler.profile.total_average.html">torch.autograd.profiler.profile.total_average</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.profiler.parse_nvprof_trace.html">torch.autograd.profiler.parse_nvprof_trace</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.profiler.EnforceUnique.html">EnforceUnique</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.profiler.KinetoStepTracker.html">KinetoStepTracker</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.profiler.record_function.html">record_function</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.profiler_util.Interval.html">Interval</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.profiler_util.Kernel.html">Kernel</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.profiler_util.MemRecordsAcc.html">MemRecordsAcc</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.profiler_util.StringTable.html">StringTable</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.profiler.load_nvprof.html">torch.autograd.profiler.load_nvprof</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.grad_mode.set_multithreading_enabled.html">set_multithreading_enabled</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.graph.Node.name.html">torch.autograd.graph.Node.name</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.graph.Node.metadata.html">torch.autograd.graph.Node.metadata</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.graph.Node.next_functions.html">torch.autograd.graph.Node.next_functions</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.graph.Node.register_hook.html">torch.autograd.graph.Node.register_hook</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.graph.Node.register_prehook.html">torch.autograd.graph.Node.register_prehook</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.graph.increment_version.html">torch.autograd.graph.increment_version</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="library.html">torch.library</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="accelerator.html">torch.accelerator</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.accelerator.device_count.html">torch.accelerator.device_count</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.accelerator.is_available.html">torch.accelerator.is_available</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.accelerator.current_accelerator.html">torch.accelerator.current_accelerator</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.accelerator.set_device_index.html">torch.accelerator.set_device_index</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.accelerator.set_device_idx.html">torch.accelerator.set_device_idx</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.accelerator.current_device_index.html">torch.accelerator.current_device_index</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.accelerator.current_device_idx.html">torch.accelerator.current_device_idx</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.accelerator.set_stream.html">torch.accelerator.set_stream</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.accelerator.current_stream.html">torch.accelerator.current_stream</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.accelerator.synchronize.html">torch.accelerator.synchronize</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="cpu.html">torch.cpu</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cpu.current_device.html">torch.cpu.current_device</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cpu.current_stream.html">torch.cpu.current_stream</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cpu.is_available.html">torch.cpu.is_available</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cpu.synchronize.html">torch.cpu.synchronize</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cpu.set_device.html">torch.cpu.set_device</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cpu.device_count.html">torch.cpu.device_count</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cpu.StreamContext.html">StreamContext</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cpu.Stream.html">torch.cpu.stream</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="cuda.html">torch.cuda</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.StreamContext.html">StreamContext</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.can_device_access_peer.html">torch.cuda.can_device_access_peer</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.current_blas_handle.html">torch.cuda.current_blas_handle</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.current_device.html">torch.cuda.current_device</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.current_stream.html">torch.cuda.current_stream</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.cudart.html">torch.cuda.cudart</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.default_stream.html">torch.cuda.default_stream</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.device.html">device</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.device_count.html">torch.cuda.device_count</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.device_memory_used.html">torch.cuda.device_memory_used</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.device_of.html">device_of</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.get_arch_list.html">torch.cuda.get_arch_list</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.get_device_capability.html">torch.cuda.get_device_capability</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.get_device_name.html">torch.cuda.get_device_name</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.get_device_properties.html">torch.cuda.get_device_properties</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.get_gencode_flags.html">torch.cuda.get_gencode_flags</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.get_sync_debug_mode.html">torch.cuda.get_sync_debug_mode</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.init.html">torch.cuda.init</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.ipc_collect.html">torch.cuda.ipc_collect</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.is_available.html">torch.cuda.is_available</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.is_initialized.html">torch.cuda.is_initialized</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.memory_usage.html">torch.cuda.memory_usage</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.set_device.html">torch.cuda.set_device</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.set_stream.html">torch.cuda.set_stream</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.set_sync_debug_mode.html">torch.cuda.set_sync_debug_mode</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.synchronize.html">torch.cuda.synchronize</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.utilization.html">torch.cuda.utilization</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.temperature.html">torch.cuda.temperature</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.power_draw.html">torch.cuda.power_draw</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.clock_rate.html">torch.cuda.clock_rate</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.OutOfMemoryError.html">torch.cuda.OutOfMemoryError</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.get_rng_state.html">torch.cuda.get_rng_state</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.get_rng_state_all.html">torch.cuda.get_rng_state_all</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.set_rng_state.html">torch.cuda.set_rng_state</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.set_rng_state_all.html">torch.cuda.set_rng_state_all</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.manual_seed.html">torch.cuda.manual_seed</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.manual_seed_all.html">torch.cuda.manual_seed_all</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.seed.html">torch.cuda.seed</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.seed_all.html">torch.cuda.seed_all</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.initial_seed.html">torch.cuda.initial_seed</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.comm.broadcast.html">torch.cuda.comm.broadcast</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.comm.broadcast_coalesced.html">torch.cuda.comm.broadcast_coalesced</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.comm.reduce_add.html">torch.cuda.comm.reduce_add</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.comm.reduce_add_coalesced.html">torch.cuda.comm.reduce_add_coalesced</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.comm.scatter.html">torch.cuda.comm.scatter</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.comm.gather.html">torch.cuda.comm.gather</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.Stream.html">torch.cuda.stream</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.ExternalStream.html">ExternalStream</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.Event.html">Event</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.is_current_stream_capturing.html">torch.cuda.is_current_stream_capturing</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.graph_pool_handle.html">torch.cuda.graph_pool_handle</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.CUDAGraph.html">CUDAGraph</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.graph.html">graph</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.make_graphed_callables.html">torch.cuda.make_graphed_callables</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.empty_cache.html">torch.cuda.empty_cache</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.get_per_process_memory_fraction.html">torch.cuda.get_per_process_memory_fraction</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.list_gpu_processes.html">torch.cuda.list_gpu_processes</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.mem_get_info.html">torch.cuda.mem_get_info</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.memory_stats.html">torch.cuda.memory_stats</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.memory_summary.html">torch.cuda.memory_summary</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.memory_snapshot.html">torch.cuda.memory_snapshot</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.memory_allocated.html">torch.cuda.memory_allocated</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.max_memory_allocated.html">torch.cuda.max_memory_allocated</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.reset_max_memory_allocated.html">torch.cuda.reset_max_memory_allocated</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.memory_reserved.html">torch.cuda.memory_reserved</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.max_memory_reserved.html">torch.cuda.max_memory_reserved</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.set_per_process_memory_fraction.html">torch.cuda.set_per_process_memory_fraction</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.memory_cached.html">torch.cuda.memory_cached</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.max_memory_cached.html">torch.cuda.max_memory_cached</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.reset_max_memory_cached.html">torch.cuda.reset_max_memory_cached</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.reset_peak_memory_stats.html">torch.cuda.reset_peak_memory_stats</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.caching_allocator_alloc.html">torch.cuda.caching_allocator_alloc</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.caching_allocator_delete.html">torch.cuda.caching_allocator_delete</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.get_allocator_backend.html">torch.cuda.get_allocator_backend</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.CUDAPluggableAllocator.html">CUDAPluggableAllocator</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.change_current_allocator.html">torch.cuda.change_current_allocator</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.MemPool.html">MemPool</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.MemPoolContext.html">MemPoolContext</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.memory.caching_allocator_enable.html">torch.cuda.memory.caching_allocator_enable</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.nvtx.mark.html">torch.cuda.nvtx.mark</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.nvtx.range_push.html">torch.cuda.nvtx.range_push</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.nvtx.range_pop.html">torch.cuda.nvtx.range_pop</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.nvtx.range.html">torch.cuda.nvtx.range</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.jiterator._create_jit_fn.html">torch.cuda.jiterator._create_jit_fn</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.jiterator._create_multi_output_jit_fn.html">torch.cuda.jiterator._create_multi_output_jit_fn</a></li>
<li class="toctree-l2"><a class="reference internal" href="cuda.tunable.html">TunableOp</a></li>
<li class="toctree-l2"><a class="reference internal" href="cuda._sanitizer.html">CUDA Stream Sanitizer</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="torch_cuda_memory.html">Understanding CUDA Memory Usage</a></li>



<li class="toctree-l1 has-children"><a class="reference internal" href="mps.html">torch.mps</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mps.device_count.html">torch.mps.device_count</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mps.synchronize.html">torch.mps.synchronize</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mps.get_rng_state.html">torch.mps.get_rng_state</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mps.set_rng_state.html">torch.mps.set_rng_state</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mps.manual_seed.html">torch.mps.manual_seed</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mps.seed.html">torch.mps.seed</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mps.empty_cache.html">torch.mps.empty_cache</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mps.set_per_process_memory_fraction.html">torch.mps.set_per_process_memory_fraction</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mps.current_allocated_memory.html">torch.mps.current_allocated_memory</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mps.driver_allocated_memory.html">torch.mps.driver_allocated_memory</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mps.recommended_max_memory.html">torch.mps.recommended_max_memory</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mps.profiler.start.html">torch.mps.profiler.start</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mps.profiler.stop.html">torch.mps.profiler.stop</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mps.profiler.profile.html">torch.mps.profiler.profile</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mps.event.Event.html">Event</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="xpu.html">torch.xpu</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xpu.StreamContext.html">StreamContext</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xpu.current_device.html">torch.xpu.current_device</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xpu.current_stream.html">torch.xpu.current_stream</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xpu.device.html">device</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xpu.device_count.html">torch.xpu.device_count</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xpu.device_of.html">device_of</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xpu.get_arch_list.html">torch.xpu.get_arch_list</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xpu.get_device_capability.html">torch.xpu.get_device_capability</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xpu.get_device_name.html">torch.xpu.get_device_name</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xpu.get_device_properties.html">torch.xpu.get_device_properties</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xpu.get_gencode_flags.html">torch.xpu.get_gencode_flags</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xpu.init.html">torch.xpu.init</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xpu.is_available.html">torch.xpu.is_available</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xpu.is_initialized.html">torch.xpu.is_initialized</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xpu.set_device.html">torch.xpu.set_device</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xpu.set_stream.html">torch.xpu.set_stream</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xpu.synchronize.html">torch.xpu.synchronize</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xpu.get_rng_state.html">torch.xpu.get_rng_state</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xpu.get_rng_state_all.html">torch.xpu.get_rng_state_all</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xpu.initial_seed.html">torch.xpu.initial_seed</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xpu.manual_seed.html">torch.xpu.manual_seed</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xpu.manual_seed_all.html">torch.xpu.manual_seed_all</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xpu.seed.html">torch.xpu.seed</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xpu.seed_all.html">torch.xpu.seed_all</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xpu.set_rng_state.html">torch.xpu.set_rng_state</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xpu.set_rng_state_all.html">torch.xpu.set_rng_state_all</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xpu.Event.html">Event</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xpu.Stream.html">torch.xpu.stream</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xpu.empty_cache.html">torch.xpu.empty_cache</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xpu.max_memory_allocated.html">torch.xpu.max_memory_allocated</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xpu.max_memory_reserved.html">torch.xpu.max_memory_reserved</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xpu.mem_get_info.html">torch.xpu.mem_get_info</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xpu.memory_allocated.html">torch.xpu.memory_allocated</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xpu.memory_reserved.html">torch.xpu.memory_reserved</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xpu.memory_stats.html">torch.xpu.memory_stats</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xpu.memory_stats_as_nested_dict.html">torch.xpu.memory_stats_as_nested_dict</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xpu.reset_accumulated_memory_stats.html">torch.xpu.reset_accumulated_memory_stats</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xpu.reset_peak_memory_stats.html">torch.xpu.reset_peak_memory_stats</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="mtia.html">torch.mtia</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mtia.StreamContext.html">StreamContext</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mtia.current_device.html">torch.mtia.current_device</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mtia.current_stream.html">torch.mtia.current_stream</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mtia.default_stream.html">torch.mtia.default_stream</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mtia.device_count.html">torch.mtia.device_count</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mtia.init.html">torch.mtia.init</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mtia.is_available.html">torch.mtia.is_available</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mtia.is_initialized.html">torch.mtia.is_initialized</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mtia.memory_stats.html">torch.mtia.memory_stats</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mtia.get_device_capability.html">torch.mtia.get_device_capability</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mtia.empty_cache.html">torch.mtia.empty_cache</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mtia.set_device.html">torch.mtia.set_device</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mtia.set_stream.html">torch.mtia.set_stream</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mtia.synchronize.html">torch.mtia.synchronize</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mtia.device.html">device</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mtia.set_rng_state.html">torch.mtia.set_rng_state</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mtia.get_rng_state.html">torch.mtia.get_rng_state</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mtia.DeferredMtiaCallError.html">torch.mtia.DeferredMtiaCallError</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mtia.Event.html">Event</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mtia.Stream.html">torch.mtia.stream</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="mtia.memory.html">torch.mtia.memory</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mtia.memory.memory_stats.html">torch.mtia.memory.memory_stats</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="meta.html">Meta device</a></li>
<li class="toctree-l1"><a class="reference internal" href="backends.html">torch.backends</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="export.html">torch.export</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="export.programming_model.html">torch.export Programming Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="export.ir_spec.html">torch.export IR Specification</a></li>
<li class="toctree-l2"><a class="reference internal" href="torch.compiler_transformations.html">Writing Graph Transformations on ATen IR</a></li>
<li class="toctree-l2"><a class="reference internal" href="torch.compiler_ir.html">IRs</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="generated/exportdb/index.html">ExportDB</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="generated/exportdb/python.control-flow.html">python.control-flow</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/exportdb/python.data-structure.html">python.data-structure</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/exportdb/torch.cond.html">torch.cond</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/exportdb/python.closure.html">python.closure</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/exportdb/torch.dynamic-value.html">torch.dynamic-value</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/exportdb/torch.escape-hatch.html">torch.escape-hatch</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/exportdb/torch.dynamic-shape.html">torch.dynamic-shape</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/exportdb/python.builtin.html">python.builtin</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/exportdb/torch.map.html">torch.map</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/exportdb/python.assert.html">python.assert</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/exportdb/torch.operator.html">torch.operator</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/exportdb/python.object-model.html">python.object-model</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/exportdb/torch.mutation.html">torch.mutation</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/exportdb/python.context-manager.html">python.context-manager</a></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="cond.html">Control Flow - Cond</a></li>
<li class="toctree-l2"><a class="reference internal" href="torch.compiler_dynamo_overview.html">Dynamo Overview</a></li>
<li class="toctree-l2"><a class="reference internal" href="torch.compiler_dynamo_deepdive.html">Dynamo Deep-Dive</a></li>
<li class="toctree-l2"><a class="reference internal" href="torch.compiler_dynamic_shapes.html">Dynamic shapes</a></li>
<li class="toctree-l2"><a class="reference internal" href="torch.compiler_fake_tensor.html">Fake tensor</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="distributed.html">torch.distributed</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed.tensor.html">torch.distributed.tensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed.algorithms.join.html">torch.distributed.algorithms.join</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="distributed.elastic.html">torch.distributed.elastic</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="elastic/quickstart.html">Quickstart</a></li>
<li class="toctree-l2"><a class="reference internal" href="elastic/train_script.html">Train script</a></li>
<li class="toctree-l2"><a class="reference internal" href="elastic/examples.html">Examples</a></li>
<li class="toctree-l2"><a class="reference internal" href="elastic/run.html">torchrun (Elastic Launch)</a></li>
<li class="toctree-l2"><a class="reference internal" href="elastic/agent.html">Elastic Agent</a></li>
<li class="toctree-l2"><a class="reference internal" href="elastic/multiprocessing.html">Multiprocessing</a></li>
<li class="toctree-l2"><a class="reference internal" href="elastic/errors.html">Error Propagation</a></li>
<li class="toctree-l2"><a class="reference internal" href="elastic/rendezvous.html">Rendezvous</a></li>
<li class="toctree-l2"><a class="reference internal" href="elastic/timer.html">Expiration Timers</a></li>
<li class="toctree-l2"><a class="reference internal" href="elastic/metrics.html">Metrics</a></li>
<li class="toctree-l2"><a class="reference internal" href="elastic/events.html">Events</a></li>
<li class="toctree-l2"><a class="reference internal" href="elastic/subprocess_handler.html">Subprocess Handling</a></li>
<li class="toctree-l2"><a class="reference internal" href="elastic/control_plane.html">Control Plane</a></li>
<li class="toctree-l2"><a class="reference internal" href="elastic/customization.html">Customization</a></li>
<li class="toctree-l2"><a class="reference internal" href="elastic/kubernetes.html">TorchElastic Kubernetes</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="fsdp.html">torch.distributed.fsdp</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed.fsdp.fully_shard.html">torch.distributed.fsdp.fully_shard</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed.tensor.parallel.html">torch.distributed.tensor.parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed.optim.html">torch.distributed.optim</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed.pipelining.html">torch.distributed.pipelining</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed.checkpoint.html">torch.distributed.checkpoint</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">torch.distributions</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="torch.compiler.html">torch.compiler</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="torch.compiler_get_started.html">Getting Started</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="torch.compiler_api.html">torch.compiler API reference</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.compiler.compile.html">torch.compiler.compile</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.compiler.reset.html">torch.compiler.reset</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.compiler.allow_in_graph.html">torch.compiler.allow_in_graph</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.compiler.substitute_in_graph.html">torch.compiler.substitute_in_graph</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.compiler.assume_constant_result.html">torch.compiler.assume_constant_result</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.compiler.list_backends.html">torch.compiler.list_backends</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.compiler.disable.html">torch.compiler.disable</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.compiler.set_stance.html">torch.compiler.set_stance</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.compiler.cudagraph_mark_step_begin.html">torch.compiler.cudagraph_mark_step_begin</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.compiler.is_compiling.html">torch.compiler.is_compiling</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.compiler.is_dynamo_compiling.html">torch.compiler.is_dynamo_compiling</a></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="torch.compiler.config.html">torch.compiler.config</a></li>
<li class="toctree-l2"><a class="reference internal" href="torch.compiler_fine_grain_apis.html">TorchDynamo APIs for fine-grained tracing</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="torch.compiler_aot_inductor.html">AOTInductor: Ahead-Of-Time Compilation for Torch.Export-ed Models</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3 has-children"><a class="reference internal" href="logging.html">torch._logging</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l4"><a class="reference internal" href="generated/torch._logging.set_logs.html">torch._logging.set_logs</a></li>
</ul>
</details></li>
<li class="toctree-l3"><a class="reference internal" href="torch.compiler_aot_inductor_minifier.html">AOTInductor Minifier</a></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="torch.compiler_inductor_profiling.html">TorchInductor GPU Profiling</a></li>
<li class="toctree-l2"><a class="reference internal" href="torch.compiler_profiling_torch_compile.html">Profiling to understand torch.compile performance</a></li>
<li class="toctree-l2"><a class="reference internal" href="torch.compiler_faq.html">Frequently Asked Questions</a></li>
<li class="toctree-l2"><a class="reference internal" href="torch.compiler_troubleshooting.html">torch.compile Troubleshooting</a></li>
<li class="toctree-l2"><a class="reference internal" href="torch.compiler_performance_dashboard.html">PyTorch 2.0 Performance Dashboard</a></li>
<li class="toctree-l2"><a class="reference internal" href="torch.compiler_dynamo_overview.html">Dynamo Overview</a></li>
<li class="toctree-l2"><a class="reference internal" href="torch.compiler_dynamo_deepdive.html">Dynamo Deep-Dive</a></li>
<li class="toctree-l2"><a class="reference internal" href="torch.compiler_dynamic_shapes.html">Dynamic shapes</a></li>
<li class="toctree-l2"><a class="reference internal" href="torch.compiler_nn_module.html">PyTorch 2.0 NNModule Support</a></li>
<li class="toctree-l2"><a class="reference internal" href="torch.compiler_best_practices_for_backends.html">Best Practices for Backends</a></li>
<li class="toctree-l2"><a class="reference internal" href="torch.compiler_cudagraph_trees.html">CUDAGraph Trees</a></li>
<li class="toctree-l2"><a class="reference internal" href="torch.compiler_fake_tensor.html">Fake tensor</a></li>
<li class="toctree-l2"><a class="reference internal" href="torch.compiler_custom_backends.html">Custom Backends</a></li>
<li class="toctree-l2"><a class="reference internal" href="torch.compiler_transformations.html">Writing Graph Transformations on ATen IR</a></li>
<li class="toctree-l2"><a class="reference internal" href="torch.compiler_ir.html">IRs</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="fft.html">torch.fft</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fft.fft.html">torch.fft.fft</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fft.ifft.html">torch.fft.ifft</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fft.fft2.html">torch.fft.fft2</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fft.ifft2.html">torch.fft.ifft2</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fft.fftn.html">torch.fft.fftn</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fft.ifftn.html">torch.fft.ifftn</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fft.rfft.html">torch.fft.rfft</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fft.irfft.html">torch.fft.irfft</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fft.rfft2.html">torch.fft.rfft2</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fft.irfft2.html">torch.fft.irfft2</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fft.rfftn.html">torch.fft.rfftn</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fft.irfftn.html">torch.fft.irfftn</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fft.hfft.html">torch.fft.hfft</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fft.ihfft.html">torch.fft.ihfft</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fft.hfft2.html">torch.fft.hfft2</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fft.ihfft2.html">torch.fft.ihfft2</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fft.hfftn.html">torch.fft.hfftn</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fft.ihfftn.html">torch.fft.ihfftn</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fft.fftfreq.html">torch.fft.fftfreq</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fft.rfftfreq.html">torch.fft.rfftfreq</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fft.fftshift.html">torch.fft.fftshift</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fft.ifftshift.html">torch.fft.ifftshift</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="func.html">torch.func</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="func.whirlwind_tour.html">torch.func Whirlwind Tour</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="func.api.html">torch.func API Reference</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.func.vmap.html">torch.func.vmap</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.func.grad.html">torch.func.grad</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.func.grad_and_value.html">torch.func.grad_and_value</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.func.vjp.html">torch.func.vjp</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.func.jvp.html">torch.func.jvp</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.func.linearize.html">torch.func.linearize</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.func.jacrev.html">torch.func.jacrev</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.func.jacfwd.html">torch.func.jacfwd</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.func.hessian.html">torch.func.hessian</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.func.functionalize.html">torch.func.functionalize</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.func.functional_call.html">torch.func.functional_call</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.func.stack_module_state.html">torch.func.stack_module_state</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.func.replace_all_batch_norm_modules_.html">torch.func.replace_all_batch_norm_modules_</a></li>
<li class="toctree-l3"><a class="reference internal" href="func.batch_norm.html">Patching Batch Norm</a></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="func.ux_limitations.html">UX Limitations</a></li>
<li class="toctree-l2"><a class="reference internal" href="func.migrating.html">Migrating from functorch to torch.func</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="futures.html">torch.futures</a></li>
<li class="toctree-l1"><a class="reference internal" href="fx.html">torch.fx</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="fx.experimental.html">torch.fx.experimental</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fx.experimental.symbolic_shapes.ShapeEnv.html">ShapeEnv</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fx.experimental.symbolic_shapes.DimDynamic.html">DimDynamic</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fx.experimental.symbolic_shapes.StrictMinMaxConstraint.html">StrictMinMaxConstraint</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fx.experimental.symbolic_shapes.RelaxedUnspecConstraint.html">RelaxedUnspecConstraint</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fx.experimental.symbolic_shapes.EqualityConstraint.html">EqualityConstraint</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fx.experimental.symbolic_shapes.SymbolicContext.html">SymbolicContext</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fx.experimental.symbolic_shapes.StatelessSymbolicContext.html">StatelessSymbolicContext</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fx.experimental.symbolic_shapes.StatefulSymbolicContext.html">StatefulSymbolicContext</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fx.experimental.symbolic_shapes.SubclassSymbolicContext.html">SubclassSymbolicContext</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fx.experimental.symbolic_shapes.DimConstraints.html">DimConstraints</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fx.experimental.symbolic_shapes.ShapeEnvSettings.html">ShapeEnvSettings</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fx.experimental.symbolic_shapes.ConvertIntKey.html">ConvertIntKey</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fx.experimental.symbolic_shapes.CallMethodKey.html">CallMethodKey</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fx.experimental.symbolic_shapes.PropagateUnbackedSymInts.html">PropagateUnbackedSymInts</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fx.experimental.symbolic_shapes.DivideByKey.html">DivideByKey</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fx.experimental.symbolic_shapes.InnerTensorKey.html">InnerTensorKey</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fx.experimental.symbolic_shapes.hint_int.html">torch.fx.experimental.symbolic_shapes.hint_int</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fx.experimental.symbolic_shapes.is_concrete_int.html">torch.fx.experimental.symbolic_shapes.is_concrete_int</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fx.experimental.symbolic_shapes.is_concrete_bool.html">torch.fx.experimental.symbolic_shapes.is_concrete_bool</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fx.experimental.symbolic_shapes.is_concrete_float.html">torch.fx.experimental.symbolic_shapes.is_concrete_float</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fx.experimental.symbolic_shapes.has_free_symbols.html">torch.fx.experimental.symbolic_shapes.has_free_symbols</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fx.experimental.symbolic_shapes.has_free_unbacked_symbols.html">torch.fx.experimental.symbolic_shapes.has_free_unbacked_symbols</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fx.experimental.symbolic_shapes.definitely_true.html">torch.fx.experimental.symbolic_shapes.definitely_true</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fx.experimental.symbolic_shapes.definitely_false.html">torch.fx.experimental.symbolic_shapes.definitely_false</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fx.experimental.symbolic_shapes.guard_size_oblivious.html">torch.fx.experimental.symbolic_shapes.guard_size_oblivious</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fx.experimental.symbolic_shapes.sym_eq.html">torch.fx.experimental.symbolic_shapes.sym_eq</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fx.experimental.symbolic_shapes.constrain_range.html">torch.fx.experimental.symbolic_shapes.constrain_range</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fx.experimental.symbolic_shapes.constrain_unify.html">torch.fx.experimental.symbolic_shapes.constrain_unify</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fx.experimental.symbolic_shapes.canonicalize_bool_expr.html">torch.fx.experimental.symbolic_shapes.canonicalize_bool_expr</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fx.experimental.symbolic_shapes.statically_known_true.html">torch.fx.experimental.symbolic_shapes.statically_known_true</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fx.experimental.symbolic_shapes.lru_cache.html">torch.fx.experimental.symbolic_shapes.lru_cache</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fx.experimental.symbolic_shapes.check_consistent.html">torch.fx.experimental.symbolic_shapes.check_consistent</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fx.experimental.symbolic_shapes.compute_unbacked_bindings.html">torch.fx.experimental.symbolic_shapes.compute_unbacked_bindings</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fx.experimental.symbolic_shapes.rebind_unbacked.html">torch.fx.experimental.symbolic_shapes.rebind_unbacked</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fx.experimental.symbolic_shapes.resolve_unbacked_bindings.html">torch.fx.experimental.symbolic_shapes.resolve_unbacked_bindings</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fx.experimental.symbolic_shapes.is_accessor_node.html">torch.fx.experimental.symbolic_shapes.is_accessor_node</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fx.experimental.proxy_tensor.make_fx.html">torch.fx.experimental.proxy_tensor.make_fx</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fx.experimental.proxy_tensor.handle_sym_dispatch.html">torch.fx.experimental.proxy_tensor.handle_sym_dispatch</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fx.experimental.proxy_tensor.get_proxy_mode.html">torch.fx.experimental.proxy_tensor.get_proxy_mode</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fx.experimental.proxy_tensor.maybe_enable_thunkify.html">torch.fx.experimental.proxy_tensor.maybe_enable_thunkify</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fx.experimental.proxy_tensor.maybe_disable_thunkify.html">torch.fx.experimental.proxy_tensor.maybe_disable_thunkify</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="hub.html">torch.hub</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="jit.html">torch.jit</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="jit_builtin_functions.html">torch.jit.supported_ops</a></li>
<li class="toctree-l2"><a class="reference internal" href="jit_language_reference.html">TorchScript Language Reference</a></li>
<li class="toctree-l2"><a class="reference internal" href="jit_language_reference_v2.html">TorchScript Language Reference</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.jit.script.html">torch.jit.script</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.jit.trace.html">torch.jit.trace</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.jit.script_if_tracing.html">torch.jit.script_if_tracing</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.jit.trace_module.html">torch.jit.trace_module</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.jit.fork.html">torch.jit.fork</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.jit.wait.html">torch.jit.wait</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.jit.ScriptModule.html">ScriptModule</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.jit.ScriptFunction.html">ScriptFunction</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.jit.freeze.html">torch.jit.freeze</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.jit.optimize_for_inference.html">torch.jit.optimize_for_inference</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.jit.enable_onednn_fusion.html">torch.jit.enable_onednn_fusion</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.jit.onednn_fusion_enabled.html">torch.jit.onednn_fusion_enabled</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.jit.set_fusion_strategy.html">torch.jit.set_fusion_strategy</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.jit.strict_fusion.html">strict_fusion</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.jit.save.html">torch.jit.save</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.jit.load.html">torch.jit.load</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.jit.ignore.html">torch.jit.ignore</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.jit.unused.html">torch.jit.unused</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.jit.interface.html">torch.jit.interface</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.jit.isinstance.html">torch.jit.isinstance</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.jit.Attribute.html">Attribute</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.jit.annotate.html">torch.jit.annotate</a></li>
<li class="toctree-l2"><a class="reference internal" href="jit_python_reference.html">Python Language Reference Coverage</a></li>
<li class="toctree-l2"><a class="reference internal" href="jit_unsupported.html">TorchScript Unsupported PyTorch Constructs</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="linalg.html">torch.linalg</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.linalg.norm.html">torch.linalg.norm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.linalg.vector_norm.html">torch.linalg.vector_norm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.linalg.matrix_norm.html">torch.linalg.matrix_norm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.linalg.diagonal.html">torch.linalg.diagonal</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.linalg.det.html">torch.linalg.det</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.linalg.slogdet.html">torch.linalg.slogdet</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.linalg.cond.html">torch.linalg.cond</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.linalg.matrix_rank.html">torch.linalg.matrix_rank</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.linalg.cholesky.html">torch.linalg.cholesky</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.linalg.qr.html">torch.linalg.qr</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.linalg.lu.html">torch.linalg.lu</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.linalg.lu_factor.html">torch.linalg.lu_factor</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.linalg.eig.html">torch.linalg.eig</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.linalg.eigvals.html">torch.linalg.eigvals</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.linalg.eigh.html">torch.linalg.eigh</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.linalg.eigvalsh.html">torch.linalg.eigvalsh</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.linalg.svd.html">torch.linalg.svd</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.linalg.svdvals.html">torch.linalg.svdvals</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.linalg.solve.html">torch.linalg.solve</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.linalg.solve_triangular.html">torch.linalg.solve_triangular</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.linalg.lu_solve.html">torch.linalg.lu_solve</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.linalg.lstsq.html">torch.linalg.lstsq</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.linalg.inv.html">torch.linalg.inv</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.linalg.pinv.html">torch.linalg.pinv</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.linalg.matrix_exp.html">torch.linalg.matrix_exp</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.linalg.matrix_power.html">torch.linalg.matrix_power</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.linalg.cross.html">torch.linalg.cross</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.linalg.matmul.html">torch.linalg.matmul</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.linalg.vecdot.html">torch.linalg.vecdot</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.linalg.multi_dot.html">torch.linalg.multi_dot</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.linalg.householder_product.html">torch.linalg.householder_product</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.linalg.tensorinv.html">torch.linalg.tensorinv</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.linalg.tensorsolve.html">torch.linalg.tensorsolve</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.linalg.vander.html">torch.linalg.vander</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.linalg.cholesky_ex.html">torch.linalg.cholesky_ex</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.linalg.inv_ex.html">torch.linalg.inv_ex</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.linalg.solve_ex.html">torch.linalg.solve_ex</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.linalg.lu_factor_ex.html">torch.linalg.lu_factor_ex</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.linalg.ldl_factor.html">torch.linalg.ldl_factor</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.linalg.ldl_factor_ex.html">torch.linalg.ldl_factor_ex</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.linalg.ldl_solve.html">torch.linalg.ldl_solve</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="monitor.html">torch.monitor</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="signal.html">torch.signal</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.signal.windows.bartlett.html">torch.signal.windows.bartlett</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.signal.windows.blackman.html">torch.signal.windows.blackman</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.signal.windows.cosine.html">torch.signal.windows.cosine</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.signal.windows.exponential.html">torch.signal.windows.exponential</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.signal.windows.gaussian.html">torch.signal.windows.gaussian</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.signal.windows.general_cosine.html">torch.signal.windows.general_cosine</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.signal.windows.general_hamming.html">torch.signal.windows.general_hamming</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.signal.windows.hamming.html">torch.signal.windows.hamming</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.signal.windows.hann.html">torch.signal.windows.hann</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.signal.windows.kaiser.html">torch.signal.windows.kaiser</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.signal.windows.nuttall.html">torch.signal.windows.nuttall</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="special.html">torch.special</a></li>
<li class="toctree-l1"><a class="reference internal" href="torch.overrides.html">torch.overrides</a></li>
<li class="toctree-l1"><a class="reference internal" href="package.html">torch.package</a></li>
<li class="toctree-l1"><a class="reference internal" href="profiler.html">torch.profiler</a></li>
<li class="toctree-l1"><a class="reference internal" href="nn.init.html">torch.nn.init</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="nn.attention.html">torch.nn.attention</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.attention.sdpa_kernel.html">torch.nn.attention.sdpa_kernel</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.attention.SDPBackend.html">SDPBackend</a></li>
<li class="toctree-l2"><a class="reference internal" href="nn.attention.flex_attention.html">torch.nn.attention.flex_attention</a></li>
<li class="toctree-l2"><a class="reference internal" href="nn.attention.bias.html">torch.nn.attention.bias</a></li>

<li class="toctree-l2"><a class="reference internal" href="nn.attention.experimental.html">torch.nn.attention.experimental</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="onnx.html">torch.onnx</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="onnx_dynamo.html">TorchDynamo-based ONNX Exporter</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="onnx_dynamo_memory_usage.html">Understanding TorchDynamo-based ONNX Exporter Memory Usage</a></li>


</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="onnx_ops.html">torch.onnx.ops</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx_verification.html">torch.onnx.verification</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx_dynamo_onnxruntime_backend.html">ONNX Backend for TorchDynamo</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="onnx_torchscript.html">TorchScript-based ONNX Exporter</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.onnx.JitScalarType.html">JitScalarType</a></li>
</ul>
</details></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="optim.html">torch.optim</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.Optimizer.add_param_group.html">torch.optim.Optimizer.add_param_group</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.Optimizer.load_state_dict.html">torch.optim.Optimizer.load_state_dict</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.Optimizer.register_load_state_dict_pre_hook.html">torch.optim.Optimizer.register_load_state_dict_pre_hook</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.Optimizer.register_load_state_dict_post_hook.html">torch.optim.Optimizer.register_load_state_dict_post_hook</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.Optimizer.state_dict.html">torch.optim.Optimizer.state_dict</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.Optimizer.register_state_dict_pre_hook.html">torch.optim.Optimizer.register_state_dict_pre_hook</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.Optimizer.register_state_dict_post_hook.html">torch.optim.Optimizer.register_state_dict_post_hook</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.Optimizer.step.html">torch.optim.Optimizer.step</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.Optimizer.register_step_pre_hook.html">torch.optim.Optimizer.register_step_pre_hook</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.Optimizer.register_step_post_hook.html">torch.optim.Optimizer.register_step_post_hook</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.Optimizer.zero_grad.html">torch.optim.Optimizer.zero_grad</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.Adadelta.html">Adadelta</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.Adafactor.html">Adafactor</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.Adagrad.html">Adagrad</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.Adam.html">Adam</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.AdamW.html">AdamW</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.SparseAdam.html">SparseAdam</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.Adamax.html">Adamax</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.ASGD.html">ASGD</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.LBFGS.html">LBFGS</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.NAdam.html">NAdam</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.RAdam.html">RAdam</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.RMSprop.html">RMSprop</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.Rprop.html">Rprop</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.SGD.html">SGD</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.lr_scheduler.LRScheduler.html">LRScheduler</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.lr_scheduler.LambdaLR.html">LambdaLR</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.lr_scheduler.MultiplicativeLR.html">MultiplicativeLR</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.lr_scheduler.StepLR.html">StepLR</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.lr_scheduler.MultiStepLR.html">MultiStepLR</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.lr_scheduler.ConstantLR.html">ConstantLR</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.lr_scheduler.LinearLR.html">LinearLR</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.lr_scheduler.ExponentialLR.html">ExponentialLR</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.lr_scheduler.PolynomialLR.html">PolynomialLR</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.lr_scheduler.CosineAnnealingLR.html">CosineAnnealingLR</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.lr_scheduler.ChainedScheduler.html">ChainedScheduler</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.lr_scheduler.SequentialLR.html">SequentialLR</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.lr_scheduler.ReduceLROnPlateau.html">ReduceLROnPlateau</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.lr_scheduler.CyclicLR.html">CyclicLR</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.lr_scheduler.OneCycleLR.html">OneCycleLR</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.lr_scheduler.CosineAnnealingWarmRestarts.html">CosineAnnealingWarmRestarts</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.swa_utils.AveragedModel.html">AveragedModel</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.swa_utils.SWALR.html">SWALR</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="complex_numbers.html">Complex Numbers</a></li>
<li class="toctree-l1"><a class="reference internal" href="ddp_comm_hooks.html">DDP Communication Hooks</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="quantization.html">Quantization</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="quantization-support.html">Quantization API Reference</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.quantize.html">quantize</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.quantize_dynamic.html">quantize_dynamic</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.quantize_qat.html">quantize_qat</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.prepare.html">prepare</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.prepare_qat.html">prepare_qat</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.convert.html">convert</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.fuse_modules.fuse_modules.html">fuse_modules</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.QuantStub.html">QuantStub</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.DeQuantStub.html">DeQuantStub</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.QuantWrapper.html">QuantWrapper</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.add_quant_dequant.html">add_quant_dequant</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.swap_module.html">swap_module</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.propagate_qconfig_.html">propagate_qconfig</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.default_eval_fn.html">default_eval_fn</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.quantize_fx.prepare_fx.html">prepare_fx</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.quantize_fx.prepare_qat_fx.html">prepare_qat_fx</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.quantize_fx.convert_fx.html">convert_fx</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.quantize_fx.fuse_fx.html">fuse_fx</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.qconfig_mapping.QConfigMapping.html">QConfigMapping</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.qconfig_mapping.get_default_qconfig_mapping.html">get_default_qconfig_mapping</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.qconfig_mapping.get_default_qat_qconfig_mapping.html">get_default_qat_qconfig_mapping</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.backend_config.BackendConfig.html">BackendConfig</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.backend_config.BackendPatternConfig.html">BackendPatternConfig</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.backend_config.DTypeConfig.html">DTypeConfig</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.backend_config.DTypeWithConstraints.html">DTypeWithConstraints</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.backend_config.ObservationType.html">ObservationType</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.fx.custom_config.FuseCustomConfig.html">FuseCustomConfig</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.fx.custom_config.PrepareCustomConfig.html">PrepareCustomConfig</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.fx.custom_config.ConvertCustomConfig.html">ConvertCustomConfig</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.fx.custom_config.StandaloneModuleConfigEntry.html">StandaloneModuleConfigEntry</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.pt2e.export_utils.model_is_exported.html">model_is_exported</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.generate_numeric_debug_handle.html">generate_numeric_debug_handle</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.CUSTOM_KEY.html">CUSTOM_KEY</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.NUMERIC_DEBUG_HANDLE_KEY.html">NUMERIC_DEBUG_HANDLE_KEY</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.prepare_for_propagation_comparison.html">prepare_for_propagation_comparison</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.extract_results_from_loggers.html">extract_results_from_loggers</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.compare_results.html">compare_results</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.quantize_per_tensor.html">torch.quantize_per_tensor</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.quantize_per_channel.html">torch.quantize_per_channel</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.dequantize.html">torch.dequantize</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.Tensor.view.html">torch.Tensor.view</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.Tensor.as_strided.html">torch.Tensor.as_strided</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.Tensor.expand.html">torch.Tensor.expand</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.Tensor.flatten.html">torch.Tensor.flatten</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.Tensor.select.html">torch.Tensor.select</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.Tensor.ne.html">torch.Tensor.ne</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.Tensor.eq.html">torch.Tensor.eq</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.Tensor.ge.html">torch.Tensor.ge</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.Tensor.le.html">torch.Tensor.le</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.Tensor.gt.html">torch.Tensor.gt</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.Tensor.lt.html">torch.Tensor.lt</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.Tensor.copy_.html">torch.Tensor.copy_</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.Tensor.clone.html">torch.Tensor.clone</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.Tensor.dequantize.html">torch.Tensor.dequantize</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.Tensor.equal.html">torch.Tensor.equal</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.Tensor.int_repr.html">torch.Tensor.int_repr</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.Tensor.max.html">torch.Tensor.max</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.Tensor.mean.html">torch.Tensor.mean</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.Tensor.min.html">torch.Tensor.min</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.Tensor.q_scale.html">torch.Tensor.q_scale</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.Tensor.q_zero_point.html">torch.Tensor.q_zero_point</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.Tensor.q_per_channel_scales.html">torch.Tensor.q_per_channel_scales</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.Tensor.q_per_channel_zero_points.html">torch.Tensor.q_per_channel_zero_points</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.Tensor.q_per_channel_axis.html">torch.Tensor.q_per_channel_axis</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.Tensor.resize_.html">torch.Tensor.resize_</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.Tensor.sort.html">torch.Tensor.sort</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.Tensor.topk.html">torch.Tensor.topk</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.observer.ObserverBase.html">ObserverBase</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.observer.MinMaxObserver.html">MinMaxObserver</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.observer.MovingAverageMinMaxObserver.html">MovingAverageMinMaxObserver</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.observer.PerChannelMinMaxObserver.html">PerChannelMinMaxObserver</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver.html">MovingAveragePerChannelMinMaxObserver</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.observer.HistogramObserver.html">HistogramObserver</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.observer.PlaceholderObserver.html">PlaceholderObserver</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.observer.RecordingObserver.html">RecordingObserver</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.observer.NoopObserver.html">NoopObserver</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.observer.get_observer_state_dict.html">get_observer_state_dict</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.observer.load_observer_state_dict.html">load_observer_state_dict</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.observer.default_observer.html">default_observer</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.observer.default_placeholder_observer.html">default_placeholder_observer</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.observer.default_debug_observer.html">default_debug_observer</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.observer.default_weight_observer.html">default_weight_observer</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.observer.default_histogram_observer.html">default_histogram_observer</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.observer.default_per_channel_weight_observer.html">default_per_channel_weight_observer</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.observer.default_dynamic_quant_observer.html">default_dynamic_quant_observer</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.observer.default_float_qparams_observer.html">default_float_qparams_observer</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.fake_quantize.FakeQuantizeBase.html">FakeQuantizeBase</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.fake_quantize.FakeQuantize.html">FakeQuantize</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.fake_quantize.FixedQParamsFakeQuantize.html">FixedQParamsFakeQuantize</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize.html">FusedMovingAvgObsFakeQuantize</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.fake_quantize.default_fake_quant.html">default_fake_quant</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.fake_quantize.default_weight_fake_quant.html">default_weight_fake_quant</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.fake_quantize.default_per_channel_weight_fake_quant.html">default_per_channel_weight_fake_quant</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.fake_quantize.default_histogram_fake_quant.html">default_histogram_fake_quant</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.fake_quantize.default_fused_act_fake_quant.html">default_fused_act_fake_quant</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.fake_quantize.default_fused_wt_fake_quant.html">default_fused_wt_fake_quant</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.fake_quantize.default_fused_per_channel_wt_fake_quant.html">default_fused_per_channel_wt_fake_quant</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.fake_quantize.disable_fake_quant.html">disable_fake_quant</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.fake_quantize.enable_fake_quant.html">enable_fake_quant</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.fake_quantize.disable_observer.html">disable_observer</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.fake_quantize.enable_observer.html">enable_observer</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.qconfig.QConfig.html">QConfig</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.qconfig.default_qconfig.html">default_qconfig</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.qconfig.default_debug_qconfig.html">default_debug_qconfig</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.qconfig.default_per_channel_qconfig.html">default_per_channel_qconfig</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.qconfig.default_dynamic_qconfig.html">default_dynamic_qconfig</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.qconfig.float16_dynamic_qconfig.html">float16_dynamic_qconfig</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.qconfig.float16_static_qconfig.html">float16_static_qconfig</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.qconfig.per_channel_dynamic_qconfig.html">per_channel_dynamic_qconfig</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.qconfig.float_qparams_weight_only_qconfig.html">float_qparams_weight_only_qconfig</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.qconfig.default_qat_qconfig.html">default_qat_qconfig</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.qconfig.default_weight_only_qconfig.html">default_weight_only_qconfig</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.qconfig.default_activation_only_qconfig.html">default_activation_only_qconfig</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.qconfig.default_qat_qconfig_v2.html">default_qat_qconfig_v2</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.intrinsic.ConvReLU1d.html">ConvReLU1d</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.intrinsic.ConvReLU2d.html">ConvReLU2d</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.intrinsic.ConvReLU3d.html">ConvReLU3d</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.intrinsic.LinearReLU.html">LinearReLU</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.intrinsic.ConvBn1d.html">ConvBn1d</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.intrinsic.ConvBn2d.html">ConvBn2d</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.intrinsic.ConvBn3d.html">ConvBn3d</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.intrinsic.ConvBnReLU1d.html">ConvBnReLU1d</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.intrinsic.ConvBnReLU2d.html">ConvBnReLU2d</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.intrinsic.ConvBnReLU3d.html">ConvBnReLU3d</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.intrinsic.BNReLU2d.html">BNReLU2d</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.intrinsic.BNReLU3d.html">BNReLU3d</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.intrinsic.qat.LinearReLU.html">LinearReLU</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.intrinsic.qat.ConvBn1d.html">ConvBn1d</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.intrinsic.qat.ConvBnReLU1d.html">ConvBnReLU1d</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.intrinsic.qat.ConvBn2d.html">ConvBn2d</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.intrinsic.qat.ConvBnReLU2d.html">ConvBnReLU2d</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.intrinsic.qat.ConvReLU2d.html">ConvReLU2d</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.intrinsic.qat.ConvBn3d.html">ConvBn3d</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.intrinsic.qat.ConvBnReLU3d.html">ConvBnReLU3d</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.intrinsic.qat.ConvReLU3d.html">ConvReLU3d</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.intrinsic.qat.update_bn_stats.html">update_bn_stats</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.intrinsic.qat.freeze_bn_stats.html">freeze_bn_stats</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.intrinsic.quantized.BNReLU2d.html">BNReLU2d</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.intrinsic.quantized.BNReLU3d.html">BNReLU3d</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.intrinsic.quantized.ConvReLU1d.html">ConvReLU1d</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.intrinsic.quantized.ConvReLU2d.html">ConvReLU2d</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.intrinsic.quantized.ConvReLU3d.html">ConvReLU3d</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.intrinsic.quantized.LinearReLU.html">LinearReLU</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.intrinsic.quantized.dynamic.LinearReLU.html">LinearReLU</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.qat.Conv2d.html">Conv2d</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.qat.Conv3d.html">Conv3d</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.qat.Linear.html">Linear</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.qat.dynamic.Linear.html">Linear</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.quantized.ReLU6.html">ReLU6</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.quantized.Hardswish.html">Hardswish</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.quantized.ELU.html">ELU</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.quantized.LeakyReLU.html">LeakyReLU</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.quantized.Sigmoid.html">Sigmoid</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.quantized.BatchNorm2d.html">BatchNorm2d</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.quantized.BatchNorm3d.html">BatchNorm3d</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.quantized.Conv1d.html">Conv1d</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.quantized.Conv2d.html">Conv2d</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.quantized.Conv3d.html">Conv3d</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.quantized.ConvTranspose1d.html">ConvTranspose1d</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.quantized.ConvTranspose2d.html">ConvTranspose2d</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.quantized.ConvTranspose3d.html">ConvTranspose3d</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.quantized.Embedding.html">Embedding</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.quantized.EmbeddingBag.html">EmbeddingBag</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.quantized.FloatFunctional.html">FloatFunctional</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.quantized.FXFloatFunctional.html">FXFloatFunctional</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.quantized.QFunctional.html">QFunctional</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.quantized.Linear.html">Linear</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.quantized.LayerNorm.html">LayerNorm</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.quantized.GroupNorm.html">GroupNorm</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.quantized.InstanceNorm1d.html">InstanceNorm1d</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.quantized.InstanceNorm2d.html">InstanceNorm2d</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.quantized.InstanceNorm3d.html">InstanceNorm3d</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.quantized.functional.avg_pool2d.html">avg_pool2d</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.quantized.functional.avg_pool3d.html">avg_pool3d</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.quantized.functional.adaptive_avg_pool2d.html">adaptive_avg_pool2d</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.quantized.functional.adaptive_avg_pool3d.html">adaptive_avg_pool3d</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.quantized.functional.conv1d.html">conv1d</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.quantized.functional.conv2d.html">conv2d</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.quantized.functional.conv3d.html">conv3d</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.quantized.functional.interpolate.html">interpolate</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.quantized.functional.linear.html">linear</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.quantized.functional.max_pool1d.html">max_pool1d</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.quantized.functional.max_pool2d.html">max_pool2d</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.quantized.functional.celu.html">celu</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.quantized.functional.leaky_relu.html">leaky_relu</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.quantized.functional.hardtanh.html">hardtanh</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.quantized.functional.hardswish.html">hardswish</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.quantized.functional.threshold.html">threshold</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.quantized.functional.elu.html">elu</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.quantized.functional.hardsigmoid.html">hardsigmoid</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.quantized.functional.clamp.html">clamp</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.quantized.functional.upsample.html">upsample</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.quantized.functional.upsample_bilinear.html">upsample_bilinear</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.quantized.functional.upsample_nearest.html">upsample_nearest</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.quantizable.LSTM.html">LSTM</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.quantizable.MultiheadAttention.html">MultiheadAttention</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.quantized.dynamic.Linear.html">Linear</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.quantized.dynamic.LSTM.html">LSTM</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.quantized.dynamic.GRU.html">GRU</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.quantized.dynamic.RNNCell.html">RNNCell</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.quantized.dynamic.LSTMCell.html">LSTMCell</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.quantized.dynamic.GRUCell.html">GRUCell</a></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="quantization-backend-configuration.html">Quantization Backend Configuration</a></li>
<li class="toctree-l2"><a class="reference internal" href="quantization-accuracy-debugging.html">Quantization Accuracy Debugging</a></li>

</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="rpc.html">Distributed RPC Framework</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="rpc/rref.html">Remote Reference Protocol</a></li>
<li class="toctree-l2"><a class="reference internal" href="rpc/distributed_autograd.html">Distributed Autograd Design</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="random.html">torch.random</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="masked.html">torch.masked</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.abs.html">torch.abs</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.absolute.html">torch.absolute</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.acos.html">torch.acos</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.arccos.html">torch.arccos</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.acosh.html">torch.acosh</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.arccosh.html">torch.arccosh</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.angle.html">torch.angle</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.asin.html">torch.asin</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.arcsin.html">torch.arcsin</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.asinh.html">torch.asinh</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.arcsinh.html">torch.arcsinh</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.atan.html">torch.atan</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.arctan.html">torch.arctan</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.atanh.html">torch.atanh</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.arctanh.html">torch.arctanh</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.bitwise_not.html">torch.bitwise_not</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.ceil.html">torch.ceil</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.clamp.html">torch.clamp</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.clip.html">torch.clip</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.conj_physical.html">torch.conj_physical</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cos.html">torch.cos</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cosh.html">torch.cosh</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.deg2rad.html">torch.deg2rad</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.digamma.html">torch.digamma</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.erf.html">torch.erf</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.erfc.html">torch.erfc</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.erfinv.html">torch.erfinv</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.exp.html">torch.exp</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.exp2.html">torch.exp2</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.expm1.html">torch.expm1</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fix.html">torch.fix</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.floor.html">torch.floor</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.frac.html">torch.frac</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.lgamma.html">torch.lgamma</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.log.html">torch.log</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.log10.html">torch.log10</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.log1p.html">torch.log1p</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.log2.html">torch.log2</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.logit.html">torch.logit</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.i0.html">torch.i0</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.isnan.html">torch.isnan</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nan_to_num.html">torch.nan_to_num</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.neg.html">torch.neg</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.negative.html">torch.negative</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.positive.html">torch.positive</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.pow.html">torch.pow</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.rad2deg.html">torch.rad2deg</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.reciprocal.html">torch.reciprocal</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.round.html">torch.round</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.rsqrt.html">torch.rsqrt</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.sigmoid.html">torch.sigmoid</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.sign.html">torch.sign</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.sgn.html">torch.sgn</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.signbit.html">torch.signbit</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.sin.html">torch.sin</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.sinc.html">torch.sinc</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.sinh.html">torch.sinh</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.sqrt.html">torch.sqrt</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.square.html">torch.square</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.tan.html">torch.tan</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.tanh.html">torch.tanh</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.trunc.html">torch.trunc</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.angle.html">torch.angle</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.positive.html">torch.positive</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.signbit.html">torch.signbit</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.isnan.html">torch.isnan</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.add.html">torch.add</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.atan2.html">torch.atan2</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.arctan2.html">torch.arctan2</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.bitwise_and.html">torch.bitwise_and</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.bitwise_or.html">torch.bitwise_or</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.bitwise_xor.html">torch.bitwise_xor</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.bitwise_left_shift.html">torch.bitwise_left_shift</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.bitwise_right_shift.html">torch.bitwise_right_shift</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.div.html">torch.div</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.divide.html">torch.divide</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.floor_divide.html">torch.floor_divide</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fmod.html">torch.fmod</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.logaddexp.html">torch.logaddexp</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.logaddexp2.html">torch.logaddexp2</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mul.html">torch.mul</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.multiply.html">torch.multiply</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nextafter.html">torch.nextafter</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.remainder.html">torch.remainder</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.sub.html">torch.sub</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.subtract.html">torch.subtract</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.true_divide.html">torch.true_divide</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.eq.html">torch.eq</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.ne.html">torch.ne</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.le.html">torch.le</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.ge.html">torch.ge</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.greater.html">torch.greater</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.greater_equal.html">torch.greater_equal</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.gt.html">torch.gt</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.less_equal.html">torch.less_equal</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.lt.html">torch.lt</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.less.html">torch.less</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.maximum.html">torch.maximum</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.minimum.html">torch.minimum</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fmax.html">torch.fmax</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fmin.html">torch.fmin</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.not_equal.html">torch.not_equal</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.logaddexp.html">torch.logaddexp</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.logaddexp2.html">torch.logaddexp2</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.equal.html">torch.equal</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fmin.html">torch.fmin</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.minimum.html">torch.minimum</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fmax.html">torch.fmax</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.sum.html">torch.sum</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mean.html">torch.mean</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.amin.html">torch.amin</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.amax.html">torch.amax</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.argmin.html">torch.argmin</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.argmax.html">torch.argmax</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.prod.html">torch.prod</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.all.html">torch.all</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.norm.html">torch.norm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.var.html">torch.var</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.std.html">torch.std</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.atleast_1d.html">torch.atleast_1d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.broadcast_tensors.html">torch.broadcast_tensors</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.broadcast_to.html">torch.broadcast_to</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cat.html">torch.cat</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.chunk.html">torch.chunk</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.column_stack.html">torch.column_stack</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.dsplit.html">torch.dsplit</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.flatten.html">torch.flatten</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.hsplit.html">torch.hsplit</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.hstack.html">torch.hstack</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.kron.html">torch.kron</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.meshgrid.html">torch.meshgrid</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.narrow.html">torch.narrow</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.unfold.html">torch.nn.functional.unfold</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.ravel.html">torch.ravel</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.select.html">torch.select</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.split.html">torch.split</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.stack.html">torch.stack</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.t.html">torch.t</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.transpose.html">torch.transpose</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.vsplit.html">torch.vsplit</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.vstack.html">torch.vstack</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.expand.html">torch.Tensor.expand</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.expand_as.html">torch.Tensor.expand_as</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.reshape.html">torch.Tensor.reshape</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.reshape_as.html">torch.Tensor.reshape_as</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.unfold.html">torch.Tensor.unfold</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.view.html">torch.Tensor.view</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="nested.html">torch.nested</a></li>
<li class="toctree-l1"><a class="reference internal" href="size.html">torch.Size</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="sparse.html">torch.sparse</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.is_sparse.html">torch.Tensor.is_sparse</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.is_sparse_csr.html">torch.Tensor.is_sparse_csr</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.dense_dim.html">torch.Tensor.dense_dim</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.sparse_dim.html">torch.Tensor.sparse_dim</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.sparse_mask.html">torch.Tensor.sparse_mask</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.to_sparse.html">torch.Tensor.to_sparse</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.to_sparse_coo.html">torch.Tensor.to_sparse_coo</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.to_sparse_csr.html">torch.Tensor.to_sparse_csr</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.to_sparse_csc.html">torch.Tensor.to_sparse_csc</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.to_sparse_bsr.html">torch.Tensor.to_sparse_bsr</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.to_sparse_bsc.html">torch.Tensor.to_sparse_bsc</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.to_dense.html">torch.Tensor.to_dense</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.values.html">torch.Tensor.values</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.coalesce.html">torch.Tensor.coalesce</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.sparse_resize_.html">torch.Tensor.sparse_resize_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.sparse_resize_and_clear_.html">torch.Tensor.sparse_resize_and_clear_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.is_coalesced.html">torch.Tensor.is_coalesced</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.indices.html">torch.Tensor.indices</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.crow_indices.html">torch.Tensor.crow_indices</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.col_indices.html">torch.Tensor.col_indices</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.row_indices.html">torch.Tensor.row_indices</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.ccol_indices.html">torch.Tensor.ccol_indices</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.sparse_coo_tensor.html">torch.sparse_coo_tensor</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.sparse_csr_tensor.html">torch.sparse_csr_tensor</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.sparse_csc_tensor.html">torch.sparse_csc_tensor</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.sparse_bsr_tensor.html">torch.sparse_bsr_tensor</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.sparse_bsc_tensor.html">torch.sparse_bsc_tensor</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.sparse_compressed_tensor.html">torch.sparse_compressed_tensor</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.sparse.sum.html">torch.sparse.sum</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.sparse.addmm.html">torch.sparse.addmm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.sparse.sampled_addmm.html">torch.sparse.sampled_addmm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.sparse.mm.html">torch.sparse.mm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.sspaddmm.html">torch.sspaddmm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.hspmm.html">torch.hspmm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.smm.html">torch.smm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.sparse.softmax.html">torch.sparse.softmax</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.sparse.spsolve.html">torch.sparse.spsolve</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.sparse.log_softmax.html">torch.sparse.log_softmax</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.sparse.spdiags.html">torch.sparse.spdiags</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.sparse.check_sparse_tensor_invariants.html">check_sparse_tensor_invariants</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.sparse.as_sparse_gradcheck.html">torch.sparse.as_sparse_gradcheck</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="storage.html">torch.Storage</a></li>
<li class="toctree-l1"><a class="reference internal" href="testing.html">torch.testing</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="utils.html">torch.utils</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.utils.rename_privateuse1_backend.html">torch.utils.rename_privateuse1_backend</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.utils.generate_methods_for_privateuse1_backend.html">torch.utils.generate_methods_for_privateuse1_backend</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.utils.get_cpp_backtrace.html">torch.utils.get_cpp_backtrace</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.utils.set_module.html">torch.utils.set_module</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.utils.swap_tensors.html">torch.utils.swap_tensors</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="benchmark_utils.html">torch.utils.benchmark</a></li>
<li class="toctree-l1"><a class="reference internal" href="bottleneck.html">torch.utils.bottleneck</a></li>
<li class="toctree-l1"><a class="reference internal" href="checkpoint.html">torch.utils.checkpoint</a></li>
<li class="toctree-l1"><a class="reference internal" href="cpp_extension.html">torch.utils.cpp_extension</a></li>
<li class="toctree-l1"><a class="reference internal" href="data.html">torch.utils.data</a></li>
<li class="toctree-l1"><a class="reference internal" href="deterministic.html">torch.utils.deterministic</a></li>
<li class="toctree-l1"><a class="reference internal" href="jit_utils.html">torch.utils.jit</a></li>
<li class="toctree-l1"><a class="reference internal" href="dlpack.html">torch.utils.dlpack</a></li>
<li class="toctree-l1"><a class="reference internal" href="mobile_optimizer.html">torch.utils.mobile_optimizer</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_zoo.html">torch.utils.model_zoo</a></li>
<li class="toctree-l1"><a class="reference internal" href="tensorboard.html">torch.utils.tensorboard</a></li>
<li class="toctree-l1"><a class="reference internal" href="module_tracker.html">torch.utils.module_tracker</a></li>
<li class="toctree-l1"><a class="reference internal" href="type_info.html">Type Info</a></li>
<li class="toctree-l1"><a class="reference internal" href="named_tensor.html">Named Tensors</a></li>
<li class="toctree-l1"><a class="reference internal" href="name_inference.html">Named Tensors operator coverage</a></li>
<li class="toctree-l1"><a class="reference internal" href="config_mod.html">torch.__config__</a></li>
<li class="toctree-l1"><a class="reference internal" href="future_mod.html">torch.__future__</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="logging.html">torch._logging</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._logging.set_logs.html">torch._logging.set_logs</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="torch_environment_variables.html">Torch Environment Variables</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="threading_environment_variables.html">Threading Environment Variables</a></li>
<li class="toctree-l2"><a class="reference internal" href="cuda_environment_variables.html">CUDA Environment Variables</a></li>
<li class="toctree-l2"><a class="reference internal" href="mps_environment_variables.html">MPS Environment Variables</a></li>
<li class="toctree-l2"><a class="reference internal" href="debugging_environment_variables.html">Debugging Environment Variables</a></li>
<li class="toctree-l2"><a class="reference internal" href="miscellaneous_environment_variables.html">Miscellaneous Environment Variables</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="logging.html">torch._logging</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="generated/torch._logging.set_logs.html">torch._logging.set_logs</a></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="torch_nccl_environment_variables.html">PYTORCH ProcessGroupNCCL Environment Variables</a></li>
</ul>
</details></li>
</ul>
</div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        
          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item">



<nav aria-label="Breadcrumb" class="d-print-none">
  <ul class="bd-breadcrumbs">
    
    <li class="breadcrumb-item breadcrumb-home">
      <a href="index.html" class="nav-link" aria-label="Home">
        <i class="fa-solid fa-home"></i>
      </a>
    </li>
    
    <li class="breadcrumb-item"><a href="pytorch-api.html" class="nav-link">Python API</a></li>
    
    <li class="breadcrumb-item active" aria-current="page">Probability...</li>
  </ul>
</nav>
</div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">
<div class="rating">
    Rate this Page
    <div class="stars">
        
        <span class="star" data-behavior="tutorial-rating" data-count="1" data-value="1"></span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="2" data-value="2"></span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="3" data-value="3"></span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="4" data-value="4"></span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="5" data-value="5"></span>
        
    </div>
</div>
</div>
      
    </div>
  
</div>
</div>
              
              
  
<div id="searchbox"></div>
  <article class="bd-article">
    
    
  <section id="module-torch.distributions">
<span id="probability-distributions-torch-distributions"></span><h1>Probability distributions - torch.distributions<a class="headerlink" href="#module-torch.distributions" title="Permalink to this heading">#</a></h1>
<p>The <code class="docutils literal notranslate"><span class="pre">distributions</span></code> package contains parameterizable probability distributions
and sampling functions. This allows the construction of stochastic computation
graphs and stochastic gradient estimators for optimization. This package
generally follows the design of the <a class="reference external" href="https://arxiv.org/abs/1711.10604">TensorFlow Distributions</a> package.</p>
<p>It is not possible to directly backpropagate through random samples. However,
there are two main methods for creating surrogate functions that can be
backpropagated through. These are the score function estimator/likelihood ratio
estimator/REINFORCE and the pathwise derivative estimator. REINFORCE is commonly
seen as the basis for policy gradient methods in reinforcement learning, and the
pathwise derivative estimator is commonly seen in the reparameterization trick
in variational autoencoders. Whilst the score function only requires the value
of samples <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">f(x)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span></span></span></span></span>, the pathwise derivative requires the derivative
<span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>f</mi><mo mathvariant="normal" lspace="0em" rspace="0em"></mo></msup><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">f&#x27;(x)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0019em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7519em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"></span></span></span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span></span></span></span></span>. The next sections discuss these two in a reinforcement learning
example. For more details see
<a class="reference external" href="https://arxiv.org/abs/1506.05254">Gradient Estimation Using Stochastic Computation Graphs</a> .</p>
<section id="score-function">
<h2>Score function<a class="headerlink" href="#score-function" title="Permalink to this heading">#</a></h2>
<p>When the probability density function is differentiable with respect to its
parameters, we only need <code class="xref py py-meth docutils literal notranslate"><span class="pre">sample()</span></code> and
<code class="xref py py-meth docutils literal notranslate"><span class="pre">log_prob()</span></code> to implement REINFORCE:</p>
<div class="math">
<span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi mathvariant="normal"></mi><mi></mi><mo>=</mo><mi></mi><mi>r</mi><mfrac><mrow><mi mathvariant="normal"></mi><mi>log</mi><mo></mo><mi>p</mi><mo stretchy="false">(</mo><mi>a</mi><mi mathvariant="normal"></mi><msup><mi></mi><mi></mi></msup><mo stretchy="false">(</mo><mi>s</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow><mrow><mi mathvariant="normal"></mi><mi></mi></mrow></mfrac></mrow><annotation encoding="application/x-tex">\Delta\theta  = \alpha r \frac{\partial\log p(a|\pi^\theta(s))}{\partial\theta}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord"></span><span class="mord mathnormal" style="margin-right:0.02778em;"></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:2.2121em;vertical-align:-0.686em;"></span><span class="mord mathnormal" style="margin-right:0.0037em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.5261em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord" style="margin-right:0.05556em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;"></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord" style="margin-right:0.05556em;"></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mop">lo<span style="margin-right:0.01389em;">g</span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal">p</span><span class="mopen">(</span><span class="mord mathnormal">a</span><span class="mord"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;"></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8491em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;"></span></span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal">s</span><span class="mclose">))</span></span></span></span><span class="vlist-s"></span></span><span class="vlist-r"><span class="vlist" style="height:0.686em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></div><p>where <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi></mi></mrow><annotation encoding="application/x-tex">\theta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;"></span></span></span></span></span> are the parameters, <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi></mi></mrow><annotation encoding="application/x-tex">\alpha</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.0037em;"></span></span></span></span></span> is the learning rate,
<span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>r</mi></mrow><annotation encoding="application/x-tex">r</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span></span></span></span></span> is the reward and <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mo stretchy="false">(</mo><mi>a</mi><mi mathvariant="normal"></mi><msup><mi></mi><mi></mi></msup><mo stretchy="false">(</mo><mi>s</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">p(a|\pi^\theta(s))</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0991em;vertical-align:-0.25em;"></span><span class="mord mathnormal">p</span><span class="mopen">(</span><span class="mord mathnormal">a</span><span class="mord"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;"></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8491em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;"></span></span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal">s</span><span class="mclose">))</span></span></span></span></span> is the probability of
taking action <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>a</mi></mrow><annotation encoding="application/x-tex">a</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">a</span></span></span></span></span> in state <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>s</mi></mrow><annotation encoding="application/x-tex">s</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">s</span></span></span></span></span> given policy <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi></mi><mi></mi></msup></mrow><annotation encoding="application/x-tex">\pi^\theta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8491em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;"></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8491em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;"></span></span></span></span></span></span></span></span></span></span></span></span>.</p>
<p>In practice we would sample an action from the output of a network, apply this
action in an environment, and then use <code class="docutils literal notranslate"><span class="pre">log_prob</span></code> to construct an equivalent
loss function. Note that we use a negative because optimizers use gradient
descent, whilst the rule above assumes gradient ascent. With a categorical
policy, the code for implementing REINFORCE would be as follows:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">probs</span> <span class="o">=</span> <span class="n">policy_network</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
<span class="c1"># Note that this is equivalent to what used to be called multinomial</span>
<span class="n">m</span> <span class="o">=</span> <span class="n">Categorical</span><span class="p">(</span><span class="n">probs</span><span class="p">)</span>
<span class="n">action</span> <span class="o">=</span> <span class="n">m</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>
<span class="n">next_state</span><span class="p">,</span> <span class="n">reward</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
<span class="n">loss</span> <span class="o">=</span> <span class="o">-</span><span class="n">m</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">action</span><span class="p">)</span> <span class="o">*</span> <span class="n">reward</span>
<span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section id="pathwise-derivative">
<h2>Pathwise derivative<a class="headerlink" href="#pathwise-derivative" title="Permalink to this heading">#</a></h2>
<p>The other way to implement these stochastic/policy gradients would be to use the
reparameterization trick from the
<code class="xref py py-meth docutils literal notranslate"><span class="pre">rsample()</span></code> method, where the
parameterized random variable can be constructed via a parameterized
deterministic function of a parameter-free random variable. The reparameterized
sample therefore becomes differentiable. The code for implementing the pathwise
derivative would be as follows:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">params</span> <span class="o">=</span> <span class="n">policy_network</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
<span class="n">m</span> <span class="o">=</span> <span class="n">Normal</span><span class="p">(</span><span class="o">*</span><span class="n">params</span><span class="p">)</span>
<span class="c1"># Any distribution with .has_rsample == True could work based on the application</span>
<span class="n">action</span> <span class="o">=</span> <span class="n">m</span><span class="o">.</span><span class="n">rsample</span><span class="p">()</span>
<span class="n">next_state</span><span class="p">,</span> <span class="n">reward</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>  <span class="c1"># Assuming that reward is differentiable</span>
<span class="n">loss</span> <span class="o">=</span> <span class="o">-</span><span class="n">reward</span>
<span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section id="distribution">
<h2><span class="hidden-section">Distribution</span><a class="headerlink" href="#distribution" title="Permalink to this heading">#</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="torch.distributions.distribution.Distribution">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torch.distributions.distribution.</span></span><span class="sig-name descname"><span class="pre">Distribution</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch_shape</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">torch.Size([])</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">event_shape</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">torch.Size([])</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">validate_args</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/distribution.py#L15"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.distribution.Distribution" title="Permalink to this definition">#</a></dt>
<dd><p>Bases: <a class="reference external" href="https://docs.python.org/3/library/functions.html#object" title="(in Python v3.13)"><code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></a></p>
<p>Distribution is the abstract base class for probability distributions.</p>
<dl class="field-list simple">
</dl>
<dl class="py property">
<dt class="sig sig-object py" id="torch.distributions.distribution.Distribution.arg_constraints">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">arg_constraints</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Dict" title="(in Python v3.13)"><span class="pre">Dict</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference internal" href="#torch.distributions.constraints.Constraint" title="torch.distributions.constraints.Constraint"><span class="pre">Constraint</span></a><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#torch.distributions.distribution.Distribution.arg_constraints" title="Permalink to this definition">#</a></dt>
<dd><p>Returns a dictionary from argument names to
<a class="reference internal" href="#torch.distributions.constraints.Constraint" title="torch.distributions.constraints.Constraint"><code class="xref py py-class docutils literal notranslate"><span class="pre">Constraint</span></code></a> objects that
should be satisfied by each argument of this distribution. Args that
are not tensors need not appear in this dict.</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="torch.distributions.distribution.Distribution.batch_shape">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">batch_shape</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><a class="reference internal" href="size.html#torch.Size" title="torch.Size"><span class="pre">Size</span></a></em><a class="headerlink" href="#torch.distributions.distribution.Distribution.batch_shape" title="Permalink to this definition">#</a></dt>
<dd><p>Returns the shape over which parameters are batched.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributions.distribution.Distribution.cdf">
<span class="sig-name descname"><span class="pre">cdf</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">value</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/distribution.py#L198"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.distribution.Distribution.cdf" title="Permalink to this definition">#</a></dt>
<dd><p>Returns the cumulative density/mass function evaluated at
<cite>value</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>value</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>)  </p>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p><a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributions.distribution.Distribution.entropy">
<span class="sig-name descname"><span class="pre">entropy</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/distribution.py#L242"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.distribution.Distribution.entropy" title="Permalink to this definition">#</a></dt>
<dd><p>Returns entropy of distribution, batched over batch_shape.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>Tensor of shape batch_shape.</p>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p><a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributions.distribution.Distribution.enumerate_support">
<span class="sig-name descname"><span class="pre">enumerate_support</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">expand</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/distribution.py#L218"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.distribution.Distribution.enumerate_support" title="Permalink to this definition">#</a></dt>
<dd><p>Returns tensor containing all values supported by a discrete
distribution. The result will enumerate over dimension 0, so the shape
of the result will be <cite>(cardinality,) + batch_shape + event_shape</cite>
(where <cite>event_shape = ()</cite> for univariate distributions).</p>
<p>Note that this enumerates over all batched tensors in lock-step
<cite>[[0, 0], [1, 1], ]</cite>. With <cite>expand=False</cite>, enumeration happens
along dim 0, but with the remaining batch dimensions being
singleton dimensions, <cite>[[0], [1], ..</cite>.</p>
<p>To iterate over the full Cartesian product use
<cite>itertools.product(m.enumerate_support())</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>expand</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><em>bool</em></a>)  whether to expand the support over the
batch dims to match the distributions <cite>batch_shape</cite>.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Tensor iterating over dimension 0.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a></p>
</dd>
</dl>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="torch.distributions.distribution.Distribution.event_shape">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">event_shape</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><a class="reference internal" href="size.html#torch.Size" title="torch.Size"><span class="pre">Size</span></a></em><a class="headerlink" href="#torch.distributions.distribution.Distribution.event_shape" title="Permalink to this definition">#</a></dt>
<dd><p>Returns the shape of a single sample (without batching).</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributions.distribution.Distribution.expand">
<span class="sig-name descname"><span class="pre">expand</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch_shape</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">_instance</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/distribution.py#L80"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.distribution.Distribution.expand" title="Permalink to this definition">#</a></dt>
<dd><p>Returns a new distribution instance (or populates an existing instance
provided by a derived class) with batch dimensions expanded to
<cite>batch_shape</cite>. This method calls <a class="reference internal" href="generated/torch.Tensor.expand.html#torch.Tensor.expand" title="torch.Tensor.expand"><code class="xref py py-class docutils literal notranslate"><span class="pre">expand</span></code></a> on
the distributions parameters. As such, this does not allocate new
memory for the expanded distribution instance. Additionally,
this does not repeat any args checking or parameter broadcasting in
<cite>__init__.py</cite>, when an instance is first created.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>batch_shape</strong> (<a class="reference internal" href="size.html#torch.Size" title="torch.Size"><em>torch.Size</em></a>)  the desired expanded size.</p></li>
<li><p><strong>_instance</strong>  new instance provided by subclasses that
need to override <cite>.expand</cite>.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>New distribution instance with batch dimensions expanded to
<cite>batch_size</cite>.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributions.distribution.Distribution.icdf">
<span class="sig-name descname"><span class="pre">icdf</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">value</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/distribution.py#L208"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.distribution.Distribution.icdf" title="Permalink to this definition">#</a></dt>
<dd><p>Returns the inverse cumulative density/mass function evaluated at
<cite>value</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>value</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>)  </p>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p><a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributions.distribution.Distribution.log_prob">
<span class="sig-name descname"><span class="pre">log_prob</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">value</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/distribution.py#L188"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.distribution.Distribution.log_prob" title="Permalink to this definition">#</a></dt>
<dd><p>Returns the log of the probability density/mass function evaluated at
<cite>value</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>value</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>)  </p>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p><a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a></p>
</dd>
</dl>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="torch.distributions.distribution.Distribution.mean">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">mean</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><span class="pre">Tensor</span></a></em><a class="headerlink" href="#torch.distributions.distribution.Distribution.mean" title="Permalink to this definition">#</a></dt>
<dd><p>Returns the mean of the distribution.</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="torch.distributions.distribution.Distribution.mode">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">mode</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><span class="pre">Tensor</span></a></em><a class="headerlink" href="#torch.distributions.distribution.Distribution.mode" title="Permalink to this definition">#</a></dt>
<dd><p>Returns the mode of the distribution.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributions.distribution.Distribution.perplexity">
<span class="sig-name descname"><span class="pre">perplexity</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/distribution.py#L251"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.distribution.Distribution.perplexity" title="Permalink to this definition">#</a></dt>
<dd><p>Returns perplexity of distribution, batched over batch_shape.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>Tensor of shape batch_shape.</p>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p><a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributions.distribution.Distribution.rsample">
<span class="sig-name descname"><span class="pre">rsample</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">sample_shape</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">torch.Size([])</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/distribution.py#L169"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.distribution.Distribution.rsample" title="Permalink to this definition">#</a></dt>
<dd><p>Generates a sample_shape shaped reparameterized sample or sample_shape
shaped batch of reparameterized samples if the distribution parameters
are batched.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributions.distribution.Distribution.sample">
<span class="sig-name descname"><span class="pre">sample</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">sample_shape</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">torch.Size([])</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/distribution.py#L161"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.distribution.Distribution.sample" title="Permalink to this definition">#</a></dt>
<dd><p>Generates a sample_shape shaped sample or sample_shape shaped batch of
samples if the distribution parameters are batched.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributions.distribution.Distribution.sample_n">
<span class="sig-name descname"><span class="pre">sample_n</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">n</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/distribution.py#L177"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.distribution.Distribution.sample_n" title="Permalink to this definition">#</a></dt>
<dd><p>Generates n samples or n batches of samples if the distribution
parameters are batched.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributions.distribution.Distribution.set_default_validate_args">
<em class="property"><span class="pre">static</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">set_default_validate_args</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">value</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/distribution.py#L24"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.distribution.Distribution.set_default_validate_args" title="Permalink to this definition">#</a></dt>
<dd><p>Sets whether validation is enabled or disabled.</p>
<p>The default behavior mimics Pythons <code class="docutils literal notranslate"><span class="pre">assert</span></code> statement: validation
is on by default, but is disabled if Python is run in optimized mode
(via <code class="docutils literal notranslate"><span class="pre">python</span> <span class="pre">-O</span></code>). Validation may be expensive, so you may want to
disable it once a model is working.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>value</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><em>bool</em></a>)  Whether to enable validation.</p>
</dd>
</dl>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="torch.distributions.distribution.Distribution.stddev">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">stddev</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><span class="pre">Tensor</span></a></em><a class="headerlink" href="#torch.distributions.distribution.Distribution.stddev" title="Permalink to this definition">#</a></dt>
<dd><p>Returns the standard deviation of the distribution.</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="torch.distributions.distribution.Distribution.support">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">support</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Optional" title="(in Python v3.13)"><span class="pre">Optional</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.13)"><span class="pre">Any</span></a><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#torch.distributions.distribution.Distribution.support" title="Permalink to this definition">#</a></dt>
<dd><p>Returns a <a class="reference internal" href="#torch.distributions.constraints.Constraint" title="torch.distributions.constraints.Constraint"><code class="xref py py-class docutils literal notranslate"><span class="pre">Constraint</span></code></a> object
representing this distributions support.</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="torch.distributions.distribution.Distribution.variance">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">variance</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><span class="pre">Tensor</span></a></em><a class="headerlink" href="#torch.distributions.distribution.Distribution.variance" title="Permalink to this definition">#</a></dt>
<dd><p>Returns the variance of the distribution.</p>
</dd></dl>

</dd></dl>

</section>
<section id="exponentialfamily">
<h2><span class="hidden-section">ExponentialFamily</span><a class="headerlink" href="#exponentialfamily" title="Permalink to this heading">#</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="torch.distributions.exp_family.ExponentialFamily">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torch.distributions.exp_family.</span></span><span class="sig-name descname"><span class="pre">ExponentialFamily</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch_shape</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">torch.Size([])</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">event_shape</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">torch.Size([])</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">validate_args</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/exp_family.py#L9"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.exp_family.ExponentialFamily" title="Permalink to this definition">#</a></dt>
<dd><p>Bases: <a class="reference internal" href="#torch.distributions.distribution.Distribution" title="torch.distributions.distribution.Distribution"><code class="xref py py-class docutils literal notranslate"><span class="pre">Distribution</span></code></a></p>
<p>ExponentialFamily is the abstract base class for probability distributions belonging to an
exponential family, whose probability mass/density function has the form is defined below</p>
<div class="math">
<span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>p</mi><mi>F</mi></msub><mo stretchy="false">(</mo><mi>x</mi><mo separator="true">;</mo><mi></mi><mo stretchy="false">)</mo><mo>=</mo><mi>exp</mi><mo></mo><mo stretchy="false">(</mo><mo stretchy="false"></mo><mi>t</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo separator="true">,</mo><mi></mi><mo stretchy="false"></mo><mo></mo><mi>F</mi><mo stretchy="false">(</mo><mi></mi><mo stretchy="false">)</mo><mo>+</mo><mi>k</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">p_{F}(x; \theta) = \exp(\langle t(x), \theta\rangle - F(\theta) + k(x))</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathnormal">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">F</span></span></span></span></span><span class="vlist-s"></span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mpunct">;</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;"></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mop">exp</span><span class="mopen">(</span><span class="mord mathnormal">t</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;"></span><span class="mclose"></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin"></span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">F</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.02778em;"></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.03148em;">k</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">))</span></span></span></span></span></div><p>where <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi></mi></mrow><annotation encoding="application/x-tex">\theta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;"></span></span></span></span></span> denotes the natural parameters, <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">t(x)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">t</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span></span></span></span></span> denotes the sufficient statistic,
<span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>F</mi><mo stretchy="false">(</mo><mi></mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">F(\theta)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">F</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.02778em;"></span><span class="mclose">)</span></span></span></span></span> is the log normalizer function for a given family and <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">k(x)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.03148em;">k</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span></span></span></span></span> is the carrier
measure.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This class is an intermediary between the <cite>Distribution</cite> class and distributions which belong
to an exponential family mainly to check the correctness of the <cite>.entropy()</cite> and analytic KL
divergence methods. We use this class to compute the entropy and KL divergence using the AD
framework and Bregman divergences (courtesy of: Frank Nielsen and Richard Nock, Entropies and
Cross-entropies of Exponential Families).</p>
</div>
<dl class="field-list simple">
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="torch.distributions.exp_family.ExponentialFamily.entropy">
<span class="sig-name descname"><span class="pre">entropy</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/exp_family.py#L53"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.exp_family.ExponentialFamily.entropy" title="Permalink to this definition">#</a></dt>
<dd><p>Method to compute the entropy using Bregman divergence of the log normalizer.</p>
</dd></dl>

</dd></dl>

</section>
<section id="bernoulli">
<h2><span class="hidden-section">Bernoulli</span><a class="headerlink" href="#bernoulli" title="Permalink to this heading">#</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="torch.distributions.bernoulli.Bernoulli">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torch.distributions.bernoulli.</span></span><span class="sig-name descname"><span class="pre">Bernoulli</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">probs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">logits</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">validate_args</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/bernoulli.py#L20"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.bernoulli.Bernoulli" title="Permalink to this definition">#</a></dt>
<dd><p>Bases: <a class="reference internal" href="#torch.distributions.exp_family.ExponentialFamily" title="torch.distributions.exp_family.ExponentialFamily"><code class="xref py py-class docutils literal notranslate"><span class="pre">ExponentialFamily</span></code></a></p>
<p>Creates a Bernoulli distribution parameterized by <a class="reference internal" href="#torch.distributions.bernoulli.Bernoulli.probs" title="torch.distributions.bernoulli.Bernoulli.probs"><code class="xref py py-attr docutils literal notranslate"><span class="pre">probs</span></code></a>
or <a class="reference internal" href="#torch.distributions.bernoulli.Bernoulli.logits" title="torch.distributions.bernoulli.Bernoulli.logits"><code class="xref py py-attr docutils literal notranslate"><span class="pre">logits</span></code></a> (but not both).</p>
<p>Samples are binary (0 or 1). They take the value <cite>1</cite> with probability <cite>p</cite>
and <cite>0</cite> with probability <cite>1 - p</cite>.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">Bernoulli</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.3</span><span class="p">]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>  <span class="c1"># 30% chance 1; 70% chance 0</span>
<span class="go">tensor([ 0.])</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>probs</strong> (<em>Number</em><em>, </em><a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>)  the probability of sampling <cite>1</cite></p></li>
<li><p><strong>logits</strong> (<em>Number</em><em>, </em><a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>)  the log-odds of sampling <cite>1</cite></p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="torch.distributions.bernoulli.Bernoulli.arg_constraints">
<span class="sig-name descname"><span class="pre">arg_constraints</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">{'logits':</span> <span class="pre">Real(),</span> <span class="pre">'probs':</span> <span class="pre">Interval(lower_bound=0.0,</span> <span class="pre">upper_bound=1.0)}</span></em><a class="headerlink" href="#torch.distributions.bernoulli.Bernoulli.arg_constraints" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributions.bernoulli.Bernoulli.entropy">
<span class="sig-name descname"><span class="pre">entropy</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/bernoulli.py#L115"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.bernoulli.Bernoulli.entropy" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributions.bernoulli.Bernoulli.enumerate_support">
<span class="sig-name descname"><span class="pre">enumerate_support</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">expand</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/bernoulli.py#L120"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.bernoulli.Bernoulli.enumerate_support" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributions.bernoulli.Bernoulli.expand">
<span class="sig-name descname"><span class="pre">expand</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch_shape</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">_instance</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/bernoulli.py#L62"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.bernoulli.Bernoulli.expand" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="torch.distributions.bernoulli.Bernoulli.has_enumerate_support">
<span class="sig-name descname"><span class="pre">has_enumerate_support</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">True</span></em><a class="headerlink" href="#torch.distributions.bernoulli.Bernoulli.has_enumerate_support" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributions.bernoulli.Bernoulli.log_prob">
<span class="sig-name descname"><span class="pre">log_prob</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">value</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/bernoulli.py#L109"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.bernoulli.Bernoulli.log_prob" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="torch.distributions.bernoulli.Bernoulli.logits">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">logits</span></span><a class="headerlink" href="#torch.distributions.bernoulli.Bernoulli.logits" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="torch.distributions.bernoulli.Bernoulli.mean">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">mean</span></span><a class="headerlink" href="#torch.distributions.bernoulli.Bernoulli.mean" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="torch.distributions.bernoulli.Bernoulli.mode">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">mode</span></span><a class="headerlink" href="#torch.distributions.bernoulli.Bernoulli.mode" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="torch.distributions.bernoulli.Bernoulli.param_shape">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">param_shape</span></span><a class="headerlink" href="#torch.distributions.bernoulli.Bernoulli.param_shape" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="torch.distributions.bernoulli.Bernoulli.probs">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">probs</span></span><a class="headerlink" href="#torch.distributions.bernoulli.Bernoulli.probs" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributions.bernoulli.Bernoulli.sample">
<span class="sig-name descname"><span class="pre">sample</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">sample_shape</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">torch.Size([])</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/bernoulli.py#L104"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.bernoulli.Bernoulli.sample" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="torch.distributions.bernoulli.Bernoulli.support">
<span class="sig-name descname"><span class="pre">support</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">Boolean()</span></em><a class="headerlink" href="#torch.distributions.bernoulli.Bernoulli.support" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="torch.distributions.bernoulli.Bernoulli.variance">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">variance</span></span><a class="headerlink" href="#torch.distributions.bernoulli.Bernoulli.variance" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="beta">
<h2><span class="hidden-section">Beta</span><a class="headerlink" href="#beta" title="Permalink to this heading">#</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="torch.distributions.beta.Beta">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torch.distributions.beta.</span></span><span class="sig-name descname"><span class="pre">Beta</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">concentration1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">concentration0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">validate_args</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/beta.py#L15"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.beta.Beta" title="Permalink to this definition">#</a></dt>
<dd><p>Bases: <a class="reference internal" href="#torch.distributions.exp_family.ExponentialFamily" title="torch.distributions.exp_family.ExponentialFamily"><code class="xref py py-class docutils literal notranslate"><span class="pre">ExponentialFamily</span></code></a></p>
<p>Beta distribution parameterized by <a class="reference internal" href="#torch.distributions.beta.Beta.concentration1" title="torch.distributions.beta.Beta.concentration1"><code class="xref py py-attr docutils literal notranslate"><span class="pre">concentration1</span></code></a> and <a class="reference internal" href="#torch.distributions.beta.Beta.concentration0" title="torch.distributions.beta.Beta.concentration0"><code class="xref py py-attr docutils literal notranslate"><span class="pre">concentration0</span></code></a>.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">Beta</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.5</span><span class="p">]),</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.5</span><span class="p">]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>  <span class="c1"># Beta distributed with concentration concentration1 and concentration0</span>
<span class="go">tensor([ 0.1046])</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>concentration1</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a><em> or </em><a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>)  1st concentration parameter of the distribution
(often referred to as alpha)</p></li>
<li><p><strong>concentration0</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a><em> or </em><a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>)  2nd concentration parameter of the distribution
(often referred to as beta)</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="torch.distributions.beta.Beta.arg_constraints">
<span class="sig-name descname"><span class="pre">arg_constraints</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">{'concentration0':</span> <span class="pre">GreaterThan(lower_bound=0.0),</span> <span class="pre">'concentration1':</span> <span class="pre">GreaterThan(lower_bound=0.0)}</span></em><a class="headerlink" href="#torch.distributions.beta.Beta.arg_constraints" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="torch.distributions.beta.Beta.concentration0">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">concentration0</span></span><a class="headerlink" href="#torch.distributions.beta.Beta.concentration0" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="torch.distributions.beta.Beta.concentration1">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">concentration1</span></span><a class="headerlink" href="#torch.distributions.beta.Beta.concentration1" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributions.beta.Beta.entropy">
<span class="sig-name descname"><span class="pre">entropy</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/beta.py#L86"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.beta.Beta.entropy" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributions.beta.Beta.expand">
<span class="sig-name descname"><span class="pre">expand</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch_shape</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">_instance</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/beta.py#L56"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.beta.Beta.expand" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="torch.distributions.beta.Beta.has_rsample">
<span class="sig-name descname"><span class="pre">has_rsample</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">True</span></em><a class="headerlink" href="#torch.distributions.beta.Beta.has_rsample" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributions.beta.Beta.log_prob">
<span class="sig-name descname"><span class="pre">log_prob</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">value</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/beta.py#L80"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.beta.Beta.log_prob" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="torch.distributions.beta.Beta.mean">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">mean</span></span><a class="headerlink" href="#torch.distributions.beta.Beta.mean" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="torch.distributions.beta.Beta.mode">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">mode</span></span><a class="headerlink" href="#torch.distributions.beta.Beta.mode" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributions.beta.Beta.rsample">
<span class="sig-name descname"><span class="pre">rsample</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">sample_shape</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">()</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/beta.py#L77"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.beta.Beta.rsample" title="Permalink to this definition">#</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a></p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="torch.distributions.beta.Beta.support">
<span class="sig-name descname"><span class="pre">support</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">Interval(lower_bound=0.0,</span> <span class="pre">upper_bound=1.0)</span></em><a class="headerlink" href="#torch.distributions.beta.Beta.support" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="torch.distributions.beta.Beta.variance">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">variance</span></span><a class="headerlink" href="#torch.distributions.beta.Beta.variance" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="binomial">
<h2><span class="hidden-section">Binomial</span><a class="headerlink" href="#binomial" title="Permalink to this heading">#</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="torch.distributions.binomial.Binomial">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torch.distributions.binomial.</span></span><span class="sig-name descname"><span class="pre">Binomial</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">total_count</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">probs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">logits</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">validate_args</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/binomial.py#L21"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.binomial.Binomial" title="Permalink to this definition">#</a></dt>
<dd><p>Bases: <a class="reference internal" href="#torch.distributions.distribution.Distribution" title="torch.distributions.distribution.Distribution"><code class="xref py py-class docutils literal notranslate"><span class="pre">Distribution</span></code></a></p>
<p>Creates a Binomial distribution parameterized by <code class="xref py py-attr docutils literal notranslate"><span class="pre">total_count</span></code> and
either <a class="reference internal" href="#torch.distributions.binomial.Binomial.probs" title="torch.distributions.binomial.Binomial.probs"><code class="xref py py-attr docutils literal notranslate"><span class="pre">probs</span></code></a> or <a class="reference internal" href="#torch.distributions.binomial.Binomial.logits" title="torch.distributions.binomial.Binomial.logits"><code class="xref py py-attr docutils literal notranslate"><span class="pre">logits</span></code></a> (but not both). <code class="xref py py-attr docutils literal notranslate"><span class="pre">total_count</span></code> must be
broadcastable with <a class="reference internal" href="#torch.distributions.binomial.Binomial.probs" title="torch.distributions.binomial.Binomial.probs"><code class="xref py py-attr docutils literal notranslate"><span class="pre">probs</span></code></a>/<a class="reference internal" href="#torch.distributions.binomial.Binomial.logits" title="torch.distributions.binomial.Binomial.logits"><code class="xref py py-attr docutils literal notranslate"><span class="pre">logits</span></code></a>.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">Binomial</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">0</span> <span class="p">,</span> <span class="mf">.2</span><span class="p">,</span> <span class="mf">.8</span><span class="p">,</span> <span class="mi">1</span><span class="p">]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">m</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>
<span class="go">tensor([   0.,   22.,   71.,  100.])</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">Binomial</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">5.</span><span class="p">],</span> <span class="p">[</span><span class="mf">10.</span><span class="p">]]),</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">m</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>
<span class="go">tensor([[ 4.,  5.],</span>
<span class="go">        [ 7.,  6.]])</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>total_count</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a><em> or </em><a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>)  number of Bernoulli trials</p></li>
<li><p><strong>probs</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>)  Event probabilities</p></li>
<li><p><strong>logits</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>)  Event log-odds</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="torch.distributions.binomial.Binomial.arg_constraints">
<span class="sig-name descname"><span class="pre">arg_constraints</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">{'logits':</span> <span class="pre">Real(),</span> <span class="pre">'probs':</span> <span class="pre">Interval(lower_bound=0.0,</span> <span class="pre">upper_bound=1.0),</span> <span class="pre">'total_count':</span> <span class="pre">IntegerGreaterThan(lower_bound=0)}</span></em><a class="headerlink" href="#torch.distributions.binomial.Binomial.arg_constraints" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributions.binomial.Binomial.entropy">
<span class="sig-name descname"><span class="pre">entropy</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/binomial.py#L145"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.binomial.Binomial.entropy" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributions.binomial.Binomial.enumerate_support">
<span class="sig-name descname"><span class="pre">enumerate_support</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">expand</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/binomial.py#L155"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.binomial.Binomial.enumerate_support" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributions.binomial.Binomial.expand">
<span class="sig-name descname"><span class="pre">expand</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch_shape</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">_instance</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/binomial.py#L73"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.binomial.Binomial.expand" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="torch.distributions.binomial.Binomial.has_enumerate_support">
<span class="sig-name descname"><span class="pre">has_enumerate_support</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">True</span></em><a class="headerlink" href="#torch.distributions.binomial.Binomial.has_enumerate_support" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributions.binomial.Binomial.log_prob">
<span class="sig-name descname"><span class="pre">log_prob</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">value</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/binomial.py#L125"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.binomial.Binomial.log_prob" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="torch.distributions.binomial.Binomial.logits">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">logits</span></span><a class="headerlink" href="#torch.distributions.binomial.Binomial.logits" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="torch.distributions.binomial.Binomial.mean">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">mean</span></span><a class="headerlink" href="#torch.distributions.binomial.Binomial.mean" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="torch.distributions.binomial.Binomial.mode">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">mode</span></span><a class="headerlink" href="#torch.distributions.binomial.Binomial.mode" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="torch.distributions.binomial.Binomial.param_shape">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">param_shape</span></span><a class="headerlink" href="#torch.distributions.binomial.Binomial.param_shape" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="torch.distributions.binomial.Binomial.probs">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">probs</span></span><a class="headerlink" href="#torch.distributions.binomial.Binomial.probs" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributions.binomial.Binomial.sample">
<span class="sig-name descname"><span class="pre">sample</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">sample_shape</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">torch.Size([])</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/binomial.py#L118"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.binomial.Binomial.sample" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="torch.distributions.binomial.Binomial.support">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">support</span></span><a class="headerlink" href="#torch.distributions.binomial.Binomial.support" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="torch.distributions.binomial.Binomial.variance">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">variance</span></span><a class="headerlink" href="#torch.distributions.binomial.Binomial.variance" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="categorical">
<h2><span class="hidden-section">Categorical</span><a class="headerlink" href="#categorical" title="Permalink to this heading">#</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="torch.distributions.categorical.Categorical">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torch.distributions.categorical.</span></span><span class="sig-name descname"><span class="pre">Categorical</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">probs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">logits</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">validate_args</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/categorical.py#L12"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.categorical.Categorical" title="Permalink to this definition">#</a></dt>
<dd><p>Bases: <a class="reference internal" href="#torch.distributions.distribution.Distribution" title="torch.distributions.distribution.Distribution"><code class="xref py py-class docutils literal notranslate"><span class="pre">Distribution</span></code></a></p>
<p>Creates a categorical distribution parameterized by either <a class="reference internal" href="#torch.distributions.categorical.Categorical.probs" title="torch.distributions.categorical.Categorical.probs"><code class="xref py py-attr docutils literal notranslate"><span class="pre">probs</span></code></a> or
<a class="reference internal" href="#torch.distributions.categorical.Categorical.logits" title="torch.distributions.categorical.Categorical.logits"><code class="xref py py-attr docutils literal notranslate"><span class="pre">logits</span></code></a> (but not both).</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>It is equivalent to the distribution that <a class="reference internal" href="generated/torch.multinomial.html#torch.multinomial" title="torch.multinomial"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.multinomial()</span></code></a>
samples from.</p>
</div>
<p>Samples are integers from <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">{</mo><mn>0</mn><mo separator="true">,</mo><mo></mo><mo separator="true">,</mo><mi>K</mi><mo></mo><mn>1</mn><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">\{0, \ldots, K-1\}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">{</span><span class="mord">0</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="minner"></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">K</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin"></span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">1</span><span class="mclose">}</span></span></span></span></span> where <cite>K</cite> is <code class="docutils literal notranslate"><span class="pre">probs.size(-1)</span></code>.</p>
<p>If <cite>probs</cite> is 1-dimensional with length-<cite>K</cite>, each element is the relative probability
of sampling the class at that index.</p>
<p>If <cite>probs</cite> is N-dimensional, the first N-1 dimensions are treated as a batch of
relative probability vectors.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The <cite>probs</cite> argument must be non-negative, finite and have a non-zero sum,
and it will be normalized to sum to 1 along the last dimension. <a class="reference internal" href="#torch.distributions.categorical.Categorical.probs" title="torch.distributions.categorical.Categorical.probs"><code class="xref py py-attr docutils literal notranslate"><span class="pre">probs</span></code></a>
will return this normalized value.
The <cite>logits</cite> argument will be interpreted as unnormalized log probabilities
and can therefore be any real number. It will likewise be normalized so that
the resulting probabilities sum to 1 along the last dimension. <a class="reference internal" href="#torch.distributions.categorical.Categorical.logits" title="torch.distributions.categorical.Categorical.logits"><code class="xref py py-attr docutils literal notranslate"><span class="pre">logits</span></code></a>
will return this normalized value.</p>
</div>
<p>See also: <a class="reference internal" href="generated/torch.multinomial.html#torch.multinomial" title="torch.multinomial"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.multinomial()</span></code></a></p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">Categorical</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span> <span class="mf">0.25</span><span class="p">,</span> <span class="mf">0.25</span><span class="p">,</span> <span class="mf">0.25</span><span class="p">,</span> <span class="mf">0.25</span> <span class="p">]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>  <span class="c1"># equal probability of 0, 1, 2, 3</span>
<span class="go">tensor(3)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>probs</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>)  event probabilities</p></li>
<li><p><strong>logits</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>)  event log probabilities (unnormalized)</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="torch.distributions.categorical.Categorical.arg_constraints">
<span class="sig-name descname"><span class="pre">arg_constraints</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">{'logits':</span> <span class="pre">IndependentConstraint(Real(),</span> <span class="pre">1),</span> <span class="pre">'probs':</span> <span class="pre">Simplex()}</span></em><a class="headerlink" href="#torch.distributions.categorical.Categorical.arg_constraints" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributions.categorical.Categorical.entropy">
<span class="sig-name descname"><span class="pre">entropy</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/categorical.py#L145"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.categorical.Categorical.entropy" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributions.categorical.Categorical.enumerate_support">
<span class="sig-name descname"><span class="pre">enumerate_support</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">expand</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/categorical.py#L151"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.categorical.Categorical.enumerate_support" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributions.categorical.Categorical.expand">
<span class="sig-name descname"><span class="pre">expand</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch_shape</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">_instance</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/categorical.py#L74"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.categorical.Categorical.expand" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="torch.distributions.categorical.Categorical.has_enumerate_support">
<span class="sig-name descname"><span class="pre">has_enumerate_support</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">True</span></em><a class="headerlink" href="#torch.distributions.categorical.Categorical.has_enumerate_support" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributions.categorical.Categorical.log_prob">
<span class="sig-name descname"><span class="pre">log_prob</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">value</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/categorical.py#L137"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.categorical.Categorical.log_prob" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="torch.distributions.categorical.Categorical.logits">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">logits</span></span><a class="headerlink" href="#torch.distributions.categorical.Categorical.logits" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="torch.distributions.categorical.Categorical.mean">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">mean</span></span><a class="headerlink" href="#torch.distributions.categorical.Categorical.mean" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="torch.distributions.categorical.Categorical.mode">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">mode</span></span><a class="headerlink" href="#torch.distributions.categorical.Categorical.mode" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="torch.distributions.categorical.Categorical.param_shape">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">param_shape</span></span><a class="headerlink" href="#torch.distributions.categorical.Categorical.param_shape" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="torch.distributions.categorical.Categorical.probs">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">probs</span></span><a class="headerlink" href="#torch.distributions.categorical.Categorical.probs" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributions.categorical.Categorical.sample">
<span class="sig-name descname"><span class="pre">sample</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">sample_shape</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">torch.Size([])</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/categorical.py#L130"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.categorical.Categorical.sample" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="torch.distributions.categorical.Categorical.support">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">support</span></span><a class="headerlink" href="#torch.distributions.categorical.Categorical.support" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="torch.distributions.categorical.Categorical.variance">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">variance</span></span><a class="headerlink" href="#torch.distributions.categorical.Categorical.variance" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="cauchy">
<h2><span class="hidden-section">Cauchy</span><a class="headerlink" href="#cauchy" title="Permalink to this heading">#</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="torch.distributions.cauchy.Cauchy">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torch.distributions.cauchy.</span></span><span class="sig-name descname"><span class="pre">Cauchy</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">loc</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">scale</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">validate_args</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/cauchy.py#L16"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.cauchy.Cauchy" title="Permalink to this definition">#</a></dt>
<dd><p>Bases: <a class="reference internal" href="#torch.distributions.distribution.Distribution" title="torch.distributions.distribution.Distribution"><code class="xref py py-class docutils literal notranslate"><span class="pre">Distribution</span></code></a></p>
<p>Samples from a Cauchy (Lorentz) distribution. The distribution of the ratio of
independent normally distributed random variables with means <cite>0</cite> follows a
Cauchy distribution.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">Cauchy</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.0</span><span class="p">]),</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.0</span><span class="p">]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>  <span class="c1"># sample from a Cauchy distribution with loc=0 and scale=1</span>
<span class="go">tensor([ 2.3214])</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>loc</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a><em> or </em><a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>)  mode or median of the distribution.</p></li>
<li><p><strong>scale</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a><em> or </em><a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>)  half width at half maximum.</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="torch.distributions.cauchy.Cauchy.arg_constraints">
<span class="sig-name descname"><span class="pre">arg_constraints</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">{'loc':</span> <span class="pre">Real(),</span> <span class="pre">'scale':</span> <span class="pre">GreaterThan(lower_bound=0.0)}</span></em><a class="headerlink" href="#torch.distributions.cauchy.Cauchy.arg_constraints" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributions.cauchy.Cauchy.cdf">
<span class="sig-name descname"><span class="pre">cdf</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">value</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/cauchy.py#L84"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.cauchy.Cauchy.cdf" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributions.cauchy.Cauchy.entropy">
<span class="sig-name descname"><span class="pre">entropy</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/cauchy.py#L92"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.cauchy.Cauchy.entropy" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributions.cauchy.Cauchy.expand">
<span class="sig-name descname"><span class="pre">expand</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch_shape</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">_instance</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/cauchy.py#L45"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.cauchy.Cauchy.expand" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="torch.distributions.cauchy.Cauchy.has_rsample">
<span class="sig-name descname"><span class="pre">has_rsample</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">True</span></em><a class="headerlink" href="#torch.distributions.cauchy.Cauchy.has_rsample" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributions.cauchy.Cauchy.icdf">
<span class="sig-name descname"><span class="pre">icdf</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">value</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/cauchy.py#L89"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.cauchy.Cauchy.icdf" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributions.cauchy.Cauchy.log_prob">
<span class="sig-name descname"><span class="pre">log_prob</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">value</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/cauchy.py#L75"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.cauchy.Cauchy.log_prob" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="torch.distributions.cauchy.Cauchy.mean">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">mean</span></span><a class="headerlink" href="#torch.distributions.cauchy.Cauchy.mean" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="torch.distributions.cauchy.Cauchy.mode">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">mode</span></span><a class="headerlink" href="#torch.distributions.cauchy.Cauchy.mode" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributions.cauchy.Cauchy.rsample">
<span class="sig-name descname"><span class="pre">rsample</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">sample_shape</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">torch.Size([])</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/cauchy.py#L70"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.cauchy.Cauchy.rsample" title="Permalink to this definition">#</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a></p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="torch.distributions.cauchy.Cauchy.support">
<span class="sig-name descname"><span class="pre">support</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">Real()</span></em><a class="headerlink" href="#torch.distributions.cauchy.Cauchy.support" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="torch.distributions.cauchy.Cauchy.variance">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">variance</span></span><a class="headerlink" href="#torch.distributions.cauchy.Cauchy.variance" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="chi2">
<h2><span class="hidden-section">Chi2</span><a class="headerlink" href="#chi2" title="Permalink to this heading">#</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="torch.distributions.chi2.Chi2">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torch.distributions.chi2.</span></span><span class="sig-name descname"><span class="pre">Chi2</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">df</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">validate_args</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/chi2.py#L9"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.chi2.Chi2" title="Permalink to this definition">#</a></dt>
<dd><p>Bases: <a class="reference internal" href="#torch.distributions.gamma.Gamma" title="torch.distributions.gamma.Gamma"><code class="xref py py-class docutils literal notranslate"><span class="pre">Gamma</span></code></a></p>
<p>Creates a Chi-squared distribution parameterized by shape parameter <a class="reference internal" href="#torch.distributions.chi2.Chi2.df" title="torch.distributions.chi2.Chi2.df"><code class="xref py py-attr docutils literal notranslate"><span class="pre">df</span></code></a>.
This is exactly equivalent to <code class="docutils literal notranslate"><span class="pre">Gamma(alpha=0.5*df,</span> <span class="pre">beta=0.5)</span></code></p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">Chi2</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.0</span><span class="p">]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>  <span class="c1"># Chi2 distributed with shape df=1</span>
<span class="go">tensor([ 0.1046])</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>df</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a><em> or </em><a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>)  shape parameter of the distribution</p>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="torch.distributions.chi2.Chi2.arg_constraints">
<span class="sig-name descname"><span class="pre">arg_constraints</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">{'df':</span> <span class="pre">GreaterThan(lower_bound=0.0)}</span></em><a class="headerlink" href="#torch.distributions.chi2.Chi2.arg_constraints" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="torch.distributions.chi2.Chi2.df">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">df</span></span><a class="headerlink" href="#torch.distributions.chi2.Chi2.df" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributions.chi2.Chi2.expand">
<span class="sig-name descname"><span class="pre">expand</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch_shape</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">_instance</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/chi2.py#L29"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.chi2.Chi2.expand" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="continuousbernoulli">
<h2><span class="hidden-section">ContinuousBernoulli</span><a class="headerlink" href="#continuousbernoulli" title="Permalink to this heading">#</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="torch.distributions.continuous_bernoulli.ContinuousBernoulli">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torch.distributions.continuous_bernoulli.</span></span><span class="sig-name descname"><span class="pre">ContinuousBernoulli</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">probs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">logits</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lims</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">(0.499,</span> <span class="pre">0.501)</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">validate_args</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/continuous_bernoulli.py#L22"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.continuous_bernoulli.ContinuousBernoulli" title="Permalink to this definition">#</a></dt>
<dd><p>Bases: <a class="reference internal" href="#torch.distributions.exp_family.ExponentialFamily" title="torch.distributions.exp_family.ExponentialFamily"><code class="xref py py-class docutils literal notranslate"><span class="pre">ExponentialFamily</span></code></a></p>
<p>Creates a continuous Bernoulli distribution parameterized by <a class="reference internal" href="#torch.distributions.continuous_bernoulli.ContinuousBernoulli.probs" title="torch.distributions.continuous_bernoulli.ContinuousBernoulli.probs"><code class="xref py py-attr docutils literal notranslate"><span class="pre">probs</span></code></a>
or <a class="reference internal" href="#torch.distributions.continuous_bernoulli.ContinuousBernoulli.logits" title="torch.distributions.continuous_bernoulli.ContinuousBernoulli.logits"><code class="xref py py-attr docutils literal notranslate"><span class="pre">logits</span></code></a> (but not both).</p>
<p>The distribution is supported in [0, 1] and parameterized by probs (in
(0,1)) or logits (real-valued). Note that, unlike the Bernoulli, probs
does not correspond to a probability and logits does not correspond to
log-odds, but the same names are used due to the similarity with the
Bernoulli. See [1] for more details.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">ContinuousBernoulli</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.3</span><span class="p">]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>
<span class="go">tensor([ 0.2538])</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>probs</strong> (<em>Number</em><em>, </em><a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>)  (0,1) valued parameters</p></li>
<li><p><strong>logits</strong> (<em>Number</em><em>, </em><a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>)  real valued parameters whose sigmoid matches probs</p></li>
</ul>
</dd>
</dl>
<p>[1] The continuous Bernoulli: fixing a pervasive error in variational
autoencoders, Loaiza-Ganem G and Cunningham JP, NeurIPS 2019.
<a class="reference external" href="https://arxiv.org/abs/1907.06845">https://arxiv.org/abs/1907.06845</a></p>
<dl class="py attribute">
<dt class="sig sig-object py" id="torch.distributions.continuous_bernoulli.ContinuousBernoulli.arg_constraints">
<span class="sig-name descname"><span class="pre">arg_constraints</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">{'logits':</span> <span class="pre">Real(),</span> <span class="pre">'probs':</span> <span class="pre">Interval(lower_bound=0.0,</span> <span class="pre">upper_bound=1.0)}</span></em><a class="headerlink" href="#torch.distributions.continuous_bernoulli.ContinuousBernoulli.arg_constraints" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributions.continuous_bernoulli.ContinuousBernoulli.cdf">
<span class="sig-name descname"><span class="pre">cdf</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">value</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/continuous_bernoulli.py#L185"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.continuous_bernoulli.ContinuousBernoulli.cdf" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributions.continuous_bernoulli.ContinuousBernoulli.entropy">
<span class="sig-name descname"><span class="pre">entropy</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/continuous_bernoulli.py#L213"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.continuous_bernoulli.ContinuousBernoulli.entropy" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributions.continuous_bernoulli.ContinuousBernoulli.expand">
<span class="sig-name descname"><span class="pre">expand</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch_shape</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">_instance</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/continuous_bernoulli.py#L80"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.continuous_bernoulli.ContinuousBernoulli.expand" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="torch.distributions.continuous_bernoulli.ContinuousBernoulli.has_rsample">
<span class="sig-name descname"><span class="pre">has_rsample</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">True</span></em><a class="headerlink" href="#torch.distributions.continuous_bernoulli.ContinuousBernoulli.has_rsample" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributions.continuous_bernoulli.ContinuousBernoulli.icdf">
<span class="sig-name descname"><span class="pre">icdf</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">value</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/continuous_bernoulli.py#L201"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.continuous_bernoulli.ContinuousBernoulli.icdf" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributions.continuous_bernoulli.ContinuousBernoulli.log_prob">
<span class="sig-name descname"><span class="pre">log_prob</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">value</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/continuous_bernoulli.py#L176"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.continuous_bernoulli.ContinuousBernoulli.log_prob" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="torch.distributions.continuous_bernoulli.ContinuousBernoulli.logits">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">logits</span></span><a class="headerlink" href="#torch.distributions.continuous_bernoulli.ContinuousBernoulli.logits" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="torch.distributions.continuous_bernoulli.ContinuousBernoulli.mean">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">mean</span></span><a class="headerlink" href="#torch.distributions.continuous_bernoulli.ContinuousBernoulli.mean" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="torch.distributions.continuous_bernoulli.ContinuousBernoulli.param_shape">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">param_shape</span></span><a class="headerlink" href="#torch.distributions.continuous_bernoulli.ContinuousBernoulli.param_shape" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="torch.distributions.continuous_bernoulli.ContinuousBernoulli.probs">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">probs</span></span><a class="headerlink" href="#torch.distributions.continuous_bernoulli.ContinuousBernoulli.probs" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributions.continuous_bernoulli.ContinuousBernoulli.rsample">
<span class="sig-name descname"><span class="pre">rsample</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">sample_shape</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">torch.Size([])</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/continuous_bernoulli.py#L171"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.continuous_bernoulli.ContinuousBernoulli.rsample" title="Permalink to this definition">#</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributions.continuous_bernoulli.ContinuousBernoulli.sample">
<span class="sig-name descname"><span class="pre">sample</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">sample_shape</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">torch.Size([])</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/continuous_bernoulli.py#L165"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.continuous_bernoulli.ContinuousBernoulli.sample" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="torch.distributions.continuous_bernoulli.ContinuousBernoulli.stddev">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">stddev</span></span><a class="headerlink" href="#torch.distributions.continuous_bernoulli.ContinuousBernoulli.stddev" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="torch.distributions.continuous_bernoulli.ContinuousBernoulli.support">
<span class="sig-name descname"><span class="pre">support</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">Interval(lower_bound=0.0,</span> <span class="pre">upper_bound=1.0)</span></em><a class="headerlink" href="#torch.distributions.continuous_bernoulli.ContinuousBernoulli.support" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="torch.distributions.continuous_bernoulli.ContinuousBernoulli.variance">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">variance</span></span><a class="headerlink" href="#torch.distributions.continuous_bernoulli.ContinuousBernoulli.variance" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="dirichlet">
<h2><span class="hidden-section">Dirichlet</span><a class="headerlink" href="#dirichlet" title="Permalink to this heading">#</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="torch.distributions.dirichlet.Dirichlet">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torch.distributions.dirichlet.</span></span><span class="sig-name descname"><span class="pre">Dirichlet</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">concentration</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">validate_args</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/dirichlet.py#L34"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.dirichlet.Dirichlet" title="Permalink to this definition">#</a></dt>
<dd><p>Bases: <a class="reference internal" href="#torch.distributions.exp_family.ExponentialFamily" title="torch.distributions.exp_family.ExponentialFamily"><code class="xref py py-class docutils literal notranslate"><span class="pre">ExponentialFamily</span></code></a></p>
<p>Creates a Dirichlet distribution parameterized by concentration <code class="xref py py-attr docutils literal notranslate"><span class="pre">concentration</span></code>.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">Dirichlet</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>  <span class="c1"># Dirichlet distributed with concentration [0.5, 0.5]</span>
<span class="go">tensor([ 0.1046,  0.8954])</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>concentration</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>)  concentration parameter of the distribution
(often referred to as alpha)</p>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="torch.distributions.dirichlet.Dirichlet.arg_constraints">
<span class="sig-name descname"><span class="pre">arg_constraints</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">{'concentration':</span> <span class="pre">IndependentConstraint(GreaterThan(lower_bound=0.0),</span> <span class="pre">1)}</span></em><a class="headerlink" href="#torch.distributions.dirichlet.Dirichlet.arg_constraints" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributions.dirichlet.Dirichlet.entropy">
<span class="sig-name descname"><span class="pre">entropy</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/dirichlet.py#L111"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.dirichlet.Dirichlet.entropy" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributions.dirichlet.Dirichlet.expand">
<span class="sig-name descname"><span class="pre">expand</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch_shape</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">_instance</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/dirichlet.py#L64"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.dirichlet.Dirichlet.expand" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="torch.distributions.dirichlet.Dirichlet.has_rsample">
<span class="sig-name descname"><span class="pre">has_rsample</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">True</span></em><a class="headerlink" href="#torch.distributions.dirichlet.Dirichlet.has_rsample" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributions.dirichlet.Dirichlet.log_prob">
<span class="sig-name descname"><span class="pre">log_prob</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">value</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/dirichlet.py#L79"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.dirichlet.Dirichlet.log_prob" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="torch.distributions.dirichlet.Dirichlet.mean">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">mean</span></span><a class="headerlink" href="#torch.distributions.dirichlet.Dirichlet.mean" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="torch.distributions.dirichlet.Dirichlet.mode">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">mode</span></span><a class="headerlink" href="#torch.distributions.dirichlet.Dirichlet.mode" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributions.dirichlet.Dirichlet.rsample">
<span class="sig-name descname"><span class="pre">rsample</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">sample_shape</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">()</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/dirichlet.py#L74"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.dirichlet.Dirichlet.rsample" title="Permalink to this definition">#</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a></p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="torch.distributions.dirichlet.Dirichlet.support">
<span class="sig-name descname"><span class="pre">support</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">Simplex()</span></em><a class="headerlink" href="#torch.distributions.dirichlet.Dirichlet.support" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="torch.distributions.dirichlet.Dirichlet.variance">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">variance</span></span><a class="headerlink" href="#torch.distributions.dirichlet.Dirichlet.variance" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="exponential">
<h2><span class="hidden-section">Exponential</span><a class="headerlink" href="#exponential" title="Permalink to this heading">#</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="torch.distributions.exponential.Exponential">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torch.distributions.exponential.</span></span><span class="sig-name descname"><span class="pre">Exponential</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">rate</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">validate_args</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/exponential.py#L14"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.exponential.Exponential" title="Permalink to this definition">#</a></dt>
<dd><p>Bases: <a class="reference internal" href="#torch.distributions.exp_family.ExponentialFamily" title="torch.distributions.exp_family.ExponentialFamily"><code class="xref py py-class docutils literal notranslate"><span class="pre">ExponentialFamily</span></code></a></p>
<p>Creates a Exponential distribution parameterized by <code class="xref py py-attr docutils literal notranslate"><span class="pre">rate</span></code>.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">Exponential</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.0</span><span class="p">]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>  <span class="c1"># Exponential distributed with rate=1</span>
<span class="go">tensor([ 0.1046])</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>rate</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a><em> or </em><a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>)  rate = 1 / scale of the distribution</p>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="torch.distributions.exponential.Exponential.arg_constraints">
<span class="sig-name descname"><span class="pre">arg_constraints</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">{'rate':</span> <span class="pre">GreaterThan(lower_bound=0.0)}</span></em><a class="headerlink" href="#torch.distributions.exponential.Exponential.arg_constraints" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributions.exponential.Exponential.cdf">
<span class="sig-name descname"><span class="pre">cdf</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">value</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/exponential.py#L71"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.exponential.Exponential.cdf" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributions.exponential.Exponential.entropy">
<span class="sig-name descname"><span class="pre">entropy</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/exponential.py#L79"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.exponential.Exponential.entropy" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributions.exponential.Exponential.expand">
<span class="sig-name descname"><span class="pre">expand</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch_shape</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">_instance</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/exponential.py#L54"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.exponential.Exponential.expand" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="torch.distributions.exponential.Exponential.has_rsample">
<span class="sig-name descname"><span class="pre">has_rsample</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">True</span></em><a class="headerlink" href="#torch.distributions.exponential.Exponential.has_rsample" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributions.exponential.Exponential.icdf">
<span class="sig-name descname"><span class="pre">icdf</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">value</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/exponential.py#L76"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.exponential.Exponential.icdf" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributions.exponential.Exponential.log_prob">
<span class="sig-name descname"><span class="pre">log_prob</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">value</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/exponential.py#L66"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.exponential.Exponential.log_prob" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="torch.distributions.exponential.Exponential.mean">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">mean</span></span><a class="headerlink" href="#torch.distributions.exponential.Exponential.mean" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="torch.distributions.exponential.Exponential.mode">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">mode</span></span><a class="headerlink" href="#torch.distributions.exponential.Exponential.mode" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributions.exponential.Exponential.rsample">
<span class="sig-name descname"><span class="pre">rsample</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">sample_shape</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">torch.Size([])</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/exponential.py#L62"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.exponential.Exponential.rsample" title="Permalink to this definition">#</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a></p>
</dd>
</dl>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="torch.distributions.exponential.Exponential.stddev">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">stddev</span></span><a class="headerlink" href="#torch.distributions.exponential.Exponential.stddev" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="torch.distributions.exponential.Exponential.support">
<span class="sig-name descname"><span class="pre">support</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">GreaterThanEq(lower_bound=0.0)</span></em><a class="headerlink" href="#torch.distributions.exponential.Exponential.support" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="torch.distributions.exponential.Exponential.variance">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">variance</span></span><a class="headerlink" href="#torch.distributions.exponential.Exponential.variance" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="fishersnedecor">
<h2><span class="hidden-section">FisherSnedecor</span><a class="headerlink" href="#fishersnedecor" title="Permalink to this heading">#</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="torch.distributions.fishersnedecor.FisherSnedecor">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torch.distributions.fishersnedecor.</span></span><span class="sig-name descname"><span class="pre">FisherSnedecor</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">df1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">df2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">validate_args</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/fishersnedecor.py#L16"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.fishersnedecor.FisherSnedecor" title="Permalink to this definition">#</a></dt>
<dd><p>Bases: <a class="reference internal" href="#torch.distributions.distribution.Distribution" title="torch.distributions.distribution.Distribution"><code class="xref py py-class docutils literal notranslate"><span class="pre">Distribution</span></code></a></p>
<p>Creates a Fisher-Snedecor distribution parameterized by <code class="xref py py-attr docutils literal notranslate"><span class="pre">df1</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">df2</span></code>.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">FisherSnedecor</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.0</span><span class="p">]),</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">2.0</span><span class="p">]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>  <span class="c1"># Fisher-Snedecor-distributed with df1=1 and df2=2</span>
<span class="go">tensor([ 0.2453])</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>df1</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a><em> or </em><a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>)  degrees of freedom parameter 1</p></li>
<li><p><strong>df2</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a><em> or </em><a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>)  degrees of freedom parameter 2</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="torch.distributions.fishersnedecor.FisherSnedecor.arg_constraints">
<span class="sig-name descname"><span class="pre">arg_constraints</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">{'df1':</span> <span class="pre">GreaterThan(lower_bound=0.0),</span> <span class="pre">'df2':</span> <span class="pre">GreaterThan(lower_bound=0.0)}</span></em><a class="headerlink" href="#torch.distributions.fishersnedecor.FisherSnedecor.arg_constraints" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributions.fishersnedecor.FisherSnedecor.expand">
<span class="sig-name descname"><span class="pre">expand</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch_shape</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">_instance</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/fishersnedecor.py#L46"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.fishersnedecor.FisherSnedecor.expand" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="torch.distributions.fishersnedecor.FisherSnedecor.has_rsample">
<span class="sig-name descname"><span class="pre">has_rsample</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">True</span></em><a class="headerlink" href="#torch.distributions.fishersnedecor.FisherSnedecor.has_rsample" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributions.fishersnedecor.FisherSnedecor.log_prob">
<span class="sig-name descname"><span class="pre">log_prob</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">value</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/fishersnedecor.py#L92"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.fishersnedecor.FisherSnedecor.log_prob" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="torch.distributions.fishersnedecor.FisherSnedecor.mean">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">mean</span></span><a class="headerlink" href="#torch.distributions.fishersnedecor.FisherSnedecor.mean" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="torch.distributions.fishersnedecor.FisherSnedecor.mode">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">mode</span></span><a class="headerlink" href="#torch.distributions.fishersnedecor.FisherSnedecor.mode" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributions.fishersnedecor.FisherSnedecor.rsample">
<span class="sig-name descname"><span class="pre">rsample</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">sample_shape</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">torch.Size([])</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/fishersnedecor.py#L80"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.fishersnedecor.FisherSnedecor.rsample" title="Permalink to this definition">#</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a></p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="torch.distributions.fishersnedecor.FisherSnedecor.support">
<span class="sig-name descname"><span class="pre">support</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">GreaterThan(lower_bound=0.0)</span></em><a class="headerlink" href="#torch.distributions.fishersnedecor.FisherSnedecor.support" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="torch.distributions.fishersnedecor.FisherSnedecor.variance">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">variance</span></span><a class="headerlink" href="#torch.distributions.fishersnedecor.FisherSnedecor.variance" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="gamma">
<h2><span class="hidden-section">Gamma</span><a class="headerlink" href="#gamma" title="Permalink to this heading">#</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="torch.distributions.gamma.Gamma">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torch.distributions.gamma.</span></span><span class="sig-name descname"><span class="pre">Gamma</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">concentration</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rate</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">validate_args</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/gamma.py#L18"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.gamma.Gamma" title="Permalink to this definition">#</a></dt>
<dd><p>Bases: <a class="reference internal" href="#torch.distributions.exp_family.ExponentialFamily" title="torch.distributions.exp_family.ExponentialFamily"><code class="xref py py-class docutils literal notranslate"><span class="pre">ExponentialFamily</span></code></a></p>
<p>Creates a Gamma distribution parameterized by shape <code class="xref py py-attr docutils literal notranslate"><span class="pre">concentration</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">rate</span></code>.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">Gamma</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.0</span><span class="p">]),</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.0</span><span class="p">]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>  <span class="c1"># Gamma distributed with concentration=1 and rate=1</span>
<span class="go">tensor([ 0.1046])</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>concentration</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a><em> or </em><a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>)  shape parameter of the distribution
(often referred to as alpha)</p></li>
<li><p><strong>rate</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a><em> or </em><a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>)  rate parameter of the distribution
(often referred to as beta), rate = 1 / scale</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="torch.distributions.gamma.Gamma.arg_constraints">
<span class="sig-name descname"><span class="pre">arg_constraints</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">{'concentration':</span> <span class="pre">GreaterThan(lower_bound=0.0),</span> <span class="pre">'rate':</span> <span class="pre">GreaterThan(lower_bound=0.0)}</span></em><a class="headerlink" href="#torch.distributions.gamma.Gamma.arg_constraints" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributions.gamma.Gamma.cdf">
<span class="sig-name descname"><span class="pre">cdf</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">value</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/gamma.py#L108"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.gamma.Gamma.cdf" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributions.gamma.Gamma.entropy">
<span class="sig-name descname"><span class="pre">entropy</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/gamma.py#L93"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.gamma.Gamma.entropy" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributions.gamma.Gamma.expand">
<span class="sig-name descname"><span class="pre">expand</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch_shape</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">_instance</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/gamma.py#L63"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.gamma.Gamma.expand" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="torch.distributions.gamma.Gamma.has_rsample">
<span class="sig-name descname"><span class="pre">has_rsample</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">True</span></em><a class="headerlink" href="#torch.distributions.gamma.Gamma.has_rsample" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributions.gamma.Gamma.log_prob">
<span class="sig-name descname"><span class="pre">log_prob</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">value</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/gamma.py#L82"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.gamma.Gamma.log_prob" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="torch.distributions.gamma.Gamma.mean">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">mean</span></span><a class="headerlink" href="#torch.distributions.gamma.Gamma.mean" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="torch.distributions.gamma.Gamma.mode">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">mode</span></span><a class="headerlink" href="#torch.distributions.gamma.Gamma.mode" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributions.gamma.Gamma.rsample">
<span class="sig-name descname"><span class="pre">rsample</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">sample_shape</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">torch.Size([])</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/gamma.py#L72"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.gamma.Gamma.rsample" title="Permalink to this definition">#</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a></p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="torch.distributions.gamma.Gamma.support">
<span class="sig-name descname"><span class="pre">support</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">GreaterThanEq(lower_bound=0.0)</span></em><a class="headerlink" href="#torch.distributions.gamma.Gamma.support" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="torch.distributions.gamma.Gamma.variance">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">variance</span></span><a class="headerlink" href="#torch.distributions.gamma.Gamma.variance" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="geometric">
<h2><span class="hidden-section">Geometric</span><a class="headerlink" href="#geometric" title="Permalink to this heading">#</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="torch.distributions.geometric.Geometric">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torch.distributions.geometric.</span></span><span class="sig-name descname"><span class="pre">Geometric</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">probs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">logits</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">validate_args</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/geometric.py#L19"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.geometric.Geometric" title="Permalink to this definition">#</a></dt>
<dd><p>Bases: <a class="reference internal" href="#torch.distributions.distribution.Distribution" title="torch.distributions.distribution.Distribution"><code class="xref py py-class docutils literal notranslate"><span class="pre">Distribution</span></code></a></p>
<p>Creates a Geometric distribution parameterized by <a class="reference internal" href="#torch.distributions.geometric.Geometric.probs" title="torch.distributions.geometric.Geometric.probs"><code class="xref py py-attr docutils literal notranslate"><span class="pre">probs</span></code></a>,
where <a class="reference internal" href="#torch.distributions.geometric.Geometric.probs" title="torch.distributions.geometric.Geometric.probs"><code class="xref py py-attr docutils literal notranslate"><span class="pre">probs</span></code></a> is the probability of success of Bernoulli trials.</p>
<div class="math">
<span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>P</mi><mo stretchy="false">(</mo><mi>X</mi><mo>=</mo><mi>k</mi><mo stretchy="false">)</mo><mo>=</mo><mo stretchy="false">(</mo><mn>1</mn><mo></mo><mi>p</mi><msup><mo stretchy="false">)</mo><mi>k</mi></msup><mi>p</mi><mo separator="true">,</mo><mi>k</mi><mo>=</mo><mn>0</mn><mo separator="true">,</mo><mn>1</mn><mo separator="true">,</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">P(X=k) = (1-p)^{k} p, k = 0, 1, ...</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.03148em;">k</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin"></span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1.1491em;vertical-align:-0.25em;"></span><span class="mord mathnormal">p</span><span class="mclose"><span class="mclose">)</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8991em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span></span></span></span></span></span><span class="mord mathnormal">p</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.03148em;">k</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.8389em;vertical-align:-0.1944em;"></span><span class="mord">0</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord">1</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord">...</span></span></span></span></span></div><div class="admonition note">
<p class="admonition-title">Note</p>
<p><a class="reference internal" href="#torch.distributions.geometric.Geometric" title="torch.distributions.geometric.Geometric"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.distributions.geometric.Geometric()</span></code></a> <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>k</mi><mo>+</mo><mn>1</mn><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(k+1)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.03148em;">k</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">1</span><span class="mclose">)</span></span></span></span></span>-th trial is the first success
hence draws samples in <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">{</mo><mn>0</mn><mo separator="true">,</mo><mn>1</mn><mo separator="true">,</mo><mo></mo><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">\{0, 1, \ldots\}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">{</span><span class="mord">0</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord">1</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="minner"></span><span class="mclose">}</span></span></span></span></span>, whereas
<a class="reference internal" href="generated/torch.Tensor.geometric_.html#torch.Tensor.geometric_" title="torch.Tensor.geometric_"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.Tensor.geometric_()</span></code></a> <cite>k</cite>-th trial is the first success hence draws samples in <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">{</mo><mn>1</mn><mo separator="true">,</mo><mn>2</mn><mo separator="true">,</mo><mo></mo><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">\{1, 2, \ldots\}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">{</span><span class="mord">1</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord">2</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="minner"></span><span class="mclose">}</span></span></span></span></span>.</p>
</div>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">Geometric</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.3</span><span class="p">]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>  <span class="c1"># underlying Bernoulli has 30% chance 1; 70% chance 0</span>
<span class="go">tensor([ 2.])</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>probs</strong> (<em>Number</em><em>, </em><a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>)  the probability of sampling <cite>1</cite>. Must be in range (0, 1]</p></li>
<li><p><strong>logits</strong> (<em>Number</em><em>, </em><a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>)  the log-odds of sampling <cite>1</cite>.</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="torch.distributions.geometric.Geometric.arg_constraints">
<span class="sig-name descname"><span class="pre">arg_constraints</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">{'logits':</span> <span class="pre">Real(),</span> <span class="pre">'probs':</span> <span class="pre">Interval(lower_bound=0.0,</span> <span class="pre">upper_bound=1.0)}</span></em><a class="headerlink" href="#torch.distributions.geometric.Geometric.arg_constraints" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributions.geometric.Geometric.entropy">
<span class="sig-name descname"><span class="pre">entropy</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/geometric.py#L126"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.geometric.Geometric.entropy" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributions.geometric.Geometric.expand">
<span class="sig-name descname"><span class="pre">expand</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch_shape</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">_instance</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/geometric.py#L75"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.geometric.Geometric.expand" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributions.geometric.Geometric.log_prob">
<span class="sig-name descname"><span class="pre">log_prob</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">value</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/geometric.py#L118"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.geometric.Geometric.log_prob" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="torch.distributions.geometric.Geometric.logits">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">logits</span></span><a class="headerlink" href="#torch.distributions.geometric.Geometric.logits" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="torch.distributions.geometric.Geometric.mean">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">mean</span></span><a class="headerlink" href="#torch.distributions.geometric.Geometric.mean" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="torch.distributions.geometric.Geometric.mode">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">mode</span></span><a class="headerlink" href="#torch.distributions.geometric.Geometric.mode" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="torch.distributions.geometric.Geometric.probs">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">probs</span></span><a class="headerlink" href="#torch.distributions.geometric.Geometric.probs" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributions.geometric.Geometric.sample">
<span class="sig-name descname"><span class="pre">sample</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">sample_shape</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">torch.Size([])</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/geometric.py#L106"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.geometric.Geometric.sample" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="torch.distributions.geometric.Geometric.support">
<span class="sig-name descname"><span class="pre">support</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">IntegerGreaterThan(lower_bound=0)</span></em><a class="headerlink" href="#torch.distributions.geometric.Geometric.support" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="torch.distributions.geometric.Geometric.variance">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">variance</span></span><a class="headerlink" href="#torch.distributions.geometric.Geometric.variance" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="gumbel">
<h2><span class="hidden-section">Gumbel</span><a class="headerlink" href="#gumbel" title="Permalink to this heading">#</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="torch.distributions.gumbel.Gumbel">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torch.distributions.gumbel.</span></span><span class="sig-name descname"><span class="pre">Gumbel</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">loc</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">scale</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">validate_args</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/gumbel.py#L16"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.gumbel.Gumbel" title="Permalink to this definition">#</a></dt>
<dd><p>Bases: <a class="reference internal" href="#torch.distributions.transformed_distribution.TransformedDistribution" title="torch.distributions.transformed_distribution.TransformedDistribution"><code class="xref py py-class docutils literal notranslate"><span class="pre">TransformedDistribution</span></code></a></p>
<p>Samples from a Gumbel Distribution.</p>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">Gumbel</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.0</span><span class="p">]),</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">2.0</span><span class="p">]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>  <span class="c1"># sample from Gumbel distribution with loc=1, scale=2</span>
<span class="go">tensor([ 1.0124])</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>loc</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a><em> or </em><a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>)  Location parameter of the distribution</p></li>
<li><p><strong>scale</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a><em> or </em><a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>)  Scale parameter of the distribution</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="torch.distributions.gumbel.Gumbel.arg_constraints">
<span class="sig-name descname"><span class="pre">arg_constraints</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference internal" href="#torch.distributions.constraints.Constraint" title="torch.distributions.constraints.Constraint"><span class="pre">constraints.Constraint</span></a><span class="p"><span class="pre">]</span></span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">{'loc':</span> <span class="pre">Real(),</span> <span class="pre">'scale':</span> <span class="pre">GreaterThan(lower_bound=0.0)}</span></em><a class="headerlink" href="#torch.distributions.gumbel.Gumbel.arg_constraints" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributions.gumbel.Gumbel.entropy">
<span class="sig-name descname"><span class="pre">entropy</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/gumbel.py#L82"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.gumbel.Gumbel.entropy" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributions.gumbel.Gumbel.expand">
<span class="sig-name descname"><span class="pre">expand</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch_shape</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">_instance</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/gumbel.py#L53"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.gumbel.Gumbel.expand" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributions.gumbel.Gumbel.log_prob">
<span class="sig-name descname"><span class="pre">log_prob</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">value</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/gumbel.py#L60"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.gumbel.Gumbel.log_prob" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="torch.distributions.gumbel.Gumbel.mean">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">mean</span></span><a class="headerlink" href="#torch.distributions.gumbel.Gumbel.mean" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="torch.distributions.gumbel.Gumbel.mode">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">mode</span></span><a class="headerlink" href="#torch.distributions.gumbel.Gumbel.mode" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="torch.distributions.gumbel.Gumbel.stddev">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">stddev</span></span><a class="headerlink" href="#torch.distributions.gumbel.Gumbel.stddev" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="torch.distributions.gumbel.Gumbel.support">
<span class="sig-name descname"><span class="pre">support</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">Real()</span></em><a class="headerlink" href="#torch.distributions.gumbel.Gumbel.support" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="torch.distributions.gumbel.Gumbel.variance">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">variance</span></span><a class="headerlink" href="#torch.distributions.gumbel.Gumbel.variance" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="halfcauchy">
<h2><span class="hidden-section">HalfCauchy</span><a class="headerlink" href="#halfcauchy" title="Permalink to this heading">#</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="torch.distributions.half_cauchy.HalfCauchy">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torch.distributions.half_cauchy.</span></span><span class="sig-name descname"><span class="pre">HalfCauchy</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">scale</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">validate_args</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/half_cauchy.py#L15"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.half_cauchy.HalfCauchy" title="Permalink to this definition">#</a></dt>
<dd><p>Bases: <a class="reference internal" href="#torch.distributions.transformed_distribution.TransformedDistribution" title="torch.distributions.transformed_distribution.TransformedDistribution"><code class="xref py py-class docutils literal notranslate"><span class="pre">TransformedDistribution</span></code></a></p>
<p>Creates a half-Cauchy distribution parameterized by <cite>scale</cite> where:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">X</span> <span class="o">~</span> <span class="n">Cauchy</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">scale</span><span class="p">)</span>
<span class="n">Y</span> <span class="o">=</span> <span class="o">|</span><span class="n">X</span><span class="o">|</span> <span class="o">~</span> <span class="n">HalfCauchy</span><span class="p">(</span><span class="n">scale</span><span class="p">)</span>
</pre></div>
</div>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">HalfCauchy</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.0</span><span class="p">]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>  <span class="c1"># half-cauchy distributed with scale=1</span>
<span class="go">tensor([ 2.3214])</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>scale</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a><em> or </em><a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>)  scale of the full Cauchy distribution</p>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="torch.distributions.half_cauchy.HalfCauchy.arg_constraints">
<span class="sig-name descname"><span class="pre">arg_constraints</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference internal" href="#torch.distributions.constraints.Constraint" title="torch.distributions.constraints.Constraint"><span class="pre">constraints.Constraint</span></a><span class="p"><span class="pre">]</span></span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">{'scale':</span> <span class="pre">GreaterThan(lower_bound=0.0)}</span></em><a class="headerlink" href="#torch.distributions.half_cauchy.HalfCauchy.arg_constraints" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributions.half_cauchy.HalfCauchy.cdf">
<span class="sig-name descname"><span class="pre">cdf</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">value</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/half_cauchy.py#L75"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.half_cauchy.HalfCauchy.cdf" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributions.half_cauchy.HalfCauchy.entropy">
<span class="sig-name descname"><span class="pre">entropy</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/half_cauchy.py#L83"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.half_cauchy.HalfCauchy.entropy" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributions.half_cauchy.HalfCauchy.expand">
<span class="sig-name descname"><span class="pre">expand</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch_shape</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">_instance</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/half_cauchy.py#L40"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.half_cauchy.HalfCauchy.expand" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="torch.distributions.half_cauchy.HalfCauchy.has_rsample">
<span class="sig-name descname"><span class="pre">has_rsample</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">True</span></em><a class="headerlink" href="#torch.distributions.half_cauchy.HalfCauchy.has_rsample" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributions.half_cauchy.HalfCauchy.icdf">
<span class="sig-name descname"><span class="pre">icdf</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">prob</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/half_cauchy.py#L80"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.half_cauchy.HalfCauchy.icdf" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributions.half_cauchy.HalfCauchy.log_prob">
<span class="sig-name descname"><span class="pre">log_prob</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">value</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/half_cauchy.py#L65"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.half_cauchy.HalfCauchy.log_prob" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="torch.distributions.half_cauchy.HalfCauchy.mean">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">mean</span></span><a class="headerlink" href="#torch.distributions.half_cauchy.HalfCauchy.mean" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="torch.distributions.half_cauchy.HalfCauchy.mode">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">mode</span></span><a class="headerlink" href="#torch.distributions.half_cauchy.HalfCauchy.mode" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="torch.distributions.half_cauchy.HalfCauchy.scale">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">scale</span></span><a class="headerlink" href="#torch.distributions.half_cauchy.HalfCauchy.scale" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="torch.distributions.half_cauchy.HalfCauchy.support">
<span class="sig-name descname"><span class="pre">support</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">GreaterThanEq(lower_bound=0.0)</span></em><a class="headerlink" href="#torch.distributions.half_cauchy.HalfCauchy.support" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="torch.distributions.half_cauchy.HalfCauchy.variance">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">variance</span></span><a class="headerlink" href="#torch.distributions.half_cauchy.HalfCauchy.variance" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="halfnormal">
<h2><span class="hidden-section">HalfNormal</span><a class="headerlink" href="#halfnormal" title="Permalink to this heading">#</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="torch.distributions.half_normal.HalfNormal">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torch.distributions.half_normal.</span></span><span class="sig-name descname"><span class="pre">HalfNormal</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">scale</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">validate_args</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/half_normal.py#L15"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.half_normal.HalfNormal" title="Permalink to this definition">#</a></dt>
<dd><p>Bases: <a class="reference internal" href="#torch.distributions.transformed_distribution.TransformedDistribution" title="torch.distributions.transformed_distribution.TransformedDistribution"><code class="xref py py-class docutils literal notranslate"><span class="pre">TransformedDistribution</span></code></a></p>
<p>Creates a half-normal distribution parameterized by <cite>scale</cite> where:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">X</span> <span class="o">~</span> <span class="n">Normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">scale</span><span class="p">)</span>
<span class="n">Y</span> <span class="o">=</span> <span class="o">|</span><span class="n">X</span><span class="o">|</span> <span class="o">~</span> <span class="n">HalfNormal</span><span class="p">(</span><span class="n">scale</span><span class="p">)</span>
</pre></div>
</div>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">HalfNormal</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.0</span><span class="p">]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>  <span class="c1"># half-normal distributed with scale=1</span>
<span class="go">tensor([ 0.1046])</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>scale</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a><em> or </em><a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>)  scale of the full Normal distribution</p>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="torch.distributions.half_normal.HalfNormal.arg_constraints">
<span class="sig-name descname"><span class="pre">arg_constraints</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference internal" href="#torch.distributions.constraints.Constraint" title="torch.distributions.constraints.Constraint"><span class="pre">constraints.Constraint</span></a><span class="p"><span class="pre">]</span></span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">{'scale':</span> <span class="pre">GreaterThan(lower_bound=0.0)}</span></em><a class="headerlink" href="#torch.distributions.half_normal.HalfNormal.arg_constraints" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributions.half_normal.HalfNormal.cdf">
<span class="sig-name descname"><span class="pre">cdf</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">value</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/half_normal.py#L67"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.half_normal.HalfNormal.cdf" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributions.half_normal.HalfNormal.entropy">
<span class="sig-name descname"><span class="pre">entropy</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/half_normal.py#L75"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.half_normal.HalfNormal.entropy" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributions.half_normal.HalfNormal.expand">
<span class="sig-name descname"><span class="pre">expand</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch_shape</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">_instance</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/half_normal.py#L40"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.half_normal.HalfNormal.expand" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="torch.distributions.half_normal.HalfNormal.has_rsample">
<span class="sig-name descname"><span class="pre">has_rsample</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">True</span></em><a class="headerlink" href="#torch.distributions.half_normal.HalfNormal.has_rsample" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributions.half_normal.HalfNormal.icdf">
<span class="sig-name descname"><span class="pre">icdf</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">prob</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/half_normal.py#L72"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.half_normal.HalfNormal.icdf" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributions.half_normal.HalfNormal.log_prob">
<span class="sig-name descname"><span class="pre">log_prob</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">value</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/half_normal.py#L60"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.half_normal.HalfNormal.log_prob" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="torch.distributions.half_normal.HalfNormal.mean">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">mean</span></span><a class="headerlink" href="#torch.distributions.half_normal.HalfNormal.mean" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="torch.distributions.half_normal.HalfNormal.mode">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">mode</span></span><a class="headerlink" href="#torch.distributions.half_normal.HalfNormal.mode" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="torch.distributions.half_normal.HalfNormal.scale">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">scale</span></span><a class="headerlink" href="#torch.distributions.half_normal.HalfNormal.scale" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="torch.distributions.half_normal.HalfNormal.support">
<span class="sig-name descname"><span class="pre">support</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">GreaterThanEq(lower_bound=0.0)</span></em><a class="headerlink" href="#torch.distributions.half_normal.HalfNormal.support" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="torch.distributions.half_normal.HalfNormal.variance">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">variance</span></span><a class="headerlink" href="#torch.distributions.half_normal.HalfNormal.variance" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="independent">
<h2><span class="hidden-section">Independent</span><a class="headerlink" href="#independent" title="Permalink to this heading">#</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="torch.distributions.independent.Independent">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torch.distributions.independent.</span></span><span class="sig-name descname"><span class="pre">Independent</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">base_distribution</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reinterpreted_batch_ndims</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">validate_args</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/independent.py#L14"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.independent.Independent" title="Permalink to this definition">#</a></dt>
<dd><p>Bases: <a class="reference internal" href="#torch.distributions.distribution.Distribution" title="torch.distributions.distribution.Distribution"><code class="xref py py-class docutils literal notranslate"><span class="pre">Distribution</span></code></a></p>
<p>Reinterprets some of the batch dims of a distribution as event dims.</p>
<p>This is mainly useful for changing the shape of the result of
<a class="reference internal" href="#torch.distributions.independent.Independent.log_prob" title="torch.distributions.independent.Independent.log_prob"><code class="xref py py-meth docutils literal notranslate"><span class="pre">log_prob()</span></code></a>. For example to create a diagonal Normal distribution with
the same shape as a Multivariate Normal distribution (so they are
interchangeable), you can:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span><span class="w"> </span><span class="nn">torch.distributions.multivariate_normal</span><span class="w"> </span><span class="kn">import</span> <span class="n">MultivariateNormal</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span><span class="w"> </span><span class="nn">torch.distributions.normal</span><span class="w"> </span><span class="kn">import</span> <span class="n">Normal</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loc</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">scale</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">mvn</span> <span class="o">=</span> <span class="n">MultivariateNormal</span><span class="p">(</span><span class="n">loc</span><span class="p">,</span> <span class="n">scale_tril</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">scale</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="n">mvn</span><span class="o">.</span><span class="n">batch_shape</span><span class="p">,</span> <span class="n">mvn</span><span class="o">.</span><span class="n">event_shape</span><span class="p">]</span>
<span class="go">[torch.Size([]), torch.Size([3])]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">normal</span> <span class="o">=</span> <span class="n">Normal</span><span class="p">(</span><span class="n">loc</span><span class="p">,</span> <span class="n">scale</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="n">normal</span><span class="o">.</span><span class="n">batch_shape</span><span class="p">,</span> <span class="n">normal</span><span class="o">.</span><span class="n">event_shape</span><span class="p">]</span>
<span class="go">[torch.Size([3]), torch.Size([])]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">diagn</span> <span class="o">=</span> <span class="n">Independent</span><span class="p">(</span><span class="n">normal</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="n">diagn</span><span class="o">.</span><span class="n">batch_shape</span><span class="p">,</span> <span class="n">diagn</span><span class="o">.</span><span class="n">event_shape</span><span class="p">]</span>
<span class="go">[torch.Size([]), torch.Size([3])]</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>base_distribution</strong> (<a class="reference internal" href="#torch.distributions.distribution.Distribution" title="torch.distributions.distribution.Distribution"><em>torch.distributions.distribution.Distribution</em></a>)  a
base distribution</p></li>
<li><p><strong>reinterpreted_batch_ndims</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>)  the number of batch dims to
reinterpret as event dims</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="torch.distributions.independent.Independent.arg_constraints">
<span class="sig-name descname"><span class="pre">arg_constraints</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Dict" title="(in Python v3.13)"><span class="pre">Dict</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference internal" href="#torch.distributions.constraints.Constraint" title="torch.distributions.constraints.Constraint"><span class="pre">Constraint</span></a><span class="p"><span class="pre">]</span></span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">{}</span></em><a class="headerlink" href="#torch.distributions.independent.Independent.arg_constraints" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributions.independent.Independent.entropy">
<span class="sig-name descname"><span class="pre">entropy</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/independent.py#L113"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.independent.Independent.entropy" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributions.independent.Independent.enumerate_support">
<span class="sig-name descname"><span class="pre">enumerate_support</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">expand</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/independent.py#L117"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.independent.Independent.enumerate_support" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributions.independent.Independent.expand">
<span class="sig-name descname"><span class="pre">expand</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch_shape</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">_instance</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/independent.py#L61"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.independent.Independent.expand" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="torch.distributions.independent.Independent.has_enumerate_support">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">has_enumerate_support</span></span><a class="headerlink" href="#torch.distributions.independent.Independent.has_enumerate_support" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="torch.distributions.independent.Independent.has_rsample">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">has_rsample</span></span><a class="headerlink" href="#torch.distributions.independent.Independent.has_rsample" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributions.independent.Independent.log_prob">
<span class="sig-name descname"><span class="pre">log_prob</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">value</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/independent.py#L109"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.independent.Independent.log_prob" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="torch.distributions.independent.Independent.mean">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">mean</span></span><a class="headerlink" href="#torch.distributions.independent.Independent.mean" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="torch.distributions.independent.Independent.mode">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">mode</span></span><a class="headerlink" href="#torch.distributions.independent.Independent.mode" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributions.independent.Independent.rsample">
<span class="sig-name descname"><span class="pre">rsample</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">sample_shape</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">torch.Size([])</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/independent.py#L106"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.independent.Independent.rsample" title="Permalink to this definition">#</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributions.independent.Independent.sample">
<span class="sig-name descname"><span class="pre">sample</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">sample_shape</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">torch.Size([])</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/independent.py#L103"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.independent.Independent.sample" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="torch.distributions.independent.Independent.support">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">support</span></span><a class="headerlink" href="#torch.distributions.independent.Independent.support" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="torch.distributions.independent.Independent.variance">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">variance</span></span><a class="headerlink" href="#torch.distributions.independent.Independent.variance" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="inversegamma">
<h2><span class="hidden-section">InverseGamma</span><a class="headerlink" href="#inversegamma" title="Permalink to this heading">#</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="torch.distributions.inverse_gamma.InverseGamma">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torch.distributions.inverse_gamma.</span></span><span class="sig-name descname"><span class="pre">InverseGamma</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">concentration</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rate</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">validate_args</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/inverse_gamma.py#L12"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.inverse_gamma.InverseGamma" title="Permalink to this definition">#</a></dt>
<dd><p>Bases: <a class="reference internal" href="#torch.distributions.transformed_distribution.TransformedDistribution" title="torch.distributions.transformed_distribution.TransformedDistribution"><code class="xref py py-class docutils literal notranslate"><span class="pre">TransformedDistribution</span></code></a></p>
<p>Creates an inverse gamma distribution parameterized by <a class="reference internal" href="#torch.distributions.inverse_gamma.InverseGamma.concentration" title="torch.distributions.inverse_gamma.InverseGamma.concentration"><code class="xref py py-attr docutils literal notranslate"><span class="pre">concentration</span></code></a> and <a class="reference internal" href="#torch.distributions.inverse_gamma.InverseGamma.rate" title="torch.distributions.inverse_gamma.InverseGamma.rate"><code class="xref py py-attr docutils literal notranslate"><span class="pre">rate</span></code></a>
where:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">X</span> <span class="o">~</span> <span class="n">Gamma</span><span class="p">(</span><span class="n">concentration</span><span class="p">,</span> <span class="n">rate</span><span class="p">)</span>
<span class="n">Y</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">X</span> <span class="o">~</span> <span class="n">InverseGamma</span><span class="p">(</span><span class="n">concentration</span><span class="p">,</span> <span class="n">rate</span><span class="p">)</span>
</pre></div>
</div>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">InverseGamma</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">2.0</span><span class="p">]),</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">3.0</span><span class="p">]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>
<span class="go">tensor([ 1.2953])</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>concentration</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a><em> or </em><a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>)  shape parameter of the distribution
(often referred to as alpha)</p></li>
<li><p><strong>rate</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a><em> or </em><a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>)  rate = 1 / scale of the distribution
(often referred to as beta)</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="torch.distributions.inverse_gamma.InverseGamma.arg_constraints">
<span class="sig-name descname"><span class="pre">arg_constraints</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference internal" href="#torch.distributions.constraints.Constraint" title="torch.distributions.constraints.Constraint"><span class="pre">constraints.Constraint</span></a><span class="p"><span class="pre">]</span></span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">{'concentration':</span> <span class="pre">GreaterThan(lower_bound=0.0),</span> <span class="pre">'rate':</span> <span class="pre">GreaterThan(lower_bound=0.0)}</span></em><a class="headerlink" href="#torch.distributions.inverse_gamma.InverseGamma.arg_constraints" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="torch.distributions.inverse_gamma.InverseGamma.concentration">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">concentration</span></span><a class="headerlink" href="#torch.distributions.inverse_gamma.InverseGamma.concentration" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributions.inverse_gamma.InverseGamma.entropy">
<span class="sig-name descname"><span class="pre">entropy</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/inverse_gamma.py#L75"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.inverse_gamma.InverseGamma.entropy" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributions.inverse_gamma.InverseGamma.expand">
<span class="sig-name descname"><span class="pre">expand</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch_shape</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">_instance</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/inverse_gamma.py#L47"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.inverse_gamma.InverseGamma.expand" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="torch.distributions.inverse_gamma.InverseGamma.has_rsample">
<span class="sig-name descname"><span class="pre">has_rsample</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">True</span></em><a class="headerlink" href="#torch.distributions.inverse_gamma.InverseGamma.has_rsample" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="torch.distributions.inverse_gamma.InverseGamma.mean">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">mean</span></span><a class="headerlink" href="#torch.distributions.inverse_gamma.InverseGamma.mean" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="torch.distributions.inverse_gamma.InverseGamma.mode">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">mode</span></span><a class="headerlink" href="#torch.distributions.inverse_gamma.InverseGamma.mode" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="torch.distributions.inverse_gamma.InverseGamma.rate">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">rate</span></span><a class="headerlink" href="#torch.distributions.inverse_gamma.InverseGamma.rate" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="torch.distributions.inverse_gamma.InverseGamma.support">
<span class="sig-name descname"><span class="pre">support</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">GreaterThan(lower_bound=0.0)</span></em><a class="headerlink" href="#torch.distributions.inverse_gamma.InverseGamma.support" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="torch.distributions.inverse_gamma.InverseGamma.variance">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">variance</span></span><a class="headerlink" href="#torch.distributions.inverse_gamma.InverseGamma.variance" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="kumaraswamy">
<h2><span class="hidden-section">Kumaraswamy</span><a class="headerlink" href="#kumaraswamy" title="Permalink to this heading">#</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="torch.distributions.kumaraswamy.Kumaraswamy">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torch.distributions.kumaraswamy.</span></span><span class="sig-name descname"><span class="pre">Kumaraswamy</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">concentration1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">concentration0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">validate_args</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/kumaraswamy.py#L23"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.kumaraswamy.Kumaraswamy" title="Permalink to this definition">#</a></dt>
<dd><p>Bases: <a class="reference internal" href="#torch.distributions.transformed_distribution.TransformedDistribution" title="torch.distributions.transformed_distribution.TransformedDistribution"><code class="xref py py-class docutils literal notranslate"><span class="pre">TransformedDistribution</span></code></a></p>
<p>Samples from a Kumaraswamy distribution.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">Kumaraswamy</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.0</span><span class="p">]),</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.0</span><span class="p">]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>  <span class="c1"># sample from a Kumaraswamy distribution with concentration alpha=1 and beta=1</span>
<span class="go">tensor([ 0.1729])</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>concentration1</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a><em> or </em><a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>)  1st concentration parameter of the distribution
(often referred to as alpha)</p></li>
<li><p><strong>concentration0</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a><em> or </em><a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>)  2nd concentration parameter of the distribution
(often referred to as beta)</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="torch.distributions.kumaraswamy.Kumaraswamy.arg_constraints">
<span class="sig-name descname"><span class="pre">arg_constraints</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference internal" href="#torch.distributions.constraints.Constraint" title="torch.distributions.constraints.Constraint"><span class="pre">constraints.Constraint</span></a><span class="p"><span class="pre">]</span></span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">{'concentration0':</span> <span class="pre">GreaterThan(lower_bound=0.0),</span> <span class="pre">'concentration1':</span> <span class="pre">GreaterThan(lower_bound=0.0)}</span></em><a class="headerlink" href="#torch.distributions.kumaraswamy.Kumaraswamy.arg_constraints" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributions.kumaraswamy.Kumaraswamy.entropy">
<span class="sig-name descname"><span class="pre">entropy</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/kumaraswamy.py#L89"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.kumaraswamy.Kumaraswamy.entropy" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributions.kumaraswamy.Kumaraswamy.expand">
<span class="sig-name descname"><span class="pre">expand</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch_shape</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">_instance</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/kumaraswamy.py#L63"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.kumaraswamy.Kumaraswamy.expand" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="torch.distributions.kumaraswamy.Kumaraswamy.has_rsample">
<span class="sig-name descname"><span class="pre">has_rsample</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">True</span></em><a class="headerlink" href="#torch.distributions.kumaraswamy.Kumaraswamy.has_rsample" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="torch.distributions.kumaraswamy.Kumaraswamy.mean">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">mean</span></span><a class="headerlink" href="#torch.distributions.kumaraswamy.Kumaraswamy.mean" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="torch.distributions.kumaraswamy.Kumaraswamy.mode">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">mode</span></span><a class="headerlink" href="#torch.distributions.kumaraswamy.Kumaraswamy.mode" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="torch.distributions.kumaraswamy.Kumaraswamy.support">
<span class="sig-name descname"><span class="pre">support</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">Interval(lower_bound=0.0,</span> <span class="pre">upper_bound=1.0)</span></em><a class="headerlink" href="#torch.distributions.kumaraswamy.Kumaraswamy.support" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="torch.distributions.kumaraswamy.Kumaraswamy.variance">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">variance</span></span><a class="headerlink" href="#torch.distributions.kumaraswamy.Kumaraswamy.variance" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="lkjcholesky">
<h2><span class="hidden-section">LKJCholesky</span><a class="headerlink" href="#lkjcholesky" title="Permalink to this heading">#</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="torch.distributions.lkj_cholesky.LKJCholesky">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torch.distributions.lkj_cholesky.</span></span><span class="sig-name descname"><span class="pre">LKJCholesky</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dim</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">concentration</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">validate_args</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/lkj_cholesky.py#L22"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.lkj_cholesky.LKJCholesky" title="Permalink to this definition">#</a></dt>
<dd><p>Bases: <a class="reference internal" href="#torch.distributions.distribution.Distribution" title="torch.distributions.distribution.Distribution"><code class="xref py py-class docutils literal notranslate"><span class="pre">Distribution</span></code></a></p>
<p>LKJ distribution for lower Cholesky factor of correlation matrices.
The distribution is controlled by <code class="docutils literal notranslate"><span class="pre">concentration</span></code> parameter <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi></mi></mrow><annotation encoding="application/x-tex">\eta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;"></span></span></span></span></span>
to make the probability of the correlation matrix <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>M</mi></mrow><annotation encoding="application/x-tex">M</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">M</span></span></span></span></span> generated from
a Cholesky factor proportional to <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>det</mi><mo></mo><mo stretchy="false">(</mo><mi>M</mi><msup><mo stretchy="false">)</mo><mrow><mi></mi><mo></mo><mn>1</mn></mrow></msup></mrow><annotation encoding="application/x-tex">\det(M)^{\eta - 1}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0641em;vertical-align:-0.25em;"></span><span class="mop">det</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.10903em;">M</span><span class="mclose"><span class="mclose">)</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;"></span><span class="mbin mtight"></span><span class="mord mtight">1</span></span></span></span></span></span></span></span></span></span></span></span></span>. Because of that,
when <code class="docutils literal notranslate"><span class="pre">concentration</span> <span class="pre">==</span> <span class="pre">1</span></code>, we have a uniform distribution over Cholesky
factors of correlation matrices:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">L</span> <span class="o">~</span> <span class="n">LKJCholesky</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">concentration</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">L</span> <span class="o">@</span> <span class="n">L</span><span class="s1">&#39; ~ LKJCorr(dim, concentration)</span>
</pre></div>
</div>
<p>Note that this distribution samples the
Cholesky factor of correlation matrices and not the correlation matrices
themselves and thereby differs slightly from the derivations in [1] for
the <cite>LKJCorr</cite> distribution. For sampling, this uses the Onion method from
[1] Section 3.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">l</span> <span class="o">=</span> <span class="n">LKJCholesky</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">l</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>  <span class="c1"># l @ l.T is a sample of a correlation 3x3 matrix</span>
<span class="go">tensor([[ 1.0000,  0.0000,  0.0000],</span>
<span class="go">        [ 0.3516,  0.9361,  0.0000],</span>
<span class="go">        [-0.1899,  0.4748,  0.8593]])</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>dimension</strong> (<em>dim</em>)  dimension of the matrices</p></li>
<li><p><strong>concentration</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a><em> or </em><a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>)  concentration/shape parameter of the
distribution (often referred to as eta)</p></li>
</ul>
</dd>
</dl>
<p><strong>References</strong></p>
<p>[1] <cite>Generating random correlation matrices based on vines and extended onion method</cite> (2009),
Daniel Lewandowski, Dorota Kurowicka, Harry Joe.
Journal of Multivariate Analysis. 100. 10.1016/j.jmva.2009.04.008</p>
<dl class="py attribute">
<dt class="sig sig-object py" id="torch.distributions.lkj_cholesky.LKJCholesky.arg_constraints">
<span class="sig-name descname"><span class="pre">arg_constraints</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">{'concentration':</span> <span class="pre">GreaterThan(lower_bound=0.0)}</span></em><a class="headerlink" href="#torch.distributions.lkj_cholesky.LKJCholesky.arg_constraints" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributions.lkj_cholesky.LKJCholesky.expand">
<span class="sig-name descname"><span class="pre">expand</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch_shape</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">_instance</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/lkj_cholesky.py#L85"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.lkj_cholesky.LKJCholesky.expand" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributions.lkj_cholesky.LKJCholesky.log_prob">
<span class="sig-name descname"><span class="pre">log_prob</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">value</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/lkj_cholesky.py#L118"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.lkj_cholesky.LKJCholesky.log_prob" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributions.lkj_cholesky.LKJCholesky.sample">
<span class="sig-name descname"><span class="pre">sample</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">sample_shape</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">torch.Size([])</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/lkj_cholesky.py#L97"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.lkj_cholesky.LKJCholesky.sample" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="torch.distributions.lkj_cholesky.LKJCholesky.support">
<span class="sig-name descname"><span class="pre">support</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">CorrCholesky()</span></em><a class="headerlink" href="#torch.distributions.lkj_cholesky.LKJCholesky.support" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="laplace">
<h2><span class="hidden-section">Laplace</span><a class="headerlink" href="#laplace" title="Permalink to this heading">#</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="torch.distributions.laplace.Laplace">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torch.distributions.laplace.</span></span><span class="sig-name descname"><span class="pre">Laplace</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">loc</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">scale</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">validate_args</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/laplace.py#L14"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.laplace.Laplace" title="Permalink to this definition">#</a></dt>
<dd><p>Bases: <a class="reference internal" href="#torch.distributions.distribution.Distribution" title="torch.distributions.distribution.Distribution"><code class="xref py py-class docutils literal notranslate"><span class="pre">Distribution</span></code></a></p>
<p>Creates a Laplace distribution parameterized by <code class="xref py py-attr docutils literal notranslate"><span class="pre">loc</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">scale</span></code>.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">Laplace</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.0</span><span class="p">]),</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.0</span><span class="p">]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>  <span class="c1"># Laplace distributed with loc=0, scale=1</span>
<span class="go">tensor([ 0.1046])</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>loc</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a><em> or </em><a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>)  mean of the distribution</p></li>
<li><p><strong>scale</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a><em> or </em><a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>)  scale of the distribution</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="torch.distributions.laplace.Laplace.arg_constraints">
<span class="sig-name descname"><span class="pre">arg_constraints</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">{'loc':</span> <span class="pre">Real(),</span> <span class="pre">'scale':</span> <span class="pre">GreaterThan(lower_bound=0.0)}</span></em><a class="headerlink" href="#torch.distributions.laplace.Laplace.arg_constraints" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributions.laplace.Laplace.cdf">
<span class="sig-name descname"><span class="pre">cdf</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">value</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/laplace.py#L85"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.laplace.Laplace.cdf" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributions.laplace.Laplace.entropy">
<span class="sig-name descname"><span class="pre">entropy</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/laplace.py#L96"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.laplace.Laplace.entropy" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributions.laplace.Laplace.expand">
<span class="sig-name descname"><span class="pre">expand</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch_shape</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">_instance</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/laplace.py#L57"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.laplace.Laplace.expand" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="torch.distributions.laplace.Laplace.has_rsample">
<span class="sig-name descname"><span class="pre">has_rsample</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">True</span></em><a class="headerlink" href="#torch.distributions.laplace.Laplace.has_rsample" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributions.laplace.Laplace.icdf">
<span class="sig-name descname"><span class="pre">icdf</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">value</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/laplace.py#L92"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.laplace.Laplace.icdf" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributions.laplace.Laplace.log_prob">
<span class="sig-name descname"><span class="pre">log_prob</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">value</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/laplace.py#L80"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.laplace.Laplace.log_prob" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="torch.distributions.laplace.Laplace.mean">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">mean</span></span><a class="headerlink" href="#torch.distributions.laplace.Laplace.mean" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="torch.distributions.laplace.Laplace.mode">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">mode</span></span><a class="headerlink" href="#torch.distributions.laplace.Laplace.mode" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributions.laplace.Laplace.rsample">
<span class="sig-name descname"><span class="pre">rsample</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">sample_shape</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">torch.Size([])</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/laplace.py#L66"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.laplace.Laplace.rsample" title="Permalink to this definition">#</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a></p>
</dd>
</dl>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="torch.distributions.laplace.Laplace.stddev">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">stddev</span></span><a class="headerlink" href="#torch.distributions.laplace.Laplace.stddev" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="torch.distributions.laplace.Laplace.support">
<span class="sig-name descname"><span class="pre">support</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">Real()</span></em><a class="headerlink" href="#torch.distributions.laplace.Laplace.support" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="torch.distributions.laplace.Laplace.variance">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">variance</span></span><a class="headerlink" href="#torch.distributions.laplace.Laplace.variance" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="lognormal">
<h2><span class="hidden-section">LogNormal</span><a class="headerlink" href="#lognormal" title="Permalink to this heading">#</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="torch.distributions.log_normal.LogNormal">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torch.distributions.log_normal.</span></span><span class="sig-name descname"><span class="pre">LogNormal</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">loc</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">scale</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">validate_args</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/log_normal.py#L11"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.log_normal.LogNormal" title="Permalink to this definition">#</a></dt>
<dd><p>Bases: <a class="reference internal" href="#torch.distributions.transformed_distribution.TransformedDistribution" title="torch.distributions.transformed_distribution.TransformedDistribution"><code class="xref py py-class docutils literal notranslate"><span class="pre">TransformedDistribution</span></code></a></p>
<p>Creates a log-normal distribution parameterized by
<a class="reference internal" href="#torch.distributions.log_normal.LogNormal.loc" title="torch.distributions.log_normal.LogNormal.loc"><code class="xref py py-attr docutils literal notranslate"><span class="pre">loc</span></code></a> and <a class="reference internal" href="#torch.distributions.log_normal.LogNormal.scale" title="torch.distributions.log_normal.LogNormal.scale"><code class="xref py py-attr docutils literal notranslate"><span class="pre">scale</span></code></a> where:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">X</span> <span class="o">~</span> <span class="n">Normal</span><span class="p">(</span><span class="n">loc</span><span class="p">,</span> <span class="n">scale</span><span class="p">)</span>
<span class="n">Y</span> <span class="o">=</span> <span class="n">exp</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="o">~</span> <span class="n">LogNormal</span><span class="p">(</span><span class="n">loc</span><span class="p">,</span> <span class="n">scale</span><span class="p">)</span>
</pre></div>
</div>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">LogNormal</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.0</span><span class="p">]),</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.0</span><span class="p">]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>  <span class="c1"># log-normal distributed with mean=0 and stddev=1</span>
<span class="go">tensor([ 0.1046])</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>loc</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a><em> or </em><a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>)  mean of log of distribution</p></li>
<li><p><strong>scale</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a><em> or </em><a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>)  standard deviation of log of the distribution</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="torch.distributions.log_normal.LogNormal.arg_constraints">
<span class="sig-name descname"><span class="pre">arg_constraints</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference internal" href="#torch.distributions.constraints.Constraint" title="torch.distributions.constraints.Constraint"><span class="pre">constraints.Constraint</span></a><span class="p"><span class="pre">]</span></span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">{'loc':</span> <span class="pre">Real(),</span> <span class="pre">'scale':</span> <span class="pre">GreaterThan(lower_bound=0.0)}</span></em><a class="headerlink" href="#torch.distributions.log_normal.LogNormal.arg_constraints" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributions.log_normal.LogNormal.entropy">
<span class="sig-name descname"><span class="pre">entropy</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/log_normal.py#L63"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.log_normal.LogNormal.entropy" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributions.log_normal.LogNormal.expand">
<span class="sig-name descname"><span class="pre">expand</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch_shape</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">_instance</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/log_normal.py#L38"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.log_normal.LogNormal.expand" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="torch.distributions.log_normal.LogNormal.has_rsample">
<span class="sig-name descname"><span class="pre">has_rsample</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">True</span></em><a class="headerlink" href="#torch.distributions.log_normal.LogNormal.has_rsample" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="torch.distributions.log_normal.LogNormal.loc">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">loc</span></span><a class="headerlink" href="#torch.distributions.log_normal.LogNormal.loc" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="torch.distributions.log_normal.LogNormal.mean">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">mean</span></span><a class="headerlink" href="#torch.distributions.log_normal.LogNormal.mean" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="torch.distributions.log_normal.LogNormal.mode">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">mode</span></span><a class="headerlink" href="#torch.distributions.log_normal.LogNormal.mode" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="torch.distributions.log_normal.LogNormal.scale">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">scale</span></span><a class="headerlink" href="#torch.distributions.log_normal.LogNormal.scale" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="torch.distributions.log_normal.LogNormal.support">
<span class="sig-name descname"><span class="pre">support</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">GreaterThan(lower_bound=0.0)</span></em><a class="headerlink" href="#torch.distributions.log_normal.LogNormal.support" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="torch.distributions.log_normal.LogNormal.variance">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">variance</span></span><a class="headerlink" href="#torch.distributions.log_normal.LogNormal.variance" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="lowrankmultivariatenormal">
<h2><span class="hidden-section">LowRankMultivariateNormal</span><a class="headerlink" href="#lowrankmultivariatenormal" title="Permalink to this heading">#</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torch.distributions.lowrank_multivariate_normal.</span></span><span class="sig-name descname"><span class="pre">LowRankMultivariateNormal</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">loc</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cov_factor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cov_diag</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">validate_args</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/lowrank_multivariate_normal.py#L53"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal" title="Permalink to this definition">#</a></dt>
<dd><p>Bases: <a class="reference internal" href="#torch.distributions.distribution.Distribution" title="torch.distributions.distribution.Distribution"><code class="xref py py-class docutils literal notranslate"><span class="pre">Distribution</span></code></a></p>
<p>Creates a multivariate normal distribution with covariance matrix having a low-rank form
parameterized by <code class="xref py py-attr docutils literal notranslate"><span class="pre">cov_factor</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">cov_diag</span></code>:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">covariance_matrix</span> <span class="o">=</span> <span class="n">cov_factor</span> <span class="o">@</span> <span class="n">cov_factor</span><span class="o">.</span><span class="n">T</span> <span class="o">+</span> <span class="n">cov_diag</span>
</pre></div>
</div>
<p class="rubric">Example</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">LowRankMultivariateNormal</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">2</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">1.</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.</span><span class="p">]]),</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>  <span class="c1"># normally distributed with mean=`[0,0]`, cov_factor=`[[1],[0]]`, cov_diag=`[1,1]`</span>
<span class="go">tensor([-0.2102, -0.5429])</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>loc</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>)  mean of the distribution with shape <cite>batch_shape + event_shape</cite></p></li>
<li><p><strong>cov_factor</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>)  factor part of low-rank form of covariance matrix with shape
<cite>batch_shape + event_shape + (rank,)</cite></p></li>
<li><p><strong>cov_diag</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>)  diagonal part of low-rank form of covariance matrix with shape
<cite>batch_shape + event_shape</cite></p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The computation for determinant and inverse of covariance matrix is avoided when
<cite>cov_factor.shape[1] &lt;&lt; cov_factor.shape[0]</cite> thanks to <a class="reference external" href="https://en.wikipedia.org/wiki/Woodbury_matrix_identity">Woodbury matrix identity</a> and
<a class="reference external" href="https://en.wikipedia.org/wiki/Matrix_determinant_lemma">matrix determinant lemma</a>.
Thanks to these formulas, we just need to compute the determinant and inverse of
the small size capacitance matrix:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">capacitance</span> <span class="o">=</span> <span class="n">I</span> <span class="o">+</span> <span class="n">cov_factor</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">inv</span><span class="p">(</span><span class="n">cov_diag</span><span class="p">)</span> <span class="o">@</span> <span class="n">cov_factor</span>
</pre></div>
</div>
</div>
<dl class="py attribute">
<dt class="sig sig-object py" id="torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.arg_constraints">
<span class="sig-name descname"><span class="pre">arg_constraints</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">{'cov_diag':</span> <span class="pre">IndependentConstraint(GreaterThan(lower_bound=0.0),</span> <span class="pre">1),</span> <span class="pre">'cov_factor':</span> <span class="pre">IndependentConstraint(Real(),</span> <span class="pre">2),</span> <span class="pre">'loc':</span> <span class="pre">IndependentConstraint(Real(),</span> <span class="pre">1)}</span></em><a class="headerlink" href="#torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.arg_constraints" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.covariance_matrix">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">covariance_matrix</span></span><a class="headerlink" href="#torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.covariance_matrix" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.entropy">
<span class="sig-name descname"><span class="pre">entropy</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/lowrank_multivariate_normal.py#L230"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.entropy" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.expand">
<span class="sig-name descname"><span class="pre">expand</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch_shape</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">_instance</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/lowrank_multivariate_normal.py#L129"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.expand" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.has_rsample">
<span class="sig-name descname"><span class="pre">has_rsample</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">True</span></em><a class="headerlink" href="#torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.has_rsample" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.log_prob">
<span class="sig-name descname"><span class="pre">log_prob</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">value</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/lowrank_multivariate_normal.py#L213"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.log_prob" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.mean">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">mean</span></span><a class="headerlink" href="#torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.mean" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.mode">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">mode</span></span><a class="headerlink" href="#torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.mode" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.precision_matrix">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">precision_matrix</span></span><a class="headerlink" href="#torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.precision_matrix" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.rsample">
<span class="sig-name descname"><span class="pre">rsample</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">sample_shape</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">torch.Size([])</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/lowrank_multivariate_normal.py#L202"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.rsample" title="Permalink to this definition">#</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a></p>
</dd>
</dl>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.scale_tril">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">scale_tril</span></span><a class="headerlink" href="#torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.scale_tril" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.support">
<span class="sig-name descname"><span class="pre">support</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">IndependentConstraint(Real(),</span> <span class="pre">1)</span></em><a class="headerlink" href="#torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.support" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.variance">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">variance</span></span><a class="headerlink" href="#torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.variance" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="mixturesamefamily">
<h2><span class="hidden-section">MixtureSameFamily</span><a class="headerlink" href="#mixturesamefamily" title="Permalink to this heading">#</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="torch.distributions.mixture_same_family.MixtureSameFamily">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torch.distributions.mixture_same_family.</span></span><span class="sig-name descname"><span class="pre">MixtureSameFamily</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">mixture_distribution</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">component_distribution</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">validate_args</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/mixture_same_family.py#L12"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.mixture_same_family.MixtureSameFamily" title="Permalink to this definition">#</a></dt>
<dd><p>Bases: <a class="reference internal" href="#torch.distributions.distribution.Distribution" title="torch.distributions.distribution.Distribution"><code class="xref py py-class docutils literal notranslate"><span class="pre">Distribution</span></code></a></p>
<p>The <cite>MixtureSameFamily</cite> distribution implements a (batch of) mixture
distribution where all component are from different parameterizations of
the same distribution type. It is parameterized by a <cite>Categorical</cite>
selecting distribution (over <cite>k</cite> component) and a component
distribution, i.e., a <cite>Distribution</cite> with a rightmost batch shape
(equal to <cite>[k]</cite>) which indexes each (batch of) component.</p>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># Construct Gaussian Mixture Model in 1D consisting of 5 equally</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># weighted normal distributions</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">mix</span> <span class="o">=</span> <span class="n">D</span><span class="o">.</span><span class="n">Categorical</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">5</span><span class="p">,))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">comp</span> <span class="o">=</span> <span class="n">D</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,),</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">5</span><span class="p">,))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">gmm</span> <span class="o">=</span> <span class="n">MixtureSameFamily</span><span class="p">(</span><span class="n">mix</span><span class="p">,</span> <span class="n">comp</span><span class="p">)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Construct Gaussian Mixture Model in 2D consisting of 5 equally</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># weighted bivariate normal distributions</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">mix</span> <span class="o">=</span> <span class="n">D</span><span class="o">.</span><span class="n">Categorical</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">5</span><span class="p">,))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">comp</span> <span class="o">=</span> <span class="n">D</span><span class="o">.</span><span class="n">Independent</span><span class="p">(</span><span class="n">D</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span>
<span class="gp">... </span>         <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span><span class="mi">2</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span><span class="mi">2</span><span class="p">)),</span> <span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">gmm</span> <span class="o">=</span> <span class="n">MixtureSameFamily</span><span class="p">(</span><span class="n">mix</span><span class="p">,</span> <span class="n">comp</span><span class="p">)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Construct a batch of 3 Gaussian Mixture Models in 2D each</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># consisting of 5 random weighted bivariate normal distributions</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">mix</span> <span class="o">=</span> <span class="n">D</span><span class="o">.</span><span class="n">Categorical</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">5</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">comp</span> <span class="o">=</span> <span class="n">D</span><span class="o">.</span><span class="n">Independent</span><span class="p">(</span><span class="n">D</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span>
<span class="gp">... </span>        <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">2</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">2</span><span class="p">)),</span> <span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">gmm</span> <span class="o">=</span> <span class="n">MixtureSameFamily</span><span class="p">(</span><span class="n">mix</span><span class="p">,</span> <span class="n">comp</span><span class="p">)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>mixture_distribution</strong>  <cite>torch.distributions.Categorical</cite>-like
instance. Manages the probability of selecting component.
The number of categories must match the rightmost batch
dimension of the <cite>component_distribution</cite>. Must have either
scalar <cite>batch_shape</cite> or <cite>batch_shape</cite> matching
<cite>component_distribution.batch_shape[:-1]</cite></p></li>
<li><p><strong>component_distribution</strong>  <cite>torch.distributions.Distribution</cite>-like
instance. Right-most batch dimension indexes component.</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="torch.distributions.mixture_same_family.MixtureSameFamily.arg_constraints">
<span class="sig-name descname"><span class="pre">arg_constraints</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Dict" title="(in Python v3.13)"><span class="pre">Dict</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference internal" href="#torch.distributions.constraints.Constraint" title="torch.distributions.constraints.Constraint"><span class="pre">Constraint</span></a><span class="p"><span class="pre">]</span></span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">{}</span></em><a class="headerlink" href="#torch.distributions.mixture_same_family.MixtureSameFamily.arg_constraints" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributions.mixture_same_family.MixtureSameFamily.cdf">
<span class="sig-name descname"><span class="pre">cdf</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/mixture_same_family.py#L154"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.mixture_same_family.MixtureSameFamily.cdf" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="torch.distributions.mixture_same_family.MixtureSameFamily.component_distribution">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">component_distribution</span></span><a class="headerlink" href="#torch.distributions.mixture_same_family.MixtureSameFamily.component_distribution" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributions.mixture_same_family.MixtureSameFamily.expand">
<span class="sig-name descname"><span class="pre">expand</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch_shape</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">_instance</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/mixture_same_family.py#L103"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.mixture_same_family.MixtureSameFamily.expand" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="torch.distributions.mixture_same_family.MixtureSameFamily.has_rsample">
<span class="sig-name descname"><span class="pre">has_rsample</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">False</span></em><a class="headerlink" href="#torch.distributions.mixture_same_family.MixtureSameFamily.has_rsample" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributions.mixture_same_family.MixtureSameFamily.log_prob">
<span class="sig-name descname"><span class="pre">log_prob</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/mixture_same_family.py#L161"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.mixture_same_family.MixtureSameFamily.log_prob" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="torch.distributions.mixture_same_family.MixtureSameFamily.mean">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">mean</span></span><a class="headerlink" href="#torch.distributions.mixture_same_family.MixtureSameFamily.mean" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="torch.distributions.mixture_same_family.MixtureSameFamily.mixture_distribution">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">mixture_distribution</span></span><a class="headerlink" href="#torch.distributions.mixture_same_family.MixtureSameFamily.mixture_distribution" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributions.mixture_same_family.MixtureSameFamily.sample">
<span class="sig-name descname"><span class="pre">sample</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">sample_shape</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">torch.Size([])</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/mixture_same_family.py#L171"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.mixture_same_family.MixtureSameFamily.sample" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="torch.distributions.mixture_same_family.MixtureSameFamily.support">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">support</span></span><a class="headerlink" href="#torch.distributions.mixture_same_family.MixtureSameFamily.support" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="torch.distributions.mixture_same_family.MixtureSameFamily.variance">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">variance</span></span><a class="headerlink" href="#torch.distributions.mixture_same_family.MixtureSameFamily.variance" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="multinomial">
<h2><span class="hidden-section">Multinomial</span><a class="headerlink" href="#multinomial" title="Permalink to this heading">#</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="torch.distributions.multinomial.Multinomial">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torch.distributions.multinomial.</span></span><span class="sig-name descname"><span class="pre">Multinomial</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">total_count</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">probs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">logits</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">validate_args</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/multinomial.py#L13"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.multinomial.Multinomial" title="Permalink to this definition">#</a></dt>
<dd><p>Bases: <a class="reference internal" href="#torch.distributions.distribution.Distribution" title="torch.distributions.distribution.Distribution"><code class="xref py py-class docutils literal notranslate"><span class="pre">Distribution</span></code></a></p>
<p>Creates a Multinomial distribution parameterized by <a class="reference internal" href="#torch.distributions.multinomial.Multinomial.total_count" title="torch.distributions.multinomial.Multinomial.total_count"><code class="xref py py-attr docutils literal notranslate"><span class="pre">total_count</span></code></a> and
either <a class="reference internal" href="#torch.distributions.multinomial.Multinomial.probs" title="torch.distributions.multinomial.Multinomial.probs"><code class="xref py py-attr docutils literal notranslate"><span class="pre">probs</span></code></a> or <a class="reference internal" href="#torch.distributions.multinomial.Multinomial.logits" title="torch.distributions.multinomial.Multinomial.logits"><code class="xref py py-attr docutils literal notranslate"><span class="pre">logits</span></code></a> (but not both). The innermost dimension of
<a class="reference internal" href="#torch.distributions.multinomial.Multinomial.probs" title="torch.distributions.multinomial.Multinomial.probs"><code class="xref py py-attr docutils literal notranslate"><span class="pre">probs</span></code></a> indexes over categories. All other dimensions index over batches.</p>
<p>Note that <a class="reference internal" href="#torch.distributions.multinomial.Multinomial.total_count" title="torch.distributions.multinomial.Multinomial.total_count"><code class="xref py py-attr docutils literal notranslate"><span class="pre">total_count</span></code></a> need not be specified if only <a class="reference internal" href="#torch.distributions.multinomial.Multinomial.log_prob" title="torch.distributions.multinomial.Multinomial.log_prob"><code class="xref py py-meth docutils literal notranslate"><span class="pre">log_prob()</span></code></a> is
called (see example below)</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The <cite>probs</cite> argument must be non-negative, finite and have a non-zero sum,
and it will be normalized to sum to 1 along the last dimension. <a class="reference internal" href="#torch.distributions.multinomial.Multinomial.probs" title="torch.distributions.multinomial.Multinomial.probs"><code class="xref py py-attr docutils literal notranslate"><span class="pre">probs</span></code></a>
will return this normalized value.
The <cite>logits</cite> argument will be interpreted as unnormalized log probabilities
and can therefore be any real number. It will likewise be normalized so that
the resulting probabilities sum to 1 along the last dimension. <a class="reference internal" href="#torch.distributions.multinomial.Multinomial.logits" title="torch.distributions.multinomial.Multinomial.logits"><code class="xref py py-attr docutils literal notranslate"><span class="pre">logits</span></code></a>
will return this normalized value.</p>
</div>
<ul class="simple">
<li><p><a class="reference internal" href="#torch.distributions.multinomial.Multinomial.sample" title="torch.distributions.multinomial.Multinomial.sample"><code class="xref py py-meth docutils literal notranslate"><span class="pre">sample()</span></code></a> requires a single shared <cite>total_count</cite> for all
parameters and samples.</p></li>
<li><p><a class="reference internal" href="#torch.distributions.multinomial.Multinomial.log_prob" title="torch.distributions.multinomial.Multinomial.log_prob"><code class="xref py py-meth docutils literal notranslate"><span class="pre">log_prob()</span></code></a> allows different <cite>total_count</cite> for each parameter and
sample.</p></li>
</ul>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">Multinomial</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">m</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>  <span class="c1"># equal probability of 0, 1, 2, 3</span>
<span class="go">tensor([ 21.,  24.,  30.,  25.])</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">Multinomial</span><span class="p">(</span><span class="n">probs</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">]))</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="go">tensor([-4.1338])</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>total_count</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>)  number of trials</p></li>
<li><p><strong>probs</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>)  event probabilities</p></li>
<li><p><strong>logits</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>)  event log probabilities (unnormalized)</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="torch.distributions.multinomial.Multinomial.arg_constraints">
<span class="sig-name descname"><span class="pre">arg_constraints</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">{'logits':</span> <span class="pre">IndependentConstraint(Real(),</span> <span class="pre">1),</span> <span class="pre">'probs':</span> <span class="pre">Simplex()}</span></em><a class="headerlink" href="#torch.distributions.multinomial.Multinomial.arg_constraints" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributions.multinomial.Multinomial.entropy">
<span class="sig-name descname"><span class="pre">entropy</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/multinomial.py#L115"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.multinomial.Multinomial.entropy" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributions.multinomial.Multinomial.expand">
<span class="sig-name descname"><span class="pre">expand</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch_shape</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">_instance</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/multinomial.py#L71"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.multinomial.Multinomial.expand" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributions.multinomial.Multinomial.log_prob">
<span class="sig-name descname"><span class="pre">log_prob</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">value</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/multinomial.py#L128"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.multinomial.Multinomial.log_prob" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="torch.distributions.multinomial.Multinomial.logits">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">logits</span></span><a class="headerlink" href="#torch.distributions.multinomial.Multinomial.logits" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="torch.distributions.multinomial.Multinomial.mean">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">mean</span></span><a class="headerlink" href="#torch.distributions.multinomial.Multinomial.mean" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="torch.distributions.multinomial.Multinomial.param_shape">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">param_shape</span></span><a class="headerlink" href="#torch.distributions.multinomial.Multinomial.param_shape" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="torch.distributions.multinomial.Multinomial.probs">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">probs</span></span><a class="headerlink" href="#torch.distributions.multinomial.Multinomial.probs" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributions.multinomial.Multinomial.sample">
<span class="sig-name descname"><span class="pre">sample</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">sample_shape</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">torch.Size([])</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/multinomial.py#L101"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.multinomial.Multinomial.sample" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="torch.distributions.multinomial.Multinomial.support">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">support</span></span><a class="headerlink" href="#torch.distributions.multinomial.Multinomial.support" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="torch.distributions.multinomial.Multinomial.total_count">
<span class="sig-name descname"><span class="pre">total_count</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><span class="pre">int</span></a></em><a class="headerlink" href="#torch.distributions.multinomial.Multinomial.total_count" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="torch.distributions.multinomial.Multinomial.variance">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">variance</span></span><a class="headerlink" href="#torch.distributions.multinomial.Multinomial.variance" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="multivariatenormal">
<h2><span class="hidden-section">MultivariateNormal</span><a class="headerlink" href="#multivariatenormal" title="Permalink to this heading">#</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="torch.distributions.multivariate_normal.MultivariateNormal">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torch.distributions.multivariate_normal.</span></span><span class="sig-name descname"><span class="pre">MultivariateNormal</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">loc</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">covariance_matrix</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">precision_matrix</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">scale_tril</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">validate_args</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/multivariate_normal.py#L87"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.multivariate_normal.MultivariateNormal" title="Permalink to this definition">#</a></dt>
<dd><p>Bases: <a class="reference internal" href="#torch.distributions.distribution.Distribution" title="torch.distributions.distribution.Distribution"><code class="xref py py-class docutils literal notranslate"><span class="pre">Distribution</span></code></a></p>
<p>Creates a multivariate normal (also called Gaussian) distribution
parameterized by a mean vector and a covariance matrix.</p>
<p>The multivariate normal distribution can be parameterized either
in terms of a positive definite covariance matrix <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold"></mi></mrow><annotation encoding="application/x-tex">\mathbf{\Sigma}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6861em;"></span><span class="mord mathbf"></span></span></span></span></span>
or a positive definite precision matrix <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi mathvariant="bold"></mi><mrow><mo></mo><mn>1</mn></mrow></msup></mrow><annotation encoding="application/x-tex">\mathbf{\Sigma}^{-1}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span class="mord mathbf"></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"></span><span class="mord mtight">1</span></span></span></span></span></span></span></span></span></span></span></span></span>
or a lower-triangular matrix <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">L</mi></mrow><annotation encoding="application/x-tex">\mathbf{L}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6861em;"></span><span class="mord mathbf">L</span></span></span></span></span> with positive-valued
diagonal entries, such that
<span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold"></mi><mo>=</mo><mi mathvariant="bold">L</mi><msup><mi mathvariant="bold">L</mi><mi mathvariant="normal"></mi></msup></mrow><annotation encoding="application/x-tex">\mathbf{\Sigma} = \mathbf{L}\mathbf{L}^\top</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6861em;"></span><span class="mord mathbf"></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.8491em;"></span><span class="mord mathbf">L</span><span class="mord"><span class="mord mathbf">L</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8491em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"></span></span></span></span></span></span></span></span></span></span></span></span>. This triangular matrix
can be obtained via e.g. Cholesky decomposition of the covariance.</p>
<p class="rubric">Example</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">MultivariateNormal</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">2</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>  <span class="c1"># normally distributed with mean=`[0,0]` and covariance_matrix=`I`</span>
<span class="go">tensor([-0.2102, -0.5429])</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>loc</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>)  mean of the distribution</p></li>
<li><p><strong>covariance_matrix</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>)  positive-definite covariance matrix</p></li>
<li><p><strong>precision_matrix</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>)  positive-definite precision matrix</p></li>
<li><p><strong>scale_tril</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>)  lower-triangular factor of covariance, with positive-valued diagonal</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Only one of <a class="reference internal" href="#torch.distributions.multivariate_normal.MultivariateNormal.covariance_matrix" title="torch.distributions.multivariate_normal.MultivariateNormal.covariance_matrix"><code class="xref py py-attr docutils literal notranslate"><span class="pre">covariance_matrix</span></code></a> or <a class="reference internal" href="#torch.distributions.multivariate_normal.MultivariateNormal.precision_matrix" title="torch.distributions.multivariate_normal.MultivariateNormal.precision_matrix"><code class="xref py py-attr docutils literal notranslate"><span class="pre">precision_matrix</span></code></a> or
<a class="reference internal" href="#torch.distributions.multivariate_normal.MultivariateNormal.scale_tril" title="torch.distributions.multivariate_normal.MultivariateNormal.scale_tril"><code class="xref py py-attr docutils literal notranslate"><span class="pre">scale_tril</span></code></a> can be specified.</p>
<p>Using <a class="reference internal" href="#torch.distributions.multivariate_normal.MultivariateNormal.scale_tril" title="torch.distributions.multivariate_normal.MultivariateNormal.scale_tril"><code class="xref py py-attr docutils literal notranslate"><span class="pre">scale_tril</span></code></a> will be more efficient: all computations internally
are based on <a class="reference internal" href="#torch.distributions.multivariate_normal.MultivariateNormal.scale_tril" title="torch.distributions.multivariate_normal.MultivariateNormal.scale_tril"><code class="xref py py-attr docutils literal notranslate"><span class="pre">scale_tril</span></code></a>. If <a class="reference internal" href="#torch.distributions.multivariate_normal.MultivariateNormal.covariance_matrix" title="torch.distributions.multivariate_normal.MultivariateNormal.covariance_matrix"><code class="xref py py-attr docutils literal notranslate"><span class="pre">covariance_matrix</span></code></a> or
<a class="reference internal" href="#torch.distributions.multivariate_normal.MultivariateNormal.precision_matrix" title="torch.distributions.multivariate_normal.MultivariateNormal.precision_matrix"><code class="xref py py-attr docutils literal notranslate"><span class="pre">precision_matrix</span></code></a> is passed instead, it is only used to compute
the corresponding lower triangular matrices using a Cholesky decomposition.</p>
</div>
<dl class="py attribute">
<dt class="sig sig-object py" id="torch.distributions.multivariate_normal.MultivariateNormal.arg_constraints">
<span class="sig-name descname"><span class="pre">arg_constraints</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">{'covariance_matrix':</span> <span class="pre">PositiveDefinite(),</span> <span class="pre">'loc':</span> <span class="pre">IndependentConstraint(Real(),</span> <span class="pre">1),</span> <span class="pre">'precision_matrix':</span> <span class="pre">PositiveDefinite(),</span> <span class="pre">'scale_tril':</span> <span class="pre">LowerCholesky()}</span></em><a class="headerlink" href="#torch.distributions.multivariate_normal.MultivariateNormal.arg_constraints" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="torch.distributions.multivariate_normal.MultivariateNormal.covariance_matrix">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">covariance_matrix</span></span><a class="headerlink" href="#torch.distributions.multivariate_normal.MultivariateNormal.covariance_matrix" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributions.multivariate_normal.MultivariateNormal.entropy">
<span class="sig-name descname"><span class="pre">entropy</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/multivariate_normal.py#L257"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.multivariate_normal.MultivariateNormal.entropy" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributions.multivariate_normal.MultivariateNormal.expand">
<span class="sig-name descname"><span class="pre">expand</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch_shape</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">_instance</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/multivariate_normal.py#L189"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.multivariate_normal.MultivariateNormal.expand" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="torch.distributions.multivariate_normal.MultivariateNormal.has_rsample">
<span class="sig-name descname"><span class="pre">has_rsample</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">True</span></em><a class="headerlink" href="#torch.distributions.multivariate_normal.MultivariateNormal.has_rsample" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributions.multivariate_normal.MultivariateNormal.log_prob">
<span class="sig-name descname"><span class="pre">log_prob</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">value</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/multivariate_normal.py#L247"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.multivariate_normal.MultivariateNormal.log_prob" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="torch.distributions.multivariate_normal.MultivariateNormal.mean">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">mean</span></span><a class="headerlink" href="#torch.distributions.multivariate_normal.MultivariateNormal.mean" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="torch.distributions.multivariate_normal.MultivariateNormal.mode">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">mode</span></span><a class="headerlink" href="#torch.distributions.multivariate_normal.MultivariateNormal.mode" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="torch.distributions.multivariate_normal.MultivariateNormal.precision_matrix">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">precision_matrix</span></span><a class="headerlink" href="#torch.distributions.multivariate_normal.MultivariateNormal.precision_matrix" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributions.multivariate_normal.MultivariateNormal.rsample">
<span class="sig-name descname"><span class="pre">rsample</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">sample_shape</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">torch.Size([])</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/multivariate_normal.py#L242"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.multivariate_normal.MultivariateNormal.rsample" title="Permalink to this definition">#</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a></p>
</dd>
</dl>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="torch.distributions.multivariate_normal.MultivariateNormal.scale_tril">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">scale_tril</span></span><a class="headerlink" href="#torch.distributions.multivariate_normal.MultivariateNormal.scale_tril" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="torch.distributions.multivariate_normal.MultivariateNormal.support">
<span class="sig-name descname"><span class="pre">support</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">IndependentConstraint(Real(),</span> <span class="pre">1)</span></em><a class="headerlink" href="#torch.distributions.multivariate_normal.MultivariateNormal.support" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="torch.distributions.multivariate_normal.MultivariateNormal.variance">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">variance</span></span><a class="headerlink" href="#torch.distributions.multivariate_normal.MultivariateNormal.variance" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="negativebinomial">
<h2><span class="hidden-section">NegativeBinomial</span><a class="headerlink" href="#negativebinomial" title="Permalink to this heading">#</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="torch.distributions.negative_binomial.NegativeBinomial">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torch.distributions.negative_binomial.</span></span><span class="sig-name descname"><span class="pre">NegativeBinomial</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">total_count</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">probs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">logits</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">validate_args</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/negative_binomial.py#L17"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.negative_binomial.NegativeBinomial" title="Permalink to this definition">#</a></dt>
<dd><p>Bases: <a class="reference internal" href="#torch.distributions.distribution.Distribution" title="torch.distributions.distribution.Distribution"><code class="xref py py-class docutils literal notranslate"><span class="pre">Distribution</span></code></a></p>
<p>Creates a Negative Binomial distribution, i.e. distribution
of the number of successful independent and identical Bernoulli trials
before <code class="xref py py-attr docutils literal notranslate"><span class="pre">total_count</span></code> failures are achieved. The probability
of success of each Bernoulli trial is <a class="reference internal" href="#torch.distributions.negative_binomial.NegativeBinomial.probs" title="torch.distributions.negative_binomial.NegativeBinomial.probs"><code class="xref py py-attr docutils literal notranslate"><span class="pre">probs</span></code></a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>total_count</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a><em> or </em><a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>)  non-negative number of negative Bernoulli
trials to stop, although the distribution is still valid for real
valued count</p></li>
<li><p><strong>probs</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>)  Event probabilities of success in the half open interval [0, 1)</p></li>
<li><p><strong>logits</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>)  Event log-odds for probabilities of success</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="torch.distributions.negative_binomial.NegativeBinomial.arg_constraints">
<span class="sig-name descname"><span class="pre">arg_constraints</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">{'logits':</span> <span class="pre">Real(),</span> <span class="pre">'probs':</span> <span class="pre">HalfOpenInterval(lower_bound=0.0,</span> <span class="pre">upper_bound=1.0),</span> <span class="pre">'total_count':</span> <span class="pre">GreaterThanEq(lower_bound=0)}</span></em><a class="headerlink" href="#torch.distributions.negative_binomial.NegativeBinomial.arg_constraints" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributions.negative_binomial.NegativeBinomial.expand">
<span class="sig-name descname"><span class="pre">expand</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch_shape</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">_instance</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/negative_binomial.py#L60"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.negative_binomial.NegativeBinomial.expand" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributions.negative_binomial.NegativeBinomial.log_prob">
<span class="sig-name descname"><span class="pre">log_prob</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">value</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/negative_binomial.py#L115"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.negative_binomial.NegativeBinomial.log_prob" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="torch.distributions.negative_binomial.NegativeBinomial.logits">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">logits</span></span><a class="headerlink" href="#torch.distributions.negative_binomial.NegativeBinomial.logits" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="torch.distributions.negative_binomial.NegativeBinomial.mean">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">mean</span></span><a class="headerlink" href="#torch.distributions.negative_binomial.NegativeBinomial.mean" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="torch.distributions.negative_binomial.NegativeBinomial.mode">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">mode</span></span><a class="headerlink" href="#torch.distributions.negative_binomial.NegativeBinomial.mode" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="torch.distributions.negative_binomial.NegativeBinomial.param_shape">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">param_shape</span></span><a class="headerlink" href="#torch.distributions.negative_binomial.NegativeBinomial.param_shape" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="torch.distributions.negative_binomial.NegativeBinomial.probs">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">probs</span></span><a class="headerlink" href="#torch.distributions.negative_binomial.NegativeBinomial.probs" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributions.negative_binomial.NegativeBinomial.sample">
<span class="sig-name descname"><span class="pre">sample</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">sample_shape</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">torch.Size([])</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/negative_binomial.py#L110"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.negative_binomial.NegativeBinomial.sample" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="torch.distributions.negative_binomial.NegativeBinomial.support">
<span class="sig-name descname"><span class="pre">support</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">IntegerGreaterThan(lower_bound=0)</span></em><a class="headerlink" href="#torch.distributions.negative_binomial.NegativeBinomial.support" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="torch.distributions.negative_binomial.NegativeBinomial.variance">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">variance</span></span><a class="headerlink" href="#torch.distributions.negative_binomial.NegativeBinomial.variance" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="normal">
<h2><span class="hidden-section">Normal</span><a class="headerlink" href="#normal" title="Permalink to this heading">#</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="torch.distributions.normal.Normal">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torch.distributions.normal.</span></span><span class="sig-name descname"><span class="pre">Normal</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">loc</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">scale</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">validate_args</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/normal.py#L15"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.normal.Normal" title="Permalink to this definition">#</a></dt>
<dd><p>Bases: <a class="reference internal" href="#torch.distributions.exp_family.ExponentialFamily" title="torch.distributions.exp_family.ExponentialFamily"><code class="xref py py-class docutils literal notranslate"><span class="pre">ExponentialFamily</span></code></a></p>
<p>Creates a normal (also called Gaussian) distribution parameterized by
<code class="xref py py-attr docutils literal notranslate"><span class="pre">loc</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">scale</span></code>.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">Normal</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.0</span><span class="p">]),</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.0</span><span class="p">]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>  <span class="c1"># normally distributed with loc=0 and scale=1</span>
<span class="go">tensor([ 0.1046])</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>loc</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a><em> or </em><a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>)  mean of the distribution (often referred to as mu)</p></li>
<li><p><strong>scale</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a><em> or </em><a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>)  standard deviation of the distribution
(often referred to as sigma)</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="torch.distributions.normal.Normal.arg_constraints">
<span class="sig-name descname"><span class="pre">arg_constraints</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">{'loc':</span> <span class="pre">Real(),</span> <span class="pre">'scale':</span> <span class="pre">GreaterThan(lower_bound=0.0)}</span></em><a class="headerlink" href="#torch.distributions.normal.Normal.arg_constraints" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributions.normal.Normal.cdf">
<span class="sig-name descname"><span class="pre">cdf</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">value</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/normal.py#L94"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.normal.Normal.cdf" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributions.normal.Normal.entropy">
<span class="sig-name descname"><span class="pre">entropy</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/normal.py#L104"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.normal.Normal.entropy" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributions.normal.Normal.expand">
<span class="sig-name descname"><span class="pre">expand</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch_shape</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">_instance</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/normal.py#L61"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.normal.Normal.expand" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="torch.distributions.normal.Normal.has_rsample">
<span class="sig-name descname"><span class="pre">has_rsample</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">True</span></em><a class="headerlink" href="#torch.distributions.normal.Normal.has_rsample" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributions.normal.Normal.icdf">
<span class="sig-name descname"><span class="pre">icdf</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">value</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/normal.py#L101"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.normal.Normal.icdf" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributions.normal.Normal.log_prob">
<span class="sig-name descname"><span class="pre">log_prob</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">value</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/normal.py#L80"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.normal.Normal.log_prob" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="torch.distributions.normal.Normal.mean">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">mean</span></span><a class="headerlink" href="#torch.distributions.normal.Normal.mean" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="torch.distributions.normal.Normal.mode">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">mode</span></span><a class="headerlink" href="#torch.distributions.normal.Normal.mode" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributions.normal.Normal.rsample">
<span class="sig-name descname"><span class="pre">rsample</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">sample_shape</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">torch.Size([])</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/normal.py#L75"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.normal.Normal.rsample" title="Permalink to this definition">#</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributions.normal.Normal.sample">
<span class="sig-name descname"><span class="pre">sample</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">sample_shape</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">torch.Size([])</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/normal.py#L70"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.normal.Normal.sample" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="torch.distributions.normal.Normal.stddev">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">stddev</span></span><a class="headerlink" href="#torch.distributions.normal.Normal.stddev" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="torch.distributions.normal.Normal.support">
<span class="sig-name descname"><span class="pre">support</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">Real()</span></em><a class="headerlink" href="#torch.distributions.normal.Normal.support" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="torch.distributions.normal.Normal.variance">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">variance</span></span><a class="headerlink" href="#torch.distributions.normal.Normal.variance" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="onehotcategorical">
<h2><span class="hidden-section">OneHotCategorical</span><a class="headerlink" href="#onehotcategorical" title="Permalink to this heading">#</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="torch.distributions.one_hot_categorical.OneHotCategorical">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torch.distributions.one_hot_categorical.</span></span><span class="sig-name descname"><span class="pre">OneHotCategorical</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">probs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">logits</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">validate_args</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/one_hot_categorical.py#L12"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.one_hot_categorical.OneHotCategorical" title="Permalink to this definition">#</a></dt>
<dd><p>Bases: <a class="reference internal" href="#torch.distributions.distribution.Distribution" title="torch.distributions.distribution.Distribution"><code class="xref py py-class docutils literal notranslate"><span class="pre">Distribution</span></code></a></p>
<p>Creates a one-hot categorical distribution parameterized by <a class="reference internal" href="#torch.distributions.one_hot_categorical.OneHotCategorical.probs" title="torch.distributions.one_hot_categorical.OneHotCategorical.probs"><code class="xref py py-attr docutils literal notranslate"><span class="pre">probs</span></code></a> or
<a class="reference internal" href="#torch.distributions.one_hot_categorical.OneHotCategorical.logits" title="torch.distributions.one_hot_categorical.OneHotCategorical.logits"><code class="xref py py-attr docutils literal notranslate"><span class="pre">logits</span></code></a>.</p>
<p>Samples are one-hot coded vectors of size <code class="docutils literal notranslate"><span class="pre">probs.size(-1)</span></code>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The <cite>probs</cite> argument must be non-negative, finite and have a non-zero sum,
and it will be normalized to sum to 1 along the last dimension. <a class="reference internal" href="#torch.distributions.one_hot_categorical.OneHotCategorical.probs" title="torch.distributions.one_hot_categorical.OneHotCategorical.probs"><code class="xref py py-attr docutils literal notranslate"><span class="pre">probs</span></code></a>
will return this normalized value.
The <cite>logits</cite> argument will be interpreted as unnormalized log probabilities
and can therefore be any real number. It will likewise be normalized so that
the resulting probabilities sum to 1 along the last dimension. <a class="reference internal" href="#torch.distributions.one_hot_categorical.OneHotCategorical.logits" title="torch.distributions.one_hot_categorical.OneHotCategorical.logits"><code class="xref py py-attr docutils literal notranslate"><span class="pre">logits</span></code></a>
will return this normalized value.</p>
</div>
<p>See also: <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.distributions.Categorical()</span></code> for specifications of
<a class="reference internal" href="#torch.distributions.one_hot_categorical.OneHotCategorical.probs" title="torch.distributions.one_hot_categorical.OneHotCategorical.probs"><code class="xref py py-attr docutils literal notranslate"><span class="pre">probs</span></code></a> and <a class="reference internal" href="#torch.distributions.one_hot_categorical.OneHotCategorical.logits" title="torch.distributions.one_hot_categorical.OneHotCategorical.logits"><code class="xref py py-attr docutils literal notranslate"><span class="pre">logits</span></code></a>.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">OneHotCategorical</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span> <span class="mf">0.25</span><span class="p">,</span> <span class="mf">0.25</span><span class="p">,</span> <span class="mf">0.25</span><span class="p">,</span> <span class="mf">0.25</span> <span class="p">]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>  <span class="c1"># equal probability of 0, 1, 2, 3</span>
<span class="go">tensor([ 0.,  0.,  0.,  1.])</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>probs</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>)  event probabilities</p></li>
<li><p><strong>logits</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>)  event log probabilities (unnormalized)</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="torch.distributions.one_hot_categorical.OneHotCategorical.arg_constraints">
<span class="sig-name descname"><span class="pre">arg_constraints</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">{'logits':</span> <span class="pre">IndependentConstraint(Real(),</span> <span class="pre">1),</span> <span class="pre">'probs':</span> <span class="pre">Simplex()}</span></em><a class="headerlink" href="#torch.distributions.one_hot_categorical.OneHotCategorical.arg_constraints" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributions.one_hot_categorical.OneHotCategorical.entropy">
<span class="sig-name descname"><span class="pre">entropy</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/one_hot_categorical.py#L107"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.one_hot_categorical.OneHotCategorical.entropy" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributions.one_hot_categorical.OneHotCategorical.enumerate_support">
<span class="sig-name descname"><span class="pre">enumerate_support</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">expand</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/one_hot_categorical.py#L110"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.one_hot_categorical.OneHotCategorical.enumerate_support" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributions.one_hot_categorical.OneHotCategorical.expand">
<span class="sig-name descname"><span class="pre">expand</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch_shape</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">_instance</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/one_hot_categorical.py#L51"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.one_hot_categorical.OneHotCategorical.expand" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="torch.distributions.one_hot_categorical.OneHotCategorical.has_enumerate_support">
<span class="sig-name descname"><span class="pre">has_enumerate_support</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">True</span></em><a class="headerlink" href="#torch.distributions.one_hot_categorical.OneHotCategorical.has_enumerate_support" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributions.one_hot_categorical.OneHotCategorical.log_prob">
<span class="sig-name descname"><span class="pre">log_prob</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">value</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/one_hot_categorical.py#L101"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.one_hot_categorical.OneHotCategorical.log_prob" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="torch.distributions.one_hot_categorical.OneHotCategorical.logits">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">logits</span></span><a class="headerlink" href="#torch.distributions.one_hot_categorical.OneHotCategorical.logits" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="torch.distributions.one_hot_categorical.OneHotCategorical.mean">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">mean</span></span><a class="headerlink" href="#torch.distributions.one_hot_categorical.OneHotCategorical.mean" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="torch.distributions.one_hot_categorical.OneHotCategorical.mode">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">mode</span></span><a class="headerlink" href="#torch.distributions.one_hot_categorical.OneHotCategorical.mode" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="torch.distributions.one_hot_categorical.OneHotCategorical.param_shape">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">param_shape</span></span><a class="headerlink" href="#torch.distributions.one_hot_categorical.OneHotCategorical.param_shape" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="torch.distributions.one_hot_categorical.OneHotCategorical.probs">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">probs</span></span><a class="headerlink" href="#torch.distributions.one_hot_categorical.OneHotCategorical.probs" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributions.one_hot_categorical.OneHotCategorical.sample">
<span class="sig-name descname"><span class="pre">sample</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">sample_shape</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">torch.Size([])</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/one_hot_categorical.py#L94"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.one_hot_categorical.OneHotCategorical.sample" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="torch.distributions.one_hot_categorical.OneHotCategorical.support">
<span class="sig-name descname"><span class="pre">support</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">OneHot()</span></em><a class="headerlink" href="#torch.distributions.one_hot_categorical.OneHotCategorical.support" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="torch.distributions.one_hot_categorical.OneHotCategorical.variance">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">variance</span></span><a class="headerlink" href="#torch.distributions.one_hot_categorical.OneHotCategorical.variance" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="pareto">
<h2><span class="hidden-section">Pareto</span><a class="headerlink" href="#pareto" title="Permalink to this heading">#</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="torch.distributions.pareto.Pareto">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torch.distributions.pareto.</span></span><span class="sig-name descname"><span class="pre">Pareto</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">scale</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">alpha</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">validate_args</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/pareto.py#L12"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.pareto.Pareto" title="Permalink to this definition">#</a></dt>
<dd><p>Bases: <a class="reference internal" href="#torch.distributions.transformed_distribution.TransformedDistribution" title="torch.distributions.transformed_distribution.TransformedDistribution"><code class="xref py py-class docutils literal notranslate"><span class="pre">TransformedDistribution</span></code></a></p>
<p>Samples from a Pareto Type 1 distribution.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">Pareto</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.0</span><span class="p">]),</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.0</span><span class="p">]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>  <span class="c1"># sample from a Pareto distribution with scale=1 and alpha=1</span>
<span class="go">tensor([ 1.5623])</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>scale</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a><em> or </em><a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>)  Scale parameter of the distribution</p></li>
<li><p><strong>alpha</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a><em> or </em><a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>)  Shape parameter of the distribution</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="torch.distributions.pareto.Pareto.arg_constraints">
<span class="sig-name descname"><span class="pre">arg_constraints</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference internal" href="#torch.distributions.constraints.Constraint" title="torch.distributions.constraints.Constraint"><span class="pre">constraints.Constraint</span></a><span class="p"><span class="pre">]</span></span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">{'alpha':</span> <span class="pre">GreaterThan(lower_bound=0.0),</span> <span class="pre">'scale':</span> <span class="pre">GreaterThan(lower_bound=0.0)}</span></em><a class="headerlink" href="#torch.distributions.pareto.Pareto.arg_constraints" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributions.pareto.Pareto.entropy">
<span class="sig-name descname"><span class="pre">entropy</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/pareto.py#L61"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.pareto.Pareto.entropy" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributions.pareto.Pareto.expand">
<span class="sig-name descname"><span class="pre">expand</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch_shape</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">_instance</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/pareto.py#L35"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.pareto.Pareto.expand" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="torch.distributions.pareto.Pareto.mean">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">mean</span></span><a class="headerlink" href="#torch.distributions.pareto.Pareto.mean" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="torch.distributions.pareto.Pareto.mode">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">mode</span></span><a class="headerlink" href="#torch.distributions.pareto.Pareto.mode" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="torch.distributions.pareto.Pareto.support">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">support</span></span><a class="headerlink" href="#torch.distributions.pareto.Pareto.support" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="torch.distributions.pareto.Pareto.variance">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">variance</span></span><a class="headerlink" href="#torch.distributions.pareto.Pareto.variance" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="poisson">
<h2><span class="hidden-section">Poisson</span><a class="headerlink" href="#poisson" title="Permalink to this heading">#</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="torch.distributions.poisson.Poisson">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torch.distributions.poisson.</span></span><span class="sig-name descname"><span class="pre">Poisson</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">rate</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">validate_args</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/poisson.py#L13"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.poisson.Poisson" title="Permalink to this definition">#</a></dt>
<dd><p>Bases: <a class="reference internal" href="#torch.distributions.exp_family.ExponentialFamily" title="torch.distributions.exp_family.ExponentialFamily"><code class="xref py py-class docutils literal notranslate"><span class="pre">ExponentialFamily</span></code></a></p>
<p>Creates a Poisson distribution parameterized by <code class="xref py py-attr docutils literal notranslate"><span class="pre">rate</span></code>, the rate parameter.</p>
<p>Samples are nonnegative integers, with a pmf given by</p>
<div class="math">
<span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msup><mrow><mi mathvariant="normal">r</mi><mi mathvariant="normal">a</mi><mi mathvariant="normal">t</mi><mi mathvariant="normal">e</mi></mrow><mi>k</mi></msup><mfrac><msup><mi>e</mi><mrow><mo></mo><mrow><mi mathvariant="normal">r</mi><mi mathvariant="normal">a</mi><mi mathvariant="normal">t</mi><mi mathvariant="normal">e</mi></mrow></mrow></msup><mrow><mi>k</mi><mo stretchy="false">!</mo></mrow></mfrac></mrow><annotation encoding="application/x-tex">\mathrm{rate}^k \frac{e^{-\mathrm{rate}}}{k!}

</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:2.1566em;vertical-align:-0.686em;"></span><span class="mord"><span class="mord"><span class="mord mathrm">rate</span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8991em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span></span></span></span></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.4706em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03148em;">k</span><span class="mclose">!</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathnormal">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7936em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"></span><span class="mord mtight"><span class="mord mathrm mtight">rate</span></span></span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s"></span></span><span class="vlist-r"><span class="vlist" style="height:0.686em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></div><p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">Poisson</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">4</span><span class="p">]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>
<span class="go">tensor([ 3.])</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>rate</strong> (<em>Number</em><em>, </em><a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>)  the rate parameter</p>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="torch.distributions.poisson.Poisson.arg_constraints">
<span class="sig-name descname"><span class="pre">arg_constraints</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">{'rate':</span> <span class="pre">GreaterThanEq(lower_bound=0.0)}</span></em><a class="headerlink" href="#torch.distributions.poisson.Poisson.arg_constraints" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributions.poisson.Poisson.expand">
<span class="sig-name descname"><span class="pre">expand</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch_shape</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">_instance</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/poisson.py#L55"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.poisson.Poisson.expand" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributions.poisson.Poisson.log_prob">
<span class="sig-name descname"><span class="pre">log_prob</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">value</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/poisson.py#L68"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.poisson.Poisson.log_prob" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="torch.distributions.poisson.Poisson.mean">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">mean</span></span><a class="headerlink" href="#torch.distributions.poisson.Poisson.mean" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="torch.distributions.poisson.Poisson.mode">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">mode</span></span><a class="headerlink" href="#torch.distributions.poisson.Poisson.mode" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributions.poisson.Poisson.sample">
<span class="sig-name descname"><span class="pre">sample</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">sample_shape</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">torch.Size([])</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/poisson.py#L63"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.poisson.Poisson.sample" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="torch.distributions.poisson.Poisson.support">
<span class="sig-name descname"><span class="pre">support</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">IntegerGreaterThan(lower_bound=0)</span></em><a class="headerlink" href="#torch.distributions.poisson.Poisson.support" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="torch.distributions.poisson.Poisson.variance">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">variance</span></span><a class="headerlink" href="#torch.distributions.poisson.Poisson.variance" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="relaxedbernoulli">
<h2><span class="hidden-section">RelaxedBernoulli</span><a class="headerlink" href="#relaxedbernoulli" title="Permalink to this heading">#</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="torch.distributions.relaxed_bernoulli.RelaxedBernoulli">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torch.distributions.relaxed_bernoulli.</span></span><span class="sig-name descname"><span class="pre">RelaxedBernoulli</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">temperature</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">probs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">logits</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">validate_args</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/relaxed_bernoulli.py#L110"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.relaxed_bernoulli.RelaxedBernoulli" title="Permalink to this definition">#</a></dt>
<dd><p>Bases: <a class="reference internal" href="#torch.distributions.transformed_distribution.TransformedDistribution" title="torch.distributions.transformed_distribution.TransformedDistribution"><code class="xref py py-class docutils literal notranslate"><span class="pre">TransformedDistribution</span></code></a></p>
<p>Creates a RelaxedBernoulli distribution, parametrized by
<a class="reference internal" href="#torch.distributions.relaxed_bernoulli.RelaxedBernoulli.temperature" title="torch.distributions.relaxed_bernoulli.RelaxedBernoulli.temperature"><code class="xref py py-attr docutils literal notranslate"><span class="pre">temperature</span></code></a>, and either <a class="reference internal" href="#torch.distributions.relaxed_bernoulli.RelaxedBernoulli.probs" title="torch.distributions.relaxed_bernoulli.RelaxedBernoulli.probs"><code class="xref py py-attr docutils literal notranslate"><span class="pre">probs</span></code></a> or <a class="reference internal" href="#torch.distributions.relaxed_bernoulli.RelaxedBernoulli.logits" title="torch.distributions.relaxed_bernoulli.RelaxedBernoulli.logits"><code class="xref py py-attr docutils literal notranslate"><span class="pre">logits</span></code></a>
(but not both). This is a relaxed version of the <cite>Bernoulli</cite> distribution,
so the values are in (0, 1), and has reparametrizable samples.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">RelaxedBernoulli</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">2.2</span><span class="p">]),</span>
<span class="gp">... </span>                     <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.99</span><span class="p">]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>
<span class="go">tensor([ 0.2951,  0.3442,  0.8918,  0.9021])</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>temperature</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>)  relaxation temperature</p></li>
<li><p><strong>probs</strong> (<em>Number</em><em>, </em><a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>)  the probability of sampling <cite>1</cite></p></li>
<li><p><strong>logits</strong> (<em>Number</em><em>, </em><a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>)  the log-odds of sampling <cite>1</cite></p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="torch.distributions.relaxed_bernoulli.RelaxedBernoulli.arg_constraints">
<span class="sig-name descname"><span class="pre">arg_constraints</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference internal" href="#torch.distributions.constraints.Constraint" title="torch.distributions.constraints.Constraint"><span class="pre">constraints.Constraint</span></a><span class="p"><span class="pre">]</span></span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">{'logits':</span> <span class="pre">Real(),</span> <span class="pre">'probs':</span> <span class="pre">Interval(lower_bound=0.0,</span> <span class="pre">upper_bound=1.0)}</span></em><a class="headerlink" href="#torch.distributions.relaxed_bernoulli.RelaxedBernoulli.arg_constraints" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributions.relaxed_bernoulli.RelaxedBernoulli.expand">
<span class="sig-name descname"><span class="pre">expand</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch_shape</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">_instance</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/relaxed_bernoulli.py#L138"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.relaxed_bernoulli.RelaxedBernoulli.expand" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="torch.distributions.relaxed_bernoulli.RelaxedBernoulli.has_rsample">
<span class="sig-name descname"><span class="pre">has_rsample</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">True</span></em><a class="headerlink" href="#torch.distributions.relaxed_bernoulli.RelaxedBernoulli.has_rsample" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="torch.distributions.relaxed_bernoulli.RelaxedBernoulli.logits">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">logits</span></span><a class="headerlink" href="#torch.distributions.relaxed_bernoulli.RelaxedBernoulli.logits" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="torch.distributions.relaxed_bernoulli.RelaxedBernoulli.probs">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">probs</span></span><a class="headerlink" href="#torch.distributions.relaxed_bernoulli.RelaxedBernoulli.probs" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="torch.distributions.relaxed_bernoulli.RelaxedBernoulli.support">
<span class="sig-name descname"><span class="pre">support</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">Interval(lower_bound=0.0,</span> <span class="pre">upper_bound=1.0)</span></em><a class="headerlink" href="#torch.distributions.relaxed_bernoulli.RelaxedBernoulli.support" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="torch.distributions.relaxed_bernoulli.RelaxedBernoulli.temperature">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">temperature</span></span><a class="headerlink" href="#torch.distributions.relaxed_bernoulli.RelaxedBernoulli.temperature" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="logitrelaxedbernoulli">
<h2><span class="hidden-section">LogitRelaxedBernoulli</span><a class="headerlink" href="#logitrelaxedbernoulli" title="Permalink to this heading">#</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torch.distributions.relaxed_bernoulli.</span></span><span class="sig-name descname"><span class="pre">LogitRelaxedBernoulli</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">temperature</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">probs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">logits</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">validate_args</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/relaxed_bernoulli.py#L22"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli" title="Permalink to this definition">#</a></dt>
<dd><p>Bases: <a class="reference internal" href="#torch.distributions.distribution.Distribution" title="torch.distributions.distribution.Distribution"><code class="xref py py-class docutils literal notranslate"><span class="pre">Distribution</span></code></a></p>
<p>Creates a LogitRelaxedBernoulli distribution parameterized by <a class="reference internal" href="#torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.probs" title="torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.probs"><code class="xref py py-attr docutils literal notranslate"><span class="pre">probs</span></code></a>
or <a class="reference internal" href="#torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.logits" title="torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.logits"><code class="xref py py-attr docutils literal notranslate"><span class="pre">logits</span></code></a> (but not both), which is the logit of a RelaxedBernoulli
distribution.</p>
<p>Samples are logits of values in (0, 1). See [1] for more details.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>temperature</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>)  relaxation temperature</p></li>
<li><p><strong>probs</strong> (<em>Number</em><em>, </em><a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>)  the probability of sampling <cite>1</cite></p></li>
<li><p><strong>logits</strong> (<em>Number</em><em>, </em><a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>)  the log-odds of sampling <cite>1</cite></p></li>
</ul>
</dd>
</dl>
<p>[1] The Concrete Distribution: A Continuous Relaxation of Discrete Random
Variables (Maddison et al., 2017)</p>
<p>[2] Categorical Reparametrization with Gumbel-Softmax
(Jang et al., 2017)</p>
<dl class="py attribute">
<dt class="sig sig-object py" id="torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.arg_constraints">
<span class="sig-name descname"><span class="pre">arg_constraints</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">{'logits':</span> <span class="pre">Real(),</span> <span class="pre">'probs':</span> <span class="pre">Interval(lower_bound=0.0,</span> <span class="pre">upper_bound=1.0)}</span></em><a class="headerlink" href="#torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.arg_constraints" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.expand">
<span class="sig-name descname"><span class="pre">expand</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch_shape</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">_instance</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/relaxed_bernoulli.py#L63"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.expand" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.log_prob">
<span class="sig-name descname"><span class="pre">log_prob</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">value</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/relaxed_bernoulli.py#L102"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.log_prob" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.logits">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">logits</span></span><a class="headerlink" href="#torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.logits" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.param_shape">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">param_shape</span></span><a class="headerlink" href="#torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.param_shape" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.probs">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">probs</span></span><a class="headerlink" href="#torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.probs" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.rsample">
<span class="sig-name descname"><span class="pre">rsample</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">sample_shape</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">torch.Size([])</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/relaxed_bernoulli.py#L92"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.rsample" title="Permalink to this definition">#</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a></p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.support">
<span class="sig-name descname"><span class="pre">support</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">Real()</span></em><a class="headerlink" href="#torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.support" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="relaxedonehotcategorical">
<h2><span class="hidden-section">RelaxedOneHotCategorical</span><a class="headerlink" href="#relaxedonehotcategorical" title="Permalink to this heading">#</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="torch.distributions.relaxed_categorical.RelaxedOneHotCategorical">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torch.distributions.relaxed_categorical.</span></span><span class="sig-name descname"><span class="pre">RelaxedOneHotCategorical</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">temperature</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">probs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">logits</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">validate_args</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/relaxed_categorical.py#L98"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.relaxed_categorical.RelaxedOneHotCategorical" title="Permalink to this definition">#</a></dt>
<dd><p>Bases: <a class="reference internal" href="#torch.distributions.transformed_distribution.TransformedDistribution" title="torch.distributions.transformed_distribution.TransformedDistribution"><code class="xref py py-class docutils literal notranslate"><span class="pre">TransformedDistribution</span></code></a></p>
<p>Creates a RelaxedOneHotCategorical distribution parametrized by
<a class="reference internal" href="#torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.temperature" title="torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.temperature"><code class="xref py py-attr docutils literal notranslate"><span class="pre">temperature</span></code></a>, and either <a class="reference internal" href="#torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.probs" title="torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.probs"><code class="xref py py-attr docutils literal notranslate"><span class="pre">probs</span></code></a> or <a class="reference internal" href="#torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.logits" title="torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.logits"><code class="xref py py-attr docutils literal notranslate"><span class="pre">logits</span></code></a>.
This is a relaxed version of the <code class="xref py py-class docutils literal notranslate"><span class="pre">OneHotCategorical</span></code> distribution, so
its samples are on simplex, and are reparametrizable.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">RelaxedOneHotCategorical</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">2.2</span><span class="p">]),</span>
<span class="gp">... </span>                             <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>
<span class="go">tensor([ 0.1294,  0.2324,  0.3859,  0.2523])</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>temperature</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>)  relaxation temperature</p></li>
<li><p><strong>probs</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>)  event probabilities</p></li>
<li><p><strong>logits</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>)  unnormalized log probability for each event</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.arg_constraints">
<span class="sig-name descname"><span class="pre">arg_constraints</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference internal" href="#torch.distributions.constraints.Constraint" title="torch.distributions.constraints.Constraint"><span class="pre">constraints.Constraint</span></a><span class="p"><span class="pre">]</span></span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">{'logits':</span> <span class="pre">IndependentConstraint(Real(),</span> <span class="pre">1),</span> <span class="pre">'probs':</span> <span class="pre">Simplex()}</span></em><a class="headerlink" href="#torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.arg_constraints" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.expand">
<span class="sig-name descname"><span class="pre">expand</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch_shape</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">_instance</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/relaxed_categorical.py#L128"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.expand" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.has_rsample">
<span class="sig-name descname"><span class="pre">has_rsample</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">True</span></em><a class="headerlink" href="#torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.has_rsample" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.logits">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">logits</span></span><a class="headerlink" href="#torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.logits" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.probs">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">probs</span></span><a class="headerlink" href="#torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.probs" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.support">
<span class="sig-name descname"><span class="pre">support</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">Simplex()</span></em><a class="headerlink" href="#torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.support" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.temperature">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">temperature</span></span><a class="headerlink" href="#torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.temperature" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="studentt">
<h2><span class="hidden-section">StudentT</span><a class="headerlink" href="#studentt" title="Permalink to this heading">#</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="torch.distributions.studentT.StudentT">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torch.distributions.studentT.</span></span><span class="sig-name descname"><span class="pre">StudentT</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">df</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">loc</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">scale</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">validate_args</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/studentT.py#L15"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.studentT.StudentT" title="Permalink to this definition">#</a></dt>
<dd><p>Bases: <a class="reference internal" href="#torch.distributions.distribution.Distribution" title="torch.distributions.distribution.Distribution"><code class="xref py py-class docutils literal notranslate"><span class="pre">Distribution</span></code></a></p>
<p>Creates a Students t-distribution parameterized by degree of
freedom <code class="xref py py-attr docutils literal notranslate"><span class="pre">df</span></code>, mean <code class="xref py py-attr docutils literal notranslate"><span class="pre">loc</span></code> and scale <code class="xref py py-attr docutils literal notranslate"><span class="pre">scale</span></code>.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">StudentT</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">2.0</span><span class="p">]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>  <span class="c1"># Student&#39;s t-distributed with degrees of freedom=2</span>
<span class="go">tensor([ 0.1046])</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>df</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a><em> or </em><a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>)  degrees of freedom</p></li>
<li><p><strong>loc</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a><em> or </em><a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>)  mean of the distribution</p></li>
<li><p><strong>scale</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a><em> or </em><a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>)  scale of the distribution</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="torch.distributions.studentT.StudentT.arg_constraints">
<span class="sig-name descname"><span class="pre">arg_constraints</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">{'df':</span> <span class="pre">GreaterThan(lower_bound=0.0),</span> <span class="pre">'loc':</span> <span class="pre">Real(),</span> <span class="pre">'scale':</span> <span class="pre">GreaterThan(lower_bound=0.0)}</span></em><a class="headerlink" href="#torch.distributions.studentT.StudentT.arg_constraints" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributions.studentT.StudentT.entropy">
<span class="sig-name descname"><span class="pre">entropy</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/studentT.py#L106"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.studentT.StudentT.entropy" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributions.studentT.StudentT.expand">
<span class="sig-name descname"><span class="pre">expand</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch_shape</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">_instance</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/studentT.py#L68"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.studentT.StudentT.expand" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="torch.distributions.studentT.StudentT.has_rsample">
<span class="sig-name descname"><span class="pre">has_rsample</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">True</span></em><a class="headerlink" href="#torch.distributions.studentT.StudentT.has_rsample" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributions.studentT.StudentT.log_prob">
<span class="sig-name descname"><span class="pre">log_prob</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">value</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/studentT.py#L93"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.studentT.StudentT.log_prob" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="torch.distributions.studentT.StudentT.mean">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">mean</span></span><a class="headerlink" href="#torch.distributions.studentT.StudentT.mean" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="torch.distributions.studentT.StudentT.mode">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">mode</span></span><a class="headerlink" href="#torch.distributions.studentT.StudentT.mode" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributions.studentT.StudentT.rsample">
<span class="sig-name descname"><span class="pre">rsample</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">sample_shape</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">torch.Size([])</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/studentT.py#L79"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.studentT.StudentT.rsample" title="Permalink to this definition">#</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a></p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="torch.distributions.studentT.StudentT.support">
<span class="sig-name descname"><span class="pre">support</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">Real()</span></em><a class="headerlink" href="#torch.distributions.studentT.StudentT.support" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="torch.distributions.studentT.StudentT.variance">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">variance</span></span><a class="headerlink" href="#torch.distributions.studentT.StudentT.variance" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="transformeddistribution">
<h2><span class="hidden-section">TransformedDistribution</span><a class="headerlink" href="#transformeddistribution" title="Permalink to this heading">#</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="torch.distributions.transformed_distribution.TransformedDistribution">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torch.distributions.transformed_distribution.</span></span><span class="sig-name descname"><span class="pre">TransformedDistribution</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">base_distribution</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">transforms</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">validate_args</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/transformed_distribution.py#L16"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.transformed_distribution.TransformedDistribution" title="Permalink to this definition">#</a></dt>
<dd><p>Bases: <a class="reference internal" href="#torch.distributions.distribution.Distribution" title="torch.distributions.distribution.Distribution"><code class="xref py py-class docutils literal notranslate"><span class="pre">Distribution</span></code></a></p>
<p>Extension of the Distribution class, which applies a sequence of Transforms
to a base distribution.  Let f be the composition of transforms applied:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">X</span> <span class="o">~</span> <span class="n">BaseDistribution</span>
<span class="n">Y</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="o">~</span> <span class="n">TransformedDistribution</span><span class="p">(</span><span class="n">BaseDistribution</span><span class="p">,</span> <span class="n">f</span><span class="p">)</span>
<span class="n">log</span> <span class="n">p</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span> <span class="o">=</span> <span class="n">log</span> <span class="n">p</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="o">+</span> <span class="n">log</span> <span class="o">|</span><span class="n">det</span> <span class="p">(</span><span class="n">dX</span><span class="o">/</span><span class="n">dY</span><span class="p">)</span><span class="o">|</span>
</pre></div>
</div>
<p>Note that the <code class="docutils literal notranslate"><span class="pre">.event_shape</span></code> of a <a class="reference internal" href="#torch.distributions.transformed_distribution.TransformedDistribution" title="torch.distributions.transformed_distribution.TransformedDistribution"><code class="xref py py-class docutils literal notranslate"><span class="pre">TransformedDistribution</span></code></a> is the
maximum shape of its base distribution and its transforms, since transforms
can introduce correlations among events.</p>
<p>An example for the usage of <a class="reference internal" href="#torch.distributions.transformed_distribution.TransformedDistribution" title="torch.distributions.transformed_distribution.TransformedDistribution"><code class="xref py py-class docutils literal notranslate"><span class="pre">TransformedDistribution</span></code></a> would be:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Building a Logistic Distribution</span>
<span class="c1"># X ~ Uniform(0, 1)</span>
<span class="c1"># f = a + b * logit(X)</span>
<span class="c1"># Y ~ f(X) ~ Logistic(a, b)</span>
<span class="n">base_distribution</span> <span class="o">=</span> <span class="n">Uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">transforms</span> <span class="o">=</span> <span class="p">[</span><span class="n">SigmoidTransform</span><span class="p">()</span><span class="o">.</span><span class="n">inv</span><span class="p">,</span> <span class="n">AffineTransform</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="n">a</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">b</span><span class="p">)]</span>
<span class="n">logistic</span> <span class="o">=</span> <span class="n">TransformedDistribution</span><span class="p">(</span><span class="n">base_distribution</span><span class="p">,</span> <span class="n">transforms</span><span class="p">)</span>
</pre></div>
</div>
<p>For more examples, please look at the implementations of
<a class="reference internal" href="#torch.distributions.gumbel.Gumbel" title="torch.distributions.gumbel.Gumbel"><code class="xref py py-class docutils literal notranslate"><span class="pre">Gumbel</span></code></a>,
<a class="reference internal" href="#torch.distributions.half_cauchy.HalfCauchy" title="torch.distributions.half_cauchy.HalfCauchy"><code class="xref py py-class docutils literal notranslate"><span class="pre">HalfCauchy</span></code></a>,
<a class="reference internal" href="#torch.distributions.half_normal.HalfNormal" title="torch.distributions.half_normal.HalfNormal"><code class="xref py py-class docutils literal notranslate"><span class="pre">HalfNormal</span></code></a>,
<a class="reference internal" href="#torch.distributions.log_normal.LogNormal" title="torch.distributions.log_normal.LogNormal"><code class="xref py py-class docutils literal notranslate"><span class="pre">LogNormal</span></code></a>,
<a class="reference internal" href="#torch.distributions.pareto.Pareto" title="torch.distributions.pareto.Pareto"><code class="xref py py-class docutils literal notranslate"><span class="pre">Pareto</span></code></a>,
<a class="reference internal" href="#torch.distributions.weibull.Weibull" title="torch.distributions.weibull.Weibull"><code class="xref py py-class docutils literal notranslate"><span class="pre">Weibull</span></code></a>,
<a class="reference internal" href="#torch.distributions.relaxed_bernoulli.RelaxedBernoulli" title="torch.distributions.relaxed_bernoulli.RelaxedBernoulli"><code class="xref py py-class docutils literal notranslate"><span class="pre">RelaxedBernoulli</span></code></a> and
<a class="reference internal" href="#torch.distributions.relaxed_categorical.RelaxedOneHotCategorical" title="torch.distributions.relaxed_categorical.RelaxedOneHotCategorical"><code class="xref py py-class docutils literal notranslate"><span class="pre">RelaxedOneHotCategorical</span></code></a></p>
<dl class="py attribute">
<dt class="sig sig-object py" id="torch.distributions.transformed_distribution.TransformedDistribution.arg_constraints">
<span class="sig-name descname"><span class="pre">arg_constraints</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Dict" title="(in Python v3.13)"><span class="pre">Dict</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference internal" href="#torch.distributions.constraints.Constraint" title="torch.distributions.constraints.Constraint"><span class="pre">Constraint</span></a><span class="p"><span class="pre">]</span></span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">{}</span></em><a class="headerlink" href="#torch.distributions.transformed_distribution.TransformedDistribution.arg_constraints" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributions.transformed_distribution.TransformedDistribution.cdf">
<span class="sig-name descname"><span class="pre">cdf</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">value</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/transformed_distribution.py#L194"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.transformed_distribution.TransformedDistribution.cdf" title="Permalink to this definition">#</a></dt>
<dd><p>Computes the cumulative distribution function by inverting the
transform(s) and computing the score of the base distribution.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributions.transformed_distribution.TransformedDistribution.expand">
<span class="sig-name descname"><span class="pre">expand</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch_shape</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">_instance</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/transformed_distribution.py#L103"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.transformed_distribution.TransformedDistribution.expand" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="torch.distributions.transformed_distribution.TransformedDistribution.has_rsample">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">has_rsample</span></span><a class="headerlink" href="#torch.distributions.transformed_distribution.TransformedDistribution.has_rsample" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributions.transformed_distribution.TransformedDistribution.icdf">
<span class="sig-name descname"><span class="pre">icdf</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">value</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/transformed_distribution.py#L207"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.transformed_distribution.TransformedDistribution.icdf" title="Permalink to this definition">#</a></dt>
<dd><p>Computes the inverse cumulative distribution function using
transform(s) and computing the score of the base distribution.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributions.transformed_distribution.TransformedDistribution.log_prob">
<span class="sig-name descname"><span class="pre">log_prob</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">value</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/transformed_distribution.py#L158"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.transformed_distribution.TransformedDistribution.log_prob" title="Permalink to this definition">#</a></dt>
<dd><p>Scores the sample by inverting the transform(s) and computing the score
using the score of the base distribution and the log abs det jacobian.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributions.transformed_distribution.TransformedDistribution.rsample">
<span class="sig-name descname"><span class="pre">rsample</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">sample_shape</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">torch.Size([])</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/transformed_distribution.py#L146"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.transformed_distribution.TransformedDistribution.rsample" title="Permalink to this definition">#</a></dt>
<dd><p>Generates a sample_shape shaped reparameterized sample or sample_shape
shaped batch of reparameterized samples if the distribution parameters
are batched. Samples first from base distribution and applies
<cite>transform()</cite> for every transform in the list.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributions.transformed_distribution.TransformedDistribution.sample">
<span class="sig-name descname"><span class="pre">sample</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">sample_shape</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">torch.Size([])</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/transformed_distribution.py#L133"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.transformed_distribution.TransformedDistribution.sample" title="Permalink to this definition">#</a></dt>
<dd><p>Generates a sample_shape shaped sample or sample_shape shaped batch of
samples if the distribution parameters are batched. Samples first from
base distribution and applies <cite>transform()</cite> for every transform in the
list.</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="torch.distributions.transformed_distribution.TransformedDistribution.support">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">support</span></span><a class="headerlink" href="#torch.distributions.transformed_distribution.TransformedDistribution.support" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="uniform">
<h2><span class="hidden-section">Uniform</span><a class="headerlink" href="#uniform" title="Permalink to this heading">#</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="torch.distributions.uniform.Uniform">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torch.distributions.uniform.</span></span><span class="sig-name descname"><span class="pre">Uniform</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">low</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">high</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">validate_args</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/uniform.py#L15"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.uniform.Uniform" title="Permalink to this definition">#</a></dt>
<dd><p>Bases: <a class="reference internal" href="#torch.distributions.distribution.Distribution" title="torch.distributions.distribution.Distribution"><code class="xref py py-class docutils literal notranslate"><span class="pre">Distribution</span></code></a></p>
<p>Generates uniformly distributed random samples from the half-open interval
<code class="docutils literal notranslate"><span class="pre">[low,</span> <span class="pre">high)</span></code>.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">Uniform</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.0</span><span class="p">]),</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">5.0</span><span class="p">]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>  <span class="c1"># uniformly distributed in the range [0.0, 5.0)</span>
<span class="go">tensor([ 2.3418])</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>low</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a><em> or </em><a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>)  lower range (inclusive).</p></li>
<li><p><strong>high</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a><em> or </em><a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>)  upper range (exclusive).</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="torch.distributions.uniform.Uniform.arg_constraints">
<span class="sig-name descname"><span class="pre">arg_constraints</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">{'high':</span> <span class="pre">Dependent(),</span> <span class="pre">'low':</span> <span class="pre">Dependent()}</span></em><a class="headerlink" href="#torch.distributions.uniform.Uniform.arg_constraints" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributions.uniform.Uniform.cdf">
<span class="sig-name descname"><span class="pre">cdf</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">value</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/uniform.py#L91"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.uniform.Uniform.cdf" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributions.uniform.Uniform.entropy">
<span class="sig-name descname"><span class="pre">entropy</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/uniform.py#L101"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.uniform.Uniform.entropy" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributions.uniform.Uniform.expand">
<span class="sig-name descname"><span class="pre">expand</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch_shape</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">_instance</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/uniform.py#L66"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.uniform.Uniform.expand" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="torch.distributions.uniform.Uniform.has_rsample">
<span class="sig-name descname"><span class="pre">has_rsample</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">True</span></em><a class="headerlink" href="#torch.distributions.uniform.Uniform.has_rsample" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributions.uniform.Uniform.icdf">
<span class="sig-name descname"><span class="pre">icdf</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">value</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/uniform.py#L97"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.uniform.Uniform.icdf" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributions.uniform.Uniform.log_prob">
<span class="sig-name descname"><span class="pre">log_prob</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">value</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/uniform.py#L84"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.uniform.Uniform.log_prob" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="torch.distributions.uniform.Uniform.mean">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">mean</span></span><a class="headerlink" href="#torch.distributions.uniform.Uniform.mean" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="torch.distributions.uniform.Uniform.mode">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">mode</span></span><a class="headerlink" href="#torch.distributions.uniform.Uniform.mode" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributions.uniform.Uniform.rsample">
<span class="sig-name descname"><span class="pre">rsample</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">sample_shape</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">torch.Size([])</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/uniform.py#L79"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.uniform.Uniform.rsample" title="Permalink to this definition">#</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a></p>
</dd>
</dl>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="torch.distributions.uniform.Uniform.stddev">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">stddev</span></span><a class="headerlink" href="#torch.distributions.uniform.Uniform.stddev" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="torch.distributions.uniform.Uniform.support">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">support</span></span><a class="headerlink" href="#torch.distributions.uniform.Uniform.support" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="torch.distributions.uniform.Uniform.variance">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">variance</span></span><a class="headerlink" href="#torch.distributions.uniform.Uniform.variance" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="vonmises">
<h2><span class="hidden-section">VonMises</span><a class="headerlink" href="#vonmises" title="Permalink to this heading">#</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="torch.distributions.von_mises.VonMises">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torch.distributions.von_mises.</span></span><span class="sig-name descname"><span class="pre">VonMises</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">loc</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">concentration</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">validate_args</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/von_mises.py#L106"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.von_mises.VonMises" title="Permalink to this definition">#</a></dt>
<dd><p>Bases: <a class="reference internal" href="#torch.distributions.distribution.Distribution" title="torch.distributions.distribution.Distribution"><code class="xref py py-class docutils literal notranslate"><span class="pre">Distribution</span></code></a></p>
<p>A circular von Mises distribution.</p>
<p>This implementation uses polar coordinates. The <code class="docutils literal notranslate"><span class="pre">loc</span></code> and <code class="docutils literal notranslate"><span class="pre">value</span></code> args
can be any real number (to facilitate unconstrained optimization), but are
interpreted as angles modulo 2 pi.</p>
<dl>
<dt>Example::</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">VonMises</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.0</span><span class="p">]),</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.0</span><span class="p">]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>  <span class="c1"># von Mises distributed with loc=1 and concentration=1</span>
<span class="go">tensor([1.9777])</span>
</pre></div>
</div>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>loc</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>torch.Tensor</em></a>)  an angle in radians.</p></li>
<li><p><strong>concentration</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>torch.Tensor</em></a>)  concentration parameter</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="torch.distributions.von_mises.VonMises.arg_constraints">
<span class="sig-name descname"><span class="pre">arg_constraints</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">{'concentration':</span> <span class="pre">GreaterThan(lower_bound=0.0),</span> <span class="pre">'loc':</span> <span class="pre">Real()}</span></em><a class="headerlink" href="#torch.distributions.von_mises.VonMises.arg_constraints" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributions.von_mises.VonMises.expand">
<span class="sig-name descname"><span class="pre">expand</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch_shape</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">_instance</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/von_mises.py#L180"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.von_mises.VonMises.expand" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="torch.distributions.von_mises.VonMises.has_rsample">
<span class="sig-name descname"><span class="pre">has_rsample</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">False</span></em><a class="headerlink" href="#torch.distributions.von_mises.VonMises.has_rsample" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributions.von_mises.VonMises.log_prob">
<span class="sig-name descname"><span class="pre">log_prob</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">value</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/von_mises.py#L134"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.von_mises.VonMises.log_prob" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="torch.distributions.von_mises.VonMises.mean">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">mean</span></span><a class="headerlink" href="#torch.distributions.von_mises.VonMises.mean" title="Permalink to this definition">#</a></dt>
<dd><p>The provided mean is the circular one.</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="torch.distributions.von_mises.VonMises.mode">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">mode</span></span><a class="headerlink" href="#torch.distributions.von_mises.VonMises.mode" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributions.von_mises.VonMises.sample">
<span class="sig-name descname"><span class="pre">sample</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">sample_shape</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">torch.Size([])</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/von_mises.py#L163"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.von_mises.VonMises.sample" title="Permalink to this definition">#</a></dt>
<dd><p>The sampling algorithm for the von Mises distribution is based on the
following paper: D.J. Best and N.I. Fisher, Efficient simulation of the
von Mises distribution. Applied Statistics (1979): 152-157.</p>
<p>Sampling is always done in double precision internally to avoid a hang
in _rejection_sample() for small values of the concentration, which
starts to happen for single precision around 1e-4 (see issue #88443).</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="torch.distributions.von_mises.VonMises.support">
<span class="sig-name descname"><span class="pre">support</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">Real()</span></em><a class="headerlink" href="#torch.distributions.von_mises.VonMises.support" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="torch.distributions.von_mises.VonMises.variance">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">variance</span></span><a class="headerlink" href="#torch.distributions.von_mises.VonMises.variance" title="Permalink to this definition">#</a></dt>
<dd><p>The provided variance is the circular one.</p>
</dd></dl>

</dd></dl>

</section>
<section id="weibull">
<h2><span class="hidden-section">Weibull</span><a class="headerlink" href="#weibull" title="Permalink to this heading">#</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="torch.distributions.weibull.Weibull">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torch.distributions.weibull.</span></span><span class="sig-name descname"><span class="pre">Weibull</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">scale</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">concentration</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">validate_args</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/weibull.py#L14"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.weibull.Weibull" title="Permalink to this definition">#</a></dt>
<dd><p>Bases: <a class="reference internal" href="#torch.distributions.transformed_distribution.TransformedDistribution" title="torch.distributions.transformed_distribution.TransformedDistribution"><code class="xref py py-class docutils literal notranslate"><span class="pre">TransformedDistribution</span></code></a></p>
<p>Samples from a two-parameter Weibull distribution.</p>
<p class="rubric">Example</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">Weibull</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.0</span><span class="p">]),</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.0</span><span class="p">]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>  <span class="c1"># sample from a Weibull distribution with scale=1, concentration=1</span>
<span class="go">tensor([ 0.4784])</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>scale</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a><em> or </em><a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>)  Scale parameter of distribution (lambda).</p></li>
<li><p><strong>concentration</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a><em> or </em><a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>)  Concentration parameter of distribution (k/shape).</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="torch.distributions.weibull.Weibull.arg_constraints">
<span class="sig-name descname"><span class="pre">arg_constraints</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference internal" href="#torch.distributions.constraints.Constraint" title="torch.distributions.constraints.Constraint"><span class="pre">constraints.Constraint</span></a><span class="p"><span class="pre">]</span></span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">{'concentration':</span> <span class="pre">GreaterThan(lower_bound=0.0),</span> <span class="pre">'scale':</span> <span class="pre">GreaterThan(lower_bound=0.0)}</span></em><a class="headerlink" href="#torch.distributions.weibull.Weibull.arg_constraints" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributions.weibull.Weibull.entropy">
<span class="sig-name descname"><span class="pre">entropy</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/weibull.py#L80"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.weibull.Weibull.entropy" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributions.weibull.Weibull.expand">
<span class="sig-name descname"><span class="pre">expand</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch_shape</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">_instance</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/weibull.py#L47"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.weibull.Weibull.expand" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="torch.distributions.weibull.Weibull.mean">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">mean</span></span><a class="headerlink" href="#torch.distributions.weibull.Weibull.mean" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="torch.distributions.weibull.Weibull.mode">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">mode</span></span><a class="headerlink" href="#torch.distributions.weibull.Weibull.mode" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="torch.distributions.weibull.Weibull.support">
<span class="sig-name descname"><span class="pre">support</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">GreaterThan(lower_bound=0.0)</span></em><a class="headerlink" href="#torch.distributions.weibull.Weibull.support" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="torch.distributions.weibull.Weibull.variance">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">variance</span></span><a class="headerlink" href="#torch.distributions.weibull.Weibull.variance" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="wishart">
<h2><span class="hidden-section">Wishart</span><a class="headerlink" href="#wishart" title="Permalink to this heading">#</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="torch.distributions.wishart.Wishart">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torch.distributions.wishart.</span></span><span class="sig-name descname"><span class="pre">Wishart</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">df</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">covariance_matrix</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">precision_matrix</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">scale_tril</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">validate_args</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/wishart.py#L34"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.wishart.Wishart" title="Permalink to this definition">#</a></dt>
<dd><p>Bases: <a class="reference internal" href="#torch.distributions.exp_family.ExponentialFamily" title="torch.distributions.exp_family.ExponentialFamily"><code class="xref py py-class docutils literal notranslate"><span class="pre">ExponentialFamily</span></code></a></p>
<p>Creates a Wishart distribution parameterized by a symmetric positive definite matrix <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal"></mi></mrow><annotation encoding="application/x-tex">\Sigma</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord"></span></span></span></span></span>,
or its Cholesky decomposition <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold"></mi><mo>=</mo><mi mathvariant="bold">L</mi><msup><mi mathvariant="bold">L</mi><mi mathvariant="normal"></mi></msup></mrow><annotation encoding="application/x-tex">\mathbf{\Sigma} = \mathbf{L}\mathbf{L}^\top</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6861em;"></span><span class="mord mathbf"></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.8491em;"></span><span class="mord mathbf">L</span><span class="mord"><span class="mord mathbf">L</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8491em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"></span></span></span></span></span></span></span></span></span></span></span></span></p>
<p class="rubric">Example</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">Wishart</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([</span><span class="mi">2</span><span class="p">]),</span> <span class="n">covariance_matrix</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>  <span class="c1"># Wishart distributed with mean=`df * I` and</span>
<span class="gp">&gt;&gt;&gt; </span>            <span class="c1"># variance(x_ij)=`df` for i != j and variance(x_ij)=`2 * df` for i == j</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>df</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a><em> or </em><a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>)  real-valued parameter larger than the (dimension of Square matrix) - 1</p></li>
<li><p><strong>covariance_matrix</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>)  positive-definite covariance matrix</p></li>
<li><p><strong>precision_matrix</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>)  positive-definite precision matrix</p></li>
<li><p><strong>scale_tril</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>)  lower-triangular factor of covariance, with positive-valued diagonal</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Only one of <a class="reference internal" href="#torch.distributions.wishart.Wishart.covariance_matrix" title="torch.distributions.wishart.Wishart.covariance_matrix"><code class="xref py py-attr docutils literal notranslate"><span class="pre">covariance_matrix</span></code></a> or <a class="reference internal" href="#torch.distributions.wishart.Wishart.precision_matrix" title="torch.distributions.wishart.Wishart.precision_matrix"><code class="xref py py-attr docutils literal notranslate"><span class="pre">precision_matrix</span></code></a> or
<a class="reference internal" href="#torch.distributions.wishart.Wishart.scale_tril" title="torch.distributions.wishart.Wishart.scale_tril"><code class="xref py py-attr docutils literal notranslate"><span class="pre">scale_tril</span></code></a> can be specified.
Using <a class="reference internal" href="#torch.distributions.wishart.Wishart.scale_tril" title="torch.distributions.wishart.Wishart.scale_tril"><code class="xref py py-attr docutils literal notranslate"><span class="pre">scale_tril</span></code></a> will be more efficient: all computations internally
are based on <a class="reference internal" href="#torch.distributions.wishart.Wishart.scale_tril" title="torch.distributions.wishart.Wishart.scale_tril"><code class="xref py py-attr docutils literal notranslate"><span class="pre">scale_tril</span></code></a>. If <a class="reference internal" href="#torch.distributions.wishart.Wishart.covariance_matrix" title="torch.distributions.wishart.Wishart.covariance_matrix"><code class="xref py py-attr docutils literal notranslate"><span class="pre">covariance_matrix</span></code></a> or
<a class="reference internal" href="#torch.distributions.wishart.Wishart.precision_matrix" title="torch.distributions.wishart.Wishart.precision_matrix"><code class="xref py py-attr docutils literal notranslate"><span class="pre">precision_matrix</span></code></a> is passed instead, it is only used to compute
the corresponding lower triangular matrices using a Cholesky decomposition.
torch.distributions.LKJCholesky is a restricted Wishart distribution.[1]</p>
</div>
<p><strong>References</strong></p>
<p>[1] Wang, Z., Wu, Y. and Chu, H., 2018. <cite>On equivalence of the LKJ distribution and the restricted Wishart distribution</cite>.
[2] Sawyer, S., 2007. <cite>Wishart Distributions and Inverse-Wishart Sampling</cite>.
[3] Anderson, T. W., 2003. <cite>An Introduction to Multivariate Statistical Analysis (3rd ed.)</cite>.
[4] Odell, P. L. &amp; Feiveson, A. H., 1966. <cite>A Numerical Procedure to Generate a SampleCovariance Matrix</cite>. JASA, 61(313):199-203.
[5] Ku, Y.-C. &amp; Bloomfield, P., 2010. <cite>Generating Random Wishart Matrices with Fractional Degrees of Freedom in OX</cite>.</p>
<dl class="py attribute">
<dt class="sig sig-object py" id="torch.distributions.wishart.Wishart.arg_constraints">
<span class="sig-name descname"><span class="pre">arg_constraints</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">{'covariance_matrix':</span> <span class="pre">PositiveDefinite(),</span> <span class="pre">'df':</span> <span class="pre">GreaterThan(lower_bound=0),</span> <span class="pre">'precision_matrix':</span> <span class="pre">PositiveDefinite(),</span> <span class="pre">'scale_tril':</span> <span class="pre">LowerCholesky()}</span></em><a class="headerlink" href="#torch.distributions.wishart.Wishart.arg_constraints" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="torch.distributions.wishart.Wishart.covariance_matrix">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">covariance_matrix</span></span><a class="headerlink" href="#torch.distributions.wishart.Wishart.covariance_matrix" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributions.wishart.Wishart.entropy">
<span class="sig-name descname"><span class="pre">entropy</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/wishart.py#L312"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.wishart.Wishart.entropy" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributions.wishart.Wishart.expand">
<span class="sig-name descname"><span class="pre">expand</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch_shape</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">_instance</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/wishart.py#L148"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.wishart.Wishart.expand" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="torch.distributions.wishart.Wishart.has_rsample">
<span class="sig-name descname"><span class="pre">has_rsample</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">True</span></em><a class="headerlink" href="#torch.distributions.wishart.Wishart.has_rsample" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributions.wishart.Wishart.log_prob">
<span class="sig-name descname"><span class="pre">log_prob</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">value</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/wishart.py#L291"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.wishart.Wishart.log_prob" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="torch.distributions.wishart.Wishart.mean">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">mean</span></span><a class="headerlink" href="#torch.distributions.wishart.Wishart.mean" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="torch.distributions.wishart.Wishart.mode">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">mode</span></span><a class="headerlink" href="#torch.distributions.wishart.Wishart.mode" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="torch.distributions.wishart.Wishart.precision_matrix">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">precision_matrix</span></span><a class="headerlink" href="#torch.distributions.wishart.Wishart.precision_matrix" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributions.wishart.Wishart.rsample">
<span class="sig-name descname"><span class="pre">rsample</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">sample_shape</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">torch.Size([])</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_try_correction</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/wishart.py#L239"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.wishart.Wishart.rsample" title="Permalink to this definition">#</a></dt>
<dd><div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>In some cases, sampling algorithm based on Bartlett decomposition may return singular matrix samples.
Several tries to correct singular samples are performed by default, but it may end up returning
singular matrix samples. Singular samples may return <cite>-inf</cite> values in <cite>.log_prob()</cite>.
In those cases, the user should validate the samples and either fix the value of <cite>df</cite>
or adjust <cite>max_try_correction</cite> value for argument in <cite>.rsample</cite> accordingly.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a></p>
</dd>
</dl>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="torch.distributions.wishart.Wishart.scale_tril">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">scale_tril</span></span><a class="headerlink" href="#torch.distributions.wishart.Wishart.scale_tril" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="torch.distributions.wishart.Wishart.support">
<span class="sig-name descname"><span class="pre">support</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">PositiveDefinite()</span></em><a class="headerlink" href="#torch.distributions.wishart.Wishart.support" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="torch.distributions.wishart.Wishart.variance">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">variance</span></span><a class="headerlink" href="#torch.distributions.wishart.Wishart.variance" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="module-torch.distributions.kl">
<span id="kl-divergence"></span><h2><cite>KL Divergence</cite><a class="headerlink" href="#module-torch.distributions.kl" title="Permalink to this heading">#</a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="torch.distributions.kl.kl_divergence">
<span class="sig-prename descclassname"><span class="pre">torch.distributions.kl.</span></span><span class="sig-name descname"><span class="pre">kl_divergence</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">p</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">q</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/kl.py#L164"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.kl.kl_divergence" title="Permalink to this definition">#</a></dt>
<dd><p>Compute Kullback-Leibler divergence <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>K</mi><mi>L</mi><mo stretchy="false">(</mo><mi>p</mi><mi mathvariant="normal"></mi><mi>q</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">KL(p \| q)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">K</span><span class="mord mathnormal">L</span><span class="mopen">(</span><span class="mord mathnormal">p</span><span class="mord"></span><span class="mord mathnormal" style="margin-right:0.03588em;">q</span><span class="mclose">)</span></span></span></span></span> between two distributions.</p>
<div class="math">
<span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>K</mi><mi>L</mi><mo stretchy="false">(</mo><mi>p</mi><mi mathvariant="normal"></mi><mi>q</mi><mo stretchy="false">)</mo><mo>=</mo><mo></mo><mi>p</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mi>log</mi><mo></mo><mfrac><mrow><mi>p</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><mrow><mi>q</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow></mfrac><mtext></mtext><mi>d</mi><mi>x</mi></mrow><annotation encoding="application/x-tex">KL(p \| q) = \int p(x) \log\frac {p(x)} {q(x)} \,dx</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">K</span><span class="mord mathnormal">L</span><span class="mopen">(</span><span class="mord mathnormal">p</span><span class="mord"></span><span class="mord mathnormal" style="margin-right:0.03588em;">q</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:2.363em;vertical-align:-0.936em;"></span><span class="mop op-symbol large-op" style="margin-right:0.44445em;position:relative;top:-0.0011em;"></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal">p</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mop">lo<span style="margin-right:0.01389em;">g</span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.427em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">q</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal">p</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span></span></span></span><span class="vlist-s"></span></span><span class="vlist-r"><span class="vlist" style="height:0.936em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal">d</span><span class="mord mathnormal">x</span></span></span></span></span></div><dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>p</strong> (<a class="reference internal" href="#torch.distributions.distribution.Distribution" title="torch.distributions.distribution.Distribution"><em>Distribution</em></a>)  A <code class="xref py py-class docutils literal notranslate"><span class="pre">Distribution</span></code> object.</p></li>
<li><p><strong>q</strong> (<a class="reference internal" href="#torch.distributions.distribution.Distribution" title="torch.distributions.distribution.Distribution"><em>Distribution</em></a>)  A <code class="xref py py-class docutils literal notranslate"><span class="pre">Distribution</span></code> object.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A batch of KL divergences of shape <cite>batch_shape</cite>.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a></p>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><p><a class="reference external" href="https://docs.python.org/3/library/exceptions.html#NotImplementedError" title="(in Python v3.13)"><strong>NotImplementedError</strong></a>  If the distribution types have not been registered via
    <a class="reference internal" href="#torch.distributions.kl.register_kl" title="torch.distributions.kl.register_kl"><code class="xref py py-meth docutils literal notranslate"><span class="pre">register_kl()</span></code></a>.</p>
</dd>
</dl>
<dl class="simple">
<dt>KL divergence is currently implemented for the following distribution pairs:</dt><dd><ul class="simple">
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Bernoulli</span></code> and <code class="xref py py-class docutils literal notranslate"><span class="pre">Bernoulli</span></code></p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Bernoulli</span></code> and <code class="xref py py-class docutils literal notranslate"><span class="pre">Poisson</span></code></p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Beta</span></code> and <code class="xref py py-class docutils literal notranslate"><span class="pre">Beta</span></code></p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Beta</span></code> and <code class="xref py py-class docutils literal notranslate"><span class="pre">ContinuousBernoulli</span></code></p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Beta</span></code> and <code class="xref py py-class docutils literal notranslate"><span class="pre">Exponential</span></code></p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Beta</span></code> and <code class="xref py py-class docutils literal notranslate"><span class="pre">Gamma</span></code></p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Beta</span></code> and <code class="xref py py-class docutils literal notranslate"><span class="pre">Normal</span></code></p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Beta</span></code> and <code class="xref py py-class docutils literal notranslate"><span class="pre">Pareto</span></code></p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Beta</span></code> and <code class="xref py py-class docutils literal notranslate"><span class="pre">Uniform</span></code></p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Binomial</span></code> and <code class="xref py py-class docutils literal notranslate"><span class="pre">Binomial</span></code></p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Categorical</span></code> and <code class="xref py py-class docutils literal notranslate"><span class="pre">Categorical</span></code></p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Cauchy</span></code> and <code class="xref py py-class docutils literal notranslate"><span class="pre">Cauchy</span></code></p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">ContinuousBernoulli</span></code> and <code class="xref py py-class docutils literal notranslate"><span class="pre">ContinuousBernoulli</span></code></p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">ContinuousBernoulli</span></code> and <code class="xref py py-class docutils literal notranslate"><span class="pre">Exponential</span></code></p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">ContinuousBernoulli</span></code> and <code class="xref py py-class docutils literal notranslate"><span class="pre">Normal</span></code></p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">ContinuousBernoulli</span></code> and <code class="xref py py-class docutils literal notranslate"><span class="pre">Pareto</span></code></p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">ContinuousBernoulli</span></code> and <code class="xref py py-class docutils literal notranslate"><span class="pre">Uniform</span></code></p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Dirichlet</span></code> and <code class="xref py py-class docutils literal notranslate"><span class="pre">Dirichlet</span></code></p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Exponential</span></code> and <code class="xref py py-class docutils literal notranslate"><span class="pre">Beta</span></code></p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Exponential</span></code> and <code class="xref py py-class docutils literal notranslate"><span class="pre">ContinuousBernoulli</span></code></p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Exponential</span></code> and <code class="xref py py-class docutils literal notranslate"><span class="pre">Exponential</span></code></p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Exponential</span></code> and <code class="xref py py-class docutils literal notranslate"><span class="pre">Gamma</span></code></p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Exponential</span></code> and <code class="xref py py-class docutils literal notranslate"><span class="pre">Gumbel</span></code></p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Exponential</span></code> and <code class="xref py py-class docutils literal notranslate"><span class="pre">Normal</span></code></p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Exponential</span></code> and <code class="xref py py-class docutils literal notranslate"><span class="pre">Pareto</span></code></p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Exponential</span></code> and <code class="xref py py-class docutils literal notranslate"><span class="pre">Uniform</span></code></p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">ExponentialFamily</span></code> and <code class="xref py py-class docutils literal notranslate"><span class="pre">ExponentialFamily</span></code></p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Gamma</span></code> and <code class="xref py py-class docutils literal notranslate"><span class="pre">Beta</span></code></p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Gamma</span></code> and <code class="xref py py-class docutils literal notranslate"><span class="pre">ContinuousBernoulli</span></code></p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Gamma</span></code> and <code class="xref py py-class docutils literal notranslate"><span class="pre">Exponential</span></code></p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Gamma</span></code> and <code class="xref py py-class docutils literal notranslate"><span class="pre">Gamma</span></code></p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Gamma</span></code> and <code class="xref py py-class docutils literal notranslate"><span class="pre">Gumbel</span></code></p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Gamma</span></code> and <code class="xref py py-class docutils literal notranslate"><span class="pre">Normal</span></code></p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Gamma</span></code> and <code class="xref py py-class docutils literal notranslate"><span class="pre">Pareto</span></code></p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Gamma</span></code> and <code class="xref py py-class docutils literal notranslate"><span class="pre">Uniform</span></code></p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Geometric</span></code> and <code class="xref py py-class docutils literal notranslate"><span class="pre">Geometric</span></code></p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Gumbel</span></code> and <code class="xref py py-class docutils literal notranslate"><span class="pre">Beta</span></code></p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Gumbel</span></code> and <code class="xref py py-class docutils literal notranslate"><span class="pre">ContinuousBernoulli</span></code></p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Gumbel</span></code> and <code class="xref py py-class docutils literal notranslate"><span class="pre">Exponential</span></code></p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Gumbel</span></code> and <code class="xref py py-class docutils literal notranslate"><span class="pre">Gamma</span></code></p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Gumbel</span></code> and <code class="xref py py-class docutils literal notranslate"><span class="pre">Gumbel</span></code></p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Gumbel</span></code> and <code class="xref py py-class docutils literal notranslate"><span class="pre">Normal</span></code></p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Gumbel</span></code> and <code class="xref py py-class docutils literal notranslate"><span class="pre">Pareto</span></code></p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Gumbel</span></code> and <code class="xref py py-class docutils literal notranslate"><span class="pre">Uniform</span></code></p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">HalfNormal</span></code> and <code class="xref py py-class docutils literal notranslate"><span class="pre">HalfNormal</span></code></p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Independent</span></code> and <code class="xref py py-class docutils literal notranslate"><span class="pre">Independent</span></code></p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Laplace</span></code> and <code class="xref py py-class docutils literal notranslate"><span class="pre">Beta</span></code></p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Laplace</span></code> and <code class="xref py py-class docutils literal notranslate"><span class="pre">ContinuousBernoulli</span></code></p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Laplace</span></code> and <code class="xref py py-class docutils literal notranslate"><span class="pre">Exponential</span></code></p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Laplace</span></code> and <code class="xref py py-class docutils literal notranslate"><span class="pre">Gamma</span></code></p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Laplace</span></code> and <code class="xref py py-class docutils literal notranslate"><span class="pre">Laplace</span></code></p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Laplace</span></code> and <code class="xref py py-class docutils literal notranslate"><span class="pre">Normal</span></code></p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Laplace</span></code> and <code class="xref py py-class docutils literal notranslate"><span class="pre">Pareto</span></code></p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Laplace</span></code> and <code class="xref py py-class docutils literal notranslate"><span class="pre">Uniform</span></code></p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">LowRankMultivariateNormal</span></code> and <code class="xref py py-class docutils literal notranslate"><span class="pre">LowRankMultivariateNormal</span></code></p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">LowRankMultivariateNormal</span></code> and <code class="xref py py-class docutils literal notranslate"><span class="pre">MultivariateNormal</span></code></p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">MultivariateNormal</span></code> and <code class="xref py py-class docutils literal notranslate"><span class="pre">LowRankMultivariateNormal</span></code></p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">MultivariateNormal</span></code> and <code class="xref py py-class docutils literal notranslate"><span class="pre">MultivariateNormal</span></code></p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Normal</span></code> and <code class="xref py py-class docutils literal notranslate"><span class="pre">Beta</span></code></p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Normal</span></code> and <code class="xref py py-class docutils literal notranslate"><span class="pre">ContinuousBernoulli</span></code></p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Normal</span></code> and <code class="xref py py-class docutils literal notranslate"><span class="pre">Exponential</span></code></p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Normal</span></code> and <code class="xref py py-class docutils literal notranslate"><span class="pre">Gamma</span></code></p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Normal</span></code> and <code class="xref py py-class docutils literal notranslate"><span class="pre">Gumbel</span></code></p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Normal</span></code> and <code class="xref py py-class docutils literal notranslate"><span class="pre">Laplace</span></code></p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Normal</span></code> and <code class="xref py py-class docutils literal notranslate"><span class="pre">Normal</span></code></p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Normal</span></code> and <code class="xref py py-class docutils literal notranslate"><span class="pre">Pareto</span></code></p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Normal</span></code> and <code class="xref py py-class docutils literal notranslate"><span class="pre">Uniform</span></code></p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">OneHotCategorical</span></code> and <code class="xref py py-class docutils literal notranslate"><span class="pre">OneHotCategorical</span></code></p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Pareto</span></code> and <code class="xref py py-class docutils literal notranslate"><span class="pre">Beta</span></code></p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Pareto</span></code> and <code class="xref py py-class docutils literal notranslate"><span class="pre">ContinuousBernoulli</span></code></p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Pareto</span></code> and <code class="xref py py-class docutils literal notranslate"><span class="pre">Exponential</span></code></p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Pareto</span></code> and <code class="xref py py-class docutils literal notranslate"><span class="pre">Gamma</span></code></p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Pareto</span></code> and <code class="xref py py-class docutils literal notranslate"><span class="pre">Normal</span></code></p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Pareto</span></code> and <code class="xref py py-class docutils literal notranslate"><span class="pre">Pareto</span></code></p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Pareto</span></code> and <code class="xref py py-class docutils literal notranslate"><span class="pre">Uniform</span></code></p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Poisson</span></code> and <code class="xref py py-class docutils literal notranslate"><span class="pre">Bernoulli</span></code></p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Poisson</span></code> and <code class="xref py py-class docutils literal notranslate"><span class="pre">Binomial</span></code></p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Poisson</span></code> and <code class="xref py py-class docutils literal notranslate"><span class="pre">Poisson</span></code></p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">TransformedDistribution</span></code> and <code class="xref py py-class docutils literal notranslate"><span class="pre">TransformedDistribution</span></code></p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Uniform</span></code> and <code class="xref py py-class docutils literal notranslate"><span class="pre">Beta</span></code></p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Uniform</span></code> and <code class="xref py py-class docutils literal notranslate"><span class="pre">ContinuousBernoulli</span></code></p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Uniform</span></code> and <code class="xref py py-class docutils literal notranslate"><span class="pre">Exponential</span></code></p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Uniform</span></code> and <code class="xref py py-class docutils literal notranslate"><span class="pre">Gamma</span></code></p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Uniform</span></code> and <code class="xref py py-class docutils literal notranslate"><span class="pre">Gumbel</span></code></p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Uniform</span></code> and <code class="xref py py-class docutils literal notranslate"><span class="pre">Normal</span></code></p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Uniform</span></code> and <code class="xref py py-class docutils literal notranslate"><span class="pre">Pareto</span></code></p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Uniform</span></code> and <code class="xref py py-class docutils literal notranslate"><span class="pre">Uniform</span></code></p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="torch.distributions.kl.register_kl">
<span class="sig-prename descclassname"><span class="pre">torch.distributions.kl.</span></span><span class="sig-name descname"><span class="pre">register_kl</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">type_p</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">type_q</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/kl.py#L51"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.kl.register_kl" title="Permalink to this definition">#</a></dt>
<dd><p>Decorator to register a pairwise function with <a class="reference internal" href="#torch.distributions.kl.kl_divergence" title="torch.distributions.kl.kl_divergence"><code class="xref py py-meth docutils literal notranslate"><span class="pre">kl_divergence()</span></code></a>.
Usage:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nd">@register_kl</span><span class="p">(</span><span class="n">Normal</span><span class="p">,</span> <span class="n">Normal</span><span class="p">)</span>
<span class="k">def</span><span class="w"> </span><span class="nf">kl_normal_normal</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">q</span><span class="p">):</span>
    <span class="c1"># insert implementation here</span>
</pre></div>
</div>
<p>Lookup returns the most specific (type,type) match ordered by subclass. If
the match is ambiguous, a <cite>RuntimeWarning</cite> is raised. For example to
resolve the ambiguous situation:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nd">@register_kl</span><span class="p">(</span><span class="n">BaseP</span><span class="p">,</span> <span class="n">DerivedQ</span><span class="p">)</span>
<span class="k">def</span><span class="w"> </span><span class="nf">kl_version1</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">q</span><span class="p">):</span> <span class="o">...</span>
<span class="nd">@register_kl</span><span class="p">(</span><span class="n">DerivedP</span><span class="p">,</span> <span class="n">BaseQ</span><span class="p">)</span>
<span class="k">def</span><span class="w"> </span><span class="nf">kl_version2</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">q</span><span class="p">):</span> <span class="o">...</span>
</pre></div>
</div>
<p>you should register a third most-specific implementation, e.g.:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">register_kl</span><span class="p">(</span><span class="n">DerivedP</span><span class="p">,</span> <span class="n">DerivedQ</span><span class="p">)(</span><span class="n">kl_version1</span><span class="p">)</span>  <span class="c1"># Break the tie.</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>type_p</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#type" title="(in Python v3.13)"><em>type</em></a>)  A subclass of <code class="xref py py-class docutils literal notranslate"><span class="pre">Distribution</span></code>.</p></li>
<li><p><strong>type_q</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#type" title="(in Python v3.13)"><em>type</em></a>)  A subclass of <code class="xref py py-class docutils literal notranslate"><span class="pre">Distribution</span></code>.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</section>
<section id="module-torch.distributions.transforms">
<span id="transforms"></span><h2><cite>Transforms</cite><a class="headerlink" href="#module-torch.distributions.transforms" title="Permalink to this heading">#</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="torch.distributions.transforms.AbsTransform">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torch.distributions.transforms.</span></span><span class="sig-name descname"><span class="pre">AbsTransform</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">cache_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/transforms.py#L692"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.transforms.AbsTransform" title="Permalink to this definition">#</a></dt>
<dd><p>Transform via the mapping <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi><mo>=</mo><mi mathvariant="normal"></mi><mi>x</mi><mi mathvariant="normal"></mi></mrow><annotation encoding="application/x-tex">y = |x|</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"></span><span class="mord mathnormal">x</span><span class="mord"></span></span></span></span></span>.</p>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="torch.distributions.transforms.AffineTransform">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torch.distributions.transforms.</span></span><span class="sig-name descname"><span class="pre">AffineTransform</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">loc</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">scale</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">event_dim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cache_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/transforms.py#L709"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.transforms.AffineTransform" title="Permalink to this definition">#</a></dt>
<dd><p>Transform via the pointwise affine mapping <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi><mo>=</mo><mtext>loc</mtext><mo>+</mo><mtext>scale</mtext><mo></mo><mi>x</mi></mrow><annotation encoding="application/x-tex">y = \text{loc} + \text{scale} \times x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.7778em;vertical-align:-0.0833em;"></span><span class="mord text"><span class="mord">loc</span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.7778em;vertical-align:-0.0833em;"></span><span class="mord text"><span class="mord">scale</span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin"></span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">x</span></span></span></span></span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>loc</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a>)  Location parameter.</p></li>
<li><p><strong>scale</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a>)  Scale parameter.</p></li>
<li><p><strong>event_dim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>)  Optional size of <cite>event_shape</cite>. This should be zero
for univariate random variables, 1 for distributions over vectors,
2 for distributions over matrices, etc.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="torch.distributions.transforms.CatTransform">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torch.distributions.transforms.</span></span><span class="sig-name descname"><span class="pre">CatTransform</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">tseq</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lengths</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cache_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/transforms.py#L1026"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.transforms.CatTransform" title="Permalink to this definition">#</a></dt>
<dd><p>Transform functor that applies a sequence of transforms <cite>tseq</cite>
component-wise to each submatrix at <cite>dim</cite>, of length <cite>lengths[dim]</cite>,
in a way compatible with <a class="reference internal" href="generated/torch.cat.html#torch.cat" title="torch.cat"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.cat()</span></code></a>.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">x0</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">torch</span><span class="o">.</span><span class="n">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">)],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">x0</span><span class="p">,</span> <span class="n">x0</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">t0</span> <span class="o">=</span> <span class="n">CatTransform</span><span class="p">([</span><span class="n">ExpTransform</span><span class="p">(),</span> <span class="n">identity_transform</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">lengths</span><span class="o">=</span><span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">])</span>
<span class="n">t</span> <span class="o">=</span> <span class="n">CatTransform</span><span class="p">([</span><span class="n">t0</span><span class="p">,</span> <span class="n">t0</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">lengths</span><span class="o">=</span><span class="p">[</span><span class="mi">20</span><span class="p">,</span> <span class="mi">20</span><span class="p">])</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">t</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="torch.distributions.transforms.ComposeTransform">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torch.distributions.transforms.</span></span><span class="sig-name descname"><span class="pre">ComposeTransform</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">parts</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cache_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/transforms.py#L277"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.transforms.ComposeTransform" title="Permalink to this definition">#</a></dt>
<dd><p>Composes multiple transforms in a chain.
The transforms being composed are responsible for caching.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>parts</strong> (list of <a class="reference internal" href="#torch.distributions.transforms.Transform" title="torch.distributions.transforms.Transform"><code class="xref py py-class docutils literal notranslate"><span class="pre">Transform</span></code></a>)  A list of transforms to compose.</p></li>
<li><p><strong>cache_size</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>)  Size of cache. If zero, no caching is done. If one,
the latest single value is cached. Only 0 and 1 are supported.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="torch.distributions.transforms.CorrCholeskyTransform">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torch.distributions.transforms.</span></span><span class="sig-name descname"><span class="pre">CorrCholeskyTransform</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">cache_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/transforms.py#L811"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.transforms.CorrCholeskyTransform" title="Permalink to this definition">#</a></dt>
<dd><p>Transforms an uncontrained real vector <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">x</span></span></span></span></span> with length <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>D</mi><mo></mo><mo stretchy="false">(</mo><mi>D</mi><mo></mo><mn>1</mn><mo stretchy="false">)</mo><mi mathvariant="normal">/</mi><mn>2</mn></mrow><annotation encoding="application/x-tex">D*(D-1)/2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">D</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin"></span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.02778em;">D</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin"></span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">1</span><span class="mclose">)</span><span class="mord">/2</span></span></span></span></span> into the
Cholesky factor of a D-dimension correlation matrix. This Cholesky factor is a lower
triangular matrix with positive diagonals and unit Euclidean norm for each row.
The transform is processed as follows:</p>
<blockquote>
<div><ol class="arabic simple">
<li><p>First we convert x into a lower triangular matrix in row order.</p></li>
<li><p>For each row <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>X</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">X_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0785em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s"></span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></span> of the lower triangular part, we apply a <em>signed</em> version of
class <a class="reference internal" href="#torch.distributions.transforms.StickBreakingTransform" title="torch.distributions.transforms.StickBreakingTransform"><code class="xref py py-class docutils literal notranslate"><span class="pre">StickBreakingTransform</span></code></a> to transform <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>X</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">X_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0785em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s"></span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></span> into a
unit Euclidean length vector using the following steps:
- Scales into the interval <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mo></mo><mn>1</mn><mo separator="true">,</mo><mn>1</mn><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(-1, 1)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord"></span><span class="mord">1</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord">1</span><span class="mclose">)</span></span></span></span></span> domain: <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>r</mi><mi>i</mi></msub><mo>=</mo><mi>tanh</mi><mo></mo><mo stretchy="false">(</mo><msub><mi>X</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">r_i = \tanh(X_i)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0278em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s"></span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mop">tanh</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0785em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s"></span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span>.
- Transforms into an unsigned domain: <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>z</mi><mi>i</mi></msub><mo>=</mo><msubsup><mi>r</mi><mi>i</mi><mn>2</mn></msubsup></mrow><annotation encoding="application/x-tex">z_i = r_i^2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.04398em;">z</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.044em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s"></span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.0728em;vertical-align:-0.2587em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-2.4413em;margin-left:-0.0278em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s"></span></span><span class="vlist-r"><span class="vlist" style="height:0.2587em;"><span></span></span></span></span></span></span></span></span></span></span>.
- Applies <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>s</mi><mi>i</mi></msub><mo>=</mo><mi>S</mi><mi>t</mi><mi>i</mi><mi>c</mi><mi>k</mi><mi>B</mi><mi>r</mi><mi>e</mi><mi>a</mi><mi>k</mi><mi>i</mi><mi>n</mi><mi>g</mi><mi>T</mi><mi>r</mi><mi>a</mi><mi>n</mi><mi>s</mi><mi>f</mi><mi>o</mi><mi>r</mi><mi>m</mi><mo stretchy="false">(</mo><msub><mi>z</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">s_i = StickBreakingTransform(z_i)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s"></span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">St</span><span class="mord mathnormal">i</span><span class="mord mathnormal">c</span><span class="mord mathnormal" style="margin-right:0.03148em;">k</span><span class="mord mathnormal" style="margin-right:0.05017em;">B</span><span class="mord mathnormal">re</span><span class="mord mathnormal">akin</span><span class="mord mathnormal" style="margin-right:0.03588em;">g</span><span class="mord mathnormal" style="margin-right:0.13889em;">T</span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mord mathnormal">an</span><span class="mord mathnormal">s</span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mord mathnormal" style="margin-right:0.02778em;">or</span><span class="mord mathnormal">m</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.04398em;">z</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.044em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s"></span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span>.
- Transforms back into signed domain: <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>y</mi><mi>i</mi></msub><mo>=</mo><mi>s</mi><mi>i</mi><mi>g</mi><mi>n</mi><mo stretchy="false">(</mo><msub><mi>r</mi><mi>i</mi></msub><mo stretchy="false">)</mo><mo></mo><msqrt><msub><mi>s</mi><mi>i</mi></msub></msqrt></mrow><annotation encoding="application/x-tex">y_i = sign(r_i) * \sqrt{s_i}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s"></span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">s</span><span class="mord mathnormal">i</span><span class="mord mathnormal" style="margin-right:0.03588em;">g</span><span class="mord mathnormal">n</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0278em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s"></span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin"></span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1.04em;vertical-align:-0.3147em;"></span><span class="mord sqrt"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.7253em;"><span class="svg-align" style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord" style="padding-left:0.833em;"><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s"></span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span><span style="top:-2.6853em;"><span class="pstrut" style="height:3em;"></span><span class="hide-tail" style="min-width:0.853em;height:1.08em;"><svg xmlns="http://www.w3.org/2000/svg" width="400em" height="1.08em" viewBox="0 0 400000 1080" preserveAspectRatio="xMinYMin slice"><path d="M95,702
c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14
c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54
c44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10
s173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429
c69,-144,104.5,-217.7,106.5,-221
l0 -0
c5.3,-9.3,12,-14,20,-14
H400000v40H845.2724
s-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7
c-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z
M834 80h400000v40h-400000z"/></svg></span></span></span><span class="vlist-s"></span></span><span class="vlist-r"><span class="vlist" style="height:0.3147em;"><span></span></span></span></span></span></span></span></span></span>.</p></li>
</ol>
</div></blockquote>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="torch.distributions.transforms.CumulativeDistributionTransform">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torch.distributions.transforms.</span></span><span class="sig-name descname"><span class="pre">CumulativeDistributionTransform</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">distribution</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cache_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/transforms.py#L1204"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.transforms.CumulativeDistributionTransform" title="Permalink to this definition">#</a></dt>
<dd><p>Transform via the cumulative distribution function of a probability distribution.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>distribution</strong> (<a class="reference internal" href="#torch.distributions.distribution.Distribution" title="torch.distributions.distribution.Distribution"><em>Distribution</em></a>)  Distribution whose cumulative distribution function to use for
the transformation.</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Construct a Gaussian copula from a multivariate normal.</span>
<span class="n">base_dist</span> <span class="o">=</span> <span class="n">MultivariateNormal</span><span class="p">(</span>
    <span class="n">loc</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">2</span><span class="p">),</span>
    <span class="n">scale_tril</span><span class="o">=</span><span class="n">LKJCholesky</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">sample</span><span class="p">(),</span>
<span class="p">)</span>
<span class="n">transform</span> <span class="o">=</span> <span class="n">CumulativeDistributionTransform</span><span class="p">(</span><span class="n">Normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="n">copula</span> <span class="o">=</span> <span class="n">TransformedDistribution</span><span class="p">(</span><span class="n">base_dist</span><span class="p">,</span> <span class="p">[</span><span class="n">transform</span><span class="p">])</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="torch.distributions.transforms.ExpTransform">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torch.distributions.transforms.</span></span><span class="sig-name descname"><span class="pre">ExpTransform</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">cache_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/transforms.py#L540"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.transforms.ExpTransform" title="Permalink to this definition">#</a></dt>
<dd><p>Transform via the mapping <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi><mo>=</mo><mi>exp</mi><mo></mo><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">y = \exp(x)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mop">exp</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span></span></span></span></span>.</p>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="torch.distributions.transforms.IndependentTransform">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torch.distributions.transforms.</span></span><span class="sig-name descname"><span class="pre">IndependentTransform</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">base_transform</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reinterpreted_batch_ndims</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cache_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/transforms.py#L402"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.transforms.IndependentTransform" title="Permalink to this definition">#</a></dt>
<dd><p>Wrapper around another transform to treat
<code class="docutils literal notranslate"><span class="pre">reinterpreted_batch_ndims</span></code>-many extra of the right most dimensions as
dependent. This has no effect on the forward or backward transforms, but
does sum out <code class="docutils literal notranslate"><span class="pre">reinterpreted_batch_ndims</span></code>-many of the rightmost dimensions
in <code class="xref py py-meth docutils literal notranslate"><span class="pre">log_abs_det_jacobian()</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>base_transform</strong> (<a class="reference internal" href="#torch.distributions.transforms.Transform" title="torch.distributions.transforms.Transform"><code class="xref py py-class docutils literal notranslate"><span class="pre">Transform</span></code></a>)  A base transform.</p></li>
<li><p><strong>reinterpreted_batch_ndims</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>)  The number of extra rightmost
dimensions to treat as dependent.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="torch.distributions.transforms.LowerCholeskyTransform">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torch.distributions.transforms.</span></span><span class="sig-name descname"><span class="pre">LowerCholeskyTransform</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">cache_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/transforms.py#L984"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.transforms.LowerCholeskyTransform" title="Permalink to this definition">#</a></dt>
<dd><p>Transform from unconstrained matrices to lower-triangular matrices with
nonnegative diagonal entries.</p>
<p>This is useful for parameterizing positive definite matrices in terms of
their Cholesky factorization.</p>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="torch.distributions.transforms.PositiveDefiniteTransform">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torch.distributions.transforms.</span></span><span class="sig-name descname"><span class="pre">PositiveDefiniteTransform</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">cache_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/transforms.py#L1006"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.transforms.PositiveDefiniteTransform" title="Permalink to this definition">#</a></dt>
<dd><p>Transform from unconstrained matrices to positive-definite matrices.</p>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="torch.distributions.transforms.PowerTransform">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torch.distributions.transforms.</span></span><span class="sig-name descname"><span class="pre">PowerTransform</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">exponent</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cache_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/transforms.py#L562"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.transforms.PowerTransform" title="Permalink to this definition">#</a></dt>
<dd><p>Transform via the mapping <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi><mo>=</mo><msup><mi>x</mi><mtext>exponent</mtext></msup></mrow><annotation encoding="application/x-tex">y = x^{\text{exponent}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.7936em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7936em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">exponent</span></span></span></span></span></span></span></span></span></span></span></span></span></span>.</p>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="torch.distributions.transforms.ReshapeTransform">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torch.distributions.transforms.</span></span><span class="sig-name descname"><span class="pre">ReshapeTransform</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">in_shape</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">out_shape</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cache_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/transforms.py#L473"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.transforms.ReshapeTransform" title="Permalink to this definition">#</a></dt>
<dd><p>Unit Jacobian transform to reshape the rightmost part of a tensor.</p>
<p>Note that <code class="docutils literal notranslate"><span class="pre">in_shape</span></code> and <code class="docutils literal notranslate"><span class="pre">out_shape</span></code> must have the same number of
elements, just as for <a class="reference internal" href="generated/torch.Tensor.reshape.html#torch.Tensor.reshape" title="torch.Tensor.reshape"><code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.Tensor.reshape()</span></code></a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>in_shape</strong> (<a class="reference internal" href="size.html#torch.Size" title="torch.Size"><em>torch.Size</em></a>)  The input event shape.</p></li>
<li><p><strong>out_shape</strong> (<a class="reference internal" href="size.html#torch.Size" title="torch.Size"><em>torch.Size</em></a>)  The output event shape.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="torch.distributions.transforms.SigmoidTransform">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torch.distributions.transforms.</span></span><span class="sig-name descname"><span class="pre">SigmoidTransform</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">cache_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/transforms.py#L609"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.transforms.SigmoidTransform" title="Permalink to this definition">#</a></dt>
<dd><p>Transform via the mapping <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi><mo>=</mo><mfrac><mn>1</mn><mrow><mn>1</mn><mo>+</mo><mi>exp</mi><mo></mo><mo stretchy="false">(</mo><mo></mo><mi>x</mi><mo stretchy="false">)</mo></mrow></mfrac></mrow><annotation encoding="application/x-tex">y = \frac{1}{1 + \exp(-x)}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.3651em;vertical-align:-0.52em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8451em;"><span style="top:-2.655em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span><span class="mbin mtight">+</span><span class="mop mtight"><span class="mtight">e</span><span class="mtight">x</span><span class="mtight">p</span></span><span class="mopen mtight">(</span><span class="mord mtight"></span><span class="mord mathnormal mtight">x</span><span class="mclose mtight">)</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s"></span></span><span class="vlist-r"><span class="vlist" style="height:0.52em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span> and <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi><mo>=</mo><mtext>logit</mtext><mo stretchy="false">(</mo><mi>y</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">x = \text{logit}(y)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">x</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord text"><span class="mord">logit</span></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mclose">)</span></span></span></span></span>.</p>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="torch.distributions.transforms.SoftplusTransform">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torch.distributions.transforms.</span></span><span class="sig-name descname"><span class="pre">SoftplusTransform</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">cache_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/transforms.py#L633"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.transforms.SoftplusTransform" title="Permalink to this definition">#</a></dt>
<dd><p>Transform via the mapping <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>Softplus</mtext><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mi>log</mi><mo></mo><mo stretchy="false">(</mo><mn>1</mn><mo>+</mo><mi>exp</mi><mo></mo><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\text{Softplus}(x) = \log(1 + \exp(x))</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord text"><span class="mord">Softplus</span></span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mop">lo<span style="margin-right:0.01389em;">g</span></span><span class="mopen">(</span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mop">exp</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">))</span></span></span></span></span>.
The implementation reverts to the linear function when <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi><mo>&gt;</mo><mn>20</mn></mrow><annotation encoding="application/x-tex">x &gt; 20</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5782em;vertical-align:-0.0391em;"></span><span class="mord mathnormal">x</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">&gt;</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">20</span></span></span></span></span>.</p>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="torch.distributions.transforms.TanhTransform">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torch.distributions.transforms.</span></span><span class="sig-name descname"><span class="pre">TanhTransform</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">cache_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/transforms.py#L656"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.transforms.TanhTransform" title="Permalink to this definition">#</a></dt>
<dd><p>Transform via the mapping <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi><mo>=</mo><mi>tanh</mi><mo></mo><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">y = \tanh(x)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mop">tanh</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span></span></span></span></span>.</p>
<p>It is equivalent to
<code class="docutils literal notranslate"><span class="pre">`</span>
<span class="pre">ComposeTransform([AffineTransform(0.,</span> <span class="pre">2.),</span> <span class="pre">SigmoidTransform(),</span> <span class="pre">AffineTransform(-1.,</span> <span class="pre">2.)])</span>
<span class="pre">`</span></code>
However this might not be numerically stable, thus it is recommended to use <cite>TanhTransform</cite>
instead.</p>
<p>Note that one should use <cite>cache_size=1</cite> when it comes to <cite>NaN/Inf</cite> values.</p>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="torch.distributions.transforms.SoftmaxTransform">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torch.distributions.transforms.</span></span><span class="sig-name descname"><span class="pre">SoftmaxTransform</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">cache_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/transforms.py#L893"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.transforms.SoftmaxTransform" title="Permalink to this definition">#</a></dt>
<dd><p>Transform from unconstrained space to the simplex via <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi><mo>=</mo><mi>exp</mi><mo></mo><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">y = \exp(x)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mop">exp</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span></span></span></span></span> then
normalizing.</p>
<p>This is not bijective and cannot be used for HMC. However this acts mostly
coordinate-wise (except for the final normalization), and thus is
appropriate for coordinate-wise optimization algorithms.</p>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="torch.distributions.transforms.StackTransform">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torch.distributions.transforms.</span></span><span class="sig-name descname"><span class="pre">StackTransform</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">tseq</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cache_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/transforms.py#L1132"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.transforms.StackTransform" title="Permalink to this definition">#</a></dt>
<dd><p>Transform functor that applies a sequence of transforms <cite>tseq</cite>
component-wise to each submatrix at <cite>dim</cite>
in a way compatible with <a class="reference internal" href="generated/torch.stack.html#torch.stack" title="torch.stack"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.stack()</span></code></a>.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">torch</span><span class="o">.</span><span class="n">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">)],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">t</span> <span class="o">=</span> <span class="n">StackTransform</span><span class="p">([</span><span class="n">ExpTransform</span><span class="p">(),</span> <span class="n">identity_transform</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">t</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="torch.distributions.transforms.StickBreakingTransform">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torch.distributions.transforms.</span></span><span class="sig-name descname"><span class="pre">StickBreakingTransform</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">cache_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/transforms.py#L928"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.transforms.StickBreakingTransform" title="Permalink to this definition">#</a></dt>
<dd><p>Transform from unconstrained space to the simplex of one additional
dimension via a stick-breaking process.</p>
<p>This transform arises as an iterated sigmoid transform in a stick-breaking
construction of the <cite>Dirichlet</cite> distribution: the first logit is
transformed via sigmoid to the first probability and the probability of
everything else, and then the process recurses.</p>
<p>This is bijective and appropriate for use in HMC; however it mixes
coordinates together and is less appropriate for optimization.</p>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="torch.distributions.transforms.Transform">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torch.distributions.transforms.</span></span><span class="sig-name descname"><span class="pre">Transform</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">cache_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/transforms.py#L46"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.transforms.Transform" title="Permalink to this definition">#</a></dt>
<dd><p>Abstract class for invertable transformations with computable log
det jacobians. They are primarily used in
<code class="xref py py-class docutils literal notranslate"><span class="pre">torch.distributions.TransformedDistribution</span></code>.</p>
<p>Caching is useful for transforms whose inverses are either expensive or
numerically unstable. Note that care must be taken with memoized values
since the autograd graph may be reversed. For example while the following
works with or without caching:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">y</span> <span class="o">=</span> <span class="n">t</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">t</span><span class="o">.</span><span class="n">log_abs_det_jacobian</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>  <span class="c1"># x will receive gradients.</span>
</pre></div>
</div>
<p>However the following will error when caching due to dependency reversal:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">y</span> <span class="o">=</span> <span class="n">t</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">z</span> <span class="o">=</span> <span class="n">t</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
<span class="n">grad</span><span class="p">(</span><span class="n">z</span><span class="o">.</span><span class="n">sum</span><span class="p">(),</span> <span class="p">[</span><span class="n">y</span><span class="p">])</span>  <span class="c1"># error because z is x</span>
</pre></div>
</div>
<p>Derived classes should implement one or both of <code class="xref py py-meth docutils literal notranslate"><span class="pre">_call()</span></code> or
<code class="xref py py-meth docutils literal notranslate"><span class="pre">_inverse()</span></code>. Derived classes that set <cite>bijective=True</cite> should also
implement <a class="reference internal" href="#torch.distributions.transforms.Transform.log_abs_det_jacobian" title="torch.distributions.transforms.Transform.log_abs_det_jacobian"><code class="xref py py-meth docutils literal notranslate"><span class="pre">log_abs_det_jacobian()</span></code></a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>cache_size</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>)  Size of cache. If zero, no caching is done. If one,
the latest single value is cached. Only 0 and 1 are supported.</p>
</dd>
<dt class="field-even">Variables<span class="colon">:</span></dt>
<dd class="field-even"><ul class="simple">
<li><p><strong>domain</strong> (<a class="reference internal" href="#torch.distributions.constraints.Constraint" title="torch.distributions.constraints.Constraint"><code class="xref py py-class docutils literal notranslate"><span class="pre">Constraint</span></code></a>)  The constraint representing valid inputs to this transform.</p></li>
<li><p><strong>codomain</strong> (<a class="reference internal" href="#torch.distributions.constraints.Constraint" title="torch.distributions.constraints.Constraint"><code class="xref py py-class docutils literal notranslate"><span class="pre">Constraint</span></code></a>)  The constraint representing valid outputs to this transform
which are inputs to the inverse transform.</p></li>
<li><p><strong>bijective</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><em>bool</em></a>)  Whether this transform is bijective. A transform
<code class="docutils literal notranslate"><span class="pre">t</span></code> is bijective iff <code class="docutils literal notranslate"><span class="pre">t.inv(t(x))</span> <span class="pre">==</span> <span class="pre">x</span></code> and
<code class="docutils literal notranslate"><span class="pre">t(t.inv(y))</span> <span class="pre">==</span> <span class="pre">y</span></code> for every <code class="docutils literal notranslate"><span class="pre">x</span></code> in the domain and <code class="docutils literal notranslate"><span class="pre">y</span></code> in
the codomain. Transforms that are not bijective should at least
maintain the weaker pseudoinverse properties
<code class="docutils literal notranslate"><span class="pre">t(t.inv(t(x))</span> <span class="pre">==</span> <span class="pre">t(x)</span></code> and <code class="docutils literal notranslate"><span class="pre">t.inv(t(t.inv(y)))</span> <span class="pre">==</span> <span class="pre">t.inv(y)</span></code>.</p></li>
<li><p><strong>sign</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a><em> or </em><a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>)  For bijective univariate transforms, this
should be +1 or -1 depending on whether transform is monotone
increasing or decreasing.</p></li>
</ul>
</dd>
</dl>
<dl class="py property">
<dt class="sig sig-object py" id="torch.distributions.transforms.Transform.inv">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">inv</span></span><a class="headerlink" href="#torch.distributions.transforms.Transform.inv" title="Permalink to this definition">#</a></dt>
<dd><p>Returns the inverse <a class="reference internal" href="#torch.distributions.transforms.Transform" title="torch.distributions.transforms.Transform"><code class="xref py py-class docutils literal notranslate"><span class="pre">Transform</span></code></a> of this transform.
This should satisfy <code class="docutils literal notranslate"><span class="pre">t.inv.inv</span> <span class="pre">is</span> <span class="pre">t</span></code>.</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="torch.distributions.transforms.Transform.sign">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">sign</span></span><a class="headerlink" href="#torch.distributions.transforms.Transform.sign" title="Permalink to this definition">#</a></dt>
<dd><p>Returns the sign of the determinant of the Jacobian, if applicable.
In general this only makes sense for bijective transforms.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributions.transforms.Transform.log_abs_det_jacobian">
<span class="sig-name descname"><span class="pre">log_abs_det_jacobian</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/transforms.py#L191"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.transforms.Transform.log_abs_det_jacobian" title="Permalink to this definition">#</a></dt>
<dd><p>Computes the log det jacobian <cite>log |dy/dx|</cite> given input and output.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributions.transforms.Transform.forward_shape">
<span class="sig-name descname"><span class="pre">forward_shape</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">shape</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/transforms.py#L200"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.transforms.Transform.forward_shape" title="Permalink to this definition">#</a></dt>
<dd><p>Infers the shape of the forward computation, given the input shape.
Defaults to preserving shape.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributions.transforms.Transform.inverse_shape">
<span class="sig-name descname"><span class="pre">inverse_shape</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">shape</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/transforms.py#L207"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.transforms.Transform.inverse_shape" title="Permalink to this definition">#</a></dt>
<dd><p>Infers the shapes of the inverse computation, given the output shape.
Defaults to preserving shape.</p>
</dd></dl>

</dd></dl>

</section>
<section id="module-torch.distributions.constraints">
<span id="constraints"></span><h2><cite>Constraints</cite><a class="headerlink" href="#module-torch.distributions.constraints" title="Permalink to this heading">#</a></h2>
<p>The following constraints are implemented:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">constraints.boolean</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">constraints.cat</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">constraints.corr_cholesky</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">constraints.dependent</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">constraints.greater_than(lower_bound)</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">constraints.greater_than_eq(lower_bound)</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">constraints.independent(constraint,</span> <span class="pre">reinterpreted_batch_ndims)</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">constraints.integer_interval(lower_bound,</span> <span class="pre">upper_bound)</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">constraints.interval(lower_bound,</span> <span class="pre">upper_bound)</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">constraints.less_than(upper_bound)</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">constraints.lower_cholesky</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">constraints.lower_triangular</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">constraints.multinomial</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">constraints.nonnegative</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">constraints.nonnegative_integer</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">constraints.one_hot</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">constraints.positive_integer</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">constraints.positive</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">constraints.positive_semidefinite</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">constraints.positive_definite</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">constraints.real_vector</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">constraints.real</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">constraints.simplex</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">constraints.symmetric</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">constraints.stack</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">constraints.square</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">constraints.symmetric</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">constraints.unit_interval</span></code></p></li>
</ul>
<dl class="py class">
<dt class="sig sig-object py" id="torch.distributions.constraints.Constraint">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torch.distributions.constraints.</span></span><span class="sig-name descname"><span class="pre">Constraint</span></span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/constraints.py#L73"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.constraints.Constraint" title="Permalink to this definition">#</a></dt>
<dd><p>Abstract base class for constraints.</p>
<p>A constraint object represents a region over which a variable is valid,
e.g. within which a variable can be optimized.</p>
<dl class="field-list simple">
<dt class="field-odd">Variables<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>is_discrete</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><em>bool</em></a>)  Whether constrained space is discrete.
Defaults to False.</p></li>
<li><p><strong>event_dim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>)  Number of rightmost dimensions that together define
an event. The <a class="reference internal" href="#torch.distributions.constraints.Constraint.check" title="torch.distributions.constraints.Constraint.check"><code class="xref py py-meth docutils literal notranslate"><span class="pre">check()</span></code></a> method will remove this many dimensions
when computing validity.</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="torch.distributions.constraints.Constraint.check">
<span class="sig-name descname"><span class="pre">check</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">value</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/constraints.py#L91"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.constraints.Constraint.check" title="Permalink to this definition">#</a></dt>
<dd><p>Returns a byte tensor of <code class="docutils literal notranslate"><span class="pre">sample_shape</span> <span class="pre">+</span> <span class="pre">batch_shape</span></code> indicating
whether each event in value satisfies this constraint.</p>
</dd></dl>

</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="torch.distributions.constraints.cat">
<span class="sig-prename descclassname"><span class="pre">torch.distributions.constraints.</span></span><span class="sig-name descname"><span class="pre">cat</span></span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/constraints.py#L584"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.constraints.cat" title="Permalink to this definition">#</a></dt>
<dd><p>alias of <code class="xref py py-class docutils literal notranslate"><span class="pre">_Cat</span></code></p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="torch.distributions.constraints.dependent_property">
<span class="sig-prename descclassname"><span class="pre">torch.distributions.constraints.</span></span><span class="sig-name descname"><span class="pre">dependent_property</span></span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/constraints.py#L175"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.constraints.dependent_property" title="Permalink to this definition">#</a></dt>
<dd><p>alias of <code class="xref py py-class docutils literal notranslate"><span class="pre">_DependentProperty</span></code></p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="torch.distributions.constraints.greater_than">
<span class="sig-prename descclassname"><span class="pre">torch.distributions.constraints.</span></span><span class="sig-name descname"><span class="pre">greater_than</span></span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/constraints.py#L359"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.constraints.greater_than" title="Permalink to this definition">#</a></dt>
<dd><p>alias of <code class="xref py py-class docutils literal notranslate"><span class="pre">_GreaterThan</span></code></p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="torch.distributions.constraints.greater_than_eq">
<span class="sig-prename descclassname"><span class="pre">torch.distributions.constraints.</span></span><span class="sig-name descname"><span class="pre">greater_than_eq</span></span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/constraints.py#L377"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.constraints.greater_than_eq" title="Permalink to this definition">#</a></dt>
<dd><p>alias of <code class="xref py py-class docutils literal notranslate"><span class="pre">_GreaterThanEq</span></code></p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="torch.distributions.constraints.independent">
<span class="sig-prename descclassname"><span class="pre">torch.distributions.constraints.</span></span><span class="sig-name descname"><span class="pre">independent</span></span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/constraints.py#L220"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.constraints.independent" title="Permalink to this definition">#</a></dt>
<dd><p>alias of <code class="xref py py-class docutils literal notranslate"><span class="pre">_IndependentConstraint</span></code></p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="torch.distributions.constraints.integer_interval">
<span class="sig-prename descclassname"><span class="pre">torch.distributions.constraints.</span></span><span class="sig-name descname"><span class="pre">integer_interval</span></span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/constraints.py#L285"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.constraints.integer_interval" title="Permalink to this definition">#</a></dt>
<dd><p>alias of <code class="xref py py-class docutils literal notranslate"><span class="pre">_IntegerInterval</span></code></p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="torch.distributions.constraints.interval">
<span class="sig-prename descclassname"><span class="pre">torch.distributions.constraints.</span></span><span class="sig-name descname"><span class="pre">interval</span></span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/constraints.py#L413"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.constraints.interval" title="Permalink to this definition">#</a></dt>
<dd><p>alias of <code class="xref py py-class docutils literal notranslate"><span class="pre">_Interval</span></code></p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="torch.distributions.constraints.half_open_interval">
<span class="sig-prename descclassname"><span class="pre">torch.distributions.constraints.</span></span><span class="sig-name descname"><span class="pre">half_open_interval</span></span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/constraints.py#L434"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.constraints.half_open_interval" title="Permalink to this definition">#</a></dt>
<dd><p>alias of <code class="xref py py-class docutils literal notranslate"><span class="pre">_HalfOpenInterval</span></code></p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="torch.distributions.constraints.is_dependent">
<span class="sig-prename descclassname"><span class="pre">torch.distributions.constraints.</span></span><span class="sig-name descname"><span class="pre">is_dependent</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">constraint</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/constraints.py#L149"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.constraints.is_dependent" title="Permalink to this definition">#</a></dt>
<dd><p>Checks if <code class="docutils literal notranslate"><span class="pre">constraint</span></code> is a <code class="docutils literal notranslate"><span class="pre">_Dependent</span></code> object.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>constraint</strong>  A <code class="docutils literal notranslate"><span class="pre">Constraint</span></code> object.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>True if <code class="docutils literal notranslate"><span class="pre">constraint</span></code> can be refined to the type <code class="docutils literal notranslate"><span class="pre">_Dependent</span></code>, False otherwise.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><code class="docutils literal notranslate"><span class="pre">bool</span></code></p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span><span class="w"> </span><span class="nn">torch.distributions</span><span class="w"> </span><span class="kn">import</span> <span class="n">Bernoulli</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span><span class="w"> </span><span class="nn">torch.distributions.constraints</span><span class="w"> </span><span class="kn">import</span> <span class="n">is_dependent</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">dist</span> <span class="o">=</span> <span class="n">Bernoulli</span><span class="p">(</span><span class="n">probs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.6</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">constraint1</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">arg_constraints</span><span class="p">[</span><span class="s2">&quot;probs&quot;</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">constraint2</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">arg_constraints</span><span class="p">[</span><span class="s2">&quot;logits&quot;</span><span class="p">]</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">constraint</span> <span class="ow">in</span> <span class="p">[</span><span class="n">constraint1</span><span class="p">,</span> <span class="n">constraint2</span><span class="p">]:</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">if</span> <span class="n">is_dependent</span><span class="p">(</span><span class="n">constraint</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">continue</span>
</pre></div>
</div>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="torch.distributions.constraints.less_than">
<span class="sig-prename descclassname"><span class="pre">torch.distributions.constraints.</span></span><span class="sig-name descname"><span class="pre">less_than</span></span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/constraints.py#L395"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.constraints.less_than" title="Permalink to this definition">#</a></dt>
<dd><p>alias of <code class="xref py py-class docutils literal notranslate"><span class="pre">_LessThan</span></code></p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="torch.distributions.constraints.multinomial">
<span class="sig-prename descclassname"><span class="pre">torch.distributions.constraints.</span></span><span class="sig-name descname"><span class="pre">multinomial</span></span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/constraints.py#L467"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.constraints.multinomial" title="Permalink to this definition">#</a></dt>
<dd><p>alias of <code class="xref py py-class docutils literal notranslate"><span class="pre">_Multinomial</span></code></p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="torch.distributions.constraints.stack">
<span class="sig-prename descclassname"><span class="pre">torch.distributions.constraints.</span></span><span class="sig-name descname"><span class="pre">stack</span></span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/constraints.py#L620"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.constraints.stack" title="Permalink to this definition">#</a></dt>
<dd><p>alias of <code class="xref py py-class docutils literal notranslate"><span class="pre">_Stack</span></code></p>
</dd></dl>

</section>
<section id="module-torch.distributions.constraint_registry">
<span id="constraint-registry"></span><h2><cite>Constraint Registry</cite><a class="headerlink" href="#module-torch.distributions.constraint_registry" title="Permalink to this heading">#</a></h2>
<p>PyTorch provides two global <a class="reference internal" href="#torch.distributions.constraint_registry.ConstraintRegistry" title="torch.distributions.constraint_registry.ConstraintRegistry"><code class="xref py py-class docutils literal notranslate"><span class="pre">ConstraintRegistry</span></code></a> objects that link
<a class="reference internal" href="#torch.distributions.constraints.Constraint" title="torch.distributions.constraints.Constraint"><code class="xref py py-class docutils literal notranslate"><span class="pre">Constraint</span></code></a> objects to
<a class="reference internal" href="#torch.distributions.transforms.Transform" title="torch.distributions.transforms.Transform"><code class="xref py py-class docutils literal notranslate"><span class="pre">Transform</span></code></a> objects. These objects both
input constraints and return transforms, but they have different guarantees on
bijectivity.</p>
<ol class="arabic simple">
<li><p><code class="docutils literal notranslate"><span class="pre">biject_to(constraint)</span></code> looks up a bijective
<a class="reference internal" href="#torch.distributions.transforms.Transform" title="torch.distributions.transforms.Transform"><code class="xref py py-class docutils literal notranslate"><span class="pre">Transform</span></code></a> from <code class="docutils literal notranslate"><span class="pre">constraints.real</span></code>
to the given <code class="docutils literal notranslate"><span class="pre">constraint</span></code>. The returned transform is guaranteed to have
<code class="docutils literal notranslate"><span class="pre">.bijective</span> <span class="pre">=</span> <span class="pre">True</span></code> and should implement <code class="docutils literal notranslate"><span class="pre">.log_abs_det_jacobian()</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">transform_to(constraint)</span></code> looks up a not-necessarily bijective
<a class="reference internal" href="#torch.distributions.transforms.Transform" title="torch.distributions.transforms.Transform"><code class="xref py py-class docutils literal notranslate"><span class="pre">Transform</span></code></a> from <code class="docutils literal notranslate"><span class="pre">constraints.real</span></code>
to the given <code class="docutils literal notranslate"><span class="pre">constraint</span></code>. The returned transform is not guaranteed to
implement <code class="docutils literal notranslate"><span class="pre">.log_abs_det_jacobian()</span></code>.</p></li>
</ol>
<p>The <code class="docutils literal notranslate"><span class="pre">transform_to()</span></code> registry is useful for performing unconstrained
optimization on constrained parameters of probability distributions, which are
indicated by each distributions <code class="docutils literal notranslate"><span class="pre">.arg_constraints</span></code> dict. These transforms often
overparameterize a space in order to avoid rotation; they are thus more
suitable for coordinate-wise optimization algorithms like Adam:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">loc</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">unconstrained</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">scale</span> <span class="o">=</span> <span class="n">transform_to</span><span class="p">(</span><span class="n">Normal</span><span class="o">.</span><span class="n">arg_constraints</span><span class="p">[</span><span class="s1">&#39;scale&#39;</span><span class="p">])(</span><span class="n">unconstrained</span><span class="p">)</span>
<span class="n">loss</span> <span class="o">=</span> <span class="o">-</span><span class="n">Normal</span><span class="p">(</span><span class="n">loc</span><span class="p">,</span> <span class="n">scale</span><span class="p">)</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">data</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
</pre></div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">biject_to()</span></code> registry is useful for Hamiltonian Monte Carlo, where
samples from a probability distribution with constrained <code class="docutils literal notranslate"><span class="pre">.support</span></code> are
propagated in an unconstrained space, and algorithms are typically rotation
invariant.:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">dist</span> <span class="o">=</span> <span class="n">Exponential</span><span class="p">(</span><span class="n">rate</span><span class="p">)</span>
<span class="n">unconstrained</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">sample</span> <span class="o">=</span> <span class="n">biject_to</span><span class="p">(</span><span class="n">dist</span><span class="o">.</span><span class="n">support</span><span class="p">)(</span><span class="n">unconstrained</span><span class="p">)</span>
<span class="n">potential_energy</span> <span class="o">=</span> <span class="o">-</span><span class="n">dist</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">sample</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>An example where <code class="docutils literal notranslate"><span class="pre">transform_to</span></code> and <code class="docutils literal notranslate"><span class="pre">biject_to</span></code> differ is
<code class="docutils literal notranslate"><span class="pre">constraints.simplex</span></code>: <code class="docutils literal notranslate"><span class="pre">transform_to(constraints.simplex)</span></code> returns a
<a class="reference internal" href="#torch.distributions.transforms.SoftmaxTransform" title="torch.distributions.transforms.SoftmaxTransform"><code class="xref py py-class docutils literal notranslate"><span class="pre">SoftmaxTransform</span></code></a> that simply
exponentiates and normalizes its inputs; this is a cheap and mostly
coordinate-wise operation appropriate for algorithms like SVI. In
contrast, <code class="docutils literal notranslate"><span class="pre">biject_to(constraints.simplex)</span></code> returns a
<a class="reference internal" href="#torch.distributions.transforms.StickBreakingTransform" title="torch.distributions.transforms.StickBreakingTransform"><code class="xref py py-class docutils literal notranslate"><span class="pre">StickBreakingTransform</span></code></a> that
bijects its input down to a one-fewer-dimensional space; this a more
expensive less numerically stable transform but is needed for algorithms
like HMC.</p>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">biject_to</span></code> and <code class="docutils literal notranslate"><span class="pre">transform_to</span></code> objects can be extended by user-defined
constraints and transforms using their <code class="docutils literal notranslate"><span class="pre">.register()</span></code> method either as a
function on singleton constraints:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">transform_to</span><span class="o">.</span><span class="n">register</span><span class="p">(</span><span class="n">my_constraint</span><span class="p">,</span> <span class="n">my_transform</span><span class="p">)</span>
</pre></div>
</div>
<p>or as a decorator on parameterized constraints:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nd">@transform_to</span><span class="o">.</span><span class="n">register</span><span class="p">(</span><span class="n">MyConstraintClass</span><span class="p">)</span>
<span class="k">def</span><span class="w"> </span><span class="nf">my_factory</span><span class="p">(</span><span class="n">constraint</span><span class="p">):</span>
    <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">constraint</span><span class="p">,</span> <span class="n">MyConstraintClass</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">MyTransform</span><span class="p">(</span><span class="n">constraint</span><span class="o">.</span><span class="n">param1</span><span class="p">,</span> <span class="n">constraint</span><span class="o">.</span><span class="n">param2</span><span class="p">)</span>
</pre></div>
</div>
<p>You can create your own registry by creating a new <a class="reference internal" href="#torch.distributions.constraint_registry.ConstraintRegistry" title="torch.distributions.constraint_registry.ConstraintRegistry"><code class="xref py py-class docutils literal notranslate"><span class="pre">ConstraintRegistry</span></code></a>
object.</p>
<dl class="py class">
<dt class="sig sig-object py" id="torch.distributions.constraint_registry.ConstraintRegistry">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torch.distributions.constraint_registry.</span></span><span class="sig-name descname"><span class="pre">ConstraintRegistry</span></span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/constraint_registry.py#L81"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.constraint_registry.ConstraintRegistry" title="Permalink to this definition">#</a></dt>
<dd><p>Registry to link constraints to transforms.</p>
<dl class="py method">
<dt class="sig sig-object py" id="torch.distributions.constraint_registry.ConstraintRegistry.register">
<span class="sig-name descname"><span class="pre">register</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">constraint</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">factory</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/distributions/constraint_registry.py#L90"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributions.constraint_registry.ConstraintRegistry.register" title="Permalink to this definition">#</a></dt>
<dd><p>Registers a <a class="reference internal" href="#torch.distributions.constraints.Constraint" title="torch.distributions.constraints.Constraint"><code class="xref py py-class docutils literal notranslate"><span class="pre">Constraint</span></code></a>
subclass in this registry. Usage:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nd">@my_registry</span><span class="o">.</span><span class="n">register</span><span class="p">(</span><span class="n">MyConstraintClass</span><span class="p">)</span>
<span class="k">def</span><span class="w"> </span><span class="nf">construct_transform</span><span class="p">(</span><span class="n">constraint</span><span class="p">):</span>
    <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">constraint</span><span class="p">,</span> <span class="n">MyConstraint</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">MyTransform</span><span class="p">(</span><span class="n">constraint</span><span class="o">.</span><span class="n">arg_constraints</span><span class="p">)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>constraint</strong> (subclass of <a class="reference internal" href="#torch.distributions.constraints.Constraint" title="torch.distributions.constraints.Constraint"><code class="xref py py-class docutils literal notranslate"><span class="pre">Constraint</span></code></a>)  A subclass of <a class="reference internal" href="#torch.distributions.constraints.Constraint" title="torch.distributions.constraints.Constraint"><code class="xref py py-class docutils literal notranslate"><span class="pre">Constraint</span></code></a>, or
a singleton object of the desired class.</p></li>
<li><p><strong>factory</strong> (<em>Callable</em>)  A callable that inputs a constraint object and returns
a  <a class="reference internal" href="#torch.distributions.transforms.Transform" title="torch.distributions.transforms.Transform"><code class="xref py py-class docutils literal notranslate"><span class="pre">Transform</span></code></a> object.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

<span class="target" id="module-torch.distributions.bernoulli"></span><span class="target" id="module-torch.distributions.beta"></span><span class="target" id="module-torch.distributions.binomial"></span><span class="target" id="module-torch.distributions.categorical"></span><span class="target" id="module-torch.distributions.cauchy"></span><span class="target" id="module-torch.distributions.chi2"></span><span class="target" id="module-torch.distributions.continuous_bernoulli"></span><span class="target" id="module-torch.distributions.dirichlet"></span><span class="target" id="module-torch.distributions.distribution"></span><span class="target" id="module-torch.distributions.exp_family"></span><span class="target" id="module-torch.distributions.exponential"></span><span class="target" id="module-torch.distributions.fishersnedecor"></span><span class="target" id="module-torch.distributions.gamma"></span><span class="target" id="module-torch.distributions.geometric"></span><span class="target" id="module-torch.distributions.gumbel"></span><span class="target" id="module-torch.distributions.half_cauchy"></span><span class="target" id="module-torch.distributions.half_normal"></span><span class="target" id="module-torch.distributions.independent"></span><span class="target" id="module-torch.distributions.inverse_gamma"></span><span class="target" id="module-torch.distributions.kumaraswamy"></span><span class="target" id="module-torch.distributions.laplace"></span><span class="target" id="module-torch.distributions.lkj_cholesky"></span><span class="target" id="module-torch.distributions.log_normal"></span><span class="target" id="module-torch.distributions.logistic_normal"></span><span class="target" id="module-torch.distributions.lowrank_multivariate_normal"></span><span class="target" id="module-torch.distributions.mixture_same_family"></span><span class="target" id="module-torch.distributions.multinomial"></span><span class="target" id="module-torch.distributions.multivariate_normal"></span><span class="target" id="module-torch.distributions.negative_binomial"></span><span class="target" id="module-torch.distributions.normal"></span><span class="target" id="module-torch.distributions.one_hot_categorical"></span><span class="target" id="module-torch.distributions.pareto"></span><span class="target" id="module-torch.distributions.poisson"></span><span class="target" id="module-torch.distributions.relaxed_bernoulli"></span><span class="target" id="module-torch.distributions.relaxed_categorical"></span><span class="target" id="module-torch.distributions.studentT"></span><span class="target" id="module-torch.distributions.transformed_distribution"></span><span class="target" id="module-torch.distributions.uniform"></span><span class="target" id="module-torch.distributions.utils"></span><span class="target" id="module-torch.distributions.von_mises"></span><span class="target" id="module-torch.distributions.weibull"></span><span class="target" id="module-torch.distributions.wishart"></span></section>
</section>


  </article>

              
              
                <footer class="bd-footer-article">
                  <div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item">
<div class="feedback">
  
<div class="rating">
    Rate this Page
    <div class="stars">
        
        <span class="star" data-behavior="tutorial-rating" data-count="1" data-value="1"></span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="2" data-value="2"></span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="3" data-value="3"></span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="4" data-value="4"></span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="5" data-value="5"></span>
        
    </div>
</div>

  <div class="feedback-send">
    <button class="feedback-btn"
            onclick="openGitHubIssue()"
            data-bs-title="Create a GitHub Issue"
            data-bs-placement="bottom"
            data-bs-toggle="tooltip">Send Feedback
    </button>
  </div>
</div>


<div class="prev-next-area">
    <a class="left-prev"
       href="distributed.checkpoint.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Distributed Checkpoint - torch.distributed.checkpoint</p>
      </div>
    </a>
    <a class="right-next"
       href="torch.compiler.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">torch.compiler</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>

<div class="footer-info">
  <p class="copyright">
    
      
         Copyright PyTorch Contributors.
      
      <br/>
    
  </p>

  <p class="theme-version">
    Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.15.4.
  </p>
</div></div>
  
</div>
                </footer>
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="distributed.checkpoint.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Distributed Checkpoint - torch.distributed.checkpoint</p>
      </div>
    </a>
    <a class="right-next"
       href="torch.compiler.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">torch.compiler</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc">
<div class="sidebar-secondary-items sidebar-secondary__inner">
    
       <div class="sidebar-secondary-item">
<div
    id="pst-page-navigation-heading-2"
    class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> On this page
  </div>
  <nav class="bd-toc-nav page-toc" aria-labelledby="pst-page-navigation-heading-2">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#score-function">Score function</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#pathwise-derivative">Pathwise derivative</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#distribution"><span class="hidden-section">Distribution</span></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.distribution.Distribution"><code class="docutils literal notranslate"><span class="pre">Distribution</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.distribution.Distribution.arg_constraints"><code class="docutils literal notranslate"><span class="pre">Distribution.arg_constraints</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.distribution.Distribution.batch_shape"><code class="docutils literal notranslate"><span class="pre">Distribution.batch_shape</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.distribution.Distribution.cdf"><code class="docutils literal notranslate"><span class="pre">Distribution.cdf()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.distribution.Distribution.entropy"><code class="docutils literal notranslate"><span class="pre">Distribution.entropy()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.distribution.Distribution.enumerate_support"><code class="docutils literal notranslate"><span class="pre">Distribution.enumerate_support()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.distribution.Distribution.event_shape"><code class="docutils literal notranslate"><span class="pre">Distribution.event_shape</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.distribution.Distribution.expand"><code class="docutils literal notranslate"><span class="pre">Distribution.expand()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.distribution.Distribution.icdf"><code class="docutils literal notranslate"><span class="pre">Distribution.icdf()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.distribution.Distribution.log_prob"><code class="docutils literal notranslate"><span class="pre">Distribution.log_prob()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.distribution.Distribution.mean"><code class="docutils literal notranslate"><span class="pre">Distribution.mean</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.distribution.Distribution.mode"><code class="docutils literal notranslate"><span class="pre">Distribution.mode</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.distribution.Distribution.perplexity"><code class="docutils literal notranslate"><span class="pre">Distribution.perplexity()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.distribution.Distribution.rsample"><code class="docutils literal notranslate"><span class="pre">Distribution.rsample()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.distribution.Distribution.sample"><code class="docutils literal notranslate"><span class="pre">Distribution.sample()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.distribution.Distribution.sample_n"><code class="docutils literal notranslate"><span class="pre">Distribution.sample_n()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.distribution.Distribution.set_default_validate_args"><code class="docutils literal notranslate"><span class="pre">Distribution.set_default_validate_args()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.distribution.Distribution.stddev"><code class="docutils literal notranslate"><span class="pre">Distribution.stddev</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.distribution.Distribution.support"><code class="docutils literal notranslate"><span class="pre">Distribution.support</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.distribution.Distribution.variance"><code class="docutils literal notranslate"><span class="pre">Distribution.variance</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#exponentialfamily"><span class="hidden-section">ExponentialFamily</span></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.exp_family.ExponentialFamily"><code class="docutils literal notranslate"><span class="pre">ExponentialFamily</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.exp_family.ExponentialFamily.entropy"><code class="docutils literal notranslate"><span class="pre">ExponentialFamily.entropy()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bernoulli"><span class="hidden-section">Bernoulli</span></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.bernoulli.Bernoulli"><code class="docutils literal notranslate"><span class="pre">Bernoulli</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.bernoulli.Bernoulli.arg_constraints"><code class="docutils literal notranslate"><span class="pre">Bernoulli.arg_constraints</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.bernoulli.Bernoulli.entropy"><code class="docutils literal notranslate"><span class="pre">Bernoulli.entropy()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.bernoulli.Bernoulli.enumerate_support"><code class="docutils literal notranslate"><span class="pre">Bernoulli.enumerate_support()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.bernoulli.Bernoulli.expand"><code class="docutils literal notranslate"><span class="pre">Bernoulli.expand()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.bernoulli.Bernoulli.has_enumerate_support"><code class="docutils literal notranslate"><span class="pre">Bernoulli.has_enumerate_support</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.bernoulli.Bernoulli.log_prob"><code class="docutils literal notranslate"><span class="pre">Bernoulli.log_prob()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.bernoulli.Bernoulli.logits"><code class="docutils literal notranslate"><span class="pre">Bernoulli.logits</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.bernoulli.Bernoulli.mean"><code class="docutils literal notranslate"><span class="pre">Bernoulli.mean</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.bernoulli.Bernoulli.mode"><code class="docutils literal notranslate"><span class="pre">Bernoulli.mode</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.bernoulli.Bernoulli.param_shape"><code class="docutils literal notranslate"><span class="pre">Bernoulli.param_shape</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.bernoulli.Bernoulli.probs"><code class="docutils literal notranslate"><span class="pre">Bernoulli.probs</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.bernoulli.Bernoulli.sample"><code class="docutils literal notranslate"><span class="pre">Bernoulli.sample()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.bernoulli.Bernoulli.support"><code class="docutils literal notranslate"><span class="pre">Bernoulli.support</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.bernoulli.Bernoulli.variance"><code class="docutils literal notranslate"><span class="pre">Bernoulli.variance</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#beta"><span class="hidden-section">Beta</span></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.beta.Beta"><code class="docutils literal notranslate"><span class="pre">Beta</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.beta.Beta.arg_constraints"><code class="docutils literal notranslate"><span class="pre">Beta.arg_constraints</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.beta.Beta.concentration0"><code class="docutils literal notranslate"><span class="pre">Beta.concentration0</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.beta.Beta.concentration1"><code class="docutils literal notranslate"><span class="pre">Beta.concentration1</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.beta.Beta.entropy"><code class="docutils literal notranslate"><span class="pre">Beta.entropy()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.beta.Beta.expand"><code class="docutils literal notranslate"><span class="pre">Beta.expand()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.beta.Beta.has_rsample"><code class="docutils literal notranslate"><span class="pre">Beta.has_rsample</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.beta.Beta.log_prob"><code class="docutils literal notranslate"><span class="pre">Beta.log_prob()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.beta.Beta.mean"><code class="docutils literal notranslate"><span class="pre">Beta.mean</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.beta.Beta.mode"><code class="docutils literal notranslate"><span class="pre">Beta.mode</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.beta.Beta.rsample"><code class="docutils literal notranslate"><span class="pre">Beta.rsample()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.beta.Beta.support"><code class="docutils literal notranslate"><span class="pre">Beta.support</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.beta.Beta.variance"><code class="docutils literal notranslate"><span class="pre">Beta.variance</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#binomial"><span class="hidden-section">Binomial</span></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.binomial.Binomial"><code class="docutils literal notranslate"><span class="pre">Binomial</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.binomial.Binomial.arg_constraints"><code class="docutils literal notranslate"><span class="pre">Binomial.arg_constraints</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.binomial.Binomial.entropy"><code class="docutils literal notranslate"><span class="pre">Binomial.entropy()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.binomial.Binomial.enumerate_support"><code class="docutils literal notranslate"><span class="pre">Binomial.enumerate_support()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.binomial.Binomial.expand"><code class="docutils literal notranslate"><span class="pre">Binomial.expand()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.binomial.Binomial.has_enumerate_support"><code class="docutils literal notranslate"><span class="pre">Binomial.has_enumerate_support</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.binomial.Binomial.log_prob"><code class="docutils literal notranslate"><span class="pre">Binomial.log_prob()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.binomial.Binomial.logits"><code class="docutils literal notranslate"><span class="pre">Binomial.logits</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.binomial.Binomial.mean"><code class="docutils literal notranslate"><span class="pre">Binomial.mean</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.binomial.Binomial.mode"><code class="docutils literal notranslate"><span class="pre">Binomial.mode</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.binomial.Binomial.param_shape"><code class="docutils literal notranslate"><span class="pre">Binomial.param_shape</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.binomial.Binomial.probs"><code class="docutils literal notranslate"><span class="pre">Binomial.probs</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.binomial.Binomial.sample"><code class="docutils literal notranslate"><span class="pre">Binomial.sample()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.binomial.Binomial.support"><code class="docutils literal notranslate"><span class="pre">Binomial.support</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.binomial.Binomial.variance"><code class="docutils literal notranslate"><span class="pre">Binomial.variance</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#categorical"><span class="hidden-section">Categorical</span></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.categorical.Categorical"><code class="docutils literal notranslate"><span class="pre">Categorical</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.categorical.Categorical.arg_constraints"><code class="docutils literal notranslate"><span class="pre">Categorical.arg_constraints</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.categorical.Categorical.entropy"><code class="docutils literal notranslate"><span class="pre">Categorical.entropy()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.categorical.Categorical.enumerate_support"><code class="docutils literal notranslate"><span class="pre">Categorical.enumerate_support()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.categorical.Categorical.expand"><code class="docutils literal notranslate"><span class="pre">Categorical.expand()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.categorical.Categorical.has_enumerate_support"><code class="docutils literal notranslate"><span class="pre">Categorical.has_enumerate_support</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.categorical.Categorical.log_prob"><code class="docutils literal notranslate"><span class="pre">Categorical.log_prob()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.categorical.Categorical.logits"><code class="docutils literal notranslate"><span class="pre">Categorical.logits</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.categorical.Categorical.mean"><code class="docutils literal notranslate"><span class="pre">Categorical.mean</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.categorical.Categorical.mode"><code class="docutils literal notranslate"><span class="pre">Categorical.mode</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.categorical.Categorical.param_shape"><code class="docutils literal notranslate"><span class="pre">Categorical.param_shape</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.categorical.Categorical.probs"><code class="docutils literal notranslate"><span class="pre">Categorical.probs</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.categorical.Categorical.sample"><code class="docutils literal notranslate"><span class="pre">Categorical.sample()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.categorical.Categorical.support"><code class="docutils literal notranslate"><span class="pre">Categorical.support</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.categorical.Categorical.variance"><code class="docutils literal notranslate"><span class="pre">Categorical.variance</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#cauchy"><span class="hidden-section">Cauchy</span></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.cauchy.Cauchy"><code class="docutils literal notranslate"><span class="pre">Cauchy</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.cauchy.Cauchy.arg_constraints"><code class="docutils literal notranslate"><span class="pre">Cauchy.arg_constraints</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.cauchy.Cauchy.cdf"><code class="docutils literal notranslate"><span class="pre">Cauchy.cdf()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.cauchy.Cauchy.entropy"><code class="docutils literal notranslate"><span class="pre">Cauchy.entropy()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.cauchy.Cauchy.expand"><code class="docutils literal notranslate"><span class="pre">Cauchy.expand()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.cauchy.Cauchy.has_rsample"><code class="docutils literal notranslate"><span class="pre">Cauchy.has_rsample</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.cauchy.Cauchy.icdf"><code class="docutils literal notranslate"><span class="pre">Cauchy.icdf()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.cauchy.Cauchy.log_prob"><code class="docutils literal notranslate"><span class="pre">Cauchy.log_prob()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.cauchy.Cauchy.mean"><code class="docutils literal notranslate"><span class="pre">Cauchy.mean</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.cauchy.Cauchy.mode"><code class="docutils literal notranslate"><span class="pre">Cauchy.mode</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.cauchy.Cauchy.rsample"><code class="docutils literal notranslate"><span class="pre">Cauchy.rsample()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.cauchy.Cauchy.support"><code class="docutils literal notranslate"><span class="pre">Cauchy.support</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.cauchy.Cauchy.variance"><code class="docutils literal notranslate"><span class="pre">Cauchy.variance</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#chi2"><span class="hidden-section">Chi2</span></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.chi2.Chi2"><code class="docutils literal notranslate"><span class="pre">Chi2</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.chi2.Chi2.arg_constraints"><code class="docutils literal notranslate"><span class="pre">Chi2.arg_constraints</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.chi2.Chi2.df"><code class="docutils literal notranslate"><span class="pre">Chi2.df</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.chi2.Chi2.expand"><code class="docutils literal notranslate"><span class="pre">Chi2.expand()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#continuousbernoulli"><span class="hidden-section">ContinuousBernoulli</span></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.continuous_bernoulli.ContinuousBernoulli"><code class="docutils literal notranslate"><span class="pre">ContinuousBernoulli</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.continuous_bernoulli.ContinuousBernoulli.arg_constraints"><code class="docutils literal notranslate"><span class="pre">ContinuousBernoulli.arg_constraints</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.continuous_bernoulli.ContinuousBernoulli.cdf"><code class="docutils literal notranslate"><span class="pre">ContinuousBernoulli.cdf()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.continuous_bernoulli.ContinuousBernoulli.entropy"><code class="docutils literal notranslate"><span class="pre">ContinuousBernoulli.entropy()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.continuous_bernoulli.ContinuousBernoulli.expand"><code class="docutils literal notranslate"><span class="pre">ContinuousBernoulli.expand()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.continuous_bernoulli.ContinuousBernoulli.has_rsample"><code class="docutils literal notranslate"><span class="pre">ContinuousBernoulli.has_rsample</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.continuous_bernoulli.ContinuousBernoulli.icdf"><code class="docutils literal notranslate"><span class="pre">ContinuousBernoulli.icdf()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.continuous_bernoulli.ContinuousBernoulli.log_prob"><code class="docutils literal notranslate"><span class="pre">ContinuousBernoulli.log_prob()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.continuous_bernoulli.ContinuousBernoulli.logits"><code class="docutils literal notranslate"><span class="pre">ContinuousBernoulli.logits</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.continuous_bernoulli.ContinuousBernoulli.mean"><code class="docutils literal notranslate"><span class="pre">ContinuousBernoulli.mean</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.continuous_bernoulli.ContinuousBernoulli.param_shape"><code class="docutils literal notranslate"><span class="pre">ContinuousBernoulli.param_shape</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.continuous_bernoulli.ContinuousBernoulli.probs"><code class="docutils literal notranslate"><span class="pre">ContinuousBernoulli.probs</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.continuous_bernoulli.ContinuousBernoulli.rsample"><code class="docutils literal notranslate"><span class="pre">ContinuousBernoulli.rsample()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.continuous_bernoulli.ContinuousBernoulli.sample"><code class="docutils literal notranslate"><span class="pre">ContinuousBernoulli.sample()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.continuous_bernoulli.ContinuousBernoulli.stddev"><code class="docutils literal notranslate"><span class="pre">ContinuousBernoulli.stddev</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.continuous_bernoulli.ContinuousBernoulli.support"><code class="docutils literal notranslate"><span class="pre">ContinuousBernoulli.support</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.continuous_bernoulli.ContinuousBernoulli.variance"><code class="docutils literal notranslate"><span class="pre">ContinuousBernoulli.variance</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#dirichlet"><span class="hidden-section">Dirichlet</span></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.dirichlet.Dirichlet"><code class="docutils literal notranslate"><span class="pre">Dirichlet</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.dirichlet.Dirichlet.arg_constraints"><code class="docutils literal notranslate"><span class="pre">Dirichlet.arg_constraints</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.dirichlet.Dirichlet.entropy"><code class="docutils literal notranslate"><span class="pre">Dirichlet.entropy()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.dirichlet.Dirichlet.expand"><code class="docutils literal notranslate"><span class="pre">Dirichlet.expand()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.dirichlet.Dirichlet.has_rsample"><code class="docutils literal notranslate"><span class="pre">Dirichlet.has_rsample</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.dirichlet.Dirichlet.log_prob"><code class="docutils literal notranslate"><span class="pre">Dirichlet.log_prob()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.dirichlet.Dirichlet.mean"><code class="docutils literal notranslate"><span class="pre">Dirichlet.mean</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.dirichlet.Dirichlet.mode"><code class="docutils literal notranslate"><span class="pre">Dirichlet.mode</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.dirichlet.Dirichlet.rsample"><code class="docutils literal notranslate"><span class="pre">Dirichlet.rsample()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.dirichlet.Dirichlet.support"><code class="docutils literal notranslate"><span class="pre">Dirichlet.support</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.dirichlet.Dirichlet.variance"><code class="docutils literal notranslate"><span class="pre">Dirichlet.variance</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#exponential"><span class="hidden-section">Exponential</span></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.exponential.Exponential"><code class="docutils literal notranslate"><span class="pre">Exponential</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.exponential.Exponential.arg_constraints"><code class="docutils literal notranslate"><span class="pre">Exponential.arg_constraints</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.exponential.Exponential.cdf"><code class="docutils literal notranslate"><span class="pre">Exponential.cdf()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.exponential.Exponential.entropy"><code class="docutils literal notranslate"><span class="pre">Exponential.entropy()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.exponential.Exponential.expand"><code class="docutils literal notranslate"><span class="pre">Exponential.expand()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.exponential.Exponential.has_rsample"><code class="docutils literal notranslate"><span class="pre">Exponential.has_rsample</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.exponential.Exponential.icdf"><code class="docutils literal notranslate"><span class="pre">Exponential.icdf()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.exponential.Exponential.log_prob"><code class="docutils literal notranslate"><span class="pre">Exponential.log_prob()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.exponential.Exponential.mean"><code class="docutils literal notranslate"><span class="pre">Exponential.mean</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.exponential.Exponential.mode"><code class="docutils literal notranslate"><span class="pre">Exponential.mode</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.exponential.Exponential.rsample"><code class="docutils literal notranslate"><span class="pre">Exponential.rsample()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.exponential.Exponential.stddev"><code class="docutils literal notranslate"><span class="pre">Exponential.stddev</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.exponential.Exponential.support"><code class="docutils literal notranslate"><span class="pre">Exponential.support</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.exponential.Exponential.variance"><code class="docutils literal notranslate"><span class="pre">Exponential.variance</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#fishersnedecor"><span class="hidden-section">FisherSnedecor</span></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.fishersnedecor.FisherSnedecor"><code class="docutils literal notranslate"><span class="pre">FisherSnedecor</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.fishersnedecor.FisherSnedecor.arg_constraints"><code class="docutils literal notranslate"><span class="pre">FisherSnedecor.arg_constraints</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.fishersnedecor.FisherSnedecor.expand"><code class="docutils literal notranslate"><span class="pre">FisherSnedecor.expand()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.fishersnedecor.FisherSnedecor.has_rsample"><code class="docutils literal notranslate"><span class="pre">FisherSnedecor.has_rsample</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.fishersnedecor.FisherSnedecor.log_prob"><code class="docutils literal notranslate"><span class="pre">FisherSnedecor.log_prob()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.fishersnedecor.FisherSnedecor.mean"><code class="docutils literal notranslate"><span class="pre">FisherSnedecor.mean</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.fishersnedecor.FisherSnedecor.mode"><code class="docutils literal notranslate"><span class="pre">FisherSnedecor.mode</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.fishersnedecor.FisherSnedecor.rsample"><code class="docutils literal notranslate"><span class="pre">FisherSnedecor.rsample()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.fishersnedecor.FisherSnedecor.support"><code class="docutils literal notranslate"><span class="pre">FisherSnedecor.support</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.fishersnedecor.FisherSnedecor.variance"><code class="docutils literal notranslate"><span class="pre">FisherSnedecor.variance</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gamma"><span class="hidden-section">Gamma</span></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.gamma.Gamma"><code class="docutils literal notranslate"><span class="pre">Gamma</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.gamma.Gamma.arg_constraints"><code class="docutils literal notranslate"><span class="pre">Gamma.arg_constraints</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.gamma.Gamma.cdf"><code class="docutils literal notranslate"><span class="pre">Gamma.cdf()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.gamma.Gamma.entropy"><code class="docutils literal notranslate"><span class="pre">Gamma.entropy()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.gamma.Gamma.expand"><code class="docutils literal notranslate"><span class="pre">Gamma.expand()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.gamma.Gamma.has_rsample"><code class="docutils literal notranslate"><span class="pre">Gamma.has_rsample</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.gamma.Gamma.log_prob"><code class="docutils literal notranslate"><span class="pre">Gamma.log_prob()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.gamma.Gamma.mean"><code class="docutils literal notranslate"><span class="pre">Gamma.mean</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.gamma.Gamma.mode"><code class="docutils literal notranslate"><span class="pre">Gamma.mode</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.gamma.Gamma.rsample"><code class="docutils literal notranslate"><span class="pre">Gamma.rsample()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.gamma.Gamma.support"><code class="docutils literal notranslate"><span class="pre">Gamma.support</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.gamma.Gamma.variance"><code class="docutils literal notranslate"><span class="pre">Gamma.variance</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#geometric"><span class="hidden-section">Geometric</span></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.geometric.Geometric"><code class="docutils literal notranslate"><span class="pre">Geometric</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.geometric.Geometric.arg_constraints"><code class="docutils literal notranslate"><span class="pre">Geometric.arg_constraints</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.geometric.Geometric.entropy"><code class="docutils literal notranslate"><span class="pre">Geometric.entropy()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.geometric.Geometric.expand"><code class="docutils literal notranslate"><span class="pre">Geometric.expand()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.geometric.Geometric.log_prob"><code class="docutils literal notranslate"><span class="pre">Geometric.log_prob()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.geometric.Geometric.logits"><code class="docutils literal notranslate"><span class="pre">Geometric.logits</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.geometric.Geometric.mean"><code class="docutils literal notranslate"><span class="pre">Geometric.mean</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.geometric.Geometric.mode"><code class="docutils literal notranslate"><span class="pre">Geometric.mode</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.geometric.Geometric.probs"><code class="docutils literal notranslate"><span class="pre">Geometric.probs</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.geometric.Geometric.sample"><code class="docutils literal notranslate"><span class="pre">Geometric.sample()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.geometric.Geometric.support"><code class="docutils literal notranslate"><span class="pre">Geometric.support</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.geometric.Geometric.variance"><code class="docutils literal notranslate"><span class="pre">Geometric.variance</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gumbel"><span class="hidden-section">Gumbel</span></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.gumbel.Gumbel"><code class="docutils literal notranslate"><span class="pre">Gumbel</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.gumbel.Gumbel.arg_constraints"><code class="docutils literal notranslate"><span class="pre">Gumbel.arg_constraints</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.gumbel.Gumbel.entropy"><code class="docutils literal notranslate"><span class="pre">Gumbel.entropy()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.gumbel.Gumbel.expand"><code class="docutils literal notranslate"><span class="pre">Gumbel.expand()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.gumbel.Gumbel.log_prob"><code class="docutils literal notranslate"><span class="pre">Gumbel.log_prob()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.gumbel.Gumbel.mean"><code class="docutils literal notranslate"><span class="pre">Gumbel.mean</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.gumbel.Gumbel.mode"><code class="docutils literal notranslate"><span class="pre">Gumbel.mode</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.gumbel.Gumbel.stddev"><code class="docutils literal notranslate"><span class="pre">Gumbel.stddev</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.gumbel.Gumbel.support"><code class="docutils literal notranslate"><span class="pre">Gumbel.support</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.gumbel.Gumbel.variance"><code class="docutils literal notranslate"><span class="pre">Gumbel.variance</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#halfcauchy"><span class="hidden-section">HalfCauchy</span></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.half_cauchy.HalfCauchy"><code class="docutils literal notranslate"><span class="pre">HalfCauchy</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.half_cauchy.HalfCauchy.arg_constraints"><code class="docutils literal notranslate"><span class="pre">HalfCauchy.arg_constraints</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.half_cauchy.HalfCauchy.cdf"><code class="docutils literal notranslate"><span class="pre">HalfCauchy.cdf()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.half_cauchy.HalfCauchy.entropy"><code class="docutils literal notranslate"><span class="pre">HalfCauchy.entropy()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.half_cauchy.HalfCauchy.expand"><code class="docutils literal notranslate"><span class="pre">HalfCauchy.expand()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.half_cauchy.HalfCauchy.has_rsample"><code class="docutils literal notranslate"><span class="pre">HalfCauchy.has_rsample</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.half_cauchy.HalfCauchy.icdf"><code class="docutils literal notranslate"><span class="pre">HalfCauchy.icdf()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.half_cauchy.HalfCauchy.log_prob"><code class="docutils literal notranslate"><span class="pre">HalfCauchy.log_prob()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.half_cauchy.HalfCauchy.mean"><code class="docutils literal notranslate"><span class="pre">HalfCauchy.mean</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.half_cauchy.HalfCauchy.mode"><code class="docutils literal notranslate"><span class="pre">HalfCauchy.mode</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.half_cauchy.HalfCauchy.scale"><code class="docutils literal notranslate"><span class="pre">HalfCauchy.scale</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.half_cauchy.HalfCauchy.support"><code class="docutils literal notranslate"><span class="pre">HalfCauchy.support</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.half_cauchy.HalfCauchy.variance"><code class="docutils literal notranslate"><span class="pre">HalfCauchy.variance</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#halfnormal"><span class="hidden-section">HalfNormal</span></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.half_normal.HalfNormal"><code class="docutils literal notranslate"><span class="pre">HalfNormal</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.half_normal.HalfNormal.arg_constraints"><code class="docutils literal notranslate"><span class="pre">HalfNormal.arg_constraints</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.half_normal.HalfNormal.cdf"><code class="docutils literal notranslate"><span class="pre">HalfNormal.cdf()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.half_normal.HalfNormal.entropy"><code class="docutils literal notranslate"><span class="pre">HalfNormal.entropy()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.half_normal.HalfNormal.expand"><code class="docutils literal notranslate"><span class="pre">HalfNormal.expand()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.half_normal.HalfNormal.has_rsample"><code class="docutils literal notranslate"><span class="pre">HalfNormal.has_rsample</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.half_normal.HalfNormal.icdf"><code class="docutils literal notranslate"><span class="pre">HalfNormal.icdf()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.half_normal.HalfNormal.log_prob"><code class="docutils literal notranslate"><span class="pre">HalfNormal.log_prob()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.half_normal.HalfNormal.mean"><code class="docutils literal notranslate"><span class="pre">HalfNormal.mean</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.half_normal.HalfNormal.mode"><code class="docutils literal notranslate"><span class="pre">HalfNormal.mode</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.half_normal.HalfNormal.scale"><code class="docutils literal notranslate"><span class="pre">HalfNormal.scale</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.half_normal.HalfNormal.support"><code class="docutils literal notranslate"><span class="pre">HalfNormal.support</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.half_normal.HalfNormal.variance"><code class="docutils literal notranslate"><span class="pre">HalfNormal.variance</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#independent"><span class="hidden-section">Independent</span></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.independent.Independent"><code class="docutils literal notranslate"><span class="pre">Independent</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.independent.Independent.arg_constraints"><code class="docutils literal notranslate"><span class="pre">Independent.arg_constraints</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.independent.Independent.entropy"><code class="docutils literal notranslate"><span class="pre">Independent.entropy()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.independent.Independent.enumerate_support"><code class="docutils literal notranslate"><span class="pre">Independent.enumerate_support()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.independent.Independent.expand"><code class="docutils literal notranslate"><span class="pre">Independent.expand()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.independent.Independent.has_enumerate_support"><code class="docutils literal notranslate"><span class="pre">Independent.has_enumerate_support</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.independent.Independent.has_rsample"><code class="docutils literal notranslate"><span class="pre">Independent.has_rsample</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.independent.Independent.log_prob"><code class="docutils literal notranslate"><span class="pre">Independent.log_prob()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.independent.Independent.mean"><code class="docutils literal notranslate"><span class="pre">Independent.mean</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.independent.Independent.mode"><code class="docutils literal notranslate"><span class="pre">Independent.mode</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.independent.Independent.rsample"><code class="docutils literal notranslate"><span class="pre">Independent.rsample()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.independent.Independent.sample"><code class="docutils literal notranslate"><span class="pre">Independent.sample()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.independent.Independent.support"><code class="docutils literal notranslate"><span class="pre">Independent.support</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.independent.Independent.variance"><code class="docutils literal notranslate"><span class="pre">Independent.variance</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#inversegamma"><span class="hidden-section">InverseGamma</span></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.inverse_gamma.InverseGamma"><code class="docutils literal notranslate"><span class="pre">InverseGamma</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.inverse_gamma.InverseGamma.arg_constraints"><code class="docutils literal notranslate"><span class="pre">InverseGamma.arg_constraints</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.inverse_gamma.InverseGamma.concentration"><code class="docutils literal notranslate"><span class="pre">InverseGamma.concentration</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.inverse_gamma.InverseGamma.entropy"><code class="docutils literal notranslate"><span class="pre">InverseGamma.entropy()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.inverse_gamma.InverseGamma.expand"><code class="docutils literal notranslate"><span class="pre">InverseGamma.expand()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.inverse_gamma.InverseGamma.has_rsample"><code class="docutils literal notranslate"><span class="pre">InverseGamma.has_rsample</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.inverse_gamma.InverseGamma.mean"><code class="docutils literal notranslate"><span class="pre">InverseGamma.mean</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.inverse_gamma.InverseGamma.mode"><code class="docutils literal notranslate"><span class="pre">InverseGamma.mode</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.inverse_gamma.InverseGamma.rate"><code class="docutils literal notranslate"><span class="pre">InverseGamma.rate</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.inverse_gamma.InverseGamma.support"><code class="docutils literal notranslate"><span class="pre">InverseGamma.support</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.inverse_gamma.InverseGamma.variance"><code class="docutils literal notranslate"><span class="pre">InverseGamma.variance</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#kumaraswamy"><span class="hidden-section">Kumaraswamy</span></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.kumaraswamy.Kumaraswamy"><code class="docutils literal notranslate"><span class="pre">Kumaraswamy</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.kumaraswamy.Kumaraswamy.arg_constraints"><code class="docutils literal notranslate"><span class="pre">Kumaraswamy.arg_constraints</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.kumaraswamy.Kumaraswamy.entropy"><code class="docutils literal notranslate"><span class="pre">Kumaraswamy.entropy()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.kumaraswamy.Kumaraswamy.expand"><code class="docutils literal notranslate"><span class="pre">Kumaraswamy.expand()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.kumaraswamy.Kumaraswamy.has_rsample"><code class="docutils literal notranslate"><span class="pre">Kumaraswamy.has_rsample</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.kumaraswamy.Kumaraswamy.mean"><code class="docutils literal notranslate"><span class="pre">Kumaraswamy.mean</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.kumaraswamy.Kumaraswamy.mode"><code class="docutils literal notranslate"><span class="pre">Kumaraswamy.mode</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.kumaraswamy.Kumaraswamy.support"><code class="docutils literal notranslate"><span class="pre">Kumaraswamy.support</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.kumaraswamy.Kumaraswamy.variance"><code class="docutils literal notranslate"><span class="pre">Kumaraswamy.variance</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#lkjcholesky"><span class="hidden-section">LKJCholesky</span></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.lkj_cholesky.LKJCholesky"><code class="docutils literal notranslate"><span class="pre">LKJCholesky</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.lkj_cholesky.LKJCholesky.arg_constraints"><code class="docutils literal notranslate"><span class="pre">LKJCholesky.arg_constraints</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.lkj_cholesky.LKJCholesky.expand"><code class="docutils literal notranslate"><span class="pre">LKJCholesky.expand()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.lkj_cholesky.LKJCholesky.log_prob"><code class="docutils literal notranslate"><span class="pre">LKJCholesky.log_prob()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.lkj_cholesky.LKJCholesky.sample"><code class="docutils literal notranslate"><span class="pre">LKJCholesky.sample()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.lkj_cholesky.LKJCholesky.support"><code class="docutils literal notranslate"><span class="pre">LKJCholesky.support</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#laplace"><span class="hidden-section">Laplace</span></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.laplace.Laplace"><code class="docutils literal notranslate"><span class="pre">Laplace</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.laplace.Laplace.arg_constraints"><code class="docutils literal notranslate"><span class="pre">Laplace.arg_constraints</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.laplace.Laplace.cdf"><code class="docutils literal notranslate"><span class="pre">Laplace.cdf()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.laplace.Laplace.entropy"><code class="docutils literal notranslate"><span class="pre">Laplace.entropy()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.laplace.Laplace.expand"><code class="docutils literal notranslate"><span class="pre">Laplace.expand()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.laplace.Laplace.has_rsample"><code class="docutils literal notranslate"><span class="pre">Laplace.has_rsample</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.laplace.Laplace.icdf"><code class="docutils literal notranslate"><span class="pre">Laplace.icdf()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.laplace.Laplace.log_prob"><code class="docutils literal notranslate"><span class="pre">Laplace.log_prob()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.laplace.Laplace.mean"><code class="docutils literal notranslate"><span class="pre">Laplace.mean</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.laplace.Laplace.mode"><code class="docutils literal notranslate"><span class="pre">Laplace.mode</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.laplace.Laplace.rsample"><code class="docutils literal notranslate"><span class="pre">Laplace.rsample()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.laplace.Laplace.stddev"><code class="docutils literal notranslate"><span class="pre">Laplace.stddev</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.laplace.Laplace.support"><code class="docutils literal notranslate"><span class="pre">Laplace.support</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.laplace.Laplace.variance"><code class="docutils literal notranslate"><span class="pre">Laplace.variance</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#lognormal"><span class="hidden-section">LogNormal</span></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.log_normal.LogNormal"><code class="docutils literal notranslate"><span class="pre">LogNormal</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.log_normal.LogNormal.arg_constraints"><code class="docutils literal notranslate"><span class="pre">LogNormal.arg_constraints</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.log_normal.LogNormal.entropy"><code class="docutils literal notranslate"><span class="pre">LogNormal.entropy()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.log_normal.LogNormal.expand"><code class="docutils literal notranslate"><span class="pre">LogNormal.expand()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.log_normal.LogNormal.has_rsample"><code class="docutils literal notranslate"><span class="pre">LogNormal.has_rsample</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.log_normal.LogNormal.loc"><code class="docutils literal notranslate"><span class="pre">LogNormal.loc</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.log_normal.LogNormal.mean"><code class="docutils literal notranslate"><span class="pre">LogNormal.mean</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.log_normal.LogNormal.mode"><code class="docutils literal notranslate"><span class="pre">LogNormal.mode</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.log_normal.LogNormal.scale"><code class="docutils literal notranslate"><span class="pre">LogNormal.scale</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.log_normal.LogNormal.support"><code class="docutils literal notranslate"><span class="pre">LogNormal.support</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.log_normal.LogNormal.variance"><code class="docutils literal notranslate"><span class="pre">LogNormal.variance</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#lowrankmultivariatenormal"><span class="hidden-section">LowRankMultivariateNormal</span></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal"><code class="docutils literal notranslate"><span class="pre">LowRankMultivariateNormal</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.arg_constraints"><code class="docutils literal notranslate"><span class="pre">LowRankMultivariateNormal.arg_constraints</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.covariance_matrix"><code class="docutils literal notranslate"><span class="pre">LowRankMultivariateNormal.covariance_matrix</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.entropy"><code class="docutils literal notranslate"><span class="pre">LowRankMultivariateNormal.entropy()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.expand"><code class="docutils literal notranslate"><span class="pre">LowRankMultivariateNormal.expand()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.has_rsample"><code class="docutils literal notranslate"><span class="pre">LowRankMultivariateNormal.has_rsample</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.log_prob"><code class="docutils literal notranslate"><span class="pre">LowRankMultivariateNormal.log_prob()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.mean"><code class="docutils literal notranslate"><span class="pre">LowRankMultivariateNormal.mean</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.mode"><code class="docutils literal notranslate"><span class="pre">LowRankMultivariateNormal.mode</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.precision_matrix"><code class="docutils literal notranslate"><span class="pre">LowRankMultivariateNormal.precision_matrix</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.rsample"><code class="docutils literal notranslate"><span class="pre">LowRankMultivariateNormal.rsample()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.scale_tril"><code class="docutils literal notranslate"><span class="pre">LowRankMultivariateNormal.scale_tril</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.support"><code class="docutils literal notranslate"><span class="pre">LowRankMultivariateNormal.support</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.variance"><code class="docutils literal notranslate"><span class="pre">LowRankMultivariateNormal.variance</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#mixturesamefamily"><span class="hidden-section">MixtureSameFamily</span></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.mixture_same_family.MixtureSameFamily"><code class="docutils literal notranslate"><span class="pre">MixtureSameFamily</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.mixture_same_family.MixtureSameFamily.arg_constraints"><code class="docutils literal notranslate"><span class="pre">MixtureSameFamily.arg_constraints</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.mixture_same_family.MixtureSameFamily.cdf"><code class="docutils literal notranslate"><span class="pre">MixtureSameFamily.cdf()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.mixture_same_family.MixtureSameFamily.component_distribution"><code class="docutils literal notranslate"><span class="pre">MixtureSameFamily.component_distribution</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.mixture_same_family.MixtureSameFamily.expand"><code class="docutils literal notranslate"><span class="pre">MixtureSameFamily.expand()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.mixture_same_family.MixtureSameFamily.has_rsample"><code class="docutils literal notranslate"><span class="pre">MixtureSameFamily.has_rsample</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.mixture_same_family.MixtureSameFamily.log_prob"><code class="docutils literal notranslate"><span class="pre">MixtureSameFamily.log_prob()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.mixture_same_family.MixtureSameFamily.mean"><code class="docutils literal notranslate"><span class="pre">MixtureSameFamily.mean</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.mixture_same_family.MixtureSameFamily.mixture_distribution"><code class="docutils literal notranslate"><span class="pre">MixtureSameFamily.mixture_distribution</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.mixture_same_family.MixtureSameFamily.sample"><code class="docutils literal notranslate"><span class="pre">MixtureSameFamily.sample()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.mixture_same_family.MixtureSameFamily.support"><code class="docutils literal notranslate"><span class="pre">MixtureSameFamily.support</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.mixture_same_family.MixtureSameFamily.variance"><code class="docutils literal notranslate"><span class="pre">MixtureSameFamily.variance</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#multinomial"><span class="hidden-section">Multinomial</span></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.multinomial.Multinomial"><code class="docutils literal notranslate"><span class="pre">Multinomial</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.multinomial.Multinomial.arg_constraints"><code class="docutils literal notranslate"><span class="pre">Multinomial.arg_constraints</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.multinomial.Multinomial.entropy"><code class="docutils literal notranslate"><span class="pre">Multinomial.entropy()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.multinomial.Multinomial.expand"><code class="docutils literal notranslate"><span class="pre">Multinomial.expand()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.multinomial.Multinomial.log_prob"><code class="docutils literal notranslate"><span class="pre">Multinomial.log_prob()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.multinomial.Multinomial.logits"><code class="docutils literal notranslate"><span class="pre">Multinomial.logits</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.multinomial.Multinomial.mean"><code class="docutils literal notranslate"><span class="pre">Multinomial.mean</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.multinomial.Multinomial.param_shape"><code class="docutils literal notranslate"><span class="pre">Multinomial.param_shape</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.multinomial.Multinomial.probs"><code class="docutils literal notranslate"><span class="pre">Multinomial.probs</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.multinomial.Multinomial.sample"><code class="docutils literal notranslate"><span class="pre">Multinomial.sample()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.multinomial.Multinomial.support"><code class="docutils literal notranslate"><span class="pre">Multinomial.support</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.multinomial.Multinomial.total_count"><code class="docutils literal notranslate"><span class="pre">Multinomial.total_count</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.multinomial.Multinomial.variance"><code class="docutils literal notranslate"><span class="pre">Multinomial.variance</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#multivariatenormal"><span class="hidden-section">MultivariateNormal</span></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.multivariate_normal.MultivariateNormal"><code class="docutils literal notranslate"><span class="pre">MultivariateNormal</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.multivariate_normal.MultivariateNormal.arg_constraints"><code class="docutils literal notranslate"><span class="pre">MultivariateNormal.arg_constraints</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.multivariate_normal.MultivariateNormal.covariance_matrix"><code class="docutils literal notranslate"><span class="pre">MultivariateNormal.covariance_matrix</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.multivariate_normal.MultivariateNormal.entropy"><code class="docutils literal notranslate"><span class="pre">MultivariateNormal.entropy()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.multivariate_normal.MultivariateNormal.expand"><code class="docutils literal notranslate"><span class="pre">MultivariateNormal.expand()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.multivariate_normal.MultivariateNormal.has_rsample"><code class="docutils literal notranslate"><span class="pre">MultivariateNormal.has_rsample</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.multivariate_normal.MultivariateNormal.log_prob"><code class="docutils literal notranslate"><span class="pre">MultivariateNormal.log_prob()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.multivariate_normal.MultivariateNormal.mean"><code class="docutils literal notranslate"><span class="pre">MultivariateNormal.mean</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.multivariate_normal.MultivariateNormal.mode"><code class="docutils literal notranslate"><span class="pre">MultivariateNormal.mode</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.multivariate_normal.MultivariateNormal.precision_matrix"><code class="docutils literal notranslate"><span class="pre">MultivariateNormal.precision_matrix</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.multivariate_normal.MultivariateNormal.rsample"><code class="docutils literal notranslate"><span class="pre">MultivariateNormal.rsample()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.multivariate_normal.MultivariateNormal.scale_tril"><code class="docutils literal notranslate"><span class="pre">MultivariateNormal.scale_tril</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.multivariate_normal.MultivariateNormal.support"><code class="docutils literal notranslate"><span class="pre">MultivariateNormal.support</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.multivariate_normal.MultivariateNormal.variance"><code class="docutils literal notranslate"><span class="pre">MultivariateNormal.variance</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#negativebinomial"><span class="hidden-section">NegativeBinomial</span></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.negative_binomial.NegativeBinomial"><code class="docutils literal notranslate"><span class="pre">NegativeBinomial</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.negative_binomial.NegativeBinomial.arg_constraints"><code class="docutils literal notranslate"><span class="pre">NegativeBinomial.arg_constraints</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.negative_binomial.NegativeBinomial.expand"><code class="docutils literal notranslate"><span class="pre">NegativeBinomial.expand()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.negative_binomial.NegativeBinomial.log_prob"><code class="docutils literal notranslate"><span class="pre">NegativeBinomial.log_prob()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.negative_binomial.NegativeBinomial.logits"><code class="docutils literal notranslate"><span class="pre">NegativeBinomial.logits</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.negative_binomial.NegativeBinomial.mean"><code class="docutils literal notranslate"><span class="pre">NegativeBinomial.mean</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.negative_binomial.NegativeBinomial.mode"><code class="docutils literal notranslate"><span class="pre">NegativeBinomial.mode</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.negative_binomial.NegativeBinomial.param_shape"><code class="docutils literal notranslate"><span class="pre">NegativeBinomial.param_shape</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.negative_binomial.NegativeBinomial.probs"><code class="docutils literal notranslate"><span class="pre">NegativeBinomial.probs</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.negative_binomial.NegativeBinomial.sample"><code class="docutils literal notranslate"><span class="pre">NegativeBinomial.sample()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.negative_binomial.NegativeBinomial.support"><code class="docutils literal notranslate"><span class="pre">NegativeBinomial.support</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.negative_binomial.NegativeBinomial.variance"><code class="docutils literal notranslate"><span class="pre">NegativeBinomial.variance</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#normal"><span class="hidden-section">Normal</span></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.normal.Normal"><code class="docutils literal notranslate"><span class="pre">Normal</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.normal.Normal.arg_constraints"><code class="docutils literal notranslate"><span class="pre">Normal.arg_constraints</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.normal.Normal.cdf"><code class="docutils literal notranslate"><span class="pre">Normal.cdf()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.normal.Normal.entropy"><code class="docutils literal notranslate"><span class="pre">Normal.entropy()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.normal.Normal.expand"><code class="docutils literal notranslate"><span class="pre">Normal.expand()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.normal.Normal.has_rsample"><code class="docutils literal notranslate"><span class="pre">Normal.has_rsample</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.normal.Normal.icdf"><code class="docutils literal notranslate"><span class="pre">Normal.icdf()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.normal.Normal.log_prob"><code class="docutils literal notranslate"><span class="pre">Normal.log_prob()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.normal.Normal.mean"><code class="docutils literal notranslate"><span class="pre">Normal.mean</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.normal.Normal.mode"><code class="docutils literal notranslate"><span class="pre">Normal.mode</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.normal.Normal.rsample"><code class="docutils literal notranslate"><span class="pre">Normal.rsample()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.normal.Normal.sample"><code class="docutils literal notranslate"><span class="pre">Normal.sample()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.normal.Normal.stddev"><code class="docutils literal notranslate"><span class="pre">Normal.stddev</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.normal.Normal.support"><code class="docutils literal notranslate"><span class="pre">Normal.support</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.normal.Normal.variance"><code class="docutils literal notranslate"><span class="pre">Normal.variance</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#onehotcategorical"><span class="hidden-section">OneHotCategorical</span></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.one_hot_categorical.OneHotCategorical"><code class="docutils literal notranslate"><span class="pre">OneHotCategorical</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.one_hot_categorical.OneHotCategorical.arg_constraints"><code class="docutils literal notranslate"><span class="pre">OneHotCategorical.arg_constraints</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.one_hot_categorical.OneHotCategorical.entropy"><code class="docutils literal notranslate"><span class="pre">OneHotCategorical.entropy()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.one_hot_categorical.OneHotCategorical.enumerate_support"><code class="docutils literal notranslate"><span class="pre">OneHotCategorical.enumerate_support()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.one_hot_categorical.OneHotCategorical.expand"><code class="docutils literal notranslate"><span class="pre">OneHotCategorical.expand()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.one_hot_categorical.OneHotCategorical.has_enumerate_support"><code class="docutils literal notranslate"><span class="pre">OneHotCategorical.has_enumerate_support</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.one_hot_categorical.OneHotCategorical.log_prob"><code class="docutils literal notranslate"><span class="pre">OneHotCategorical.log_prob()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.one_hot_categorical.OneHotCategorical.logits"><code class="docutils literal notranslate"><span class="pre">OneHotCategorical.logits</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.one_hot_categorical.OneHotCategorical.mean"><code class="docutils literal notranslate"><span class="pre">OneHotCategorical.mean</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.one_hot_categorical.OneHotCategorical.mode"><code class="docutils literal notranslate"><span class="pre">OneHotCategorical.mode</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.one_hot_categorical.OneHotCategorical.param_shape"><code class="docutils literal notranslate"><span class="pre">OneHotCategorical.param_shape</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.one_hot_categorical.OneHotCategorical.probs"><code class="docutils literal notranslate"><span class="pre">OneHotCategorical.probs</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.one_hot_categorical.OneHotCategorical.sample"><code class="docutils literal notranslate"><span class="pre">OneHotCategorical.sample()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.one_hot_categorical.OneHotCategorical.support"><code class="docutils literal notranslate"><span class="pre">OneHotCategorical.support</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.one_hot_categorical.OneHotCategorical.variance"><code class="docutils literal notranslate"><span class="pre">OneHotCategorical.variance</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#pareto"><span class="hidden-section">Pareto</span></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.pareto.Pareto"><code class="docutils literal notranslate"><span class="pre">Pareto</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.pareto.Pareto.arg_constraints"><code class="docutils literal notranslate"><span class="pre">Pareto.arg_constraints</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.pareto.Pareto.entropy"><code class="docutils literal notranslate"><span class="pre">Pareto.entropy()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.pareto.Pareto.expand"><code class="docutils literal notranslate"><span class="pre">Pareto.expand()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.pareto.Pareto.mean"><code class="docutils literal notranslate"><span class="pre">Pareto.mean</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.pareto.Pareto.mode"><code class="docutils literal notranslate"><span class="pre">Pareto.mode</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.pareto.Pareto.support"><code class="docutils literal notranslate"><span class="pre">Pareto.support</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.pareto.Pareto.variance"><code class="docutils literal notranslate"><span class="pre">Pareto.variance</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#poisson"><span class="hidden-section">Poisson</span></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.poisson.Poisson"><code class="docutils literal notranslate"><span class="pre">Poisson</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.poisson.Poisson.arg_constraints"><code class="docutils literal notranslate"><span class="pre">Poisson.arg_constraints</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.poisson.Poisson.expand"><code class="docutils literal notranslate"><span class="pre">Poisson.expand()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.poisson.Poisson.log_prob"><code class="docutils literal notranslate"><span class="pre">Poisson.log_prob()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.poisson.Poisson.mean"><code class="docutils literal notranslate"><span class="pre">Poisson.mean</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.poisson.Poisson.mode"><code class="docutils literal notranslate"><span class="pre">Poisson.mode</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.poisson.Poisson.sample"><code class="docutils literal notranslate"><span class="pre">Poisson.sample()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.poisson.Poisson.support"><code class="docutils literal notranslate"><span class="pre">Poisson.support</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.poisson.Poisson.variance"><code class="docutils literal notranslate"><span class="pre">Poisson.variance</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#relaxedbernoulli"><span class="hidden-section">RelaxedBernoulli</span></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.relaxed_bernoulli.RelaxedBernoulli"><code class="docutils literal notranslate"><span class="pre">RelaxedBernoulli</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.relaxed_bernoulli.RelaxedBernoulli.arg_constraints"><code class="docutils literal notranslate"><span class="pre">RelaxedBernoulli.arg_constraints</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.relaxed_bernoulli.RelaxedBernoulli.expand"><code class="docutils literal notranslate"><span class="pre">RelaxedBernoulli.expand()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.relaxed_bernoulli.RelaxedBernoulli.has_rsample"><code class="docutils literal notranslate"><span class="pre">RelaxedBernoulli.has_rsample</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.relaxed_bernoulli.RelaxedBernoulli.logits"><code class="docutils literal notranslate"><span class="pre">RelaxedBernoulli.logits</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.relaxed_bernoulli.RelaxedBernoulli.probs"><code class="docutils literal notranslate"><span class="pre">RelaxedBernoulli.probs</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.relaxed_bernoulli.RelaxedBernoulli.support"><code class="docutils literal notranslate"><span class="pre">RelaxedBernoulli.support</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.relaxed_bernoulli.RelaxedBernoulli.temperature"><code class="docutils literal notranslate"><span class="pre">RelaxedBernoulli.temperature</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#logitrelaxedbernoulli"><span class="hidden-section">LogitRelaxedBernoulli</span></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli"><code class="docutils literal notranslate"><span class="pre">LogitRelaxedBernoulli</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.arg_constraints"><code class="docutils literal notranslate"><span class="pre">LogitRelaxedBernoulli.arg_constraints</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.expand"><code class="docutils literal notranslate"><span class="pre">LogitRelaxedBernoulli.expand()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.log_prob"><code class="docutils literal notranslate"><span class="pre">LogitRelaxedBernoulli.log_prob()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.logits"><code class="docutils literal notranslate"><span class="pre">LogitRelaxedBernoulli.logits</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.param_shape"><code class="docutils literal notranslate"><span class="pre">LogitRelaxedBernoulli.param_shape</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.probs"><code class="docutils literal notranslate"><span class="pre">LogitRelaxedBernoulli.probs</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.rsample"><code class="docutils literal notranslate"><span class="pre">LogitRelaxedBernoulli.rsample()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.support"><code class="docutils literal notranslate"><span class="pre">LogitRelaxedBernoulli.support</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#relaxedonehotcategorical"><span class="hidden-section">RelaxedOneHotCategorical</span></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.relaxed_categorical.RelaxedOneHotCategorical"><code class="docutils literal notranslate"><span class="pre">RelaxedOneHotCategorical</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.arg_constraints"><code class="docutils literal notranslate"><span class="pre">RelaxedOneHotCategorical.arg_constraints</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.expand"><code class="docutils literal notranslate"><span class="pre">RelaxedOneHotCategorical.expand()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.has_rsample"><code class="docutils literal notranslate"><span class="pre">RelaxedOneHotCategorical.has_rsample</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.logits"><code class="docutils literal notranslate"><span class="pre">RelaxedOneHotCategorical.logits</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.probs"><code class="docutils literal notranslate"><span class="pre">RelaxedOneHotCategorical.probs</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.support"><code class="docutils literal notranslate"><span class="pre">RelaxedOneHotCategorical.support</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.temperature"><code class="docutils literal notranslate"><span class="pre">RelaxedOneHotCategorical.temperature</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#studentt"><span class="hidden-section">StudentT</span></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.studentT.StudentT"><code class="docutils literal notranslate"><span class="pre">StudentT</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.studentT.StudentT.arg_constraints"><code class="docutils literal notranslate"><span class="pre">StudentT.arg_constraints</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.studentT.StudentT.entropy"><code class="docutils literal notranslate"><span class="pre">StudentT.entropy()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.studentT.StudentT.expand"><code class="docutils literal notranslate"><span class="pre">StudentT.expand()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.studentT.StudentT.has_rsample"><code class="docutils literal notranslate"><span class="pre">StudentT.has_rsample</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.studentT.StudentT.log_prob"><code class="docutils literal notranslate"><span class="pre">StudentT.log_prob()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.studentT.StudentT.mean"><code class="docutils literal notranslate"><span class="pre">StudentT.mean</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.studentT.StudentT.mode"><code class="docutils literal notranslate"><span class="pre">StudentT.mode</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.studentT.StudentT.rsample"><code class="docutils literal notranslate"><span class="pre">StudentT.rsample()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.studentT.StudentT.support"><code class="docutils literal notranslate"><span class="pre">StudentT.support</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.studentT.StudentT.variance"><code class="docutils literal notranslate"><span class="pre">StudentT.variance</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#transformeddistribution"><span class="hidden-section">TransformedDistribution</span></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.transformed_distribution.TransformedDistribution"><code class="docutils literal notranslate"><span class="pre">TransformedDistribution</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.transformed_distribution.TransformedDistribution.arg_constraints"><code class="docutils literal notranslate"><span class="pre">TransformedDistribution.arg_constraints</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.transformed_distribution.TransformedDistribution.cdf"><code class="docutils literal notranslate"><span class="pre">TransformedDistribution.cdf()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.transformed_distribution.TransformedDistribution.expand"><code class="docutils literal notranslate"><span class="pre">TransformedDistribution.expand()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.transformed_distribution.TransformedDistribution.has_rsample"><code class="docutils literal notranslate"><span class="pre">TransformedDistribution.has_rsample</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.transformed_distribution.TransformedDistribution.icdf"><code class="docutils literal notranslate"><span class="pre">TransformedDistribution.icdf()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.transformed_distribution.TransformedDistribution.log_prob"><code class="docutils literal notranslate"><span class="pre">TransformedDistribution.log_prob()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.transformed_distribution.TransformedDistribution.rsample"><code class="docutils literal notranslate"><span class="pre">TransformedDistribution.rsample()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.transformed_distribution.TransformedDistribution.sample"><code class="docutils literal notranslate"><span class="pre">TransformedDistribution.sample()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.transformed_distribution.TransformedDistribution.support"><code class="docutils literal notranslate"><span class="pre">TransformedDistribution.support</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#uniform"><span class="hidden-section">Uniform</span></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.uniform.Uniform"><code class="docutils literal notranslate"><span class="pre">Uniform</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.uniform.Uniform.arg_constraints"><code class="docutils literal notranslate"><span class="pre">Uniform.arg_constraints</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.uniform.Uniform.cdf"><code class="docutils literal notranslate"><span class="pre">Uniform.cdf()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.uniform.Uniform.entropy"><code class="docutils literal notranslate"><span class="pre">Uniform.entropy()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.uniform.Uniform.expand"><code class="docutils literal notranslate"><span class="pre">Uniform.expand()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.uniform.Uniform.has_rsample"><code class="docutils literal notranslate"><span class="pre">Uniform.has_rsample</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.uniform.Uniform.icdf"><code class="docutils literal notranslate"><span class="pre">Uniform.icdf()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.uniform.Uniform.log_prob"><code class="docutils literal notranslate"><span class="pre">Uniform.log_prob()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.uniform.Uniform.mean"><code class="docutils literal notranslate"><span class="pre">Uniform.mean</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.uniform.Uniform.mode"><code class="docutils literal notranslate"><span class="pre">Uniform.mode</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.uniform.Uniform.rsample"><code class="docutils literal notranslate"><span class="pre">Uniform.rsample()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.uniform.Uniform.stddev"><code class="docutils literal notranslate"><span class="pre">Uniform.stddev</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.uniform.Uniform.support"><code class="docutils literal notranslate"><span class="pre">Uniform.support</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.uniform.Uniform.variance"><code class="docutils literal notranslate"><span class="pre">Uniform.variance</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#vonmises"><span class="hidden-section">VonMises</span></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.von_mises.VonMises"><code class="docutils literal notranslate"><span class="pre">VonMises</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.von_mises.VonMises.arg_constraints"><code class="docutils literal notranslate"><span class="pre">VonMises.arg_constraints</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.von_mises.VonMises.expand"><code class="docutils literal notranslate"><span class="pre">VonMises.expand()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.von_mises.VonMises.has_rsample"><code class="docutils literal notranslate"><span class="pre">VonMises.has_rsample</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.von_mises.VonMises.log_prob"><code class="docutils literal notranslate"><span class="pre">VonMises.log_prob()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.von_mises.VonMises.mean"><code class="docutils literal notranslate"><span class="pre">VonMises.mean</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.von_mises.VonMises.mode"><code class="docutils literal notranslate"><span class="pre">VonMises.mode</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.von_mises.VonMises.sample"><code class="docutils literal notranslate"><span class="pre">VonMises.sample()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.von_mises.VonMises.support"><code class="docutils literal notranslate"><span class="pre">VonMises.support</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.von_mises.VonMises.variance"><code class="docutils literal notranslate"><span class="pre">VonMises.variance</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#weibull"><span class="hidden-section">Weibull</span></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.weibull.Weibull"><code class="docutils literal notranslate"><span class="pre">Weibull</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.weibull.Weibull.arg_constraints"><code class="docutils literal notranslate"><span class="pre">Weibull.arg_constraints</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.weibull.Weibull.entropy"><code class="docutils literal notranslate"><span class="pre">Weibull.entropy()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.weibull.Weibull.expand"><code class="docutils literal notranslate"><span class="pre">Weibull.expand()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.weibull.Weibull.mean"><code class="docutils literal notranslate"><span class="pre">Weibull.mean</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.weibull.Weibull.mode"><code class="docutils literal notranslate"><span class="pre">Weibull.mode</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.weibull.Weibull.support"><code class="docutils literal notranslate"><span class="pre">Weibull.support</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.weibull.Weibull.variance"><code class="docutils literal notranslate"><span class="pre">Weibull.variance</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#wishart"><span class="hidden-section">Wishart</span></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.wishart.Wishart"><code class="docutils literal notranslate"><span class="pre">Wishart</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.wishart.Wishart.arg_constraints"><code class="docutils literal notranslate"><span class="pre">Wishart.arg_constraints</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.wishart.Wishart.covariance_matrix"><code class="docutils literal notranslate"><span class="pre">Wishart.covariance_matrix</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.wishart.Wishart.entropy"><code class="docutils literal notranslate"><span class="pre">Wishart.entropy()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.wishart.Wishart.expand"><code class="docutils literal notranslate"><span class="pre">Wishart.expand()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.wishart.Wishart.has_rsample"><code class="docutils literal notranslate"><span class="pre">Wishart.has_rsample</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.wishart.Wishart.log_prob"><code class="docutils literal notranslate"><span class="pre">Wishart.log_prob()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.wishart.Wishart.mean"><code class="docutils literal notranslate"><span class="pre">Wishart.mean</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.wishart.Wishart.mode"><code class="docutils literal notranslate"><span class="pre">Wishart.mode</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.wishart.Wishart.precision_matrix"><code class="docutils literal notranslate"><span class="pre">Wishart.precision_matrix</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.wishart.Wishart.rsample"><code class="docutils literal notranslate"><span class="pre">Wishart.rsample()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.wishart.Wishart.scale_tril"><code class="docutils literal notranslate"><span class="pre">Wishart.scale_tril</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.wishart.Wishart.support"><code class="docutils literal notranslate"><span class="pre">Wishart.support</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.wishart.Wishart.variance"><code class="docutils literal notranslate"><span class="pre">Wishart.variance</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#module-torch.distributions.kl"><cite>KL Divergence</cite></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.kl.kl_divergence"><code class="docutils literal notranslate"><span class="pre">kl_divergence()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.kl.register_kl"><code class="docutils literal notranslate"><span class="pre">register_kl()</span></code></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#module-torch.distributions.transforms"><cite>Transforms</cite></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.transforms.AbsTransform"><code class="docutils literal notranslate"><span class="pre">AbsTransform</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.transforms.AffineTransform"><code class="docutils literal notranslate"><span class="pre">AffineTransform</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.transforms.CatTransform"><code class="docutils literal notranslate"><span class="pre">CatTransform</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.transforms.ComposeTransform"><code class="docutils literal notranslate"><span class="pre">ComposeTransform</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.transforms.CorrCholeskyTransform"><code class="docutils literal notranslate"><span class="pre">CorrCholeskyTransform</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.transforms.CumulativeDistributionTransform"><code class="docutils literal notranslate"><span class="pre">CumulativeDistributionTransform</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.transforms.ExpTransform"><code class="docutils literal notranslate"><span class="pre">ExpTransform</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.transforms.IndependentTransform"><code class="docutils literal notranslate"><span class="pre">IndependentTransform</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.transforms.LowerCholeskyTransform"><code class="docutils literal notranslate"><span class="pre">LowerCholeskyTransform</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.transforms.PositiveDefiniteTransform"><code class="docutils literal notranslate"><span class="pre">PositiveDefiniteTransform</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.transforms.PowerTransform"><code class="docutils literal notranslate"><span class="pre">PowerTransform</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.transforms.ReshapeTransform"><code class="docutils literal notranslate"><span class="pre">ReshapeTransform</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.transforms.SigmoidTransform"><code class="docutils literal notranslate"><span class="pre">SigmoidTransform</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.transforms.SoftplusTransform"><code class="docutils literal notranslate"><span class="pre">SoftplusTransform</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.transforms.TanhTransform"><code class="docutils literal notranslate"><span class="pre">TanhTransform</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.transforms.SoftmaxTransform"><code class="docutils literal notranslate"><span class="pre">SoftmaxTransform</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.transforms.StackTransform"><code class="docutils literal notranslate"><span class="pre">StackTransform</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.transforms.StickBreakingTransform"><code class="docutils literal notranslate"><span class="pre">StickBreakingTransform</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.transforms.Transform"><code class="docutils literal notranslate"><span class="pre">Transform</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.transforms.Transform.inv"><code class="docutils literal notranslate"><span class="pre">Transform.inv</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.transforms.Transform.sign"><code class="docutils literal notranslate"><span class="pre">Transform.sign</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.transforms.Transform.log_abs_det_jacobian"><code class="docutils literal notranslate"><span class="pre">Transform.log_abs_det_jacobian()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.transforms.Transform.forward_shape"><code class="docutils literal notranslate"><span class="pre">Transform.forward_shape()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.transforms.Transform.inverse_shape"><code class="docutils literal notranslate"><span class="pre">Transform.inverse_shape()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#module-torch.distributions.constraints"><cite>Constraints</cite></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.constraints.Constraint"><code class="docutils literal notranslate"><span class="pre">Constraint</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.constraints.Constraint.check"><code class="docutils literal notranslate"><span class="pre">Constraint.check()</span></code></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.constraints.cat"><code class="docutils literal notranslate"><span class="pre">cat</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.constraints.dependent_property"><code class="docutils literal notranslate"><span class="pre">dependent_property</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.constraints.greater_than"><code class="docutils literal notranslate"><span class="pre">greater_than</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.constraints.greater_than_eq"><code class="docutils literal notranslate"><span class="pre">greater_than_eq</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.constraints.independent"><code class="docutils literal notranslate"><span class="pre">independent</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.constraints.integer_interval"><code class="docutils literal notranslate"><span class="pre">integer_interval</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.constraints.interval"><code class="docutils literal notranslate"><span class="pre">interval</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.constraints.half_open_interval"><code class="docutils literal notranslate"><span class="pre">half_open_interval</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.constraints.is_dependent"><code class="docutils literal notranslate"><span class="pre">is_dependent()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.constraints.less_than"><code class="docutils literal notranslate"><span class="pre">less_than</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.constraints.multinomial"><code class="docutils literal notranslate"><span class="pre">multinomial</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.constraints.stack"><code class="docutils literal notranslate"><span class="pre">stack</span></code></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#module-torch.distributions.constraint_registry"><cite>Constraint Registry</cite></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.constraint_registry.ConstraintRegistry"><code class="docutils literal notranslate"><span class="pre">ConstraintRegistry</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributions.constraint_registry.ConstraintRegistry.register"><code class="docutils literal notranslate"><span class="pre">ConstraintRegistry.register()</span></code></a></li>
</ul>
</li>
</ul>
</li>
</ul>
  </nav></div>
    
       <div class="sidebar-secondary-item">

  
  <div class="tocsection editthispage">
    <a href="https://github.com/pytorch/pytorch/edit/main/docs/cpp/source/distributions.rst">
      <i class="fa-solid fa-pencil"></i>
      
      
        
          Edit on GitHub
        
      
    </a>
  </div>
</div>
    
       <div class="sidebar-secondary-item">
    <div class="tocsection sourcelink">
      <a href="_sources/distributions.rst.txt">
        <i class="fa-solid fa-file-lines"></i> Show Source
      </a>
    </div>
</div>
    


<div class="sidebar-secondary-item">
  <div class="sidebar-heading">PyTorch Community</div>
  <ul style="list-style-type: none; padding: 0;">
  
   <li><a class="nav-link nav-external" href="https://pytorch.org/docs/stable/community/index.html" style="color: var(--pst-color-text-muted)">PyTorch Governance</a></li>
  
   <li><a class="nav-link nav-external" href="https://pytorch.org/docs/stable/community/design.html" style="color: var(--pst-color-text-muted)">PyTorch Design Philosophy</a></li>
  
   <li><a class="nav-link nav-external" href="https://github.com/pytorch/pytorch/wiki/The-Ultimate-Guide-to-PyTorch-Contributions" style="color: var(--pst-color-text-muted)">The Ultimate Guide to PyTorch Contributions</a></li>
  
  </ul>
</div>


<div class="sidebar-secondary-item">
 <div class="sidebar-heading">Language Bindings</div>
 <ul style="list-style-type: none; padding: 0;">
 
  <li><a class="nav-link nav-external" href="https://pytorch.org/docs/stable/cpp_index.html" style="color: var(--pst-color-text-muted)">C++</a></li>
 
  <li><a class="nav-link nav-external" href="https://pytorch.org/javadoc/" style="color: var(--pst-color-text-muted)">Javadoc</a></li>
 
  <li><a class="nav-link nav-external" href="https://github.com/pytorch/multipy" style="color: var(--pst-color-text-muted)">torch.multiply</a></li>
 
 </ul>
</div>


<div class="sidebar-secondary-item">
  <div class="sidebar-heading">PyTorch Libraries</div>
  <ul style="list-style-type: none; padding: 0; padding-bottom: 80px;">
  
   <li><a class="nav-link nav-external" href="https://pytorch.org/audio/stable/" style="color: var(--pst-color-text-muted)">torchaudio</a></li>
  
   <li><a class="nav-link nav-external" href="https://pytorch.org/ao" style="color: var(--pst-color-text-muted)">torchao</a></li>
  
   <li><a class="nav-link nav-external" href="https://pytorch.org/executorch" style="color: var(--pst-color-text-muted)">ExecuTorch</a></li>
  
   <li><a class="nav-link nav-external" href="https://pytorch.org/torchrec" style="color: var(--pst-color-text-muted)">torchrec</a></li>
  
   <li><a class="nav-link nav-external" href="https://pytorch.org/serve/" style="color: var(--pst-color-text-muted)">torchserve</a></li>
  
   <li><a class="nav-link nav-external" href="https://pytorch.org/data" style="color: var(--pst-color-text-muted)">torchdata</a></li>
  
   <li><a class="nav-link nav-external" href="https://pytorch.org/data" style="color: var(--pst-color-text-muted)">torchvision</a></li>
  
   <li><a class="nav-link nav-external" href="https://pytorch.org/xla" style="color: var(--pst-color-text-muted)">PyTorch on XLA Devices</a></li>
  
  </ul>
</div>

</div>
</div>
              
            
          </div>
          <footer class="bd-footer-content">
            
          </footer>
        
      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

   <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
      <div class="container">
        <div class="row">
          <div class="col-md-4 text-center">
            <h2>Docs</h2>
            <p>Access comprehensive developer documentation for PyTorch</p>
            <a class="with-right-arrow" href="">View Docs</a>
          </div>

          <div class="col-md-4 text-center">
            <h2>Tutorials</h2>
            <p>Get in-depth tutorials for beginners and advanced developers</p>
            <a class="with-right-arrow" href="">View Tutorials</a>
          </div>

          <div class="col-md-4 text-center">
            <h2>Resources</h2>
            <p>Find development resources and get your questions answered</p>
            <a class="with-right-arrow" href="">View Resources</a>
          </div>
        </div>
      </div>
  </div>

  <footer class="site-footer">
      <div class="container footer-container">
        <div class="footer-logo-wrapper">
          <a href="" class="footer-logo"></a>
        </div>

        <div class="footer-links-wrapper">
          <div class="footer-links-col">
            <ul>
              <li class="list-title"><a href="">PyTorch</a></li>
              <li><a href="">Get Started</a></li>
              <li><a href="">Features</a></li>
              <li><a href="">Ecosystem</a></li>
              <li><a href="">Blog</a></li>
              <li><a href="">Contributing</a></li>
            </ul>
          </div>

          <div class="footer-links-col">
            <ul>
              <li class="list-title"><a href="">Resources</a></li>
              <li><a href="">Tutorials</a></li>
              <li><a href="">Docs</a></li>
              <li><a href="" target="_blank">Discuss</a></li>
              <li><a href="" target="_blank">Github Issues</a></li>
              <li><a href="" target="_blank">Brand Guidelines</a></li>
            </ul>
          </div>

          <div class="footer-links-col">
            <ul>
              <li class="list-title">Stay up to date</li>
              <li><a href="" target="_blank">Facebook</a></li>
              <li><a href="" target="_blank">Twitter</a></li>
              <li><a href="" target="_blank">YouTube</a></li>
              <li><a href="" target="_blank">LinkedIn</a></li>
            </ul>
            </div>

          <div class="footer-links-col">
            <ul>
              <li class="list-title">PyTorch Podcasts</li>
              <li><a href="" target="_blank">Spotify</a></li>
              <li><a href="" target="_blank">Apple</a></li>
              <li><a href="" target="_blank">Google</a></li>
              <li><a href="" target="_blank">Amazon</a></li>
            </ul>
           </div>
          </div>

          <div class="privacy-policy">
            <ul>
              <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/terms/" target="_blank">Terms</a></li>
              <li class="privacy-policy-links">|</li>
              <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/privacy-policy/" target="_blank">Privacy</a></li>
            </ul>
          </div>
          <div class="copyright">
          <p> Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation.
            For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see
            <a href="https://www.linuxfoundation.org/policies/">www.linuxfoundation.org/policies/</a>. The PyTorch Foundation supports the PyTorch open source
            project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC,
            please see <a href="https://www.lfprojects.org/policies/">www.lfprojects.org/policies/</a>.</p>
        </div>
       </div>

  <div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebooks Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="_static/img/pytorch-x.svg">
  </div>
</div>
 </footer>
   
  <footer class="bd-footer"><div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebooks Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="_static/img/pytorch-x.svg">
  </div>
</div>
<div class="bd-footer__inner bd-page-width">
  
    <div class="footer-items__start">
      
        <div class="footer-item">

  <p class="copyright">
    
       Copyright PyTorch Contributors.
      <br/>
    
  </p>
</div>
      
        <div class="footer-item">

  <p class="sphinx-version">
    Created using <a href="https://www.sphinx-doc.org/">Sphinx</a> 5.3.0.
    <br/>
  </p>
</div>
      
    </div>
  
  
  
    <div class="footer-items__end">
      
        <div class="footer-item">
<p class="theme-version">
  Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.15.4.
</p></div>
      
    </div>
  
</div>

  </footer>

  </body>
</html>