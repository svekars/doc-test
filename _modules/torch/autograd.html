
<!DOCTYPE html>


<html lang="en" data-content_root="../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>torch.autograd &#8212; PyTorch main documentation</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  <!--
    this give us a css class that will be invisible only if js is disabled
  -->
  <noscript>
    <style>
      .pst-js-only { display: none !important; }

    </style>
  </noscript>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css?v=949a1ff5" />
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/katex.min.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/katex-math.css?v=91adb8b6" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="../../_static/css/jit.css?v=8de1ea5d" />
    <link rel="stylesheet" type="text/css" href="../../_static/css/custom.css?v=3ccf5357" />
    <link rel="stylesheet" type="text/css" href="../../_static/css/custom2.css?v=baa440dc" />
  
  <!-- So that users can add custom icons -->
  <script src="../../_static/scripts/fontawesome.js?digest=8878045cc6db502f8baf"></script>
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf" />

    <script src="../../_static/documentation_options.js?v=a8da1a53"></script>
    <script src="../../_static/doctools.js?v=9bcbadda"></script>
    <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../_static/copybutton.js?v=940804e7"></script>
    <script src="../../_static/design-tabs.js?v=f930bc37"></script>
    <script type="module" src="https://cdn.jsdelivr.net/npm/mermaid@11.2.0/dist/mermaid.esm.min.mjs"></script>
    <script type="module" src="https://cdn.jsdelivr.net/npm/@mermaid-js/layout-elk@0.1.4/dist/mermaid-layout-elk.esm.min.mjs"></script>
    <script type="module">import mermaid from "https://cdn.jsdelivr.net/npm/mermaid@11.2.0/dist/mermaid.esm.min.mjs";import elkLayouts from "https://cdn.jsdelivr.net/npm/@mermaid-js/layout-elk@0.1.4/dist/mermaid-layout-elk.esm.min.mjs";mermaid.registerLayoutLoaders(elkLayouts);mermaid.initialize({startOnLoad:false});</script>
    <script src="https://cdn.jsdelivr.net/npm/d3@7.9.0/dist/d3.min.js"></script>
    <script type="module">
import mermaid from "https://cdn.jsdelivr.net/npm/mermaid@11.2.0/dist/mermaid.esm.min.mjs";
window.addEventListener("load", () => mermaid.run());
</script>
    <script>DOCUMENTATION_OPTIONS.pagename = '_modules/torch/autograd';</script>
    <script src="../../_static/js/star-rating.js?v=8861fcb6"></script>
    <script src="../../_static/js/send-feedback.js?v=5646bf45"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />

  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  <meta name="docsearch:version" content="main (2.6.0 )" />
<script src="https://code.jquery.com/jquery-3.7.1.min.js"></script>
<script type="text/javascript" src="../../_static/js/send-feedback.js"></script>
<script type="text/javascript" src="../../_static/js/star-rating.js"></script>
<script type="text/javascript" src="../../_static/js/cookie-banner.js"></script>

<script async src="https://www.googletagmanager.com/gtag/js?id=G-TEST12345"></script>
    <script type="text/javascript">
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-TEST12345');
    </script>

  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
<body data-feedback-url="https://github.com/pytorch/pytorch">
  <div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>

          <li class="main-menu-item">
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Learn
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="">
                  <span class=dropdown-title>Get Started</span>
                  <p>Run PyTorch locally or get started quickly with one of the supported cloud platforms</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Tutorials</span>
                  <p>Whats new in PyTorch tutorials</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Learn the Basics</span>
                  <p>Familiarize yourself with PyTorch concepts and modules</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">PyTorch Recipes</span>
                  <p>Bite-size, ready-to-deploy PyTorch code examples</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Intro to PyTorch - YouTube Series</span>
                  <p>Master PyTorch basics with our engaging YouTube tutorial series</p>
                </a>
              </div>
            </div>
          </li>

          <li>
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Ecosystem
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Tools</span>
                  <p>Learn about the tools and frameworks in the PyTorch Ecosystem</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class=dropdown-title>Community</span>
                  <p>Join the PyTorch developer community to contribute, learn, and get your questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="" target="_blank">
                  <span class=dropdown-title>Forums</span>
                  <p>A place to discuss PyTorch code, issues, install, research</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class=dropdown-title>Developer Resources</span>
                  <p>Find resources and get questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Contributor Awards - 2024</span>
                  <p>Award winners announced at this year's PyTorch Conference</p>
                </a>
              </div>
            </div>
          </li>

          <li>
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Edge
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">About PyTorch Edge</span>
                  <p>Build innovative and privacy-aware AI experiences for edge devices</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">ExecuTorch</span>
                  <p>End-to-end solution for enabling on-device inference capabilities across mobile and edge devices</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">ExecuTorch Docs</span>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Docs
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">PyTorch</span>
                  <p>Explore the documentation for comprehensive guidance on how to use PyTorch</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">PyTorch Domains</span>
                  <p>Read the PyTorch Domains documentation to learn more about domain-specific libraries</p>
                </a>
              </div>
            </div>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Blogs & News
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">PyTorch Blog</span>
                  <p>Catch up on the latest technical news and happenings</p>
                </a>
                 <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Community Blog</span>
                  <p>Stories from the PyTorch ecosystem</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Videos</span>
                  <p>Learn about the latest PyTorch tutorials, new, and more </p>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Community Stories</span>
                  <p>Learn how our community solves real, everyday machine learning problems with PyTorch</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Events</span>
                  <p>Find events, webinars, and podcasts</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Newsletter</span>
                  <p>Stay up-to-date with the latest updates</p>
                </a>
            </div>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                About
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">PyTorch Foundation</span>
                  <p>Learn more about the PyTorch Foundation</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Governing Board</span>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Cloud Credit Program</span>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Technical Advisory Council</span>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Staff</span>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Contact Us</span>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
            <div class="no-dropdown">
              <a href="" data-cta="join">
                Become a Member
              </a>
            </div>
          </li>
          <li>
           <div class="main-menu-item">
             <a href="" class="github-icon">
             </a>
           </div>
          </li>
          <!--- TODO: This block adds the search icon to the nav bar. We will enable it later.
          <li>
            <div class="main-menu-item">
             <a href="" class="search-icon">
             </a>
            </div>
          </li>
          --->
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>
  </div>
</div>

  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <dialog id="pst-search-dialog">
    
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </dialog>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
<div class="bd-header__inner bd-page-width">
  <button class="pst-navbar-icon sidebar-toggle primary-toggle" aria-label="Site navigation">
    <span class="fa-solid fa-bars"></span>
  </button>
  
  
  <div class=" navbar-header-items__start">
    
      <div class="navbar-item">

  
    
  

<a class="navbar-brand logo" href="../../index.html">
  
  
  
  
  
  
    <p class="title logo__title">Home</p>
  
</a></div>
    
  </div>
  
  <div class=" navbar-header-items">
    
    <div class="me-auto navbar-header-items__center">
      
        <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../python-api/index.html">
    Python API
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../notes/index.html">
    Developer Notes
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-external" href="https://pytorch.org/tutorials/">
    Tutorials
  </a>
</li>

  </ul>
</nav></div>
      
    </div>
    
    
    <div class="navbar-header-items__end">
      
        <div class="navbar-item navbar-persistent--container">
          

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button>
        </div>
      
      
        <div class="navbar-item">
<div class="version-switcher__container dropdown pst-js-only">
  <button id="pst-version-switcher-button-2"
    type="button"
    class="version-switcher__button btn btn-sm dropdown-toggle"
    data-bs-toggle="dropdown"
    aria-haspopup="listbox"
    aria-controls="pst-version-switcher-list-2"
    aria-label="Version switcher list"
  >
    Choose version  <!-- this text may get changed later by javascript -->
    <span class="caret"></span>
  </button>
  <div id="pst-version-switcher-list-2"
    class="version-switcher__menu dropdown-menu list-group-flush py-0"
    role="listbox" aria-labelledby="pst-version-switcher-button-2">
    <!-- dropdown will be populated by javascript on page load -->
  </div>
</div></div>
      
        <div class="navbar-item">

<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button></div>
      
        <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://x.com/PyTorch" title="X" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-x-twitter fa-lg" aria-hidden="true"></i>
            <span class="sr-only">X</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/pytorch/pytorch" title="GitHub" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">GitHub</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://pypi.org/project/torch" title="PyPI" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-custom fa-pypi fa-lg" aria-hidden="true"></i>
            <span class="sr-only">PyPI</span></a>
        </li>
</ul></div>
      
    </div>
    
  </div>
  
  
    <div class="navbar-persistent--mobile">

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button>
    </div>
  

  
</div>

    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
        
      
      <dialog id="pst-primary-sidebar-modal"></dialog>
      <div id="pst-primary-sidebar" class="bd-sidebar-primary bd-sidebar hide-on-wide">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
      <div class="sidebar-header-items__center">
        
          
          
            <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../python-api/index.html">
    Python API
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../notes/index.html">
    Developer Notes
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-external" href="https://pytorch.org/tutorials/">
    Tutorials
  </a>
</li>

  </ul>
</nav></div>
          
        
      </div>
    
    
    
      <div class="sidebar-header-items__end">
        
          <div class="navbar-item">
<div class="version-switcher__container dropdown pst-js-only">
  <button id="pst-version-switcher-button-3"
    type="button"
    class="version-switcher__button btn btn-sm dropdown-toggle"
    data-bs-toggle="dropdown"
    aria-haspopup="listbox"
    aria-controls="pst-version-switcher-list-3"
    aria-label="Version switcher list"
  >
    Choose version  <!-- this text may get changed later by javascript -->
    <span class="caret"></span>
  </button>
  <div id="pst-version-switcher-list-3"
    class="version-switcher__menu dropdown-menu list-group-flush py-0"
    role="listbox" aria-labelledby="pst-version-switcher-button-3">
    <!-- dropdown will be populated by javascript on page load -->
  </div>
</div></div>
        
          <div class="navbar-item">

<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button></div>
        
          <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://x.com/PyTorch" title="X" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-x-twitter fa-lg" aria-hidden="true"></i>
            <span class="sr-only">X</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/pytorch/pytorch" title="GitHub" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">GitHub</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://pypi.org/project/torch" title="PyPI" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-custom fa-pypi fa-lg" aria-hidden="true"></i>
            <span class="sr-only">PyPI</span></a>
        </li>
</ul></div>
        
      </div>
    
  </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
      <div class="sidebar-primary-item">
<div id="ethical-ad-placement"
      class="flat"
      data-ea-publisher="readthedocs"
      data-ea-type="readthedocs-sidebar"
      data-ea-manual="true">
</div></div>
  </div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        
          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item">

<nav aria-label="Breadcrumb" class="d-print-none">
  <ul class="bd-breadcrumbs">
    
    <li class="breadcrumb-item breadcrumb-home">
      <a href="../../index.html" class="nav-link" aria-label="Home">
        <i class="fa-solid fa-home"></i>
      </a>
    </li>
    
    <li class="breadcrumb-item"><a href="../index.html" class="nav-link">Module code</a></li>
    
    
    <li class="breadcrumb-item"><a href="../torch.html" class="nav-link">torch</a></li>
    
    <li class="breadcrumb-item active" aria-current="page"><span class="ellipsis">torch.autograd</span></li>
  </ul>
</nav>
</div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">
<div class="rating">
    Rate this Page
    <div class="stars">
        
        <span class="star" data-value="1">★</span>
        
        <span class="star" data-value="2">★</span>
        
        <span class="star" data-value="3">★</span>
        
        <span class="star" data-value="4">★</span>
        
        <span class="star" data-value="5">★</span>
        
    </div>
</div>
</div>
      
    </div>
  
</div>
</div>
              
              
              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <h1>Source code for torch.autograd</h1><div class="highlight"><pre>
<span></span><span class="c1"># mypy: allow-untyped-defs</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">``torch.autograd`` provides classes and functions implementing automatic differentiation of arbitrary scalar valued functions.</span>

<span class="sd">It requires minimal changes to the existing code - you only need to declare :class:`Tensor` s</span>
<span class="sd">for which gradients should be computed with the ``requires_grad=True`` keyword.</span>
<span class="sd">As of now, we only support autograd for floating point :class:`Tensor` types (</span>
<span class="sd">half, float, double and bfloat16) and complex :class:`Tensor` types (cfloat, cdouble).</span>
<span class="sd">&quot;&quot;&quot;</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">warnings</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">typing</span><span class="w"> </span><span class="kn">import</span> <span class="n">cast</span><span class="p">,</span> <span class="n">List</span><span class="p">,</span> <span class="n">Optional</span><span class="p">,</span> <span class="n">Sequence</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">,</span> <span class="n">Union</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch</span><span class="w"> </span><span class="kn">import</span> <span class="n">_vmap_internals</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch.overrides</span><span class="w"> </span><span class="kn">import</span> <span class="n">handle_torch_function</span><span class="p">,</span> <span class="n">has_torch_function</span><span class="p">,</span> <span class="n">is_tensor_like</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch.types</span><span class="w"> </span><span class="kn">import</span> <span class="n">_size</span><span class="p">,</span> <span class="n">_TensorOrTensors</span><span class="p">,</span> <span class="n">_TensorOrTensorsOrGradEdge</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">.</span><span class="w"> </span><span class="kn">import</span> <span class="n">forward_ad</span><span class="p">,</span> <span class="n">functional</span><span class="p">,</span> <span class="n">graph</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">.anomaly_mode</span><span class="w"> </span><span class="kn">import</span> <span class="n">detect_anomaly</span><span class="p">,</span> <span class="n">set_detect_anomaly</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">.function</span><span class="w"> </span><span class="kn">import</span> <span class="n">Function</span><span class="p">,</span> <span class="n">NestedIOFunction</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">.grad_mode</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">_force_original_view_tracking</span><span class="p">,</span>
    <span class="n">_unsafe_preserve_version_counter</span><span class="p">,</span>
    <span class="n">enable_grad</span><span class="p">,</span>
    <span class="n">inference_mode</span><span class="p">,</span>
    <span class="n">no_grad</span><span class="p">,</span>
    <span class="n">set_grad_enabled</span><span class="p">,</span>
    <span class="n">set_multithreading_enabled</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">.gradcheck</span><span class="w"> </span><span class="kn">import</span> <span class="n">gradcheck</span><span class="p">,</span> <span class="n">gradgradcheck</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">.graph</span><span class="w"> </span><span class="kn">import</span> <span class="n">_engine_run_backward</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">.variable</span><span class="w"> </span><span class="kn">import</span> <span class="n">Variable</span>


<span class="n">__all__</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s2">&quot;Variable&quot;</span><span class="p">,</span>
    <span class="s2">&quot;Function&quot;</span><span class="p">,</span>
    <span class="s2">&quot;backward&quot;</span><span class="p">,</span>
    <span class="s2">&quot;grad_mode&quot;</span><span class="p">,</span>
    <span class="s2">&quot;NestedIOFunction&quot;</span><span class="p">,</span>
    <span class="s2">&quot;detect_anomaly&quot;</span><span class="p">,</span>
    <span class="s2">&quot;enable_grad&quot;</span><span class="p">,</span>
    <span class="s2">&quot;grad&quot;</span><span class="p">,</span>
    <span class="s2">&quot;gradcheck&quot;</span><span class="p">,</span>
    <span class="s2">&quot;gradgradcheck&quot;</span><span class="p">,</span>
    <span class="s2">&quot;inference_mode&quot;</span><span class="p">,</span>
    <span class="s2">&quot;no_grad&quot;</span><span class="p">,</span>
    <span class="s2">&quot;set_detect_anomaly&quot;</span><span class="p">,</span>
    <span class="s2">&quot;set_grad_enabled&quot;</span><span class="p">,</span>
    <span class="s2">&quot;set_multithreading_enabled&quot;</span><span class="p">,</span>
    <span class="s2">&quot;variable&quot;</span><span class="p">,</span>
<span class="p">]</span>

<span class="n">_OptionalTensor</span> <span class="o">=</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span>
<span class="n">_ShapeorNestedShape</span> <span class="o">=</span> <span class="n">Union</span><span class="p">[</span><span class="n">_size</span><span class="p">,</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">_size</span><span class="p">],</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_calculate_shape</span><span class="p">(</span>
    <span class="n">output</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">graph</span><span class="o">.</span><span class="n">GradientEdge</span><span class="p">],</span>
    <span class="n">grad</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">is_grads_batched</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">_ShapeorNestedShape</span><span class="p">,</span> <span class="n">_ShapeorNestedShape</span><span class="p">]:</span>
    <span class="c1"># is_same_size ensures that both tensors are either nested or non nested</span>
    <span class="c1"># circular import</span>
    <span class="kn">from</span><span class="w"> </span><span class="nn">torch.nested._internal.nested_tensor</span><span class="w"> </span><span class="kn">import</span> <span class="n">NestedTensor</span>

    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">graph</span><span class="o">.</span><span class="n">GradientEdge</span><span class="p">):</span>
        <span class="c1"># We have already checked that we are not a C++ NestedTensor</span>
        <span class="k">if</span> <span class="n">is_grads_batched</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;Batched grads are not supported with GradientEdge&quot;</span><span class="p">)</span>
        <span class="n">out_metadata</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">node</span><span class="o">.</span><span class="n">_input_metadata</span><span class="p">[</span><span class="n">output</span><span class="o">.</span><span class="n">output_nr</span><span class="p">]</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">(</span><span class="n">out_metadata</span><span class="o">.</span><span class="n">shape</span><span class="p">),</span> <span class="n">grad</span><span class="o">.</span><span class="n">shape</span>

    <span class="k">if</span> <span class="n">output</span><span class="o">.</span><span class="n">is_nested</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">NestedTensor</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">is_grads_batched</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;Batched grads are not supported with Nested Tensor.&quot;</span><span class="p">)</span>
        <span class="n">out_shape</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">_nested_tensor_size</span><span class="p">()</span>
        <span class="n">grad_shape</span> <span class="o">=</span> <span class="n">grad</span><span class="o">.</span><span class="n">_nested_tensor_size</span><span class="p">()</span>

        <span class="k">return</span> <span class="n">out_shape</span><span class="p">,</span> <span class="n">grad_shape</span>

    <span class="n">reg_out_shape</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">reg_grad_shape</span> <span class="o">=</span> <span class="n">grad</span><span class="o">.</span><span class="n">shape</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">is_grads_batched</span> <span class="k">else</span> <span class="n">grad</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span>
    <span class="k">return</span> <span class="n">reg_out_shape</span><span class="p">,</span> <span class="n">reg_grad_shape</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_make_grads</span><span class="p">(</span>
    <span class="n">outputs</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">Sequence</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">graph</span><span class="o">.</span><span class="n">GradientEdge</span><span class="p">]],</span>
    <span class="n">grads</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">_OptionalTensor</span><span class="p">],</span>
    <span class="n">is_grads_batched</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">_OptionalTensor</span><span class="p">,</span> <span class="o">...</span><span class="p">]:</span>
    <span class="n">new_grads</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">_OptionalTensor</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">out</span><span class="p">,</span> <span class="n">grad</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">grads</span><span class="p">):</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">cast</span><span class="p">(</span><span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">graph</span><span class="o">.</span><span class="n">GradientEdge</span><span class="p">],</span> <span class="n">out</span><span class="p">)</span>
        <span class="n">out_size</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="n">out_device</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">graph</span><span class="o">.</span><span class="n">GradientEdge</span><span class="p">):</span>
            <span class="n">out_metadata</span> <span class="o">=</span> <span class="n">out</span><span class="o">.</span><span class="n">node</span><span class="o">.</span><span class="n">_input_metadata</span><span class="p">[</span><span class="n">out</span><span class="o">.</span><span class="n">output_nr</span><span class="p">]</span>
            <span class="n">out_size</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">(</span><span class="n">out_metadata</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
            <span class="n">out_dtype</span> <span class="o">=</span> <span class="n">out_metadata</span><span class="o">.</span><span class="n">dtype</span>
            <span class="n">out_device</span> <span class="o">=</span> <span class="n">out_metadata</span><span class="o">.</span><span class="n">device</span>
            <span class="n">out_is_nested</span> <span class="o">=</span> <span class="n">out_metadata</span><span class="o">.</span><span class="n">is_nested_tensor</span>
            <span class="k">if</span> <span class="n">out_metadata</span><span class="o">.</span><span class="n">is_cpp_nested_tensor</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                    <span class="s2">&quot;C++ NestedTensor are not supported with GradientEdge&quot;</span>
                <span class="p">)</span>
            <span class="n">out_is_cpp_nested</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># circular import</span>
            <span class="kn">from</span><span class="w"> </span><span class="nn">torch.nested._internal.nested_tensor</span><span class="w"> </span><span class="kn">import</span> <span class="n">NestedTensor</span>

            <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span>
            <span class="n">out_dtype</span> <span class="o">=</span> <span class="n">out</span><span class="o">.</span><span class="n">dtype</span>
            <span class="n">out_is_nested</span> <span class="o">=</span> <span class="n">out</span><span class="o">.</span><span class="n">is_nested</span>
            <span class="n">out_is_cpp_nested</span> <span class="o">=</span> <span class="n">out_is_nested</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">NestedTensor</span><span class="p">)</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">out_is_cpp_nested</span><span class="p">:</span>
                <span class="n">out_size</span> <span class="o">=</span> <span class="n">out</span><span class="o">.</span><span class="n">shape</span>

        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">grad</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
            <span class="kn">from</span><span class="w"> </span><span class="nn">torch.fx.experimental.symbolic_shapes</span><span class="w"> </span><span class="kn">import</span> <span class="n">expect_true</span><span class="p">,</span> <span class="n">sym_eq</span>

            <span class="n">first_grad</span> <span class="o">=</span> <span class="n">grad</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">is_grads_batched</span> <span class="k">else</span> <span class="n">grad</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

            <span class="c1"># TODO: We can remove this conditional once we uniformly use</span>
            <span class="c1"># singleton int to represent jagged dimension, so that size() call</span>
            <span class="c1"># on nested tensor works.</span>
            <span class="k">if</span> <span class="n">out_is_cpp_nested</span><span class="p">:</span>
                <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span>
                <span class="n">shape_matches</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">is_same_size</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">first_grad</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="c1"># We need to do a regular size check, without going through</span>
                <span class="c1"># the operator, to be able to handle unbacked symints</span>
                <span class="c1"># (expect_true ensures we can deal with unbacked)</span>
                <span class="k">assert</span> <span class="n">out_size</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
                <span class="n">shape_matches</span> <span class="o">=</span> <span class="n">expect_true</span><span class="p">(</span><span class="n">sym_eq</span><span class="p">(</span><span class="n">out_size</span><span class="p">,</span> <span class="n">first_grad</span><span class="o">.</span><span class="n">size</span><span class="p">()))</span>

            <span class="k">if</span> <span class="ow">not</span> <span class="n">shape_matches</span><span class="p">:</span>
                <span class="n">out</span> <span class="o">=</span> <span class="n">cast</span><span class="p">(</span><span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">graph</span><span class="o">.</span><span class="n">GradientEdge</span><span class="p">],</span> <span class="n">out</span><span class="p">)</span>
                <span class="n">out_shape</span><span class="p">,</span> <span class="n">grad_shape</span> <span class="o">=</span> <span class="n">_calculate_shape</span><span class="p">(</span>
                    <span class="n">out</span><span class="p">,</span> <span class="n">first_grad</span><span class="p">,</span> <span class="n">is_grads_batched</span>
                <span class="p">)</span>
                <span class="k">if</span> <span class="n">is_grads_batched</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                        <span class="s2">&quot;If `is_grads_batched=True`, we interpret the first &quot;</span>
                        <span class="s2">&quot;dimension of each grad_output as the batch dimension. &quot;</span>
                        <span class="s2">&quot;The sizes of the remaining dimensions are expected to match &quot;</span>
                        <span class="s2">&quot;the shape of corresponding output, but a mismatch &quot;</span>
                        <span class="s2">&quot;was detected: grad_output[&quot;</span>
                        <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">grads</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="n">grad</span><span class="p">))</span>
                        <span class="o">+</span> <span class="s2">&quot;] has a shape of &quot;</span>
                        <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">grad_shape</span><span class="p">)</span>
                        <span class="o">+</span> <span class="s2">&quot; and output[&quot;</span>
                        <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">outputs</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="n">out</span><span class="p">))</span>
                        <span class="o">+</span> <span class="s2">&quot;] has a shape of &quot;</span>
                        <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">out_shape</span><span class="p">)</span>
                        <span class="o">+</span> <span class="s2">&quot;. &quot;</span>
                        <span class="s2">&quot;If you only want some tensors in `grad_output` to be considered &quot;</span>
                        <span class="s2">&quot;batched, consider using vmap.&quot;</span>
                    <span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                        <span class="s2">&quot;Mismatch in shape: grad_output[&quot;</span>
                        <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">grads</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="n">grad</span><span class="p">))</span>
                        <span class="o">+</span> <span class="s2">&quot;] has a shape of &quot;</span>
                        <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">grad_shape</span><span class="p">)</span>
                        <span class="o">+</span> <span class="s2">&quot; and output[&quot;</span>
                        <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">outputs</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="n">out</span><span class="p">))</span>
                        <span class="o">+</span> <span class="s2">&quot;] has a shape of &quot;</span>
                        <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">out_shape</span><span class="p">)</span>
                        <span class="o">+</span> <span class="s2">&quot;.&quot;</span>
                    <span class="p">)</span>
            <span class="k">if</span> <span class="n">out_dtype</span><span class="o">.</span><span class="n">is_complex</span> <span class="o">!=</span> <span class="n">grad</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">is_complex</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                    <span class="s2">&quot;For complex Tensors, both grad_output and output&quot;</span>
                    <span class="s2">&quot; are required to have the same dtype.&quot;</span>
                    <span class="s2">&quot; Mismatch in dtype: grad_output[&quot;</span>
                    <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">grads</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="n">grad</span><span class="p">))</span>
                    <span class="o">+</span> <span class="s2">&quot;] has a dtype of &quot;</span>
                    <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">grad</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
                    <span class="o">+</span> <span class="s2">&quot; and output[&quot;</span>
                    <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">outputs</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="n">out</span><span class="p">))</span>
                    <span class="o">+</span> <span class="s2">&quot;] has a dtype of &quot;</span>
                    <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">out_dtype</span><span class="p">)</span>
                    <span class="o">+</span> <span class="s2">&quot;.&quot;</span>
                <span class="p">)</span>
            <span class="n">new_grads</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">grad</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">grad</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">graph</span><span class="o">.</span><span class="n">GradientEdge</span><span class="p">)</span> <span class="ow">or</span> <span class="n">out</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">:</span>  <span class="c1"># type: ignore[attr-defined]</span>
                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">graph</span><span class="o">.</span><span class="n">GradientEdge</span><span class="p">):</span>
                    <span class="k">assert</span> <span class="n">out_size</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
                    <span class="n">out_numel_is_1</span> <span class="o">=</span> <span class="nb">all</span><span class="p">(</span><span class="n">o</span> <span class="o">==</span> <span class="mi">1</span> <span class="k">for</span> <span class="n">o</span> <span class="ow">in</span> <span class="n">out_size</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span>
                    <span class="n">out_numel_is_1</span> <span class="o">=</span> <span class="n">out</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="o">==</span> <span class="mi">1</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="n">out_numel_is_1</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                        <span class="s2">&quot;grad can be implicitly created only for scalar outputs&quot;</span>
                    <span class="p">)</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="n">out_dtype</span><span class="o">.</span><span class="n">is_floating_point</span><span class="p">:</span>
                    <span class="n">msg</span> <span class="o">=</span> <span class="p">(</span>
                        <span class="s2">&quot;grad can be implicitly created only for real scalar outputs&quot;</span>
                        <span class="sa">f</span><span class="s2">&quot; but got </span><span class="si">{</span><span class="n">out_dtype</span><span class="si">}</span><span class="s2">&quot;</span>
                    <span class="p">)</span>
                    <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="n">msg</span><span class="p">)</span>
                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">graph</span><span class="o">.</span><span class="n">GradientEdge</span><span class="p">):</span>
                    <span class="k">assert</span> <span class="n">out_size</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
                    <span class="k">assert</span> <span class="n">out_device</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
                    <span class="n">new_grads</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                        <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span>
                            <span class="n">out_size</span><span class="p">,</span>
                            <span class="n">dtype</span><span class="o">=</span><span class="n">out_dtype</span><span class="p">,</span>
                            <span class="n">device</span><span class="o">=</span><span class="n">out_device</span><span class="p">,</span>
                        <span class="p">)</span>
                    <span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span>
                    <span class="n">new_grads</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                        <span class="n">torch</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">memory_format</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">preserve_format</span><span class="p">)</span>
                    <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">new_grads</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="kc">None</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span>
                <span class="s2">&quot;gradients can be either Tensors or None, but got &quot;</span>
                <span class="o">+</span> <span class="nb">type</span><span class="p">(</span><span class="n">grad</span><span class="p">)</span><span class="o">.</span><span class="vm">__name__</span>
            <span class="p">)</span>
    <span class="k">return</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">new_grads</span><span class="p">)</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_tensor_or_tensors_to_tuple</span><span class="p">(</span>
    <span class="n">tensors</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">_TensorOrTensors</span><span class="p">],</span> <span class="n">length</span><span class="p">:</span> <span class="nb">int</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">_OptionalTensor</span><span class="p">,</span> <span class="o">...</span><span class="p">]:</span>
    <span class="k">if</span> <span class="n">tensors</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">return</span> <span class="p">(</span><span class="kc">None</span><span class="p">,)</span> <span class="o">*</span> <span class="n">length</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">tensors</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">tensors</span><span class="p">,)</span>
    <span class="k">return</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">tensors</span><span class="p">)</span>


<div class="viewcode-block" id="backward">
<a class="viewcode-back" href="../../python-api/generated/torch.autograd.backward.html#torch.autograd.backward">[docs]</a>
<span class="k">def</span><span class="w"> </span><span class="nf">backward</span><span class="p">(</span>
    <span class="n">tensors</span><span class="p">:</span> <span class="n">_TensorOrTensors</span><span class="p">,</span>
    <span class="n">grad_tensors</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">_TensorOrTensors</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">retain_graph</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">create_graph</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">grad_variables</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">_TensorOrTensors</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">inputs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">_TensorOrTensorsOrGradEdge</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Compute the sum of gradients of given tensors with respect to graph leaves.</span>

<span class="sd">    The graph is differentiated using the chain rule. If any of ``tensors``</span>
<span class="sd">    are non-scalar (i.e. their data has more than one element) and require</span>
<span class="sd">    gradient, then the Jacobian-vector product would be computed, in this</span>
<span class="sd">    case the function additionally requires specifying ``grad_tensors``.</span>
<span class="sd">    It should be a sequence of matching length, that contains the &quot;vector&quot;</span>
<span class="sd">    in the Jacobian-vector product, usually the gradient of the differentiated</span>
<span class="sd">    function w.r.t. corresponding tensors (``None`` is an acceptable value for</span>
<span class="sd">    all tensors that don&#39;t need gradient tensors).</span>

<span class="sd">    This function accumulates gradients in the leaves - you might need to zero</span>
<span class="sd">    ``.grad`` attributes or set them to ``None`` before calling it.</span>
<span class="sd">    See :ref:`Default gradient layouts&lt;default-grad-layouts&gt;`</span>
<span class="sd">    for details on the memory layout of accumulated gradients.</span>

<span class="sd">    .. note::</span>
<span class="sd">        Using this method with ``create_graph=True`` will create a reference cycle</span>
<span class="sd">        between the parameter and its gradient which can cause a memory leak.</span>
<span class="sd">        We recommend using ``autograd.grad`` when creating the graph to avoid this.</span>
<span class="sd">        If you have to use this function, make sure to reset the ``.grad`` fields of your</span>
<span class="sd">        parameters to ``None`` after use to break the cycle and avoid the leak.</span>

<span class="sd">    .. note::</span>

<span class="sd">        If you run any forward ops, create ``grad_tensors``, and/or call ``backward``</span>
<span class="sd">        in a user-specified CUDA stream context, see</span>
<span class="sd">        :ref:`Stream semantics of backward passes&lt;bwd-cuda-stream-semantics&gt;`.</span>

<span class="sd">    .. note::</span>

<span class="sd">        When ``inputs`` are provided and a given input is not a leaf,</span>
<span class="sd">        the current implementation will call its grad_fn (even though it is not strictly needed to get this gradients).</span>
<span class="sd">        It is an implementation detail on which the user should not rely.</span>
<span class="sd">        See https://github.com/pytorch/pytorch/pull/60521#issuecomment-867061780 for more details.</span>

<span class="sd">    Args:</span>
<span class="sd">        tensors (Sequence[Tensor] or Tensor): Tensors of which the derivative will be</span>
<span class="sd">            computed.</span>
<span class="sd">        grad_tensors (Sequence[Tensor or None] or Tensor, optional): The &quot;vector&quot; in</span>
<span class="sd">            the Jacobian-vector product, usually gradients w.r.t. each element of</span>
<span class="sd">            corresponding tensors. None values can be specified for scalar Tensors or</span>
<span class="sd">            ones that don&#39;t require grad. If a None value would be acceptable for all</span>
<span class="sd">            grad_tensors, then this argument is optional.</span>
<span class="sd">        retain_graph (bool, optional): If ``False``, the graph used to compute the grad</span>
<span class="sd">            will be freed. Note that in nearly all cases setting this option to ``True``</span>
<span class="sd">            is not needed and often can be worked around in a much more efficient</span>
<span class="sd">            way. Defaults to the value of ``create_graph``.</span>
<span class="sd">        create_graph (bool, optional): If ``True``, graph of the derivative will</span>
<span class="sd">            be constructed, allowing to compute higher order derivative products.</span>
<span class="sd">            Defaults to ``False``.</span>
<span class="sd">        inputs (Sequence[Tensor] or Tensor or Sequence[GradientEdge], optional): Inputs w.r.t. which the gradient</span>
<span class="sd">            be will accumulated into ``.grad``. All other Tensors will be ignored. If</span>
<span class="sd">            not provided, the gradient is accumulated into all the leaf Tensors that</span>
<span class="sd">            were used to compute the :attr:`tensors`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_are_functorch_transforms_active</span><span class="p">():</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
            <span class="s2">&quot;backward() called inside a functorch transform. This is not &quot;</span>
            <span class="s2">&quot;supported, please use functorch.grad or functorch.vjp instead &quot;</span>
            <span class="s2">&quot;or call backward() outside of functorch transforms.&quot;</span>
        <span class="p">)</span>

    <span class="k">if</span> <span class="n">grad_variables</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
            <span class="s2">&quot;`grad_variables` is deprecated. Use `grad_tensors` instead.&quot;</span><span class="p">,</span>
            <span class="ne">FutureWarning</span><span class="p">,</span>
            <span class="n">stacklevel</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="n">grad_tensors</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">grad_tensors</span> <span class="o">=</span> <span class="n">grad_variables</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                <span class="s2">&quot;`grad_tensors` and `grad_variables` (deprecated) &quot;</span>
                <span class="s2">&quot;arguments both passed to `backward()`. Please only &quot;</span>
                <span class="s2">&quot;use `grad_tensors`.&quot;</span>
            <span class="p">)</span>
    <span class="k">if</span> <span class="n">inputs</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;`inputs` argument to `backward()` cannot be empty.&quot;</span><span class="p">)</span>

    <span class="n">tensors</span> <span class="o">=</span> <span class="p">(</span><span class="n">tensors</span><span class="p">,)</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">tensors</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="k">else</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">tensors</span><span class="p">)</span>
    <span class="n">inputs</span> <span class="o">=</span> <span class="p">(</span>
        <span class="p">(</span><span class="n">inputs</span><span class="p">,)</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">graph</span><span class="o">.</span><span class="n">GradientEdge</span><span class="p">))</span>
        <span class="k">else</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">inputs</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="k">else</span> <span class="p">()</span>
    <span class="p">)</span>

    <span class="n">grad_tensors_</span> <span class="o">=</span> <span class="n">_tensor_or_tensors_to_tuple</span><span class="p">(</span><span class="n">grad_tensors</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">tensors</span><span class="p">))</span>
    <span class="n">grad_tensors_</span> <span class="o">=</span> <span class="n">_make_grads</span><span class="p">(</span><span class="n">tensors</span><span class="p">,</span> <span class="n">grad_tensors_</span><span class="p">,</span> <span class="n">is_grads_batched</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">retain_graph</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">retain_graph</span> <span class="o">=</span> <span class="n">create_graph</span>

    <span class="c1"># The reason we repeat the same comment below is that</span>
    <span class="c1"># some Python versions print out the first line of a multi-line function</span>
    <span class="c1"># calls in the traceback and some print out the last line</span>
    <span class="n">_engine_run_backward</span><span class="p">(</span>
        <span class="n">tensors</span><span class="p">,</span>
        <span class="n">grad_tensors_</span><span class="p">,</span>
        <span class="n">retain_graph</span><span class="p">,</span>
        <span class="n">create_graph</span><span class="p">,</span>
        <span class="n">inputs</span><span class="p">,</span>
        <span class="n">allow_unreachable</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">accumulate_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="p">)</span></div>



<div class="viewcode-block" id="grad">
<a class="viewcode-back" href="../../python-api/generated/torch.autograd.grad.html#torch.autograd.grad">[docs]</a>
<span class="k">def</span><span class="w"> </span><span class="nf">grad</span><span class="p">(</span>
    <span class="n">outputs</span><span class="p">:</span> <span class="n">_TensorOrTensorsOrGradEdge</span><span class="p">,</span>
    <span class="n">inputs</span><span class="p">:</span> <span class="n">_TensorOrTensorsOrGradEdge</span><span class="p">,</span>
    <span class="n">grad_outputs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">_TensorOrTensors</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">retain_graph</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">create_graph</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">only_inputs</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">allow_unused</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">is_grads_batched</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">materialize_grads</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="o">...</span><span class="p">]:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Compute and return the sum of gradients of outputs with respect to the inputs.</span>

<span class="sd">    ``grad_outputs`` should be a sequence of length matching ``output``</span>
<span class="sd">    containing the &quot;vector&quot; in vector-Jacobian product, usually the pre-computed</span>
<span class="sd">    gradients w.r.t. each of the outputs. If an output doesn&#39;t require_grad,</span>
<span class="sd">    then the gradient can be ``None``).</span>

<span class="sd">    .. note::</span>

<span class="sd">        If you run any forward ops, create ``grad_outputs``, and/or call ``grad``</span>
<span class="sd">        in a user-specified CUDA stream context, see</span>
<span class="sd">        :ref:`Stream semantics of backward passes&lt;bwd-cuda-stream-semantics&gt;`.</span>

<span class="sd">    .. note::</span>

<span class="sd">        ``only_inputs`` argument is deprecated and is ignored now (defaults to ``True``).</span>
<span class="sd">        To accumulate gradient for other parts of the graph, please use</span>
<span class="sd">        ``torch.autograd.backward``.</span>

<span class="sd">    Args:</span>
<span class="sd">        outputs (sequence of Tensor or GradientEdge): outputs of the differentiated function.</span>
<span class="sd">        inputs (sequence of Tensor or GradientEdge): Inputs w.r.t. which the gradient will be</span>
<span class="sd">            returned (and not accumulated into ``.grad``).</span>
<span class="sd">        grad_outputs (sequence of Tensor): The &quot;vector&quot; in the vector-Jacobian product.</span>
<span class="sd">            Usually gradients w.r.t. each output. None values can be specified for scalar</span>
<span class="sd">            Tensors or ones that don&#39;t require grad. If a None value would be acceptable</span>
<span class="sd">            for all grad_tensors, then this argument is optional. Default: None.</span>
<span class="sd">        retain_graph (bool, optional): If ``False``, the graph used to compute the grad</span>
<span class="sd">            will be freed. Note that in nearly all cases setting this option to ``True``</span>
<span class="sd">            is not needed and often can be worked around in a much more efficient</span>
<span class="sd">            way. Defaults to the value of ``create_graph``.</span>
<span class="sd">        create_graph (bool, optional): If ``True``, graph of the derivative will</span>
<span class="sd">            be constructed, allowing to compute higher order derivative products.</span>
<span class="sd">            Default: ``False``.</span>
<span class="sd">        allow_unused (Optional[bool], optional): If ``False``, specifying inputs</span>
<span class="sd">            that were not used when computing outputs (and therefore their grad is</span>
<span class="sd">            always zero) is an error. Defaults to the value of ``materialize_grads``.</span>
<span class="sd">        is_grads_batched (bool, optional): If ``True``, the first dimension of each</span>
<span class="sd">            tensor in ``grad_outputs`` will be interpreted as the batch dimension.</span>
<span class="sd">            Instead of computing a single vector-Jacobian product, we compute a</span>
<span class="sd">            batch of vector-Jacobian products for each &quot;vector&quot; in the batch.</span>
<span class="sd">            We use the vmap prototype feature as the backend to vectorize calls</span>
<span class="sd">            to the autograd engine so that this computation can be performed in a</span>
<span class="sd">            single call. This should lead to performance improvements when compared</span>
<span class="sd">            to manually looping and performing backward multiple times. Note that</span>
<span class="sd">            due to this feature being experimental, there may be performance</span>
<span class="sd">            cliffs. Please use ``torch._C._debug_only_display_vmap_fallback_warnings(True)``</span>
<span class="sd">            to show any performance warnings and file an issue on github if warnings exist</span>
<span class="sd">            for your use case. Defaults to ``False``.</span>
<span class="sd">        materialize_grads (bool, optional): If ``True``, set the gradient for unused inputs</span>
<span class="sd">            to zero instead of None. This is useful when computing higher-order derivatives.</span>
<span class="sd">            If ``materialize_grads`` is ``True`` and ``allow_unused`` is ``False``, an error</span>
<span class="sd">            will be raised. Defaults to ``False``.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">materialize_grads</span> <span class="ow">and</span> <span class="n">allow_unused</span> <span class="ow">is</span> <span class="kc">False</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s2">&quot;Expected allow_unused to be True or not passed when materialize_grads=True, &quot;</span>
            <span class="s2">&quot;but got: allow_unused=False.&quot;</span>
        <span class="p">)</span>
    <span class="k">if</span> <span class="n">allow_unused</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">allow_unused</span> <span class="o">=</span> <span class="n">materialize_grads</span>
    <span class="k">if</span> <span class="n">is_tensor_like</span><span class="p">(</span><span class="n">outputs</span><span class="p">)</span> <span class="ow">or</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">graph</span><span class="o">.</span><span class="n">GradientEdge</span><span class="p">):</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="n">cast</span><span class="p">(</span>
            <span class="n">Union</span><span class="p">[</span><span class="n">Sequence</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">graph</span><span class="o">.</span><span class="n">GradientEdge</span><span class="p">]],</span> <span class="p">(</span><span class="n">outputs</span><span class="p">,)</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">outputs</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">is_tensor_like</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span> <span class="ow">or</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">graph</span><span class="o">.</span><span class="n">GradientEdge</span><span class="p">):</span>
        <span class="n">inputs</span> <span class="o">=</span> <span class="n">cast</span><span class="p">(</span><span class="n">_TensorOrTensorsOrGradEdge</span><span class="p">,</span> <span class="p">(</span><span class="n">inputs</span><span class="p">,))</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">inputs</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
    <span class="n">t_outputs</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">outputs</span> <span class="k">if</span> <span class="n">is_tensor_like</span><span class="p">(</span><span class="n">i</span><span class="p">))</span>
    <span class="n">t_inputs</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">inputs</span> <span class="k">if</span> <span class="n">is_tensor_like</span><span class="p">(</span><span class="n">i</span><span class="p">))</span>
    <span class="n">overridable_args</span> <span class="o">=</span> <span class="n">t_outputs</span> <span class="o">+</span> <span class="n">t_inputs</span>
    <span class="k">if</span> <span class="n">has_torch_function</span><span class="p">(</span><span class="n">overridable_args</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span>
            <span class="n">grad</span><span class="p">,</span>
            <span class="n">overridable_args</span><span class="p">,</span>
            <span class="n">outputs</span><span class="p">,</span>
            <span class="n">inputs</span><span class="p">,</span>
            <span class="n">grad_outputs</span><span class="o">=</span><span class="n">grad_outputs</span><span class="p">,</span>
            <span class="n">retain_graph</span><span class="o">=</span><span class="n">retain_graph</span><span class="p">,</span>
            <span class="n">create_graph</span><span class="o">=</span><span class="n">create_graph</span><span class="p">,</span>
            <span class="n">only_inputs</span><span class="o">=</span><span class="n">only_inputs</span><span class="p">,</span>
            <span class="n">allow_unused</span><span class="o">=</span><span class="n">allow_unused</span><span class="p">,</span>
            <span class="n">is_grads_batched</span><span class="o">=</span><span class="n">is_grads_batched</span><span class="p">,</span>
            <span class="n">materialize_grads</span><span class="o">=</span><span class="n">materialize_grads</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">only_inputs</span><span class="p">:</span>
        <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
            <span class="s2">&quot;only_inputs argument is deprecated and is ignored now &quot;</span>
            <span class="s2">&quot;(defaults to True). To accumulate gradient for other &quot;</span>
            <span class="s2">&quot;parts of the graph, please use torch.autograd.backward.&quot;</span><span class="p">,</span>
            <span class="ne">FutureWarning</span><span class="p">,</span>
            <span class="n">stacklevel</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="n">grad_outputs_</span> <span class="o">=</span> <span class="n">_tensor_or_tensors_to_tuple</span><span class="p">(</span><span class="n">grad_outputs</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">outputs</span><span class="p">))</span>
    <span class="n">grad_outputs_</span> <span class="o">=</span> <span class="n">_make_grads</span><span class="p">(</span>
        <span class="n">outputs</span><span class="p">,</span> <span class="n">grad_outputs_</span><span class="p">,</span> <span class="n">is_grads_batched</span><span class="o">=</span><span class="n">is_grads_batched</span>
    <span class="p">)</span>

    <span class="k">if</span> <span class="n">retain_graph</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">retain_graph</span> <span class="o">=</span> <span class="n">create_graph</span>

    <span class="c1"># The reason we repeat the same comment several times below is because</span>
    <span class="c1"># some Python versions print out the first line of multi-line function</span>
    <span class="c1"># calls in the traceback and some print out the last line</span>
    <span class="k">if</span> <span class="n">is_grads_batched</span><span class="p">:</span>

        <span class="k">def</span><span class="w"> </span><span class="nf">vjp</span><span class="p">(</span><span class="n">gO</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">_engine_run_backward</span><span class="p">(</span>
                <span class="n">outputs</span><span class="p">,</span>
                <span class="n">gO</span><span class="p">,</span>
                <span class="n">retain_graph</span><span class="p">,</span>
                <span class="n">create_graph</span><span class="p">,</span>
                <span class="n">inputs</span><span class="p">,</span>
                <span class="n">allow_unused</span><span class="p">,</span>
                <span class="n">accumulate_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="n">result</span> <span class="o">=</span> <span class="n">_vmap_internals</span><span class="o">.</span><span class="n">_vmap</span><span class="p">(</span><span class="n">vjp</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">allow_none_pass_through</span><span class="o">=</span><span class="kc">True</span><span class="p">)(</span>
            <span class="n">grad_outputs_</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">result</span> <span class="o">=</span> <span class="n">_engine_run_backward</span><span class="p">(</span>
            <span class="n">outputs</span><span class="p">,</span>
            <span class="n">grad_outputs_</span><span class="p">,</span>
            <span class="n">retain_graph</span><span class="p">,</span>
            <span class="n">create_graph</span><span class="p">,</span>
            <span class="n">inputs</span><span class="p">,</span>
            <span class="n">allow_unused</span><span class="p">,</span>
            <span class="n">accumulate_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="p">)</span>
    <span class="k">if</span> <span class="n">materialize_grads</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">any</span><span class="p">(</span>
            <span class="n">result</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">is_tensor_like</span><span class="p">(</span><span class="n">inputs</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">inputs</span><span class="p">))</span>
        <span class="p">):</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                <span class="s2">&quot;materialize_grads cannot be used when the given input is a GradientEdge&quot;</span>
            <span class="p">)</span>
        <span class="n">result</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span>
            <span class="n">output</span>
            <span class="k">if</span> <span class="n">output</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
            <span class="k">else</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="k">for</span> <span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="nb">input</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">result</span><span class="p">,</span> <span class="n">inputs</span><span class="p">)</span>
        <span class="p">)</span>
    <span class="k">return</span> <span class="n">result</span></div>



<span class="c1"># This function applies in case of gradient checkpointing for memory</span>
<span class="c1"># optimization. Currently, gradient checkpointing is supported only if the</span>
<span class="c1"># execution engine is invoked through torch.autograd.backward() and its</span>
<span class="c1"># inputs argument is not passed. It is not supported for torch.autograd.grad().</span>
<span class="c1"># This is because if inputs are specified, the gradient won&#39;t be calculated for</span>
<span class="c1"># anything else e.g. model parameters like weights, bias etc.</span>
<span class="c1">#</span>
<span class="c1"># This function returns whether the checkpointing is valid i.e. torch.autograd.backward</span>
<span class="c1"># or not i.e. torch.autograd.grad. The implementation works by maintaining a thread</span>
<span class="c1"># local variable in torch/csrc/autograd/engine.cpp which looks at the NodeTask</span>
<span class="c1"># in the stack and before a NodeTask is executed in evaluate_function, it</span>
<span class="c1"># checks for whether reentrant backwards is imperative or not.</span>
<span class="c1"># See https://github.com/pytorch/pytorch/pull/4594 for more discussion/context</span>
<span class="k">def</span><span class="w"> </span><span class="nf">_is_checkpoint_valid</span><span class="p">():</span>
    <span class="k">return</span> <span class="n">Variable</span><span class="o">.</span><span class="n">_execution_engine</span><span class="o">.</span><span class="n">is_checkpoint_valid</span><span class="p">()</span>


<span class="k">def</span><span class="w"> </span><span class="nf">variable</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>  <span class="c1"># noqa: D103</span>
    <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
        <span class="s2">&quot;torch.autograd.variable(...) is deprecated, use torch.tensor(...) instead&quot;</span>
    <span class="p">)</span>


<span class="c1"># Monkey patching variable.Variable to fix FX codegen. FX generates a call by roughly doing</span>
<span class="c1"># f&quot;{fn.__module__}.{fn.__name__}(...). This yields torch.autograd.variable.Variable(...) in the</span>
<span class="c1"># output of an FX graph.  Unfortunately the module name torch.autograd.variable is shadowed by the</span>
<span class="c1"># deprecated function - variable(...).</span>
<span class="n">variable</span><span class="o">.</span><span class="n">Variable</span> <span class="o">=</span> <span class="n">Variable</span>  <span class="c1"># type: ignore[attr-defined]</span>

<span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_autograd_init</span><span class="p">():</span>
    <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;autograd initialization failed&quot;</span><span class="p">)</span>

<span class="c1"># Import all native method/classes</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch._C._autograd</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">_add_metadata_json</span><span class="p">,</span>
    <span class="n">_disable_profiler</span><span class="p">,</span>
    <span class="n">_disable_profiler_legacy</span><span class="p">,</span>
    <span class="n">_enable_profiler</span><span class="p">,</span>
    <span class="n">_enable_profiler_legacy</span><span class="p">,</span>
    <span class="n">_enable_record_function</span><span class="p">,</span>
    <span class="n">_get_sequence_nr</span><span class="p">,</span>
    <span class="n">_kineto_step</span><span class="p">,</span>
    <span class="n">_KinetoEvent</span><span class="p">,</span>
    <span class="n">_pop_saved_tensors_default_hooks</span><span class="p">,</span>
    <span class="n">_prepare_profiler</span><span class="p">,</span>
    <span class="n">_profiler_enabled</span><span class="p">,</span>
    <span class="n">_ProfilerResult</span><span class="p">,</span>
    <span class="n">_push_saved_tensors_default_hooks</span><span class="p">,</span>
    <span class="n">_record_function_with_args_enter</span><span class="p">,</span>
    <span class="n">_record_function_with_args_exit</span><span class="p">,</span>
    <span class="n">_set_empty_test_observer</span><span class="p">,</span>
    <span class="n">_supported_activities</span><span class="p">,</span>
    <span class="n">_toggle_collection_dynamic</span><span class="p">,</span>
    <span class="n">DeviceType</span><span class="p">,</span>
    <span class="n">kineto_available</span><span class="p">,</span>
    <span class="n">ProfilerEvent</span><span class="p">,</span>
    <span class="n">SavedTensor</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch._C._profiler</span><span class="w"> </span><span class="kn">import</span> <span class="n">ProfilerActivity</span><span class="p">,</span> <span class="n">ProfilerConfig</span><span class="p">,</span> <span class="n">ProfilerState</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">.</span><span class="w"> </span><span class="kn">import</span> <span class="n">profiler</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_register_py_tensor_class_for_device</span><span class="p">(</span><span class="n">device</span><span class="p">,</span> <span class="bp">cls</span><span class="p">):</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="nb">type</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;cls isn&#39;t a typeinfo object&quot;</span><span class="p">)</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_register_py_class_for_device</span><span class="p">(</span><span class="n">device</span><span class="p">,</span> <span class="bp">cls</span><span class="p">)</span>


<span class="n">is_multithreading_enabled</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_is_multithreading_enabled</span>
<span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_add_docstr</span><span class="p">(</span>
    <span class="n">is_multithreading_enabled</span><span class="p">,</span> <span class="s2">&quot;Returns True if multithreading is currently enabled.&quot;</span>
<span class="p">)</span>

<span class="n">is_view_replay_enabled</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_is_view_replay_enabled</span>
<span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_add_docstr</span><span class="p">(</span>
    <span class="n">is_view_replay_enabled</span><span class="p">,</span> <span class="s2">&quot;Returns True if view-replay is currently enabled.&quot;</span>
<span class="p">)</span>
</pre></div>

                </article>
              
              
              
                <footer class="bd-footer-article">
                  <div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item">
<div class="feedback">
  
<div class="rating">
    Rate this Page
    <div class="stars">
        
        <span class="star" data-value="1">★</span>
        
        <span class="star" data-value="2">★</span>
        
        <span class="star" data-value="3">★</span>
        
        <span class="star" data-value="4">★</span>
        
        <span class="star" data-value="5">★</span>
        
    </div>
</div>

  <div class="feedback-send">
    <button class="feedback-btn" onclick="openGitHubIssue()">Send Feedback</button>
  </div>
</div>
</div>
  
</div>
                </footer>
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
</div>
                </footer>
              
            </div>
            
            
              
            
          </div>
          <footer class="bd-footer-content">
            
          </footer>
        
      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script defer src="../../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf"></script>
<script defer src="../../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf"></script>


  <footer class="bd-footer">
    
    <div class="bd-footer__inner bd-page-width">
      
        <div class="footer-items__start">
          
            <div class="footer-item">

  <p class="copyright">
    
      © Copyright PyTorch Contributors.
      <br/>
    
  </p>
</div>
          
            <div class="footer-item">

  <p class="sphinx-version">
    Created using <a href="https://www.sphinx-doc.org/">Sphinx</a> 8.1.0.
    <br/>
  </p>
</div>
          
        </div>
      
    
    
      <div class="footer-items__end">
        
          <div class="footer-item">
<p class="theme-version">
  <!-- # L10n: Setting the PST URL as an argument as this does not need to be localized -->
  Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.16.1.
</p></div>
        
      </div>
    
   </div>
   

   <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
      <div class="container">
        <div class="row">
          <div class="col-md-4 text-center">
            <h2>Docs</h2>
            <p>Access comprehensive developer documentation for PyTorch</p>
            <a class="with-right-arrow" href="">View Docs</a>
          </div>

          <div class="col-md-4 text-center">
            <h2>Tutorials</h2>
            <p>Get in-depth tutorials for beginners and advanced developers</p>
            <a class="with-right-arrow" href="">View Tutorials</a>
          </div>

          <div class="col-md-4 text-center">
            <h2>Resources</h2>
            <p>Find development resources and get your questions answered</p>
            <a class="with-right-arrow" href="">View Resources</a>
          </div>
        </div>
      </div>
  </div>

  <footer class="site-footer">
      <div class="container footer-container">
        <div class="footer-logo-wrapper">
          <a href="" class="footer-logo"></a>
        </div>

        <div class="footer-links-wrapper">
          <div class="footer-links-col">
            <ul>
              <li class="list-title"><a href="">PyTorch</a></li>
              <li><a href="">Get Started</a></li>
              <li><a href="">Features</a></li>
              <li><a href="">Ecosystem</a></li>
              <li><a href="">Blog</a></li>
              <li><a href="">Contributing</a></li>
            </ul>
          </div>

          <div class="footer-links-col">
            <ul>
              <li class="list-title"><a href="">Resources</a></li>
              <li><a href="">Tutorials</a></li>
              <li><a href="">Docs</a></li>
              <li><a href="" target="_blank">Discuss</a></li>
              <li><a href="" target="_blank">Github Issues</a></li>
              <li><a href="" target="_blank">Brand Guidelines</a></li>
            </ul>
          </div>

          <div class="footer-links-col">
            <ul>
              <li class="list-title">Stay up to date</li>
              <li><a href="" target="_blank">Facebook</a></li>
              <li><a href="" target="_blank">Twitter</a></li>
              <li><a href="" target="_blank">YouTube</a></li>
              <li><a href="" target="_blank">LinkedIn</a></li>
            </ul>
            </div>

          <div class="footer-links-col">
            <ul>
              <li class="list-title">PyTorch Podcasts</li>
              <li><a href="" target="_blank">Spotify</a></li>
              <li><a href="" target="_blank">Apple</a></li>
              <li><a href="" target="_blank">Google</a></li>
              <li><a href="" target="_blank">Amazon</a></li>
            </ul>
           </div>
          </div>

          <div class="privacy-policy">
            <ul>
              <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/terms/" target="_blank">Terms</a></li>
              <li class="privacy-policy-links">|</li>
              <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/privacy-policy/" target="_blank">Privacy</a></li>
            </ul>
          </div>
          <div class="copyright">
          <p>© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation.
            For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see
            <a href="https://www.linuxfoundation.org/policies/">www.linuxfoundation.org/policies/</a>. The PyTorch Foundation supports the PyTorch open source
            project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC,
            please see <a href="https://www.lfprojects.org/policies/">www.lfprojects.org/policies/</a>.</p>
        </div>
       </div>

  <div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="../../_static/img/pytorch-x.svg">
  </div>
</div>
  </footer>


  </body>
</html>