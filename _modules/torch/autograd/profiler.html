
<!DOCTYPE html>


<html lang="en" data-content_root="../../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>torch.autograd.profiler &#8212; PyTorch main documentation</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  <!--
    this give us a css class that will be invisible only if js is disabled
  -->
  <noscript>
    <style>
      .pst-js-only { display: none !important; }

    </style>
  </noscript>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../../_static/styles/theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />
<link href="../../../_static/styles/pydata-sphinx-theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />

    <link rel="stylesheet" type="text/css" href="../../../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../../../_static/copybutton.css?v=949a1ff5" />
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/katex.min.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/katex-math.css?v=91adb8b6" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="../../../_static/css/jit.css?v=8de1ea5d" />
    <link rel="stylesheet" type="text/css" href="../../../_static/css/custom.css?v=3ccf5357" />
    <link rel="stylesheet" type="text/css" href="../../../_static/css/custom2.css?v=baa440dc" />
  
  <!-- So that users can add custom icons -->
  <script src="../../../_static/scripts/fontawesome.js?digest=8878045cc6db502f8baf"></script>
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf" />
<link rel="preload" as="script" href="../../../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf" />

    <script src="../../../_static/documentation_options.js?v=a8da1a53"></script>
    <script src="../../../_static/doctools.js?v=9bcbadda"></script>
    <script src="../../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../../_static/copybutton.js?v=940804e7"></script>
    <script src="../../../_static/design-tabs.js?v=f930bc37"></script>
    <script type="module" src="https://cdn.jsdelivr.net/npm/mermaid@11.2.0/dist/mermaid.esm.min.mjs"></script>
    <script type="module" src="https://cdn.jsdelivr.net/npm/@mermaid-js/layout-elk@0.1.4/dist/mermaid-layout-elk.esm.min.mjs"></script>
    <script type="module">import mermaid from "https://cdn.jsdelivr.net/npm/mermaid@11.2.0/dist/mermaid.esm.min.mjs";import elkLayouts from "https://cdn.jsdelivr.net/npm/@mermaid-js/layout-elk@0.1.4/dist/mermaid-layout-elk.esm.min.mjs";mermaid.registerLayoutLoaders(elkLayouts);mermaid.initialize({startOnLoad:false});</script>
    <script src="https://cdn.jsdelivr.net/npm/d3@7.9.0/dist/d3.min.js"></script>
    <script type="module">
import mermaid from "https://cdn.jsdelivr.net/npm/mermaid@11.2.0/dist/mermaid.esm.min.mjs";
window.addEventListener("load", () => mermaid.run());
</script>
    <script>DOCUMENTATION_OPTIONS.pagename = '_modules/torch/autograd/profiler';</script>
    <script src="../../../_static/js/star-rating.js?v=8861fcb6"></script>
    <script src="../../../_static/js/send-feedback.js?v=5646bf45"></script>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />

  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  <meta name="docsearch:version" content="main (2.6.0 )" />
<script src="https://code.jquery.com/jquery-3.7.1.min.js"></script>
<script type="text/javascript" src="../../../_static/js/send-feedback.js"></script>
<script type="text/javascript" src="../../../_static/js/star-rating.js"></script>
<script type="text/javascript" src="../../../_static/js/cookie-banner.js"></script>

<script async src="https://www.googletagmanager.com/gtag/js?id=G-TEST12345"></script>
    <script type="text/javascript">
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-TEST12345');
    </script>

  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
<body data-feedback-url="https://github.com/pytorch/pytorch">
  <div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>

          <li class="main-menu-item">
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Learn
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="">
                  <span class=dropdown-title>Get Started</span>
                  <p>Run PyTorch locally or get started quickly with one of the supported cloud platforms</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Tutorials</span>
                  <p>Whats new in PyTorch tutorials</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Learn the Basics</span>
                  <p>Familiarize yourself with PyTorch concepts and modules</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">PyTorch Recipes</span>
                  <p>Bite-size, ready-to-deploy PyTorch code examples</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Intro to PyTorch - YouTube Series</span>
                  <p>Master PyTorch basics with our engaging YouTube tutorial series</p>
                </a>
              </div>
            </div>
          </li>

          <li>
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Ecosystem
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Tools</span>
                  <p>Learn about the tools and frameworks in the PyTorch Ecosystem</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class=dropdown-title>Community</span>
                  <p>Join the PyTorch developer community to contribute, learn, and get your questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="" target="_blank">
                  <span class=dropdown-title>Forums</span>
                  <p>A place to discuss PyTorch code, issues, install, research</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class=dropdown-title>Developer Resources</span>
                  <p>Find resources and get questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Contributor Awards - 2024</span>
                  <p>Award winners announced at this year's PyTorch Conference</p>
                </a>
              </div>
            </div>
          </li>

          <li>
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Edge
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">About PyTorch Edge</span>
                  <p>Build innovative and privacy-aware AI experiences for edge devices</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">ExecuTorch</span>
                  <p>End-to-end solution for enabling on-device inference capabilities across mobile and edge devices</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">ExecuTorch Docs</span>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Docs
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">PyTorch</span>
                  <p>Explore the documentation for comprehensive guidance on how to use PyTorch</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">PyTorch Domains</span>
                  <p>Read the PyTorch Domains documentation to learn more about domain-specific libraries</p>
                </a>
              </div>
            </div>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Blogs & News
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">PyTorch Blog</span>
                  <p>Catch up on the latest technical news and happenings</p>
                </a>
                 <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Community Blog</span>
                  <p>Stories from the PyTorch ecosystem</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Videos</span>
                  <p>Learn about the latest PyTorch tutorials, new, and more </p>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Community Stories</span>
                  <p>Learn how our community solves real, everyday machine learning problems with PyTorch</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Events</span>
                  <p>Find events, webinars, and podcasts</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Newsletter</span>
                  <p>Stay up-to-date with the latest updates</p>
                </a>
            </div>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                About
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">PyTorch Foundation</span>
                  <p>Learn more about the PyTorch Foundation</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Governing Board</span>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Cloud Credit Program</span>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Technical Advisory Council</span>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Staff</span>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Contact Us</span>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
            <div class="no-dropdown">
              <a href="" data-cta="join">
                Become a Member
              </a>
            </div>
          </li>
          <li>
           <div class="main-menu-item">
             <a href="" class="github-icon">
             </a>
           </div>
          </li>
          <!--- TODO: This block adds the search icon to the nav bar. We will enable it later.
          <li>
            <div class="main-menu-item">
             <a href="" class="search-icon">
             </a>
            </div>
          </li>
          --->
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>
  </div>
</div>

  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <dialog id="pst-search-dialog">
    
<form class="bd-search d-flex align-items-center"
      action="../../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </dialog>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
<div class="bd-header__inner bd-page-width">
  <button class="pst-navbar-icon sidebar-toggle primary-toggle" aria-label="Site navigation">
    <span class="fa-solid fa-bars"></span>
  </button>
  
  
  <div class=" navbar-header-items__start">
    
      <div class="navbar-item">

  
    
  

<a class="navbar-brand logo" href="../../../index.html">
  
  
  
  
  
  
    <p class="title logo__title">Home</p>
  
</a></div>
    
  </div>
  
  <div class=" navbar-header-items">
    
    <div class="me-auto navbar-header-items__center">
      
        <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../python-api/index.html">
    Python API
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../notes/index.html">
    Developer Notes
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-external" href="https://pytorch.org/tutorials/">
    Tutorials
  </a>
</li>

  </ul>
</nav></div>
      
    </div>
    
    
    <div class="navbar-header-items__end">
      
        <div class="navbar-item navbar-persistent--container">
          

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button>
        </div>
      
      
        <div class="navbar-item">
<div class="version-switcher__container dropdown pst-js-only">
  <button id="pst-version-switcher-button-2"
    type="button"
    class="version-switcher__button btn btn-sm dropdown-toggle"
    data-bs-toggle="dropdown"
    aria-haspopup="listbox"
    aria-controls="pst-version-switcher-list-2"
    aria-label="Version switcher list"
  >
    Choose version  <!-- this text may get changed later by javascript -->
    <span class="caret"></span>
  </button>
  <div id="pst-version-switcher-list-2"
    class="version-switcher__menu dropdown-menu list-group-flush py-0"
    role="listbox" aria-labelledby="pst-version-switcher-button-2">
    <!-- dropdown will be populated by javascript on page load -->
  </div>
</div></div>
      
        <div class="navbar-item">

<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button></div>
      
        <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://x.com/PyTorch" title="X" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-x-twitter fa-lg" aria-hidden="true"></i>
            <span class="sr-only">X</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/pytorch/pytorch" title="GitHub" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">GitHub</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://pypi.org/project/torch" title="PyPI" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-custom fa-pypi fa-lg" aria-hidden="true"></i>
            <span class="sr-only">PyPI</span></a>
        </li>
</ul></div>
      
    </div>
    
  </div>
  
  
    <div class="navbar-persistent--mobile">

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button>
    </div>
  

  
</div>

    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
        
      
      <dialog id="pst-primary-sidebar-modal"></dialog>
      <div id="pst-primary-sidebar" class="bd-sidebar-primary bd-sidebar hide-on-wide">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
      <div class="sidebar-header-items__center">
        
          
          
            <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../python-api/index.html">
    Python API
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../notes/index.html">
    Developer Notes
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-external" href="https://pytorch.org/tutorials/">
    Tutorials
  </a>
</li>

  </ul>
</nav></div>
          
        
      </div>
    
    
    
      <div class="sidebar-header-items__end">
        
          <div class="navbar-item">
<div class="version-switcher__container dropdown pst-js-only">
  <button id="pst-version-switcher-button-3"
    type="button"
    class="version-switcher__button btn btn-sm dropdown-toggle"
    data-bs-toggle="dropdown"
    aria-haspopup="listbox"
    aria-controls="pst-version-switcher-list-3"
    aria-label="Version switcher list"
  >
    Choose version  <!-- this text may get changed later by javascript -->
    <span class="caret"></span>
  </button>
  <div id="pst-version-switcher-list-3"
    class="version-switcher__menu dropdown-menu list-group-flush py-0"
    role="listbox" aria-labelledby="pst-version-switcher-button-3">
    <!-- dropdown will be populated by javascript on page load -->
  </div>
</div></div>
        
          <div class="navbar-item">

<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button></div>
        
          <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://x.com/PyTorch" title="X" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-x-twitter fa-lg" aria-hidden="true"></i>
            <span class="sr-only">X</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/pytorch/pytorch" title="GitHub" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">GitHub</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://pypi.org/project/torch" title="PyPI" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-custom fa-pypi fa-lg" aria-hidden="true"></i>
            <span class="sr-only">PyPI</span></a>
        </li>
</ul></div>
        
      </div>
    
  </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
      <div class="sidebar-primary-item">
<div id="ethical-ad-placement"
      class="flat"
      data-ea-publisher="readthedocs"
      data-ea-type="readthedocs-sidebar"
      data-ea-manual="true">
</div></div>
  </div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        
          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item">

<nav aria-label="Breadcrumb" class="d-print-none">
  <ul class="bd-breadcrumbs">
    
    <li class="breadcrumb-item breadcrumb-home">
      <a href="../../../index.html" class="nav-link" aria-label="Home">
        <i class="fa-solid fa-home"></i>
      </a>
    </li>
    
    <li class="breadcrumb-item"><a href="../../index.html" class="nav-link">Module code</a></li>
    
    
    <li class="breadcrumb-item"><a href="../../torch.html" class="nav-link">torch</a></li>
    
    
    <li class="breadcrumb-item"><a href="../autograd.html" class="nav-link">torch.autograd</a></li>
    
    <li class="breadcrumb-item active" aria-current="page"><span class="ellipsis">torch.autograd.profiler</span></li>
  </ul>
</nav>
</div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">
<div class="rating">
    Rate this Page
    <div class="stars">
        
        <span class="star" data-value="1">★</span>
        
        <span class="star" data-value="2">★</span>
        
        <span class="star" data-value="3">★</span>
        
        <span class="star" data-value="4">★</span>
        
        <span class="star" data-value="5">★</span>
        
    </div>
</div>
</div>
      
    </div>
  
</div>
</div>
              
              
              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <h1>Source code for torch.autograd.profiler</h1><div class="highlight"><pre>
<span></span><span class="c1"># mypy: allow-untyped-defs</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">uuid</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">collections</span><span class="w"> </span><span class="kn">import</span> <span class="n">defaultdict</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">dataclasses</span><span class="w"> </span><span class="kn">import</span> <span class="n">dataclass</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">time</span><span class="w"> </span><span class="kn">import</span> <span class="n">perf_counter_ns</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">typing</span><span class="w"> </span><span class="kn">import</span> <span class="n">Any</span><span class="p">,</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">Iterable</span><span class="p">,</span> <span class="n">List</span><span class="p">,</span> <span class="n">Optional</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">warnings</span><span class="w"> </span><span class="kn">import</span> <span class="n">warn</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.cuda</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch._C</span><span class="w"> </span><span class="kn">import</span> <span class="n">_get_privateuse1_backend_name</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch._C._profiler</span><span class="w"> </span><span class="kn">import</span> <span class="n">_ExperimentalConfig</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch.autograd</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">_disable_profiler</span><span class="p">,</span>
    <span class="n">_enable_profiler</span><span class="p">,</span>
    <span class="n">_kineto_step</span><span class="p">,</span>
    <span class="n">_prepare_profiler</span><span class="p">,</span>
    <span class="n">_ProfilerResult</span><span class="p">,</span>
    <span class="n">_supported_activities</span><span class="p">,</span>
    <span class="n">_toggle_collection_dynamic</span><span class="p">,</span>
    <span class="n">DeviceType</span><span class="p">,</span>
    <span class="n">kineto_available</span><span class="p">,</span>
    <span class="n">ProfilerActivity</span><span class="p">,</span>
    <span class="n">ProfilerConfig</span><span class="p">,</span>
    <span class="n">ProfilerState</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch.autograd.profiler_util</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">_filter_name</span><span class="p">,</span>
    <span class="n">_filter_stack_entry</span><span class="p">,</span>
    <span class="n">_rewrite_name</span><span class="p">,</span>
    <span class="n">EventList</span><span class="p">,</span>
    <span class="n">FunctionEvent</span><span class="p">,</span>
    <span class="n">MEMORY_EVENT_NAME</span><span class="p">,</span>
    <span class="n">MemRecordsAcc</span><span class="p">,</span>
    <span class="n">OUT_OF_MEMORY_EVENT_NAME</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch.futures</span><span class="w"> </span><span class="kn">import</span> <span class="n">Future</span>


<span class="n">__all__</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s2">&quot;profile&quot;</span><span class="p">,</span>
    <span class="s2">&quot;record_function&quot;</span><span class="p">,</span>
    <span class="s2">&quot;emit_itt&quot;</span><span class="p">,</span>
    <span class="s2">&quot;emit_nvtx&quot;</span><span class="p">,</span>
    <span class="s2">&quot;load_nvprof&quot;</span><span class="p">,</span>
    <span class="s2">&quot;EnforceUnique&quot;</span><span class="p">,</span>
    <span class="s2">&quot;parse_nvprof_trace&quot;</span><span class="p">,</span>
    <span class="s2">&quot;KinetoStepTracker&quot;</span><span class="p">,</span>
    <span class="s2">&quot;EventList&quot;</span><span class="p">,</span>
    <span class="s2">&quot;FunctionEvent&quot;</span><span class="p">,</span>
    <span class="s2">&quot;MemRecordsAcc&quot;</span><span class="p">,</span>
<span class="p">]</span>

<span class="k">try</span><span class="p">:</span>
    <span class="c1"># Available in Python &gt;= 3.2</span>
    <span class="kn">from</span><span class="w"> </span><span class="nn">contextlib</span><span class="w"> </span><span class="kn">import</span> <span class="n">ContextDecorator</span> <span class="k">as</span> <span class="n">_ContextDecorator</span>
<span class="k">except</span> <span class="ne">ImportError</span><span class="p">:</span>
    <span class="kn">import</span><span class="w"> </span><span class="nn">functools</span>

    <span class="k">class</span><span class="w"> </span><span class="nc">_ContextDecorator</span><span class="p">:</span>  <span class="c1"># type: ignore[no-redef]</span>
        <span class="k">def</span><span class="w"> </span><span class="fm">__enter__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">NotImplementedError</span>

        <span class="k">def</span><span class="w"> </span><span class="fm">__exit__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">exc_type</span><span class="p">,</span> <span class="n">exc_val</span><span class="p">,</span> <span class="n">exc_tb</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">NotImplementedError</span>

        <span class="k">def</span><span class="w"> </span><span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">func</span><span class="p">):</span>
            <span class="nd">@functools</span><span class="o">.</span><span class="n">wraps</span><span class="p">(</span><span class="n">func</span><span class="p">)</span>
            <span class="k">def</span><span class="w"> </span><span class="nf">wrapped</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
                <span class="k">with</span> <span class="bp">self</span><span class="p">:</span>
                    <span class="k">return</span> <span class="n">func</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

            <span class="k">return</span> <span class="n">wrapped</span>


<span class="c1"># global python state - whether profiler is currently enabled</span>
<span class="c1"># useful for fast python checks to reduce latency</span>
<span class="n">_is_profiler_enabled</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_set_is_profiler_enabled</span><span class="p">(</span><span class="n">enable</span><span class="p">:</span> <span class="nb">bool</span><span class="p">):</span>
    <span class="k">global</span> <span class="n">_is_profiler_enabled</span>
    <span class="n">_is_profiler_enabled</span> <span class="o">=</span> <span class="n">enable</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_run_on_profiler_start</span><span class="p">():</span>
    <span class="n">_set_is_profiler_enabled</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_run_on_profiler_stop</span><span class="p">():</span>
    <span class="n">_set_is_profiler_enabled</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>


<span class="nd">@dataclass</span>
<span class="k">class</span><span class="w"> </span><span class="nc">_ProfilerStats</span><span class="p">:</span>
    <span class="s2">&quot;Profiler timing and stats used by developers to catch issues/regressions&quot;</span>
    <span class="n">profiling_window_duration_sec</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">number_of_events</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">profiler_prepare_call_duration_us</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">profiler_enable_call_duration_us</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">profiler_disable_call_duration_us</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">parse_kineto_call_duration_us</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">function_events_build_tree_call_duration_us</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span>


<div class="viewcode-block" id="profile">
<a class="viewcode-back" href="../../../python-api/autograd.html#torch.autograd.profiler.profile">[docs]</a>
<span class="k">class</span><span class="w"> </span><span class="nc">profile</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Context manager that manages autograd profiler state and holds a summary of results.</span>

<span class="sd">    Under the hood it just records events of functions being executed in C++ and</span>
<span class="sd">    exposes those events to Python. You can wrap any code into it and it will</span>
<span class="sd">    only report runtime of PyTorch functions.</span>
<span class="sd">    Note: profiler is thread local and is automatically propagated into the async tasks</span>

<span class="sd">    Args:</span>
<span class="sd">        enabled (bool, optional): Setting this to False makes this context manager a no-op.</span>

<span class="sd">        use_cuda (bool, optional): Enables timing of CUDA events as well</span>
<span class="sd">            using the cudaEvent API. (will be deprecated)</span>

<span class="sd">        use_device (str, optional): Enables timing of device events.</span>
<span class="sd">            Adds approximately 4us of overhead to each tensor operation when use cuda.</span>
<span class="sd">            The valid devices options are &#39;cuda&#39;, &#39;xpu&#39;, &#39;mtia&#39; and &#39;privateuseone&#39;.</span>

<span class="sd">        record_shapes (bool, optional): If shapes recording is set, information</span>
<span class="sd">            about input dimensions will be collected. This allows one to see which</span>
<span class="sd">            dimensions have been used under the hood and further group by them</span>
<span class="sd">            using prof.key_averages(group_by_input_shape=True). Please note that</span>
<span class="sd">            shape recording might skew your profiling data. It is recommended to</span>
<span class="sd">            use separate runs with and without shape recording to validate the timing.</span>
<span class="sd">            Most likely the skew will be negligible for bottom most events (in a case</span>
<span class="sd">            of nested function calls). But for higher level functions the total</span>
<span class="sd">            self cpu time might be artificially increased because of the shape</span>
<span class="sd">            collection.</span>

<span class="sd">        with_flops (bool, optional): If with_flops is set, the profiler will estimate</span>
<span class="sd">            the FLOPs (floating point operations) value using the operator&#39;s input shape.</span>
<span class="sd">            This allows one to estimate the hardware performance. Currently,</span>
<span class="sd">            this option only works for the matrix multiplication and 2D convolution operators.</span>

<span class="sd">        profile_memory (bool, optional): track tensor memory allocation/deallocation.</span>

<span class="sd">        with_stack (bool, optional): record source information (file and line number) for the ops.</span>

<span class="sd">        with_modules (bool): record module hierarchy (including function names)</span>
<span class="sd">            corresponding to the callstack of the op. e.g. If module A&#39;s forward call&#39;s</span>
<span class="sd">            module B&#39;s forward which contains an aten::add op,</span>
<span class="sd">            then aten::add&#39;s module hierarchy is A.B</span>
<span class="sd">            Note that this support exist, at the moment, only for TorchScript models</span>
<span class="sd">            and not eager mode models.</span>

<span class="sd">        use_kineto (bool, optional): experimental, enable profiling with Kineto profiler.</span>

<span class="sd">        use_cpu (bool, optional): profile CPU events; setting to ``False`` requires</span>
<span class="sd">            ``use_kineto=True`` and can be used to lower the overhead for GPU-only profiling.</span>

<span class="sd">        experimental_config (_ExperimentalConfig) : A set of experimental options</span>
<span class="sd">            used by profiler libraries like Kineto. Note, backward compatibility is not guaranteed.</span>

<span class="sd">        acc_events (bool): Enable the accumulation of FunctionEvents across multiple profiling cycles</span>


<span class="sd">    .. warning:</span>
<span class="sd">        Enabling memory profiling or source attribution incurs additional profiler</span>
<span class="sd">        overhead</span>

<span class="sd">    .. warning:</span>
<span class="sd">        This context managers should not be called recursively, i.e. no nested</span>
<span class="sd">        instances are allowed</span>

<span class="sd">    .. warning:</span>
<span class="sd">        Due to some CUDA multiprocessing limitations (multiprocessing-cuda-note_),</span>
<span class="sd">        one cannot use the profiler with ``use_device = &#39;cuda&#39;`` to benchmark</span>
<span class="sd">        DataLoaders with ``num_workers &gt; 0``. If you wish to benchmark data loading,</span>
<span class="sd">        please use ``use_device = None`` or ``num_workers = 0``.</span>

<span class="sd">    Example:</span>
<span class="sd">        &gt;&gt;&gt; # xdoctest: +SKIP</span>
<span class="sd">        &gt;&gt;&gt; # xdoctest: +REQUIRES(env:TORCH_DOCTEST_AUTOGRAD_PROFILER)</span>
<span class="sd">        &gt;&gt;&gt; x = torch.randn((1, 1), requires_grad=True)</span>
<span class="sd">        &gt;&gt;&gt; with torch.autograd.profiler.profile() as prof:</span>
<span class="sd">        &gt;&gt;&gt;     for _ in range(100):  # any normal python code, really!</span>
<span class="sd">        &gt;&gt;&gt;         y = x ** 2</span>
<span class="sd">        &gt;&gt;&gt;         y.backward()</span>
<span class="sd">        &gt;&gt;&gt; # NOTE: some columns were removed for brevity</span>
<span class="sd">        &gt;&gt;&gt; print(prof.key_averages().table(sort_by=&quot;self_cpu_time_total&quot;))</span>
<span class="sd">        -----------------------------------  ---------------  ---------------  ---------------</span>
<span class="sd">        Name                                 Self CPU total   CPU time avg     Number of Calls</span>
<span class="sd">        -----------------------------------  ---------------  ---------------  ---------------</span>
<span class="sd">        mul                                  32.048ms         32.048ms         200</span>
<span class="sd">        pow                                  27.041ms         27.041ms         200</span>
<span class="sd">        PowBackward0                         9.727ms          55.483ms         100</span>
<span class="sd">        torch::autograd::AccumulateGrad      9.148ms          9.148ms          100</span>
<span class="sd">        torch::autograd::GraphRoot           691.816us        691.816us        100</span>
<span class="sd">        -----------------------------------  ---------------  ---------------  ---------------</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">enabled</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">use_cuda</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>  <span class="c1"># Deprecated</span>
        <span class="n">use_device</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">record_shapes</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">with_flops</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">profile_memory</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">with_stack</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">with_modules</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">use_kineto</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">use_cpu</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">experimental_config</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">acc_events</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">custom_trace_id_callback</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">enabled</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="n">enabled</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">enabled</span><span class="p">:</span>
            <span class="k">return</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">use_cuda</span> <span class="o">=</span> <span class="n">use_cuda</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_cuda</span><span class="p">:</span>
            <span class="n">warn</span><span class="p">(</span>
                <span class="s2">&quot;The attribute `use_cuda` will be deprecated soon, &quot;</span>
                <span class="s2">&quot;please use ``use_device = &#39;cuda&#39;`` instead.&quot;</span><span class="p">,</span>
                <span class="ne">FutureWarning</span><span class="p">,</span>
                <span class="n">stacklevel</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">use_device</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;cuda&quot;</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">use_device</span> <span class="o">=</span> <span class="n">use_device</span>
        <span class="c1"># TODO Consider changing _function_events into data structure with size cap</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_function_events</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">EventList</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_old_function_events</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">EventList</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="c1"># Function event processing is done lazily</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_needs_processing</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">entered</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">record_shapes</span> <span class="o">=</span> <span class="n">record_shapes</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">with_flops</span> <span class="o">=</span> <span class="n">with_flops</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">record_shapes</span> <span class="o">|=</span> <span class="bp">self</span><span class="o">.</span><span class="n">with_flops</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">profile_memory</span> <span class="o">=</span> <span class="n">profile_memory</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">with_stack</span> <span class="o">=</span> <span class="n">with_stack</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">with_modules</span> <span class="o">=</span> <span class="n">with_modules</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">use_cpu</span> <span class="o">=</span> <span class="n">use_cpu</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">acc_events</span> <span class="o">=</span> <span class="n">acc_events</span>
        <span class="k">if</span> <span class="n">experimental_config</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">experimental_config</span> <span class="o">=</span> <span class="n">_ExperimentalConfig</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">experimental_config</span> <span class="o">=</span> <span class="n">experimental_config</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">kineto_results</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">_ProfilerResult</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">profiling_start_time_ns</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">profiling_end_time_ns</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_stats</span> <span class="o">=</span> <span class="n">_ProfilerStats</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">custom_trace_id_callback</span> <span class="o">=</span> <span class="n">custom_trace_id_callback</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">trace_id</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_cpu</span><span class="p">:</span>
            <span class="k">assert</span> <span class="p">(</span>
                <span class="n">use_kineto</span>
            <span class="p">),</span> <span class="s2">&quot;Device-only events supported only with Kineto (use_kineto=True)&quot;</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_device</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">VALID_DEVICE_OPTIONS</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;cuda&quot;</span><span class="p">,</span> <span class="s2">&quot;xpu&quot;</span><span class="p">,</span> <span class="s2">&quot;mtia&quot;</span><span class="p">]</span>
            <span class="k">if</span> <span class="n">_get_privateuse1_backend_name</span><span class="p">()</span> <span class="o">!=</span> <span class="s2">&quot;privateuseone&quot;</span><span class="p">:</span>
                <span class="n">VALID_DEVICE_OPTIONS</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">_get_privateuse1_backend_name</span><span class="p">())</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_device</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">VALID_DEVICE_OPTIONS</span><span class="p">:</span>
                <span class="n">warn</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;The </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">use_device</span><span class="si">}</span><span class="s2"> is not a valid device option.&quot;</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">use_device</span> <span class="o">=</span> <span class="kc">None</span>

            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_device</span> <span class="o">==</span> <span class="s2">&quot;cuda&quot;</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">():</span>
                <span class="n">warn</span><span class="p">(</span><span class="s2">&quot;CUDA is not available, disabling CUDA profiling&quot;</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">use_cuda</span> <span class="o">=</span> <span class="kc">False</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">use_device</span> <span class="o">=</span> <span class="kc">None</span>

            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_device</span> <span class="o">==</span> <span class="s2">&quot;xpu&quot;</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">xpu</span><span class="o">.</span><span class="n">is_available</span><span class="p">():</span>
                <span class="n">warn</span><span class="p">(</span><span class="s2">&quot;XPU is not available, disabling XPU profiling&quot;</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">use_device</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">kineto_activities</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_cpu</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">kineto_activities</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">ProfilerActivity</span><span class="o">.</span><span class="n">CPU</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">profiler_kind</span> <span class="o">=</span> <span class="n">ProfilerState</span><span class="o">.</span><span class="n">KINETO</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_device</span> <span class="o">==</span> <span class="s2">&quot;cuda&quot;</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">use_kineto</span> <span class="ow">or</span> <span class="n">ProfilerActivity</span><span class="o">.</span><span class="n">CUDA</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">_supported_activities</span><span class="p">():</span>
                <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_cpu</span><span class="p">,</span> <span class="s2">&quot;Legacy CUDA profiling requires use_cpu=True&quot;</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">profiler_kind</span> <span class="o">=</span> <span class="n">ProfilerState</span><span class="o">.</span><span class="n">KINETO_GPU_FALLBACK</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">kineto_activities</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">ProfilerActivity</span><span class="o">.</span><span class="n">CUDA</span><span class="p">)</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_device</span> <span class="o">==</span> <span class="s2">&quot;xpu&quot;</span><span class="p">:</span>
            <span class="k">assert</span> <span class="p">(</span>
                <span class="n">use_kineto</span> <span class="ow">and</span> <span class="n">ProfilerActivity</span><span class="o">.</span><span class="n">XPU</span> <span class="ow">in</span> <span class="n">_supported_activities</span><span class="p">()</span>
            <span class="p">),</span> <span class="s2">&quot;Legacy XPU profiling is not supported. Requires use_kineto=True on XPU devices.&quot;</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">kineto_activities</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">ProfilerActivity</span><span class="o">.</span><span class="n">XPU</span><span class="p">)</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_device</span> <span class="o">==</span> <span class="s2">&quot;mtia&quot;</span><span class="p">:</span>
            <span class="k">assert</span> <span class="p">(</span>
                <span class="n">use_kineto</span> <span class="ow">and</span> <span class="n">ProfilerActivity</span><span class="o">.</span><span class="n">MTIA</span> <span class="ow">in</span> <span class="n">_supported_activities</span><span class="p">()</span>
            <span class="p">),</span> <span class="s2">&quot;Legacy MTIA profiling is not supported. Requires use_kineto=True on MTIA devices.&quot;</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">kineto_activities</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">ProfilerActivity</span><span class="o">.</span><span class="n">MTIA</span><span class="p">)</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_device</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_device</span> <span class="o">!=</span> <span class="s2">&quot;privateuseone&quot;</span><span class="p">:</span>
            <span class="k">if</span> <span class="p">(</span>
                <span class="ow">not</span> <span class="n">use_kineto</span>
                <span class="ow">or</span> <span class="n">ProfilerActivity</span><span class="o">.</span><span class="n">PrivateUse1</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">_supported_activities</span><span class="p">()</span>
            <span class="p">):</span>
                <span class="k">assert</span> <span class="p">(</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">use_cpu</span>
                <span class="p">),</span> <span class="s2">&quot;Legacy custombackend profiling requires use_cpu=True&quot;</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">profiler_kind</span> <span class="o">=</span> <span class="n">ProfilerState</span><span class="o">.</span><span class="n">KINETO_PRIVATEUSE1_FALLBACK</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">kineto_activities</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">ProfilerActivity</span><span class="o">.</span><span class="n">PrivateUse1</span><span class="p">)</span>

        <span class="k">assert</span> <span class="p">(</span>
            <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">kineto_activities</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span>
        <span class="p">),</span> <span class="s2">&quot;No activities specified for the profiler&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">default_trace_id</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># Generate a UUID</span>
        <span class="n">uuid_raw</span> <span class="o">=</span> <span class="n">uuid</span><span class="o">.</span><span class="n">uuid4</span><span class="p">()</span>

        <span class="k">return</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">uuid_raw</span><span class="o">.</span><span class="n">int</span><span class="si">:</span><span class="s2">032X</span><span class="si">}</span><span class="s2">&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">create_trace_id</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">custom_trace_id_callback</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">custom_trace_id_callback</span><span class="p">()</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">default_trace_id</span><span class="p">()</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">config</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">create_trace_id</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="c1"># only need to generate new trace id upon prepare trace not start trace</span>
        <span class="k">if</span> <span class="n">create_trace_id</span><span class="p">:</span>
            <span class="n">trace_id</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">create_trace_id</span><span class="p">()</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">trace_id</span> <span class="o">=</span> <span class="n">trace_id</span>
        <span class="k">return</span> <span class="n">ProfilerConfig</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">profiler_kind</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">record_shapes</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">profile_memory</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">with_stack</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">with_flops</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">with_modules</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">experimental_config</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">trace_id</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__enter__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">enabled</span><span class="p">:</span>
            <span class="k">return</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">entered</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;Profiler context manager is not reentrant&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_prepare_trace</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_start_trace</span><span class="p">()</span>
        <span class="k">return</span> <span class="bp">self</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_prepare_trace</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">entered</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="n">t0</span> <span class="o">=</span> <span class="n">perf_counter_ns</span><span class="p">()</span>
        <span class="n">_prepare_profiler</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">(</span><span class="n">create_trace_id</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">kineto_activities</span><span class="p">)</span>
        <span class="n">t1</span> <span class="o">=</span> <span class="n">perf_counter_ns</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_stats</span><span class="o">.</span><span class="n">profiler_prepare_call_duration_us</span> <span class="o">=</span> <span class="nb">int</span><span class="p">((</span><span class="n">t1</span> <span class="o">-</span> <span class="n">t0</span><span class="p">)</span> <span class="o">/</span> <span class="mi">1000</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_start_trace</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">entered</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="n">_run_on_profiler_start</span><span class="p">()</span>
        <span class="n">t0</span> <span class="o">=</span> <span class="n">perf_counter_ns</span><span class="p">()</span>
        <span class="n">_enable_profiler</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">(</span><span class="n">create_trace_id</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">kineto_activities</span><span class="p">)</span>
        <span class="n">t1</span> <span class="o">=</span> <span class="n">perf_counter_ns</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_stats</span><span class="o">.</span><span class="n">profiler_enable_call_duration_us</span> <span class="o">=</span> <span class="nb">int</span><span class="p">((</span><span class="n">t1</span> <span class="o">-</span> <span class="n">t0</span><span class="p">)</span> <span class="o">/</span> <span class="mi">1000</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">profiling_start_time_ns</span> <span class="o">=</span> <span class="n">t1</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__exit__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">exc_type</span><span class="p">,</span> <span class="n">exc_val</span><span class="p">,</span> <span class="n">exc_tb</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">enabled</span><span class="p">:</span>
            <span class="k">return</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_device</span> <span class="ow">and</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">torch</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_device</span><span class="p">):</span>
            <span class="n">device_module</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">torch</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_device</span><span class="p">)</span>
            <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">device_module</span><span class="p">,</span> <span class="s2">&quot;synchronize&quot;</span><span class="p">):</span>
                <span class="n">device_module</span><span class="o">.</span><span class="n">synchronize</span><span class="p">()</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_function_events</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">acc_events</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_old_function_events</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_function_events</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_function_events</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_needs_processing</span> <span class="o">=</span> <span class="kc">True</span>

        <span class="n">t0</span> <span class="o">=</span> <span class="n">perf_counter_ns</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">kineto_results</span> <span class="o">=</span> <span class="n">_disable_profiler</span><span class="p">()</span>
        <span class="n">t1</span> <span class="o">=</span> <span class="n">perf_counter_ns</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_stats</span><span class="o">.</span><span class="n">profiler_disable_call_duration_us</span> <span class="o">=</span> <span class="nb">int</span><span class="p">((</span><span class="n">t1</span> <span class="o">-</span> <span class="n">t0</span><span class="p">)</span> <span class="o">/</span> <span class="mi">1000</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">profiling_end_time_ns</span> <span class="o">=</span> <span class="n">t0</span>

        <span class="n">_run_on_profiler_stop</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_stats</span><span class="o">.</span><span class="n">profiling_window_duration_sec</span> <span class="o">=</span> <span class="p">(</span>
            <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">profiling_end_time_ns</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">profiling_start_time_ns</span><span class="p">)</span> <span class="o">*</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="mf">1e9</span>
        <span class="p">)</span>

        <span class="c1"># If we plan to accumulate events we should post process the function events</span>
        <span class="c1"># right away to retain the state across mulitple start/stop calls</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">acc_events</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_ensure_function_events</span><span class="p">()</span>
        <span class="k">return</span> <span class="kc">False</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_needs_processing</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_ensure_function_events</span><span class="p">()</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_function_events</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="s2">&quot;&lt;unfinished torch.autograd.profile&gt;&quot;</span>
        <span class="k">return</span> <span class="nb">repr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_function_events</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__str__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_needs_processing</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_ensure_function_events</span><span class="p">()</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_function_events</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="s2">&quot;&lt;unfinished torch.autograd.profile&gt;&quot;</span>
        <span class="k">return</span> <span class="nb">str</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_function_events</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_ensure_function_events</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Process function events lazily if required&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_function_events</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_needs_processing</span> <span class="o">=</span> <span class="kc">False</span>

        <span class="n">t0</span> <span class="o">=</span> <span class="n">perf_counter_ns</span><span class="p">()</span>
        <span class="n">parsed_results</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">kineto_results</span><span class="p">:</span>
            <span class="n">parsed_results</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_parse_kineto_results</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">kineto_results</span><span class="p">)</span>
        <span class="n">t1</span> <span class="o">=</span> <span class="n">perf_counter_ns</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_stats</span><span class="o">.</span><span class="n">parse_kineto_call_duration_us</span> <span class="o">=</span> <span class="nb">int</span><span class="p">((</span><span class="n">t1</span> <span class="o">-</span> <span class="n">t0</span><span class="p">)</span> <span class="o">/</span> <span class="mi">1000</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_function_events</span> <span class="o">=</span> <span class="n">EventList</span><span class="p">(</span>
            <span class="n">parsed_results</span><span class="p">,</span>
            <span class="n">use_device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">use_device</span><span class="p">,</span>
            <span class="n">profile_memory</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">profile_memory</span><span class="p">,</span>
            <span class="n">with_flops</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">with_flops</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">t0</span> <span class="o">=</span> <span class="n">perf_counter_ns</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_function_events</span><span class="o">.</span><span class="n">_build_tree</span><span class="p">()</span>
        <span class="n">t1</span> <span class="o">=</span> <span class="n">perf_counter_ns</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_stats</span><span class="o">.</span><span class="n">function_events_build_tree_call_duration_us</span> <span class="o">=</span> <span class="nb">int</span><span class="p">((</span><span class="n">t1</span> <span class="o">-</span> <span class="n">t0</span><span class="p">)</span> <span class="o">/</span> <span class="mi">1000</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_stats</span><span class="o">.</span><span class="n">number_of_events</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_function_events</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_old_function_events</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">acc_events</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">evt</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_old_function_events</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_function_events</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">evt</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_old_function_events</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_function_events</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;Profiler didn&#39;t finish running&quot;</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">function_events</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_function_events</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">_needs_processing</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_ensure_function_events</span><span class="p">()</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_function_events</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">table</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">sort_by</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">row_limit</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
        <span class="n">max_src_column_width</span><span class="o">=</span><span class="mi">75</span><span class="p">,</span>
        <span class="n">max_name_column_width</span><span class="o">=</span><span class="mi">55</span><span class="p">,</span>
        <span class="n">max_shapes_column_width</span><span class="o">=</span><span class="mi">80</span><span class="p">,</span>
        <span class="n">header</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">top_level_events_only</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_ensure_function_events</span><span class="p">()</span>
        <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">_function_events</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_function_events</span><span class="o">.</span><span class="n">table</span><span class="p">(</span>
            <span class="n">sort_by</span><span class="o">=</span><span class="n">sort_by</span><span class="p">,</span>
            <span class="n">row_limit</span><span class="o">=</span><span class="n">row_limit</span><span class="p">,</span>
            <span class="n">max_src_column_width</span><span class="o">=</span><span class="n">max_src_column_width</span><span class="p">,</span>
            <span class="n">max_name_column_width</span><span class="o">=</span><span class="n">max_name_column_width</span><span class="p">,</span>
            <span class="n">max_shapes_column_width</span><span class="o">=</span><span class="n">max_shapes_column_width</span><span class="p">,</span>
            <span class="n">header</span><span class="o">=</span><span class="n">header</span><span class="p">,</span>
            <span class="n">top_level_events_only</span><span class="o">=</span><span class="n">top_level_events_only</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="n">table</span><span class="o">.</span><span class="vm">__doc__</span> <span class="o">=</span> <span class="n">EventList</span><span class="o">.</span><span class="n">table</span><span class="o">.</span><span class="vm">__doc__</span>

<div class="viewcode-block" id="profile.export_chrome_trace">
<a class="viewcode-back" href="../../../python-api/generated/torch.autograd.profiler.profile.export_chrome_trace.html#torch.autograd.profiler.profile.export_chrome_trace">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">export_chrome_trace</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">path</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Exports the collected trace in Chrome JSON format. If kineto is enabled, only</span>
<span class="sd">        last cycle in schedule is exported.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">kineto_available</span><span class="p">():</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">kineto_results</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">path</span><span class="p">)</span>  <span class="c1"># type: ignore[union-attr]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_ensure_function_events</span><span class="p">()</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_function_events</span><span class="o">.</span><span class="n">export_chrome_trace</span><span class="p">(</span><span class="n">path</span><span class="p">)</span>  <span class="c1"># type: ignore[union-attr]</span></div>


    <span class="n">export_chrome_trace</span><span class="o">.</span><span class="vm">__doc__</span> <span class="o">=</span> <span class="n">EventList</span><span class="o">.</span><span class="n">export_chrome_trace</span><span class="o">.</span><span class="vm">__doc__</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">export_stacks</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">path</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">metric</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;self_cpu_time_total&quot;</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_ensure_function_events</span><span class="p">()</span>
        <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">_function_events</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">,</span> <span class="s2">&quot;Expected profiling results&quot;</span>
        <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">with_stack</span><span class="p">,</span> <span class="s2">&quot;export_stacks() requires with_stack=True&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_function_events</span><span class="o">.</span><span class="n">export_stacks</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="n">metric</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">toggle_collection_dynamic</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">enabled</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span> <span class="n">activities</span><span class="p">:</span> <span class="n">Iterable</span><span class="p">[</span><span class="n">ProfilerActivity</span><span class="p">]</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Toggles the collection of activities for the current profiler instance.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">_toggle_collection_dynamic</span><span class="p">(</span><span class="n">enabled</span><span class="p">,</span> <span class="nb">set</span><span class="p">(</span><span class="n">activities</span><span class="p">))</span>

<div class="viewcode-block" id="profile.key_averages">
<a class="viewcode-back" href="../../../python-api/generated/torch.autograd.profiler.profile.key_averages.html#torch.autograd.profiler.profile.key_averages">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">key_averages</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">group_by_input_shape</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">group_by_stack_n</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_ensure_function_events</span><span class="p">()</span>
        <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">_function_events</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">,</span> <span class="s2">&quot;Expected profiling results&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_function_events</span><span class="o">.</span><span class="n">key_averages</span><span class="p">(</span>
            <span class="n">group_by_input_shape</span><span class="p">,</span> <span class="n">group_by_stack_n</span>
        <span class="p">)</span></div>


    <span class="n">key_averages</span><span class="o">.</span><span class="vm">__doc__</span> <span class="o">=</span> <span class="n">EventList</span><span class="o">.</span><span class="n">key_averages</span><span class="o">.</span><span class="vm">__doc__</span>

<div class="viewcode-block" id="profile.total_average">
<a class="viewcode-back" href="../../../python-api/generated/torch.autograd.profiler.profile.total_average.html#torch.autograd.profiler.profile.total_average">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">total_average</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_ensure_function_events</span><span class="p">()</span>
        <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">_function_events</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">,</span> <span class="s2">&quot;Expected profiling results&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_function_events</span><span class="o">.</span><span class="n">total_average</span><span class="p">()</span></div>


    <span class="n">total_average</span><span class="o">.</span><span class="vm">__doc__</span> <span class="o">=</span> <span class="n">EventList</span><span class="o">.</span><span class="n">total_average</span><span class="o">.</span><span class="vm">__doc__</span>

    <span class="nd">@property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">self_cpu_time_total</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Returns total time spent on CPU.</span>

<span class="sd">        The total time is a sum of all self times across all the events.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_ensure_function_events</span><span class="p">()</span>
        <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">_function_events</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_function_events</span><span class="o">.</span><span class="n">self_cpu_time_total</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_parse_kineto_results</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">result</span><span class="p">:</span> <span class="n">_ProfilerResult</span><span class="p">):</span>
        <span class="c1"># result.events() has most of the events - PyTorch op-level and device-level events</span>

        <span class="n">trace_start_ns</span> <span class="o">=</span> <span class="n">result</span><span class="o">.</span><span class="n">trace_start_ns</span><span class="p">()</span>
        <span class="n">mem_records</span> <span class="o">=</span> <span class="p">[</span>
            <span class="p">[</span><span class="n">evt</span><span class="p">,</span> <span class="kc">False</span><span class="p">]</span> <span class="k">for</span> <span class="n">evt</span> <span class="ow">in</span> <span class="n">result</span><span class="o">.</span><span class="n">events</span><span class="p">()</span> <span class="k">if</span> <span class="n">evt</span><span class="o">.</span><span class="n">name</span><span class="p">()</span> <span class="o">==</span> <span class="n">MEMORY_EVENT_NAME</span>
        <span class="p">]</span>
        <span class="n">oom_records</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">evt</span> <span class="k">for</span> <span class="n">evt</span> <span class="ow">in</span> <span class="n">result</span><span class="o">.</span><span class="n">events</span><span class="p">()</span> <span class="k">if</span> <span class="n">evt</span><span class="o">.</span><span class="n">name</span><span class="p">()</span> <span class="o">==</span> <span class="n">OUT_OF_MEMORY_EVENT_NAME</span>
        <span class="p">]</span>
        <span class="n">mem_records_acc</span> <span class="o">=</span> <span class="n">MemRecordsAcc</span><span class="p">(</span><span class="n">mem_records</span><span class="p">)</span>

        <span class="k">def</span><span class="w"> </span><span class="nf">_cpu_memory_usage</span><span class="p">(</span><span class="n">mem_record</span><span class="p">):</span>
            <span class="k">return</span> <span class="p">(</span>
                <span class="n">mem_record</span><span class="o">.</span><span class="n">nbytes</span><span class="p">()</span>
                <span class="k">if</span> <span class="n">mem_record</span><span class="o">.</span><span class="n">device_type</span><span class="p">()</span>
                <span class="ow">in</span> <span class="p">[</span><span class="n">DeviceType</span><span class="o">.</span><span class="n">CPU</span><span class="p">,</span> <span class="n">DeviceType</span><span class="o">.</span><span class="n">MKLDNN</span><span class="p">,</span> <span class="n">DeviceType</span><span class="o">.</span><span class="n">IDEEP</span><span class="p">]</span>
                <span class="k">else</span> <span class="mi">0</span>
            <span class="p">)</span>

        <span class="k">def</span><span class="w"> </span><span class="nf">_device_memory_usage</span><span class="p">(</span><span class="n">mem_record</span><span class="p">):</span>
            <span class="k">return</span> <span class="p">(</span>
                <span class="n">mem_record</span><span class="o">.</span><span class="n">nbytes</span><span class="p">()</span>
                <span class="k">if</span> <span class="n">mem_record</span><span class="o">.</span><span class="n">device_type</span><span class="p">()</span>
                <span class="ow">in</span> <span class="p">[</span><span class="n">DeviceType</span><span class="o">.</span><span class="n">CUDA</span><span class="p">,</span> <span class="n">DeviceType</span><span class="o">.</span><span class="n">PrivateUse1</span><span class="p">,</span> <span class="n">DeviceType</span><span class="o">.</span><span class="n">HIP</span><span class="p">]</span>
                <span class="k">else</span> <span class="mi">0</span>
            <span class="p">)</span>

        <span class="c1"># Create and return FunctionEvent list, which contains all function events</span>
        <span class="c1"># Here 2 function events are created:</span>
        <span class="c1"># all_function_events contains all events associated with each kineto event from result</span>
        <span class="n">all_function_events</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="c1"># frontend_function_events contains the events in aten or torch frontend level,</span>
        <span class="c1"># whose correlation id is 0</span>
        <span class="n">frontend_function_events</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">device_corr_map</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">FunctionEvent</span><span class="p">]]</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="n">max_evt_id</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">kineto_event</span> <span class="ow">in</span> <span class="n">result</span><span class="o">.</span><span class="n">events</span><span class="p">():</span>
            <span class="k">if</span> <span class="n">_filter_name</span><span class="p">(</span><span class="n">kineto_event</span><span class="o">.</span><span class="n">name</span><span class="p">()):</span>
                <span class="k">continue</span>
            <span class="n">rel_start_ns</span> <span class="o">=</span> <span class="n">kineto_event</span><span class="o">.</span><span class="n">start_ns</span><span class="p">()</span> <span class="o">-</span> <span class="n">trace_start_ns</span>
            <span class="n">rel_end_ns</span> <span class="o">=</span> <span class="n">kineto_event</span><span class="o">.</span><span class="n">end_ns</span><span class="p">()</span> <span class="o">-</span> <span class="n">trace_start_ns</span>
            <span class="n">abs_end_ns</span> <span class="o">=</span> <span class="n">kineto_event</span><span class="o">.</span><span class="n">end_ns</span><span class="p">()</span>

            <span class="n">cpu_memory_usage</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="n">device_memory_usage</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="k">if</span> <span class="n">kineto_event</span><span class="o">.</span><span class="n">device_type</span><span class="p">()</span> <span class="o">==</span> <span class="n">DeviceType</span><span class="o">.</span><span class="n">CPU</span><span class="p">:</span>
                <span class="c1"># find the corresponding memory allocation events</span>
                <span class="k">for</span> <span class="n">mem_record</span> <span class="ow">in</span> <span class="n">mem_records_acc</span><span class="o">.</span><span class="n">in_interval</span><span class="p">(</span>
                    <span class="n">kineto_event</span><span class="o">.</span><span class="n">start_ns</span><span class="p">()</span> <span class="o">/</span> <span class="mi">1000</span><span class="p">,</span> <span class="n">abs_end_ns</span> <span class="o">/</span> <span class="mi">1000</span>
                <span class="p">):</span>
                    <span class="n">cpu_memory_usage</span> <span class="o">+=</span> <span class="n">_cpu_memory_usage</span><span class="p">(</span><span class="n">mem_record</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
                    <span class="n">device_memory_usage</span> <span class="o">+=</span> <span class="n">_device_memory_usage</span><span class="p">(</span><span class="n">mem_record</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
                    <span class="n">mem_record</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>

            <span class="n">is_async</span> <span class="o">=</span> <span class="n">kineto_event</span><span class="o">.</span><span class="n">is_async</span><span class="p">()</span> <span class="ow">or</span> <span class="p">(</span>
                <span class="n">kineto_event</span><span class="o">.</span><span class="n">start_thread_id</span><span class="p">()</span> <span class="o">!=</span> <span class="n">kineto_event</span><span class="o">.</span><span class="n">end_thread_id</span><span class="p">()</span>
            <span class="p">)</span>

            <span class="n">fe</span> <span class="o">=</span> <span class="n">FunctionEvent</span><span class="p">(</span>
                <span class="nb">id</span><span class="o">=</span><span class="n">kineto_event</span><span class="o">.</span><span class="n">correlation_id</span><span class="p">(),</span>
                <span class="n">name</span><span class="o">=</span><span class="n">_rewrite_name</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="n">kineto_event</span><span class="o">.</span><span class="n">name</span><span class="p">(),</span> <span class="n">with_wildcard</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
                <span class="n">trace_name</span><span class="o">=</span><span class="n">_rewrite_name</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="n">kineto_event</span><span class="o">.</span><span class="n">name</span><span class="p">(),</span> <span class="n">with_wildcard</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span>
                <span class="n">thread</span><span class="o">=</span><span class="n">kineto_event</span><span class="o">.</span><span class="n">start_thread_id</span><span class="p">(),</span>
                <span class="n">start_us</span><span class="o">=</span><span class="n">rel_start_ns</span> <span class="o">/</span> <span class="mi">1000</span><span class="p">,</span>
                <span class="n">end_us</span><span class="o">=</span><span class="n">rel_end_ns</span> <span class="o">/</span> <span class="mi">1000</span><span class="p">,</span>
                <span class="n">fwd_thread</span><span class="o">=</span><span class="n">kineto_event</span><span class="o">.</span><span class="n">fwd_thread_id</span><span class="p">(),</span>
                <span class="n">input_shapes</span><span class="o">=</span><span class="n">kineto_event</span><span class="o">.</span><span class="n">shapes</span><span class="p">(),</span>
                <span class="n">concrete_inputs</span><span class="o">=</span><span class="n">kineto_event</span><span class="o">.</span><span class="n">concrete_inputs</span><span class="p">(),</span>
                <span class="n">kwinputs</span><span class="o">=</span><span class="n">kineto_event</span><span class="o">.</span><span class="n">kwinputs</span><span class="p">(),</span>
                <span class="n">stack</span><span class="o">=</span><span class="p">[</span>
                    <span class="n">entry</span>
                    <span class="k">for</span> <span class="n">entry</span> <span class="ow">in</span> <span class="n">kineto_event</span><span class="o">.</span><span class="n">stack</span><span class="p">()</span>
                    <span class="k">if</span> <span class="n">_filter_stack_entry</span><span class="p">(</span><span class="n">entry</span><span class="p">)</span>
                <span class="p">],</span>
                <span class="n">scope</span><span class="o">=</span><span class="n">kineto_event</span><span class="o">.</span><span class="n">scope</span><span class="p">(),</span>
                <span class="n">use_device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">use_device</span><span class="p">,</span>
                <span class="n">cpu_memory_usage</span><span class="o">=</span><span class="n">cpu_memory_usage</span><span class="p">,</span>
                <span class="n">device_memory_usage</span><span class="o">=</span><span class="n">device_memory_usage</span><span class="p">,</span>
                <span class="n">is_async</span><span class="o">=</span><span class="n">is_async</span><span class="p">,</span>
                <span class="n">sequence_nr</span><span class="o">=</span><span class="n">kineto_event</span><span class="o">.</span><span class="n">sequence_nr</span><span class="p">(),</span>
                <span class="n">device_type</span><span class="o">=</span><span class="n">kineto_event</span><span class="o">.</span><span class="n">device_type</span><span class="p">(),</span>
                <span class="n">device_index</span><span class="o">=</span><span class="n">kineto_event</span><span class="o">.</span><span class="n">device_index</span><span class="p">(),</span>
                <span class="n">device_resource_id</span><span class="o">=</span><span class="n">kineto_event</span><span class="o">.</span><span class="n">device_resource_id</span><span class="p">(),</span>
                <span class="n">flops</span><span class="o">=</span><span class="n">kineto_event</span><span class="o">.</span><span class="n">flops</span><span class="p">(),</span>
                <span class="n">is_user_annotation</span><span class="o">=</span><span class="n">kineto_event</span><span class="o">.</span><span class="n">is_user_annotation</span><span class="p">(),</span>
            <span class="p">)</span>
            <span class="n">max_evt_id</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">max_evt_id</span><span class="p">,</span> <span class="n">fe</span><span class="o">.</span><span class="n">id</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">fe</span><span class="o">.</span><span class="n">device_type</span> <span class="o">==</span> <span class="n">DeviceType</span><span class="o">.</span><span class="n">CPU</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">fe</span><span class="o">.</span><span class="n">is_async</span><span class="p">:</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_device</span> <span class="o">==</span> <span class="s2">&quot;privateuseone&quot;</span><span class="p">:</span>
                    <span class="n">privateuse1_time</span> <span class="o">=</span> <span class="n">kineto_event</span><span class="o">.</span><span class="n">privateuse1_elapsed_us</span><span class="p">()</span>
                    <span class="k">if</span> <span class="n">privateuse1_time</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                        <span class="n">fe</span><span class="o">.</span><span class="n">append_kernel</span><span class="p">(</span><span class="n">fe</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">fe</span><span class="o">.</span><span class="n">device_index</span><span class="p">,</span> <span class="n">privateuse1_time</span><span class="p">)</span>
                        <span class="n">fe</span><span class="o">.</span><span class="n">is_legacy</span> <span class="o">=</span> <span class="kc">True</span>
                <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_device</span> <span class="o">==</span> <span class="s2">&quot;cuda&quot;</span><span class="p">:</span>
                    <span class="c1"># Check if we have CUDA time as a fallback</span>
                    <span class="n">cuda_time</span> <span class="o">=</span> <span class="n">kineto_event</span><span class="o">.</span><span class="n">cuda_elapsed_us</span><span class="p">()</span>
                    <span class="k">if</span> <span class="n">cuda_time</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                        <span class="n">fe</span><span class="o">.</span><span class="n">append_kernel</span><span class="p">(</span><span class="n">fe</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">fe</span><span class="o">.</span><span class="n">device_index</span><span class="p">,</span> <span class="n">cuda_time</span><span class="p">)</span>
                        <span class="n">fe</span><span class="o">.</span><span class="n">is_legacy</span> <span class="o">=</span> <span class="kc">True</span>
            <span class="n">all_function_events</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">fe</span><span class="p">)</span>
            <span class="n">corr_id</span> <span class="o">=</span> <span class="n">kineto_event</span><span class="o">.</span><span class="n">linked_correlation_id</span><span class="p">()</span>
            <span class="k">if</span> <span class="n">corr_id</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">corr_id</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">device_corr_map</span><span class="p">:</span>
                    <span class="n">device_corr_map</span><span class="p">[</span><span class="n">corr_id</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
                <span class="n">device_corr_map</span><span class="p">[</span><span class="n">corr_id</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">fe</span><span class="p">)</span>
            <span class="k">elif</span> <span class="n">corr_id</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">frontend_function_events</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">fe</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;Got negative correlation id </span><span class="si">{</span><span class="n">corr_id</span><span class="si">}</span><span class="s2"> in profiler post processing&quot;</span>
                <span class="p">)</span>

        <span class="c1"># associate device kernels and device runtime (CPU) with CPU events</span>
        <span class="k">for</span> <span class="n">fe</span> <span class="ow">in</span> <span class="n">frontend_function_events</span><span class="p">:</span>
            <span class="k">if</span> <span class="p">(</span>
                <span class="n">fe</span><span class="o">.</span><span class="n">device_type</span> <span class="o">==</span> <span class="n">DeviceType</span><span class="o">.</span><span class="n">CPU</span>
                <span class="ow">and</span> <span class="ow">not</span> <span class="n">fe</span><span class="o">.</span><span class="n">is_async</span>
                <span class="ow">and</span> <span class="n">fe</span><span class="o">.</span><span class="n">id</span> <span class="ow">in</span> <span class="n">device_corr_map</span>
            <span class="p">):</span>
                <span class="k">for</span> <span class="n">f_evt</span> <span class="ow">in</span> <span class="n">device_corr_map</span><span class="p">[</span><span class="n">fe</span><span class="o">.</span><span class="n">id</span><span class="p">]:</span>
                    <span class="k">if</span> <span class="p">(</span>
                        <span class="n">f_evt</span><span class="o">.</span><span class="n">device_type</span> <span class="o">==</span> <span class="n">DeviceType</span><span class="o">.</span><span class="n">CUDA</span>
                        <span class="ow">or</span> <span class="n">f_evt</span><span class="o">.</span><span class="n">device_type</span> <span class="o">==</span> <span class="n">DeviceType</span><span class="o">.</span><span class="n">PrivateUse1</span>
                    <span class="p">):</span>
                        <span class="n">fe</span><span class="o">.</span><span class="n">append_kernel</span><span class="p">(</span>
                            <span class="n">f_evt</span><span class="o">.</span><span class="n">name</span><span class="p">,</span>
                            <span class="n">f_evt</span><span class="o">.</span><span class="n">device_index</span><span class="p">,</span>
                            <span class="n">f_evt</span><span class="o">.</span><span class="n">time_range</span><span class="o">.</span><span class="n">end</span> <span class="o">-</span> <span class="n">f_evt</span><span class="o">.</span><span class="n">time_range</span><span class="o">.</span><span class="n">start</span><span class="p">,</span>
                        <span class="p">)</span>
                    <span class="k">elif</span> <span class="n">f_evt</span><span class="o">.</span><span class="n">device_type</span> <span class="o">==</span> <span class="n">DeviceType</span><span class="o">.</span><span class="n">CPU</span><span class="p">:</span>
                        <span class="c1"># make sure that &#39;thread&#39; of a CPU Kineto (e.g. Device Runtime) event is associated</span>
                        <span class="c1"># with the &#39;thread&#39; of the corresponding linked PyTorch event to properly track</span>
                        <span class="c1"># parents and children</span>
                        <span class="n">f_evt</span><span class="o">.</span><span class="n">thread</span> <span class="o">=</span> <span class="n">fe</span><span class="o">.</span><span class="n">thread</span>

        <span class="k">def</span><span class="w"> </span><span class="nf">createFunctionEventForMemoryEvents</span><span class="p">(</span><span class="n">evt</span><span class="p">):</span>
            <span class="n">rel_start_ns</span> <span class="o">=</span> <span class="n">evt</span><span class="o">.</span><span class="n">start_ns</span><span class="p">()</span> <span class="o">-</span> <span class="n">trace_start_ns</span>
            <span class="n">fe</span> <span class="o">=</span> <span class="n">FunctionEvent</span><span class="p">(</span>
                <span class="nb">id</span><span class="o">=</span><span class="n">max_evt_id</span><span class="p">,</span>
                <span class="n">name</span><span class="o">=</span><span class="n">evt</span><span class="o">.</span><span class="n">name</span><span class="p">(),</span>
                <span class="n">trace_name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>  <span class="c1"># not outputting in the trace</span>
                <span class="n">thread</span><span class="o">=</span><span class="n">evt</span><span class="o">.</span><span class="n">start_thread_id</span><span class="p">(),</span>
                <span class="n">start_us</span><span class="o">=</span><span class="n">rel_start_ns</span> <span class="o">/</span> <span class="mi">1000</span><span class="p">,</span>
                <span class="n">end_us</span><span class="o">=</span><span class="n">rel_start_ns</span> <span class="o">/</span> <span class="mi">1000</span><span class="p">,</span>  <span class="c1"># no duration</span>
                <span class="n">fwd_thread</span><span class="o">=</span><span class="n">evt</span><span class="o">.</span><span class="n">start_thread_id</span><span class="p">(),</span>
                <span class="n">input_shapes</span><span class="o">=</span><span class="p">[],</span>
                <span class="n">stack</span><span class="o">=</span><span class="p">[],</span>
                <span class="n">scope</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>  <span class="c1"># RecordScope::FUNCTION</span>
                <span class="n">use_device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">use_device</span><span class="p">,</span>
                <span class="n">cpu_memory_usage</span><span class="o">=</span><span class="n">_cpu_memory_usage</span><span class="p">(</span><span class="n">evt</span><span class="p">),</span>
                <span class="n">device_memory_usage</span><span class="o">=</span><span class="n">_device_memory_usage</span><span class="p">(</span><span class="n">evt</span><span class="p">),</span>
                <span class="n">is_async</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                <span class="n">sequence_nr</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span>
                <span class="n">device_type</span><span class="o">=</span><span class="n">DeviceType</span><span class="o">.</span><span class="n">CPU</span><span class="p">,</span>
                <span class="n">device_index</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="k">return</span> <span class="n">fe</span>

        <span class="c1"># output top-level memory events</span>
        <span class="k">for</span> <span class="n">mem_record</span> <span class="ow">in</span> <span class="n">mem_records</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">mem_record</span><span class="p">[</span><span class="mi">1</span><span class="p">]:</span>
                <span class="n">max_evt_id</span> <span class="o">+=</span> <span class="mi">1</span>
                <span class="n">fe</span> <span class="o">=</span> <span class="n">createFunctionEventForMemoryEvents</span><span class="p">(</span><span class="n">mem_record</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
                <span class="n">all_function_events</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">fe</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">oom_record</span> <span class="ow">in</span> <span class="n">oom_records</span><span class="p">:</span>
            <span class="n">max_evt_id</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="n">fe</span> <span class="o">=</span> <span class="n">createFunctionEventForMemoryEvents</span><span class="p">(</span><span class="n">oom_record</span><span class="p">)</span>
            <span class="n">all_function_events</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">fe</span><span class="p">)</span>

        <span class="n">all_function_events</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span>
            <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">evt</span><span class="p">:</span> <span class="p">[</span><span class="n">evt</span><span class="o">.</span><span class="n">time_range</span><span class="o">.</span><span class="n">start</span><span class="p">,</span> <span class="o">-</span><span class="n">evt</span><span class="o">.</span><span class="n">time_range</span><span class="o">.</span><span class="n">end</span><span class="p">]</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">all_function_events</span></div>



<div class="viewcode-block" id="record_function">
<a class="viewcode-back" href="../../../python-api/generated/torch.autograd.profiler.record_function.html#torch.autograd.profiler.record_function">[docs]</a>
<span class="k">class</span><span class="w"> </span><span class="nc">record_function</span><span class="p">(</span><span class="n">_ContextDecorator</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Context manager/function decorator that adds a label to a code block/function when running autograd profiler.</span>
<span class="sd">    Label will only appear if CPU activity tracing is enabled.</span>

<span class="sd">    It is useful when tracing the code profile.</span>

<span class="sd">    Args:</span>
<span class="sd">        name (str): Label assigned to the block of code.</span>
<span class="sd">        node_id (int): ID of node, for distributed profiling. Unset in</span>
<span class="sd">        non-distributed cases.</span>

<span class="sd">    Example:</span>
<span class="sd">        &gt;&gt;&gt; # xdoctest: +REQUIRES(env:TORCH_DOCTEST_AUTOGRAD_PROFILER)</span>
<span class="sd">        &gt;&gt;&gt; x = torch.randn((1, 1), requires_grad=True)</span>
<span class="sd">        &gt;&gt;&gt; with torch.autograd.profiler.profile() as prof:</span>
<span class="sd">        ...     y = x ** 2</span>
<span class="sd">        ...     with torch.autograd.profiler.record_function(&quot;label-z&quot;): # label the block</span>
<span class="sd">        ...         z = y ** 3</span>
<span class="sd">        ...     y.backward()</span>
<span class="sd">        ...</span>
<span class="sd">        &gt;&gt;&gt; # xdoctest: +IGNORE_WANT</span>
<span class="sd">        &gt;&gt;&gt; # NOTE: some columns were removed for brevity</span>
<span class="sd">        &gt;&gt;&gt; print(prof.key_averages().table(sort_by=&quot;self_cpu_time_total&quot;))</span>
<span class="sd">        -----------------------------------  ---------------  ---------------  ---------------</span>
<span class="sd">        Name                                 Self CPU total %  CPU time avg     Number of Calls</span>
<span class="sd">        -----------------------------------  ---------------  ---------------  ---------------</span>
<span class="sd">        pow                                  60.77%           47.470us         3</span>
<span class="sd">        mul                                  21.73%           25.465us         2</span>
<span class="sd">        PowBackward0                         12.03%           121.891us        1</span>
<span class="sd">        torch::autograd::AccumulateGrad      2.70%            6.324us          1</span>
<span class="sd">        label-z                              2.13%            12.421us         1</span>
<span class="sd">        torch::autograd::GraphRoot           0.64%            1.503us          1</span>
<span class="sd">        -----------------------------------  ---------------  ---------------  ---------------</span>
<span class="sd">        Self CPU time total: 234.344us</span>
<span class="sd">        CUDA time total: 0.000us</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">args</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="n">name</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="n">args</span>
        <span class="c1"># Whether or not we should run record function&#39;s end callbacks when exiting.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">run_callbacks_on_exit</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="c1"># TODO: TorchScript ignores standard type annotation here</span>
        <span class="c1"># self.record: Optional[&quot;torch.classes.profiler._RecordFunction&quot;] = None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">record</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span>
            <span class="n">Optional</span><span class="p">[</span><span class="s2">&quot;torch.classes.profiler._RecordFunction&quot;</span><span class="p">],</span> <span class="kc">None</span>
        <span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__enter__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">record</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">profiler</span><span class="o">.</span><span class="n">_record_function_enter_new</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">args</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__exit__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">exc_type</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="n">exc_value</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="n">traceback</span><span class="p">:</span> <span class="n">Any</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">run_callbacks_on_exit</span><span class="p">:</span>
            <span class="k">return</span>

        <span class="c1"># Local variable is needed by TorchScript to refine Optional[T] to T</span>
        <span class="n">record</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">record</span>
        <span class="k">assert</span> <span class="n">record</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>

        <span class="c1"># TODO: Too slow with __torch_function__ handling enabled</span>
        <span class="c1"># See https://github.com/pytorch/pytorch/issues/76410</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">is_scripting</span><span class="p">():</span>
            <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">DisableTorchFunctionSubclass</span><span class="p">():</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">profiler</span><span class="o">.</span><span class="n">_record_function_exit</span><span class="o">.</span><span class="n">_RecordFunction</span><span class="p">(</span><span class="n">record</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">profiler</span><span class="o">.</span><span class="n">_record_function_exit</span><span class="p">(</span><span class="n">record</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_call_end_callbacks_on_future</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">fut</span><span class="p">:</span> <span class="n">Future</span><span class="p">[</span><span class="n">Any</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Future</span><span class="p">[</span><span class="n">Any</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Use for profiling async calls that return a future.</span>

<span class="sd">        Calling this function will extend recording beyond this scope, until the future is</span>
<span class="sd">        satisfied. It is useful for profiling the end to end time of asynchronous calls.</span>
<span class="sd">        This function should only be called once to attach the callback onto the future, and</span>
<span class="sd">        will throw if called multiple times.</span>

<span class="sd">        Args:</span>
<span class="sd">            fut: (torch._C.Future): future for which to schedule</span>
<span class="sd">            callback for.</span>

<span class="sd">        Returns:</span>
<span class="sd">            A future that completes with the value of the passed in future when</span>
<span class="sd">            the profiling callbacks have ran.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Throw if we have already attached a callback onto the future.</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">run_callbacks_on_exit</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;_call_end_callbacks_on_future can only be called once.&quot;</span><span class="p">)</span>

        <span class="c1"># We are scheduling to run this RecordFunction&#39;s end callbacks when the</span>
        <span class="c1"># passed in future completes, so don&#39;t run end callbacks on exit.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">run_callbacks_on_exit</span> <span class="o">=</span> <span class="kc">False</span>

        <span class="c1"># Local variable is needed by TorchScript to refine Optional[T] to T</span>
        <span class="n">record</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">record</span>
        <span class="k">assert</span> <span class="n">record</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>

        <span class="c1"># TODO: Too slow with __torch_function__ handling enabled</span>
        <span class="c1"># See https://github.com/pytorch/pytorch/issues/76410</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">is_scripting</span><span class="p">():</span>
            <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">DisableTorchFunctionSubclass</span><span class="p">():</span>
                <span class="n">profiled_future</span> <span class="o">=</span> <span class="p">(</span>
                    <span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">profiler</span><span class="o">.</span><span class="n">_call_end_callbacks_on_jit_fut</span><span class="o">.</span><span class="n">_RecordFunction</span><span class="p">(</span>
                        <span class="n">record</span><span class="p">,</span> <span class="n">fut</span>
                    <span class="p">)</span>
                <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">profiled_future</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">profiler</span><span class="o">.</span><span class="n">_call_end_callbacks_on_jit_fut</span><span class="p">(</span>
                <span class="n">record</span><span class="p">,</span> <span class="n">fut</span>
            <span class="p">)</span>
        <span class="k">return</span> <span class="n">profiled_future</span></div>



<div class="viewcode-block" id="emit_itt">
<a class="viewcode-back" href="../../../python-api/autograd.html#torch.autograd.profiler.emit_itt">[docs]</a>
<span class="k">class</span><span class="w"> </span><span class="nc">emit_itt</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Context manager that makes every autograd operation emit an ITT range.</span>

<span class="sd">    It is useful when running the program under Intel(R) VTune Profiler::</span>

<span class="sd">        vtune &lt;--vtune-flags&gt; &lt;regular command here&gt;</span>

<span class="sd">    The Instrumentation and Tracing Technology (ITT) API enables your application to generate and</span>
<span class="sd">    control the collection of trace data during its execution across different Intel tools.</span>
<span class="sd">    This context manager is to annotate Intel(R) VTune Profiling trace. With help of this context manager,</span>
<span class="sd">    you will be able to see labled ranges in Intel(R) VTune Profiler GUI.</span>

<span class="sd">    .. warning:</span>
<span class="sd">        This context manager should not be called recursively, i.e. at most one</span>
<span class="sd">        instance should be enabled at any given time.</span>

<span class="sd">    Args:</span>
<span class="sd">        enabled (bool, optional): Setting ``enabled=False`` makes this context manager a no-op.</span>
<span class="sd">            Default: ``True``.</span>
<span class="sd">        record_shapes (bool, optional): If ``record_shapes=True``, the itt range wrapping</span>
<span class="sd">            each autograd op will append information about the sizes of Tensor arguments received</span>
<span class="sd">            by that op, in the following format:</span>
<span class="sd">            ``[[arg0.size(0), arg0.size(1), ...], [arg1.size(0), arg1.size(1), ...], ...]``</span>
<span class="sd">            Non-tensor arguments will be represented by ``[]``.</span>
<span class="sd">            Arguments will be listed in the order they are received by the backend op.</span>
<span class="sd">            Please note that this order may not match the order in which those arguments were passed</span>
<span class="sd">            on the Python side.  Also note that shape recording may increase the overhead of itt range creation.</span>
<span class="sd">            Default: ``False``</span>

<span class="sd">    Example:</span>
<span class="sd">        &gt;&gt;&gt; # xdoctest: +SKIP(&quot;Undefined variables&quot;)</span>
<span class="sd">        &gt;&gt;&gt; # xdoctest: +REQUIRES(env:TORCH_DOCTEST_AUTOGRAD_PROFILER)</span>
<span class="sd">        &gt;&gt;&gt; with torch.autograd.profiler.emit_itt():</span>
<span class="sd">        ...     model(x)</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">enabled</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">record_shapes</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">enabled</span> <span class="o">=</span> <span class="n">enabled</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">entered</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">record_shapes</span> <span class="o">=</span> <span class="n">record_shapes</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__enter__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">enabled</span><span class="p">:</span>
            <span class="k">return</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">entered</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;ITT annotation context manager is not reentrant&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">entered</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="n">_run_on_profiler_start</span><span class="p">()</span>
        <span class="n">_enable_profiler</span><span class="p">(</span>
            <span class="n">ProfilerConfig</span><span class="p">(</span>
                <span class="n">ProfilerState</span><span class="o">.</span><span class="n">ITT</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">record_shapes</span><span class="p">,</span>
                <span class="kc">False</span><span class="p">,</span>
                <span class="kc">False</span><span class="p">,</span>
                <span class="kc">False</span><span class="p">,</span>
                <span class="kc">False</span><span class="p">,</span>
                <span class="n">_ExperimentalConfig</span><span class="p">(),</span>
            <span class="p">),</span>
            <span class="nb">set</span><span class="p">(),</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__exit__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">exc_type</span><span class="p">,</span> <span class="n">exc_val</span><span class="p">,</span> <span class="n">exc_tb</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">enabled</span><span class="p">:</span>
            <span class="k">return</span>
        <span class="n">_disable_profiler</span><span class="p">()</span>
        <span class="n">_run_on_profiler_stop</span><span class="p">()</span>
        <span class="k">return</span> <span class="kc">False</span></div>



<div class="viewcode-block" id="emit_nvtx">
<a class="viewcode-back" href="../../../python-api/autograd.html#torch.autograd.profiler.emit_nvtx">[docs]</a>
<span class="k">class</span><span class="w"> </span><span class="nc">emit_nvtx</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Context manager that makes every autograd operation emit an NVTX range.</span>

<span class="sd">    It is useful when running the program under nvprof::</span>

<span class="sd">        nvprof --profile-from-start off -o trace_name.prof -- &lt;regular command here&gt;</span>

<span class="sd">    Unfortunately, there&#39;s no way to force nvprof to flush the data it collected</span>
<span class="sd">    to disk, so for CUDA profiling one has to use this context manager to annotate</span>
<span class="sd">    nvprof traces and wait for the process to exit before inspecting them.</span>
<span class="sd">    Then, either NVIDIA Visual Profiler (nvvp) can be used to visualize the timeline, or</span>
<span class="sd">    :func:`torch.autograd.profiler.load_nvprof` can load the results for inspection</span>
<span class="sd">    e.g. in Python REPL.</span>

<span class="sd">    .. warning:</span>
<span class="sd">        This context manager should not be called recursively, i.e. at most one</span>
<span class="sd">        instance should be enabled at any given time.</span>

<span class="sd">    Args:</span>
<span class="sd">        enabled (bool, optional): Setting ``enabled=False`` makes this context manager a no-op.</span>
<span class="sd">            Default: ``True``.</span>
<span class="sd">        record_shapes (bool, optional): If ``record_shapes=True``, the nvtx range wrapping</span>
<span class="sd">            each autograd op will append information about the sizes of Tensor arguments received</span>
<span class="sd">            by that op, in the following format:</span>
<span class="sd">            ``[[arg0.size(0), arg0.size(1), ...], [arg1.size(0), arg1.size(1), ...], ...]``</span>
<span class="sd">            Non-tensor arguments will be represented by ``[]``.</span>
<span class="sd">            Arguments will be listed in the order they are received by the backend op.</span>
<span class="sd">            Please note that this order may not match the order in which those arguments were passed</span>
<span class="sd">            on the Python side.  Also note that shape recording may increase the overhead of nvtx range creation.</span>
<span class="sd">            Default: ``False``</span>

<span class="sd">    Example:</span>
<span class="sd">        &gt;&gt;&gt; # xdoctest: +SKIP(&quot;undefined variables&quot;)</span>
<span class="sd">        &gt;&gt;&gt; # xdoctest: +REQUIRES(env:TORCH_DOCTEST_AUTOGRAD_PROFILER)</span>
<span class="sd">        &gt;&gt;&gt; with torch.cuda.profiler.profile():</span>
<span class="sd">        ...     model(x)  # Warmup CUDA memory allocator and profiler</span>
<span class="sd">        ...     with torch.autograd.profiler.emit_nvtx():</span>
<span class="sd">        ...         model(x)</span>

<span class="sd">    **Forward-backward correlation**</span>

<span class="sd">    When viewing a profile created using :class:`emit_nvtx` in the Nvidia Visual Profiler,</span>
<span class="sd">    correlating each backward-pass op with the corresponding forward-pass op can be difficult.</span>
<span class="sd">    To ease this task, :class:`emit_nvtx` appends sequence number information to the ranges it</span>
<span class="sd">    generates.</span>

<span class="sd">    During the forward pass, each function range is decorated with ``seq=&lt;N&gt;``.  ``seq`` is a running</span>
<span class="sd">    counter, incremented each time a new backward Function object is created and stashed for backward.</span>
<span class="sd">    Thus, the ``seq=&lt;N&gt;`` annotation associated with each forward function range tells you that</span>
<span class="sd">    if a backward Function object is created by this forward function,</span>
<span class="sd">    the backward object will receive sequence number N.</span>
<span class="sd">    During the backward pass, the top-level range wrapping each C++ backward Function&#39;s</span>
<span class="sd">    ``apply()`` call is decorated with ``stashed seq=&lt;M&gt;``.  ``M`` is the sequence number that</span>
<span class="sd">    the backward object was created with.  By comparing ``stashed seq`` numbers in backward with ``seq``</span>
<span class="sd">    numbers in forward, you can track down which forward op created each backward Function.</span>

<span class="sd">    Any functions executed during the backward pass are also decorated with ``seq=&lt;N&gt;``.  During</span>
<span class="sd">    default backward (with ``create_graph=False``) this information is irrelevant, and in fact,</span>
<span class="sd">    ``N`` may simply be 0 for all such functions.  Only the top-level ranges associated with</span>
<span class="sd">    backward Function objects&#39; ``apply()`` methods are useful, as a way to correlate these Function</span>
<span class="sd">    objects with the earlier forward pass.</span>

<span class="sd">    **Double-backward**</span>

<span class="sd">    If, on the other hand, a backward pass with ``create_graph=True`` is underway (in other words,</span>
<span class="sd">    if you are setting up for a double-backward), each function&#39;s execution during backward</span>
<span class="sd">    is given a nonzero, useful ``seq=&lt;N&gt;``.  Those functions may themselves create Function objects</span>
<span class="sd">    to be executed later during double-backward, just as the original functions in the forward pass did.</span>
<span class="sd">    The relationship between backward and double-backward is conceptually the same as the relationship</span>
<span class="sd">    between forward and backward: The functions still emit current-sequence-number-tagged ranges,</span>
<span class="sd">    the Function objects they create still stash those sequence numbers, and during the eventual</span>
<span class="sd">    double-backward, the Function objects&#39; ``apply()`` ranges are still tagged with ``stashed seq``</span>
<span class="sd">    numbers, which can be compared to `seq` numbers from the backward pass.</span>

<span class="sd">    .. warning:</span>
<span class="sd">        The sequence number is thread-local, and some forward functions don&#39;t create an associated</span>
<span class="sd">        backward Function object (instead delegating that to sub-functions further down the call chain).</span>
<span class="sd">        For these reasons, the correspondence of stashed sequence numbers in</span>
<span class="sd">        backward Function ``apply()`` ranges with `seq` numbers in forward-pass ranges is</span>
<span class="sd">        not guaranteed to be 1 to 1.  The sequence numbers alone may not be enough to fully</span>
<span class="sd">        disambiguate which forward function created which</span>
<span class="sd">        backward Function object.  You may need to make a judgment based on analytic knowledge of what</span>
<span class="sd">        the expected correspondence should be.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">enabled</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">record_shapes</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">enabled</span> <span class="o">=</span> <span class="n">enabled</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">entered</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">record_shapes</span> <span class="o">=</span> <span class="n">record_shapes</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__enter__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">enabled</span><span class="p">:</span>
            <span class="k">return</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">entered</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;NVTX annotation context manager is not reentrant&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">entered</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">synchronize</span><span class="p">()</span>
        <span class="n">_run_on_profiler_start</span><span class="p">()</span>
        <span class="n">_enable_profiler</span><span class="p">(</span>
            <span class="n">ProfilerConfig</span><span class="p">(</span>
                <span class="n">ProfilerState</span><span class="o">.</span><span class="n">NVTX</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">record_shapes</span><span class="p">,</span>
                <span class="kc">False</span><span class="p">,</span>
                <span class="kc">False</span><span class="p">,</span>
                <span class="kc">False</span><span class="p">,</span>
                <span class="kc">False</span><span class="p">,</span>
                <span class="n">_ExperimentalConfig</span><span class="p">(),</span>
            <span class="p">),</span>
            <span class="nb">set</span><span class="p">(),</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__exit__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">exc_type</span><span class="p">,</span> <span class="n">exc_val</span><span class="p">,</span> <span class="n">exc_tb</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">enabled</span><span class="p">:</span>
            <span class="k">return</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">synchronize</span><span class="p">()</span>
        <span class="n">_disable_profiler</span><span class="p">()</span>
        <span class="n">_run_on_profiler_stop</span><span class="p">()</span>
        <span class="k">return</span> <span class="kc">False</span></div>



<div class="viewcode-block" id="load_nvprof">
<a class="viewcode-back" href="../../../python-api/generated/torch.autograd.profiler.load_nvprof.html#torch.autograd.profiler.load_nvprof">[docs]</a>
<span class="k">def</span><span class="w"> </span><span class="nf">load_nvprof</span><span class="p">(</span><span class="n">path</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Open an nvprof trace file and parses autograd annotations.</span>

<span class="sd">    Args:</span>
<span class="sd">        path (str): path to nvprof trace</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">EventList</span><span class="p">(</span><span class="n">parse_nvprof_trace</span><span class="p">(</span><span class="n">path</span><span class="p">))</span></div>



<div class="viewcode-block" id="EnforceUnique">
<a class="viewcode-back" href="../../../python-api/generated/torch.autograd.profiler.EnforceUnique.html#torch.autograd.profiler.EnforceUnique">[docs]</a>
<span class="k">class</span><span class="w"> </span><span class="nc">EnforceUnique</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Raises an error if a key is seen more than once.&quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">seen</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>

<div class="viewcode-block" id="EnforceUnique.see">
<a class="viewcode-back" href="../../../python-api/generated/torch.autograd.profiler.EnforceUnique.html#torch.autograd.profiler.EnforceUnique.see">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">see</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">key</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Observe a key and raise an error if it is seen multiple times.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">key</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">seen</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;duplicate key: &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">key</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">seen</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">key</span><span class="p">)</span></div>
</div>



<div class="viewcode-block" id="parse_nvprof_trace">
<a class="viewcode-back" href="../../../python-api/generated/torch.autograd.profiler.parse_nvprof_trace.html#torch.autograd.profiler.parse_nvprof_trace">[docs]</a>
<span class="k">def</span><span class="w"> </span><span class="nf">parse_nvprof_trace</span><span class="p">(</span><span class="n">path</span><span class="p">):</span>
    <span class="kn">import</span><span class="w"> </span><span class="nn">sqlite3</span>

    <span class="n">conn</span> <span class="o">=</span> <span class="n">sqlite3</span><span class="o">.</span><span class="n">connect</span><span class="p">(</span><span class="n">path</span><span class="p">)</span>
    <span class="n">conn</span><span class="o">.</span><span class="n">row_factory</span> <span class="o">=</span> <span class="n">sqlite3</span><span class="o">.</span><span class="n">Row</span>

    <span class="c1"># Parse strings table</span>
    <span class="n">strings</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="k">for</span> <span class="n">r</span> <span class="ow">in</span> <span class="n">conn</span><span class="o">.</span><span class="n">execute</span><span class="p">(</span><span class="s2">&quot;SELECT _id_ as id, value FROM StringTable&quot;</span><span class="p">):</span>
        <span class="n">strings</span><span class="p">[</span><span class="n">r</span><span class="p">[</span><span class="s2">&quot;id&quot;</span><span class="p">]]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_demangle</span><span class="p">(</span><span class="n">r</span><span class="p">[</span><span class="s2">&quot;value&quot;</span><span class="p">])</span>

    <span class="c1"># First, find all functions and create FunctionEvents for them</span>
    <span class="n">marker_query</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot;</span>
<span class="s2">    SELECT</span>
<span class="s2">        start.id AS marker_id, start.name, start.timestamp AS start_time, end.timestamp AS end_time</span>
<span class="s2">    FROM</span>
<span class="s2">        CUPTI_ACTIVITY_KIND_MARKER AS start INNER JOIN CUPTI_ACTIVITY_KIND_MARKER AS end</span>
<span class="s2">        ON start.id = end.id</span>
<span class="s2">    WHERE</span>
<span class="s2">        start.name != 0 AND end.name = 0</span>
<span class="s2">    &quot;&quot;&quot;</span>
    <span class="n">functions</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">functions_map</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="n">unique</span> <span class="o">=</span> <span class="n">EnforceUnique</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">row</span> <span class="ow">in</span> <span class="n">conn</span><span class="o">.</span><span class="n">execute</span><span class="p">(</span><span class="n">marker_query</span><span class="p">):</span>
        <span class="n">unique</span><span class="o">.</span><span class="n">see</span><span class="p">(</span><span class="n">row</span><span class="p">[</span><span class="s2">&quot;marker_id&quot;</span><span class="p">])</span>
        <span class="n">evt</span> <span class="o">=</span> <span class="n">FunctionEvent</span><span class="p">(</span>
            <span class="nb">id</span><span class="o">=</span><span class="n">row</span><span class="p">[</span><span class="s2">&quot;marker_id&quot;</span><span class="p">],</span>
            <span class="n">node_id</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>  <span class="c1"># missing a node_id when calling FunctionEvent. This is just to ensure</span>
            <span class="c1"># that pytorch doesn&#39;t crash when creating a FunctionEvent() object</span>
            <span class="n">name</span><span class="o">=</span><span class="n">strings</span><span class="p">[</span><span class="n">row</span><span class="p">[</span><span class="s2">&quot;name&quot;</span><span class="p">]],</span>
            <span class="n">start_us</span><span class="o">=</span><span class="n">row</span><span class="p">[</span><span class="s2">&quot;start_time&quot;</span><span class="p">],</span>
            <span class="n">end_us</span><span class="o">=</span><span class="n">row</span><span class="p">[</span><span class="s2">&quot;end_time&quot;</span><span class="p">],</span>
            <span class="n">thread</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
        <span class="p">)</span>  <span class="c1"># TODO: find in sqlite database</span>
        <span class="n">functions</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">evt</span><span class="p">)</span>
        <span class="n">functions_map</span><span class="p">[</span><span class="n">evt</span><span class="o">.</span><span class="n">id</span><span class="p">]</span> <span class="o">=</span> <span class="n">evt</span>

    <span class="c1"># Now, correlate all kernels with FunctionEvents</span>
    <span class="n">kernel_query</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot;</span>
<span class="s2">    SELECT</span>
<span class="s2">        start.id AS marker_id, start.name, start.timestamp, end.timestamp,</span>
<span class="s2">        runtime._id_ AS runtime_id, runtime.cbid, runtime.start AS runtime_start, runtime.end AS runtime_end,</span>
<span class="s2">        kernel.start AS kernel_start, kernel.end AS kernel_end, kernel.name AS kernel_name</span>
<span class="s2">    FROM</span>
<span class="s2">        CUPTI_ACTIVITY_KIND_MARKER AS start</span>
<span class="s2">        INNER JOIN CUPTI_ACTIVITY_KIND_MARKER AS end</span>
<span class="s2">            ON start.id = end.id</span>
<span class="s2">        INNER JOIN CUPTI_ACTIVITY_KIND_RUNTIME as runtime</span>
<span class="s2">            ON (start.timestamp &lt; runtime.start AND runtime.end &lt; end.timestamp)</span>
<span class="s2">        INNER JOIN CUPTI_ACTIVITY_KIND_CONCURRENT_KERNEL AS kernel</span>
<span class="s2">            ON kernel.correlationId = runtime.correlationId</span>
<span class="s2">    &quot;&quot;&quot;</span>
    <span class="n">unique</span> <span class="o">=</span> <span class="n">EnforceUnique</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">row</span> <span class="ow">in</span> <span class="n">conn</span><span class="o">.</span><span class="n">execute</span><span class="p">(</span><span class="n">kernel_query</span><span class="p">):</span>
        <span class="n">unique</span><span class="o">.</span><span class="n">see</span><span class="p">(</span><span class="n">row</span><span class="p">[</span><span class="s2">&quot;marker_id&quot;</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s2">&quot;runtime_id&quot;</span><span class="p">])</span>
        <span class="c1"># 211 is cudaKernelLaunch for cuda &gt;= 9.2</span>
        <span class="k">assert</span> <span class="n">row</span><span class="p">[</span><span class="s2">&quot;cbid&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="mi">211</span>
        <span class="n">evt</span> <span class="o">=</span> <span class="n">functions_map</span><span class="p">[</span><span class="n">row</span><span class="p">[</span><span class="s2">&quot;marker_id&quot;</span><span class="p">]]</span>
        <span class="n">evt</span><span class="o">.</span><span class="n">append_kernel</span><span class="p">(</span>
            <span class="n">row</span><span class="p">[</span><span class="s2">&quot;kernel_name&quot;</span><span class="p">],</span> <span class="mi">0</span><span class="p">,</span> <span class="n">row</span><span class="p">[</span><span class="s2">&quot;kernel_end&quot;</span><span class="p">]</span> <span class="o">-</span> <span class="n">row</span><span class="p">[</span><span class="s2">&quot;kernel_start&quot;</span><span class="p">]</span>
        <span class="p">)</span>

    <span class="n">functions</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">evt</span><span class="p">:</span> <span class="n">evt</span><span class="o">.</span><span class="n">time_range</span><span class="o">.</span><span class="n">start</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">functions</span></div>



<div class="viewcode-block" id="KinetoStepTracker">
<a class="viewcode-back" href="../../../python-api/generated/torch.autograd.profiler.KinetoStepTracker.html#torch.autograd.profiler.KinetoStepTracker">[docs]</a>
<span class="k">class</span><span class="w"> </span><span class="nc">KinetoStepTracker</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Provides an abstraction for incrementing the step count globally.</span>

<span class="sd">    Previously, we only had one place to mark that a step() has occurred</span>
<span class="sd">    in the program via pytorch profiler step(). We will now add step hooks</span>
<span class="sd">    in the Optimizer class https://github.com/pytorch/pytorch/issues/88446</span>

<span class="sd">    - This could mean programs that already call profiler.step() every</span>
<span class="sd">      iteration can end up double incrementing step count.</span>
<span class="sd">    - If a model uses multiple optimizers we can also have double or more</span>
<span class="sd">      counting of the step.</span>

<span class="sd">    We fix this by adding a layer of abstraction before calling step()</span>
<span class="sd">    to the kineto library. The idea is to maintain steps per requester in a dict:</span>

<span class="sd">    .. code-block::</span>

<span class="sd">        {</span>
<span class="sd">           &quot;ProfilerStep&quot;: 100,  # triggered by profiler step() call</span>
<span class="sd">           &quot;Optimizer1Step&quot;: 100,   # Optimizer 1 or 2 are just examples, could be SGD, Adam etc</span>
<span class="sd">           &quot;Optimizer2Step&quot;: 100,</span>
<span class="sd">        }</span>

<span class="sd">    To figure out the global step count just take the max of dict values (100).</span>

<span class="sd">    If one of the count increments the max will go up.</span>

<span class="sd">    .. code-block::</span>

<span class="sd">        {</span>
<span class="sd">           &quot;ProfilerStep&quot;: 100,</span>
<span class="sd">           &quot;Optimizer1Step&quot;: 101,   # Optimizer1 got incremented first say</span>
<span class="sd">           &quot;Optimizer2Step&quot;: 100,</span>
<span class="sd">        }</span>

<span class="sd">    Then global step count is 101</span>
<span class="sd">    We only call the kineto step() function when global count increments.</span>

<span class="sd">    NOTE: Please do not use the KinetoStepTracker in modules beside the Optimizer</span>
<span class="sd">    for now. The result could be incorrect increments of the step count.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">_current_step</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">_step_dict</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="n">defaultdict</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>

<div class="viewcode-block" id="KinetoStepTracker.init_step_count">
<a class="viewcode-back" href="../../../python-api/generated/torch.autograd.profiler.KinetoStepTracker.html#torch.autograd.profiler.KinetoStepTracker.init_step_count">[docs]</a>
    <span class="nd">@classmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">init_step_count</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">requester</span><span class="p">:</span> <span class="nb">str</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Initialize for a given requester.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">cls</span><span class="o">.</span><span class="n">_step_dict</span><span class="p">[</span><span class="n">requester</span><span class="p">]</span> <span class="o">=</span> <span class="bp">cls</span><span class="o">.</span><span class="n">_current_step</span></div>


<div class="viewcode-block" id="KinetoStepTracker.erase_step_count">
<a class="viewcode-back" href="../../../python-api/generated/torch.autograd.profiler.KinetoStepTracker.html#torch.autograd.profiler.KinetoStepTracker.erase_step_count">[docs]</a>
    <span class="nd">@classmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">erase_step_count</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">requester</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Remove a given requester.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">cls</span><span class="o">.</span><span class="n">_step_dict</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="n">requester</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span></div>


<div class="viewcode-block" id="KinetoStepTracker.increment_step">
<a class="viewcode-back" href="../../../python-api/generated/torch.autograd.profiler.KinetoStepTracker.html#torch.autograd.profiler.KinetoStepTracker.increment_step">[docs]</a>
    <span class="nd">@classmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">increment_step</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">requester</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Increments the step count for the requester.</span>

<span class="sd">        Additionally if the max over all step counts has incremented then</span>
<span class="sd">        trigger the _kineto_step() returns global step count</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">requester</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">cls</span><span class="o">.</span><span class="n">_step_dict</span><span class="p">:</span>
            <span class="bp">cls</span><span class="o">.</span><span class="n">init_step_count</span><span class="p">(</span><span class="n">requester</span><span class="p">)</span>
        <span class="bp">cls</span><span class="o">.</span><span class="n">_step_dict</span><span class="p">[</span><span class="n">requester</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>

        <span class="n">new_step</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="bp">cls</span><span class="o">.</span><span class="n">_step_dict</span><span class="o">.</span><span class="n">values</span><span class="p">())</span>
        <span class="k">if</span> <span class="n">new_step</span> <span class="o">&gt;</span> <span class="bp">cls</span><span class="o">.</span><span class="n">_current_step</span><span class="p">:</span>
            <span class="n">delta</span> <span class="o">=</span> <span class="n">new_step</span> <span class="o">-</span> <span class="bp">cls</span><span class="o">.</span><span class="n">_current_step</span>
            <span class="k">if</span> <span class="n">delta</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
                <span class="n">warn</span><span class="p">(</span>
                    <span class="s2">&quot;Profiler step count has increased more than 1 - &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;current_step = </span><span class="si">{</span><span class="bp">cls</span><span class="o">.</span><span class="n">_current_step</span><span class="si">}</span><span class="s2"> step dict =  </span><span class="si">{</span><span class="bp">cls</span><span class="o">.</span><span class="n">_step_dict</span><span class="si">}</span><span class="s2">&quot;</span>
                <span class="p">)</span>
            <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">delta</span><span class="p">):</span>
                <span class="n">_kineto_step</span><span class="p">()</span>
            <span class="bp">cls</span><span class="o">.</span><span class="n">_current_step</span> <span class="o">=</span> <span class="n">new_step</span>
        <span class="k">return</span> <span class="bp">cls</span><span class="o">.</span><span class="n">_current_step</span></div>


<div class="viewcode-block" id="KinetoStepTracker.current_step">
<a class="viewcode-back" href="../../../python-api/generated/torch.autograd.profiler.KinetoStepTracker.html#torch.autograd.profiler.KinetoStepTracker.current_step">[docs]</a>
    <span class="nd">@classmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">current_step</span><span class="p">(</span><span class="bp">cls</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Get the latest step for any requester</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">cls</span><span class="o">.</span><span class="n">_current_step</span></div>
</div>

</pre></div>

                </article>
              
              
              
                <footer class="bd-footer-article">
                  <div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item">
<div class="feedback">
  
<div class="rating">
    Rate this Page
    <div class="stars">
        
        <span class="star" data-value="1">★</span>
        
        <span class="star" data-value="2">★</span>
        
        <span class="star" data-value="3">★</span>
        
        <span class="star" data-value="4">★</span>
        
        <span class="star" data-value="5">★</span>
        
    </div>
</div>

  <div class="feedback-send">
    <button class="feedback-btn" onclick="openGitHubIssue()">Send Feedback</button>
  </div>
</div>
</div>
  
</div>
                </footer>
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
</div>
                </footer>
              
            </div>
            
            
              
            
          </div>
          <footer class="bd-footer-content">
            
          </footer>
        
      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script defer src="../../../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf"></script>
<script defer src="../../../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf"></script>


  <footer class="bd-footer">
    
    <div class="bd-footer__inner bd-page-width">
      
        <div class="footer-items__start">
          
            <div class="footer-item">

  <p class="copyright">
    
      © Copyright PyTorch Contributors.
      <br/>
    
  </p>
</div>
          
            <div class="footer-item">

  <p class="sphinx-version">
    Created using <a href="https://www.sphinx-doc.org/">Sphinx</a> 8.1.0.
    <br/>
  </p>
</div>
          
        </div>
      
    
    
      <div class="footer-items__end">
        
          <div class="footer-item">
<p class="theme-version">
  <!-- # L10n: Setting the PST URL as an argument as this does not need to be localized -->
  Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.16.1.
</p></div>
        
      </div>
    
   </div>
   

   <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
      <div class="container">
        <div class="row">
          <div class="col-md-4 text-center">
            <h2>Docs</h2>
            <p>Access comprehensive developer documentation for PyTorch</p>
            <a class="with-right-arrow" href="">View Docs</a>
          </div>

          <div class="col-md-4 text-center">
            <h2>Tutorials</h2>
            <p>Get in-depth tutorials for beginners and advanced developers</p>
            <a class="with-right-arrow" href="">View Tutorials</a>
          </div>

          <div class="col-md-4 text-center">
            <h2>Resources</h2>
            <p>Find development resources and get your questions answered</p>
            <a class="with-right-arrow" href="">View Resources</a>
          </div>
        </div>
      </div>
  </div>

  <footer class="site-footer">
      <div class="container footer-container">
        <div class="footer-logo-wrapper">
          <a href="" class="footer-logo"></a>
        </div>

        <div class="footer-links-wrapper">
          <div class="footer-links-col">
            <ul>
              <li class="list-title"><a href="">PyTorch</a></li>
              <li><a href="">Get Started</a></li>
              <li><a href="">Features</a></li>
              <li><a href="">Ecosystem</a></li>
              <li><a href="">Blog</a></li>
              <li><a href="">Contributing</a></li>
            </ul>
          </div>

          <div class="footer-links-col">
            <ul>
              <li class="list-title"><a href="">Resources</a></li>
              <li><a href="">Tutorials</a></li>
              <li><a href="">Docs</a></li>
              <li><a href="" target="_blank">Discuss</a></li>
              <li><a href="" target="_blank">Github Issues</a></li>
              <li><a href="" target="_blank">Brand Guidelines</a></li>
            </ul>
          </div>

          <div class="footer-links-col">
            <ul>
              <li class="list-title">Stay up to date</li>
              <li><a href="" target="_blank">Facebook</a></li>
              <li><a href="" target="_blank">Twitter</a></li>
              <li><a href="" target="_blank">YouTube</a></li>
              <li><a href="" target="_blank">LinkedIn</a></li>
            </ul>
            </div>

          <div class="footer-links-col">
            <ul>
              <li class="list-title">PyTorch Podcasts</li>
              <li><a href="" target="_blank">Spotify</a></li>
              <li><a href="" target="_blank">Apple</a></li>
              <li><a href="" target="_blank">Google</a></li>
              <li><a href="" target="_blank">Amazon</a></li>
            </ul>
           </div>
          </div>

          <div class="privacy-policy">
            <ul>
              <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/terms/" target="_blank">Terms</a></li>
              <li class="privacy-policy-links">|</li>
              <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/privacy-policy/" target="_blank">Privacy</a></li>
            </ul>
          </div>
          <div class="copyright">
          <p>© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation.
            For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see
            <a href="https://www.linuxfoundation.org/policies/">www.linuxfoundation.org/policies/</a>. The PyTorch Foundation supports the PyTorch open source
            project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC,
            please see <a href="https://www.lfprojects.org/policies/">www.lfprojects.org/policies/</a>.</p>
        </div>
       </div>

  <div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="../../../_static/img/pytorch-x.svg">
  </div>
</div>
  </footer>


  </body>
</html>