
<!DOCTYPE html>


<html lang="en" data-content_root="../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>torch.cuda &#8212; PyTorch main documentation</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  <!--
    this give us a css class that will be invisible only if js is disabled
  -->
  <noscript>
    <style>
      .pst-js-only { display: none !important; }

    </style>
  </noscript>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css?v=949a1ff5" />
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/katex.min.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/katex-math.css?v=91adb8b6" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="../../_static/css/jit.css?v=8de1ea5d" />
    <link rel="stylesheet" type="text/css" href="../../_static/css/custom.css?v=3ccf5357" />
    <link rel="stylesheet" type="text/css" href="../../_static/css/custom2.css?v=189c4a6a" />
  
  <!-- So that users can add custom icons -->
  <script src="../../_static/scripts/fontawesome.js?digest=8878045cc6db502f8baf"></script>
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf" />

    <script src="../../_static/documentation_options.js?v=53c08c8d"></script>
    <script src="../../_static/doctools.js?v=9bcbadda"></script>
    <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../_static/copybutton.js?v=940804e7"></script>
    <script src="../../_static/design-tabs.js?v=f930bc37"></script>
    <script type="module" src="https://cdn.jsdelivr.net/npm/mermaid@11.2.0/dist/mermaid.esm.min.mjs"></script>
    <script type="module" src="https://cdn.jsdelivr.net/npm/@mermaid-js/layout-elk@0.1.4/dist/mermaid-layout-elk.esm.min.mjs"></script>
    <script type="module">import mermaid from "https://cdn.jsdelivr.net/npm/mermaid@11.2.0/dist/mermaid.esm.min.mjs";import elkLayouts from "https://cdn.jsdelivr.net/npm/@mermaid-js/layout-elk@0.1.4/dist/mermaid-layout-elk.esm.min.mjs";mermaid.registerLayoutLoaders(elkLayouts);mermaid.initialize({startOnLoad:false});</script>
    <script src="https://cdn.jsdelivr.net/npm/d3@7.9.0/dist/d3.min.js"></script>
    <script type="module">
import mermaid from "https://cdn.jsdelivr.net/npm/mermaid@11.2.0/dist/mermaid.esm.min.mjs";
window.addEventListener("load", () => mermaid.run());
</script>
    <script>DOCUMENTATION_OPTIONS.pagename = '_modules/torch/cuda';</script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  <meta name="docsearch:version" content="main (2.7.0a0+git74cfb4f )" />
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>

          <li class="main-menu-item">
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Learn
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="">
                  <span class=dropdown-title>Get Started</span>
                  <p>Run PyTorch locally or get started quickly with one of the supported cloud platforms</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Tutorials</span>
                  <p>Whats new in PyTorch tutorials</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Learn the Basics</span>
                  <p>Familiarize yourself with PyTorch concepts and modules</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">PyTorch Recipes</span>
                  <p>Bite-size, ready-to-deploy PyTorch code examples</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Intro to PyTorch - YouTube Series</span>
                  <p>Master PyTorch basics with our engaging YouTube tutorial series</p>
                </a>
              </div>
            </div>
          </li>

          <li>
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Ecosystem
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Tools</span>
                  <p>Learn about the tools and frameworks in the PyTorch Ecosystem</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class=dropdown-title>Community</span>
                  <p>Join the PyTorch developer community to contribute, learn, and get your questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="" target="_blank">
                  <span class=dropdown-title>Forums</span>
                  <p>A place to discuss PyTorch code, issues, install, research</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class=dropdown-title>Developer Resources</span>
                  <p>Find resources and get questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Contributor Awards - 2024</span>
                  <p>Award winners announced at this year's PyTorch Conference</p>
                </a>
              </div>
            </div>
          </li>

          <li>
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Edge
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">About PyTorch Edge</span>
                  <p>Build innovative and privacy-aware AI experiences for edge devices</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">ExecuTorch</span>
                  <p>End-to-end solution for enabling on-device inference capabilities across mobile and edge devices</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">ExecuTorch Docs</span>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Docs
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">PyTorch</span>
                  <p>Explore the documentation for comprehensive guidance on how to use PyTorch</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">PyTorch Domains</span>
                  <p>Read the PyTorch Domains documentation to learn more about domain-specific libraries</p>
                </a>
              </div>
            </div>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Blogs & News
              lnhetrlnle</a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">PyTorch Blog</span>
                  <p>Catch up on the latest technical news and happenings</p>
                </a>
                 <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Community Blog</span>
                  <p>Stories from the PyTorch ecosystem</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Videos</span>
                  <p>Learn about the latest PyTorch tutorials, new, and more </p>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Community Stories</span>
                  <p>Learn how our community solves real, everyday machine learning problems with PyTorch</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Events</span>
                  <p>Find events, webinars, and podcasts</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Newsletter</span>
                  <p>Stay up-to-date with the latest updates</p>
                </a>
            </div>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                About
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">PyTorch Foundation</span>
                  <p>Learn more about the PyTorch Foundation</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Governing Board</span>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Cloud Credit Program</span>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Technical Advisory Council</span>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Staff</span>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Contact Us</span>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
            <div class="no-dropdown">
              <a href="" data-cta="join">
                Become a Member
              </a>
            </div>
          </li>
          <li>
           <div class="main-menu-item">
             <a href="" class="github-icon">
             </a>
           </div>
          </li>
          <!--- TODO: This block adds the search icon to the nav bar. We will enable it later.
          <li>
            <div class="main-menu-item">
             <a href="" class="search-icon">
             </a>
            </div>
          </li>
          --->
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>
  </div>
</div>


  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <dialog id="pst-search-dialog">
    
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </dialog>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
<div class="bd-header__inner bd-page-width">
  <button class="pst-navbar-icon sidebar-toggle primary-toggle" aria-label="Site navigation">
    <span class="fa-solid fa-bars"></span>
  </button>
  
  
    <div class="navbar-header-items__start" style="display: flex; align-items: center; justify-content: flex-start;">
    <div class="navbar-item">
      <a class="nav-link nav-internal" href="/index.html">Home</a></div>
    
      <div class="navbar-item">

  
    
  

<a class="navbar-brand logo" href="../../index.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../../_static/pytorch-logo-dark-unstable.png" class="logo__image only-light" alt="PyTorch main documentation - Home"/>
    <img src="../../_static/pytorch-logo-dark-unstable.png" class="logo__image only-dark pst-js-only" alt="PyTorch main documentation - Home"/>
  
  
</a></div>
    
  </div>
  
  <div class="col-lg-9 navbar-header-items">
    
    <div class="me-auto navbar-header-items__center">
      
        <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../python-api/index.html">
    Python API
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../notes/index.html">
    Developer Notes
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../community/index.html">
    Community
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../lang-bindings/index.html">
    Language Bindings
  </a>
</li>

  </ul>
</nav></div>
      
    </div>
    
    
    <div class="navbar-header-items__end">
      
        <div class="navbar-item navbar-persistent--container">
          

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button>
        </div>
      
      
        <div class="navbar-item">

<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button></div>
      
    </div>
    
  </div>
  
  
    <div class="navbar-persistent--mobile">

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button>
    </div>
  

  
</div>

    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
        
      
      <dialog id="pst-primary-sidebar-modal"></dialog>
      <div id="pst-primary-sidebar" class="bd-sidebar-primary bd-sidebar hide-on-wide">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
      <div class="sidebar-header-items__center">
        
          
          
            <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../python-api/index.html">
    Python API
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../notes/index.html">
    Developer Notes
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../community/index.html">
    Community
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../lang-bindings/index.html">
    Language Bindings
  </a>
</li>

  </ul>
</nav></div>
          
        
      </div>
    
    
    
      <div class="sidebar-header-items__end">
        
          <div class="navbar-item">

<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button></div>
        
      </div>
    
  </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
      <div class="sidebar-primary-item">
<div id="ethical-ad-placement"
      class="flat"
      data-ea-publisher="readthedocs"
      data-ea-type="readthedocs-sidebar"
      data-ea-manual="true">
</div></div>
  </div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        
          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item">

<nav aria-label="Breadcrumb" class="d-print-none">
  <ul class="bd-breadcrumbs">
    
    <li class="breadcrumb-item breadcrumb-home">
      <a href="../../index.html" class="nav-link" aria-label="Home">
        <i class="fa-solid fa-home"></i>
      </a>
    </li>
    
    <li class="breadcrumb-item"><a href="../index.html" class="nav-link">Module code</a></li>
    
    
    <li class="breadcrumb-item"><a href="../torch.html" class="nav-link">torch</a></li>
    
    <li class="breadcrumb-item active" aria-current="page"><span class="ellipsis">torch.cuda</span></li>
  </ul>
</nav>
</div>
      
    </div>
  
  
</div>
</div>
              
              
              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <h1>Source code for torch.cuda</h1><div class="highlight"><pre>
<span></span><span class="c1"># mypy: allow-untyped-defs</span>
<span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">This package adds support for CUDA tensor types.</span>

<span class="sd">It implements the same function as CPU tensors, but they utilize</span>
<span class="sd">GPUs for computation.</span>

<span class="sd">It is lazily initialized, so you can always import it, and use</span>
<span class="sd">:func:`is_available()` to determine if your system supports CUDA.</span>

<span class="sd">:ref:`cuda-semantics` has more details about working with CUDA.</span>
<span class="sd">&quot;&quot;&quot;</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">importlib</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">os</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">threading</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">traceback</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">warnings</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">functools</span><span class="w"> </span><span class="kn">import</span> <span class="n">lru_cache</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">typing</span><span class="w"> </span><span class="kn">import</span> <span class="n">Any</span><span class="p">,</span> <span class="n">Callable</span><span class="p">,</span> <span class="n">cast</span><span class="p">,</span> <span class="n">List</span><span class="p">,</span> <span class="n">Optional</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">,</span> <span class="n">Union</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch._C</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch</span><span class="w"> </span><span class="kn">import</span> <span class="n">device</span> <span class="k">as</span> <span class="n">_device</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch._utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">_dummy_type</span><span class="p">,</span> <span class="n">_LazySeedTracker</span><span class="p">,</span> <span class="n">classproperty</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch.types</span><span class="w"> </span><span class="kn">import</span> <span class="n">Device</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">.</span><span class="w"> </span><span class="kn">import</span> <span class="n">gds</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">._utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">_get_device_index</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">.graphs</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">CUDAGraph</span><span class="p">,</span>
    <span class="n">graph</span><span class="p">,</span>
    <span class="n">graph_pool_handle</span><span class="p">,</span>
    <span class="n">is_current_stream_capturing</span><span class="p">,</span>
    <span class="n">make_graphed_callables</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">.streams</span><span class="w"> </span><span class="kn">import</span> <span class="n">Event</span><span class="p">,</span> <span class="n">ExternalStream</span><span class="p">,</span> <span class="n">Stream</span>


<span class="k">try</span><span class="p">:</span>
    <span class="kn">from</span><span class="w"> </span><span class="nn">torch._C</span><span class="w"> </span><span class="kn">import</span> <span class="n">_cudart</span>  <span class="c1"># type: ignore[attr-defined]</span>
<span class="k">except</span> <span class="ne">ImportError</span><span class="p">:</span>
    <span class="n">_cudart</span> <span class="o">=</span> <span class="kc">None</span>

<span class="n">_initialized</span> <span class="o">=</span> <span class="kc">False</span>
<span class="n">_tls</span> <span class="o">=</span> <span class="n">threading</span><span class="o">.</span><span class="n">local</span><span class="p">()</span>
<span class="n">_initialization_lock</span> <span class="o">=</span> <span class="n">threading</span><span class="o">.</span><span class="n">Lock</span><span class="p">()</span>
<span class="n">_queued_calls</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span>
    <span class="nb">tuple</span><span class="p">[</span><span class="n">Callable</span><span class="p">[[],</span> <span class="kc">None</span><span class="p">],</span> <span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span>
<span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>  <span class="c1"># don&#39;t invoke these until initialization occurs</span>
<span class="n">_is_in_bad_fork</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="p">,</span> <span class="s2">&quot;_cuda_isInBadFork&quot;</span><span class="p">,</span> <span class="k">lambda</span><span class="p">:</span> <span class="kc">False</span><span class="p">)</span>
<span class="n">_device_t</span> <span class="o">=</span> <span class="n">Union</span><span class="p">[</span><span class="n">_device</span><span class="p">,</span> <span class="nb">str</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="kc">None</span><span class="p">]</span>

<span class="n">_HAS_PYNVML</span> <span class="o">=</span> <span class="kc">False</span>
<span class="n">_PYNVML_ERR</span> <span class="o">=</span> <span class="kc">None</span>
<span class="k">try</span><span class="p">:</span>
    <span class="kn">from</span><span class="w"> </span><span class="nn">torch</span><span class="w"> </span><span class="kn">import</span> <span class="n">version</span> <span class="k">as</span> <span class="n">_version</span>

    <span class="k">try</span><span class="p">:</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">_version</span><span class="o">.</span><span class="n">hip</span><span class="p">:</span>
            <span class="kn">import</span><span class="w"> </span><span class="nn">pynvml</span>  <span class="c1"># type: ignore[import]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="kn">import</span><span class="w"> </span><span class="nn">amdsmi</span>  <span class="c1"># type: ignore[import]</span>

        <span class="n">_HAS_PYNVML</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="k">except</span> <span class="ne">ModuleNotFoundError</span><span class="p">:</span>
        <span class="k">pass</span>
    <span class="k">finally</span><span class="p">:</span>
        <span class="k">del</span> <span class="n">_version</span>
<span class="k">except</span> <span class="ne">ImportError</span> <span class="k">as</span> <span class="n">err</span><span class="p">:</span>
    <span class="n">_PYNVML_ERR</span> <span class="o">=</span> <span class="n">err</span>  <span class="c1"># sometimes a lib is installed but the import fails for some other reason, so we log the error for later</span>

<span class="n">_lazy_seed_tracker</span> <span class="o">=</span> <span class="n">_LazySeedTracker</span><span class="p">()</span>

<span class="c1"># Define dummy _CudaDeviceProperties type if PyTorch was compiled without CUDA</span>
<span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="p">,</span> <span class="s2">&quot;_CudaDeviceProperties&quot;</span><span class="p">):</span>
    <span class="n">_CudaDeviceProperties</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_CudaDeviceProperties</span>
<span class="k">else</span><span class="p">:</span>
    <span class="n">_CudaDeviceProperties</span> <span class="o">=</span> <span class="n">_dummy_type</span><span class="p">(</span><span class="s2">&quot;_CudaDeviceProperties&quot;</span><span class="p">)</span>  <span class="c1"># type: ignore[assignment, misc]</span>

<span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="p">,</span> <span class="s2">&quot;_cuda_exchangeDevice&quot;</span><span class="p">):</span>
    <span class="n">_exchange_device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_cuda_exchangeDevice</span>
<span class="k">else</span><span class="p">:</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_exchange_device</span><span class="p">(</span><span class="n">device</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">device</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">return</span> <span class="o">-</span><span class="mi">1</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;PyTorch was compiled without CUDA support&quot;</span><span class="p">)</span>


<span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="p">,</span> <span class="s2">&quot;_cuda_maybeExchangeDevice&quot;</span><span class="p">):</span>
    <span class="n">_maybe_exchange_device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_cuda_maybeExchangeDevice</span>
<span class="k">else</span><span class="p">:</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_maybe_exchange_device</span><span class="p">(</span><span class="n">device</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">device</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">return</span> <span class="o">-</span><span class="mi">1</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;PyTorch was compiled without CUDA support&quot;</span><span class="p">)</span>


<span class="n">has_half</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span>
<span class="n">has_magma</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_has_magma</span>

<span class="n">default_generators</span><span class="p">:</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">Generator</span><span class="p">]</span> <span class="o">=</span> <span class="p">()</span>  <span class="c1"># type: ignore[assignment]</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_is_compiled</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Return true if compile with CUDA support.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="p">,</span> <span class="s2">&quot;_cuda_getDeviceCount&quot;</span><span class="p">)</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_nvml_based_avail</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s2">&quot;PYTORCH_NVML_BASED_CUDA_CHECK&quot;</span><span class="p">)</span> <span class="o">==</span> <span class="s2">&quot;1&quot;</span>


<div class="viewcode-block" id="is_available">
<a class="viewcode-back" href="../../python-api/torch.cuda.is_available.html#torch.cuda.is_available">[docs]</a>
<span class="k">def</span><span class="w"> </span><span class="nf">is_available</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Return a bool indicating if CUDA is currently available.&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">_is_compiled</span><span class="p">():</span>
        <span class="k">return</span> <span class="kc">False</span>
    <span class="k">if</span> <span class="n">_nvml_based_avail</span><span class="p">():</span>
        <span class="c1"># The user has set an env variable to request this availability check that attempts to avoid fork poisoning by</span>
        <span class="c1"># using NVML at the cost of a weaker CUDA availability assessment. Note that if NVML discovery/initialization</span>
        <span class="c1"># fails, this assessment falls back to the default CUDA Runtime API assessment (`cudaGetDeviceCount`)</span>
        <span class="k">return</span> <span class="n">device_count</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">0</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># The default availability inspection never throws and returns 0 if the driver is missing or can&#39;t</span>
        <span class="c1"># be initialized. This uses the CUDA Runtime API `cudaGetDeviceCount` which in turn initializes the CUDA Driver</span>
        <span class="c1"># API via `cuInit`</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_cuda_getDeviceCount</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">0</span></div>



<span class="k">def</span><span class="w"> </span><span class="nf">is_bf16_supported</span><span class="p">(</span><span class="n">including_emulation</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Return a bool indicating if the current CUDA/ROCm device supports dtype bfloat16.&quot;&quot;&quot;</span>
    <span class="c1"># Check for ROCm, if true return true, no ROCM_VERSION check required,</span>
    <span class="c1"># since it is supported on AMD GPU archs.</span>
    <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">version</span><span class="o">.</span><span class="n">hip</span><span class="p">:</span>
        <span class="k">return</span> <span class="kc">True</span>

    <span class="c1"># If CUDA is not available, than it does not support bf16 either</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">is_available</span><span class="p">():</span>
        <span class="k">return</span> <span class="kc">False</span>

    <span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">current_device</span><span class="p">()</span>

    <span class="c1"># Check for CUDA version and device compute capability.</span>
    <span class="c1"># This is a fast way to check for it.</span>
    <span class="n">cuda_version</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">version</span><span class="o">.</span><span class="n">cuda</span>
    <span class="k">if</span> <span class="p">(</span>
        <span class="n">cuda_version</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="ow">and</span> <span class="nb">int</span><span class="p">(</span><span class="n">cuda_version</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;.&quot;</span><span class="p">)[</span><span class="mi">0</span><span class="p">])</span> <span class="o">&gt;=</span> <span class="mi">11</span>
        <span class="ow">and</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">get_device_properties</span><span class="p">(</span><span class="n">device</span><span class="p">)</span><span class="o">.</span><span class="n">major</span> <span class="o">&gt;=</span> <span class="mi">8</span>
    <span class="p">):</span>
        <span class="k">return</span> <span class="kc">True</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">including_emulation</span><span class="p">:</span>
        <span class="k">return</span> <span class="kc">False</span>

    <span class="c1"># Finally try to create a bfloat16 device.</span>
    <span class="k">return</span> <span class="n">_check_bf16_tensor_supported</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>


<span class="nd">@lru_cache</span><span class="p">(</span><span class="n">maxsize</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
<span class="k">def</span><span class="w"> </span><span class="nf">_check_bf16_tensor_supported</span><span class="p">(</span><span class="n">device</span><span class="p">:</span> <span class="n">_device_t</span><span class="p">):</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.0</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
        <span class="k">return</span> <span class="kc">True</span>
    <span class="k">except</span> <span class="ne">Exception</span><span class="p">:</span>
        <span class="k">return</span> <span class="kc">False</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_sleep</span><span class="p">(</span><span class="n">cycles</span><span class="p">):</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_cuda_sleep</span><span class="p">(</span><span class="n">cycles</span><span class="p">)</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_extract_arch_version</span><span class="p">(</span><span class="n">arch_string</span><span class="p">:</span> <span class="nb">str</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Extracts the architecture string from a CUDA version&quot;&quot;&quot;</span>
    <span class="n">base</span> <span class="o">=</span> <span class="n">arch_string</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;_&quot;</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">base</span><span class="o">.</span><span class="n">endswith</span><span class="p">(</span><span class="s2">&quot;a&quot;</span><span class="p">):</span>
        <span class="n">base</span> <span class="o">=</span> <span class="n">base</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="k">return</span> <span class="nb">int</span><span class="p">(</span><span class="n">base</span><span class="p">)</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_check_capability</span><span class="p">():</span>
    <span class="n">incorrect_binary_warn</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot;</span>
<span class="s2">    Found GPU</span><span class="si">%d</span><span class="s2"> </span><span class="si">%s</span><span class="s2"> which requires CUDA_VERSION &gt;= </span><span class="si">%d</span><span class="s2"> to</span>
<span class="s2">     work properly, but your PyTorch was compiled</span>
<span class="s2">     with CUDA_VERSION </span><span class="si">%d</span><span class="s2">. Please install the correct PyTorch binary</span>
<span class="s2">     using instructions from https://pytorch.org</span>
<span class="s2">    &quot;&quot;&quot;</span>  <span class="c1"># noqa: F841</span>

    <span class="n">old_gpu_warn</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot;</span>
<span class="s2">    Found GPU</span><span class="si">%d</span><span class="s2"> </span><span class="si">%s</span><span class="s2"> which is of cuda capability </span><span class="si">%d</span><span class="s2">.</span><span class="si">%d</span><span class="s2">.</span>
<span class="s2">    PyTorch no longer supports this GPU because it is too old.</span>
<span class="s2">    The minimum cuda capability supported by this library is </span><span class="si">%d</span><span class="s2">.</span><span class="si">%d</span><span class="s2">.</span>
<span class="s2">    &quot;&quot;&quot;</span>

    <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">version</span><span class="o">.</span><span class="n">cuda</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>  <span class="c1"># on ROCm we don&#39;t want this check</span>
        <span class="n">CUDA_VERSION</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_cuda_getCompiledVersion</span><span class="p">()</span>  <span class="c1"># noqa: F841</span>
        <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">device_count</span><span class="p">()):</span>
            <span class="n">capability</span> <span class="o">=</span> <span class="n">get_device_capability</span><span class="p">(</span><span class="n">d</span><span class="p">)</span>
            <span class="n">major</span> <span class="o">=</span> <span class="n">capability</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">minor</span> <span class="o">=</span> <span class="n">capability</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
            <span class="n">name</span> <span class="o">=</span> <span class="n">get_device_name</span><span class="p">(</span><span class="n">d</span><span class="p">)</span>
            <span class="n">current_arch</span> <span class="o">=</span> <span class="n">major</span> <span class="o">*</span> <span class="mi">10</span> <span class="o">+</span> <span class="n">minor</span>
            <span class="n">min_arch</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span>
                <span class="p">(</span><span class="n">_extract_arch_version</span><span class="p">(</span><span class="n">arch</span><span class="p">)</span> <span class="k">for</span> <span class="n">arch</span> <span class="ow">in</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">get_arch_list</span><span class="p">()),</span>
                <span class="n">default</span><span class="o">=</span><span class="mi">35</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="k">if</span> <span class="n">current_arch</span> <span class="o">&lt;</span> <span class="n">min_arch</span><span class="p">:</span>
                <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
                    <span class="n">old_gpu_warn</span>
                    <span class="o">%</span> <span class="p">(</span><span class="n">d</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">major</span><span class="p">,</span> <span class="n">minor</span><span class="p">,</span> <span class="n">min_arch</span> <span class="o">//</span> <span class="mi">10</span><span class="p">,</span> <span class="n">min_arch</span> <span class="o">%</span> <span class="mi">10</span><span class="p">)</span>
                <span class="p">)</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_check_cubins</span><span class="p">():</span>
    <span class="n">incompatible_device_warn</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot;</span>
<span class="si">{}</span><span class="s2"> with CUDA capability sm_</span><span class="si">{}</span><span class="s2"> is not compatible with the current PyTorch installation.</span>
<span class="s2">The current PyTorch install supports CUDA capabilities </span><span class="si">{}</span><span class="s2">.</span>
<span class="s2">If you want to use the </span><span class="si">{}</span><span class="s2"> GPU with PyTorch, please check the instructions at https://pytorch.org/get-started/locally/</span>
<span class="s2">&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">version</span><span class="o">.</span><span class="n">cuda</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>  <span class="c1"># on ROCm we don&#39;t want this check</span>
        <span class="k">return</span>
    <span class="n">arch_list</span> <span class="o">=</span> <span class="n">get_arch_list</span><span class="p">()</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">arch_list</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">return</span>
    <span class="n">supported_sm</span> <span class="o">=</span> <span class="p">[</span><span class="n">_extract_arch_version</span><span class="p">(</span><span class="n">arch</span><span class="p">)</span> <span class="k">for</span> <span class="n">arch</span> <span class="ow">in</span> <span class="n">arch_list</span> <span class="k">if</span> <span class="s2">&quot;sm_&quot;</span> <span class="ow">in</span> <span class="n">arch</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">device_count</span><span class="p">()):</span>
        <span class="n">cap_major</span><span class="p">,</span> <span class="n">cap_minor</span> <span class="o">=</span> <span class="n">get_device_capability</span><span class="p">(</span><span class="n">idx</span><span class="p">)</span>
        <span class="c1"># NVIDIA GPU compute architectures are backward compatible within major version</span>
        <span class="n">supported</span> <span class="o">=</span> <span class="nb">any</span><span class="p">(</span><span class="n">sm</span> <span class="o">//</span> <span class="mi">10</span> <span class="o">==</span> <span class="n">cap_major</span> <span class="k">for</span> <span class="n">sm</span> <span class="ow">in</span> <span class="n">supported_sm</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">supported</span><span class="p">:</span>
            <span class="n">device_name</span> <span class="o">=</span> <span class="n">get_device_name</span><span class="p">(</span><span class="n">idx</span><span class="p">)</span>
            <span class="n">capability</span> <span class="o">=</span> <span class="n">cap_major</span> <span class="o">*</span> <span class="mi">10</span> <span class="o">+</span> <span class="n">cap_minor</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
                <span class="n">incompatible_device_warn</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                    <span class="n">device_name</span><span class="p">,</span> <span class="n">capability</span><span class="p">,</span> <span class="s2">&quot; &quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">arch_list</span><span class="p">),</span> <span class="n">device_name</span>
                <span class="p">)</span>
            <span class="p">)</span>


<div class="viewcode-block" id="is_initialized">
<a class="viewcode-back" href="../../generated/torch.cuda.is_initialized.html#torch.cuda.is_initialized">[docs]</a>
<span class="k">def</span><span class="w"> </span><span class="nf">is_initialized</span><span class="p">():</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Return whether PyTorch&#39;s CUDA state has been initialized.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">_initialized</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">_is_in_bad_fork</span><span class="p">()</span></div>



<span class="k">def</span><span class="w"> </span><span class="nf">_lazy_call</span><span class="p">(</span><span class="nb">callable</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="k">with</span> <span class="n">_initialization_lock</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">is_initialized</span><span class="p">():</span>
            <span class="nb">callable</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># TODO(torch_deploy): this accesses linecache, which attempts to read the</span>
            <span class="c1"># file system to get traceback info. Patch linecache or do something</span>
            <span class="c1"># else here if this ends up being important.</span>
            <span class="k">global</span> <span class="n">_lazy_seed_tracker</span>
            <span class="k">if</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;seed_all&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">):</span>
                <span class="n">_lazy_seed_tracker</span><span class="o">.</span><span class="n">queue_seed_all</span><span class="p">(</span><span class="nb">callable</span><span class="p">,</span> <span class="n">traceback</span><span class="o">.</span><span class="n">format_stack</span><span class="p">())</span>
            <span class="k">elif</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;seed&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">):</span>
                <span class="n">_lazy_seed_tracker</span><span class="o">.</span><span class="n">queue_seed</span><span class="p">(</span><span class="nb">callable</span><span class="p">,</span> <span class="n">traceback</span><span class="o">.</span><span class="n">format_stack</span><span class="p">())</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="c1"># Don&#39;t store the actual traceback to avoid memory cycle</span>
                <span class="n">_queued_calls</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="nb">callable</span><span class="p">,</span> <span class="n">traceback</span><span class="o">.</span><span class="n">format_stack</span><span class="p">()))</span>


<span class="n">_lazy_call</span><span class="p">(</span><span class="n">_check_capability</span><span class="p">)</span>
<span class="n">_lazy_call</span><span class="p">(</span><span class="n">_check_cubins</span><span class="p">)</span>


<span class="k">class</span><span class="w"> </span><span class="nc">DeferredCudaCallError</span><span class="p">(</span><span class="ne">Exception</span><span class="p">):</span>
    <span class="k">pass</span>


<span class="n">OutOfMemoryError</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">OutOfMemoryError</span>


<div class="viewcode-block" id="init">
<a class="viewcode-back" href="../../generated/torch.cuda.init.html#torch.cuda.init">[docs]</a>
<span class="k">def</span><span class="w"> </span><span class="nf">init</span><span class="p">():</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Initialize PyTorch&#39;s CUDA state.</span>

<span class="sd">    You may need to call this explicitly if you are interacting with</span>
<span class="sd">    PyTorch via its C API, as Python bindings for CUDA functionality</span>
<span class="sd">    will not be available until this initialization takes place.</span>
<span class="sd">    Ordinary users should not need this, as all of PyTorch&#39;s CUDA methods</span>
<span class="sd">    automatically initialize CUDA state on-demand.</span>

<span class="sd">    Does nothing if the CUDA state is already initialized.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_lazy_init</span><span class="p">()</span></div>



<span class="k">def</span><span class="w"> </span><span class="nf">_lazy_init</span><span class="p">():</span>
    <span class="k">global</span> <span class="n">_initialized</span><span class="p">,</span> <span class="n">_queued_calls</span>
    <span class="k">if</span> <span class="n">is_initialized</span><span class="p">()</span> <span class="ow">or</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">_tls</span><span class="p">,</span> <span class="s2">&quot;is_initializing&quot;</span><span class="p">):</span>
        <span class="k">return</span>
    <span class="k">with</span> <span class="n">_initialization_lock</span><span class="p">:</span>
        <span class="c1"># We be double-checked locking, boys!  This is OK because</span>
        <span class="c1"># the above test was GIL protected anyway.  The inner test</span>
        <span class="c1"># is for when a thread blocked on some other thread which was</span>
        <span class="c1"># doing the initialization; when they get the lock, they will</span>
        <span class="c1"># find there is nothing left to do.</span>
        <span class="k">if</span> <span class="n">is_initialized</span><span class="p">():</span>
            <span class="k">return</span>
        <span class="c1"># It is important to prevent other threads from entering _lazy_init</span>
        <span class="c1"># immediately, while we are still guaranteed to have the GIL, because some</span>
        <span class="c1"># of the C calls we make below will release the GIL</span>
        <span class="k">if</span> <span class="n">_is_in_bad_fork</span><span class="p">():</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                <span class="s2">&quot;Cannot re-initialize CUDA in forked subprocess. To use CUDA with &quot;</span>
                <span class="s2">&quot;multiprocessing, you must use the &#39;spawn&#39; start method&quot;</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="p">,</span> <span class="s2">&quot;_cuda_getDeviceCount&quot;</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">AssertionError</span><span class="p">(</span><span class="s2">&quot;Torch not compiled with CUDA enabled&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">_cudart</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">AssertionError</span><span class="p">(</span>
                <span class="s2">&quot;libcudart functions unavailable. It looks like you have a broken build?&quot;</span>
            <span class="p">)</span>
        <span class="c1"># This function throws if there&#39;s a driver initialization error, no GPUs</span>
        <span class="c1"># are found or any other error occurs</span>
        <span class="k">if</span> <span class="s2">&quot;CUDA_MODULE_LOADING&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">:</span>
            <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;CUDA_MODULE_LOADING&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;LAZY&quot;</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_cuda_init</span><span class="p">()</span>
        <span class="c1"># Some of the queued calls may reentrantly call _lazy_init();</span>
        <span class="c1"># we need to just return without initializing in that case.</span>
        <span class="c1"># However, we must not let any *other* threads in!</span>
        <span class="n">_tls</span><span class="o">.</span><span class="n">is_initializing</span> <span class="o">=</span> <span class="kc">True</span>

        <span class="n">_queued_calls</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">calls</span> <span class="k">for</span> <span class="n">calls</span> <span class="ow">in</span> <span class="n">_lazy_seed_tracker</span><span class="o">.</span><span class="n">get_calls</span><span class="p">()</span> <span class="k">if</span> <span class="n">calls</span><span class="p">)</span>

        <span class="k">try</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">queued_call</span><span class="p">,</span> <span class="n">orig_traceback</span> <span class="ow">in</span> <span class="n">_queued_calls</span><span class="p">:</span>
                <span class="k">try</span><span class="p">:</span>
                    <span class="n">queued_call</span><span class="p">()</span>
                <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
                    <span class="n">msg</span> <span class="o">=</span> <span class="p">(</span>
                        <span class="sa">f</span><span class="s2">&quot;CUDA call failed lazily at initialization with error: </span><span class="si">{</span><span class="nb">str</span><span class="p">(</span><span class="n">e</span><span class="p">)</span><span class="si">}</span><span class="se">\n\n</span><span class="s2">&quot;</span>
                        <span class="sa">f</span><span class="s2">&quot;CUDA call was originally invoked at:</span><span class="se">\n\n</span><span class="si">{</span><span class="s1">&#39;&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">orig_traceback</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span>
                    <span class="p">)</span>
                    <span class="k">raise</span> <span class="n">DeferredCudaCallError</span><span class="p">(</span><span class="n">msg</span><span class="p">)</span> <span class="kn">from</span><span class="w"> </span><span class="nn">e</span>
        <span class="k">finally</span><span class="p">:</span>
            <span class="nb">delattr</span><span class="p">(</span><span class="n">_tls</span><span class="p">,</span> <span class="s2">&quot;is_initializing&quot;</span><span class="p">)</span>
        <span class="n">_initialized</span> <span class="o">=</span> <span class="kc">True</span>


<div class="viewcode-block" id="cudart">
<a class="viewcode-back" href="../../python-api/torch.cuda.cudart.html#torch.cuda.cudart">[docs]</a>
<span class="k">def</span><span class="w"> </span><span class="nf">cudart</span><span class="p">():</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Retrieves the CUDA runtime API module.</span>


<span class="sd">    This function initializes the CUDA runtime environment if it is not already</span>
<span class="sd">    initialized and returns the CUDA runtime API module (_cudart). The CUDA</span>
<span class="sd">    runtime API module provides access to various CUDA runtime functions.</span>

<span class="sd">    Args:</span>
<span class="sd">        ``None``</span>

<span class="sd">    Returns:</span>
<span class="sd">        module: The CUDA runtime API module (_cudart).</span>

<span class="sd">    Raises:</span>
<span class="sd">        RuntimeError: If CUDA cannot be re-initialized in a forked subprocess.</span>
<span class="sd">        AssertionError: If PyTorch is not compiled with CUDA support or if libcudart functions are unavailable.</span>

<span class="sd">    Example of CUDA operations with profiling:</span>
<span class="sd">        &gt;&gt;&gt; import torch</span>
<span class="sd">        &gt;&gt;&gt; from torch.cuda import cudart, check_error</span>
<span class="sd">        &gt;&gt;&gt; import os</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; os.environ[&#39;CUDA_PROFILE&#39;] = &#39;1&#39;</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; def perform_cuda_operations_with_streams():</span>
<span class="sd">        &gt;&gt;&gt;     stream = torch.cuda.Stream()</span>
<span class="sd">        &gt;&gt;&gt;     with torch.cuda.stream(stream):</span>
<span class="sd">        &gt;&gt;&gt;         x = torch.randn(100, 100, device=&#39;cuda&#39;)</span>
<span class="sd">        &gt;&gt;&gt;         y = torch.randn(100, 100, device=&#39;cuda&#39;)</span>
<span class="sd">        &gt;&gt;&gt;         z = torch.mul(x, y)</span>
<span class="sd">        &gt;&gt;&gt;     return z</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; torch.cuda.synchronize()</span>
<span class="sd">        &gt;&gt;&gt; print(&quot;====== Start nsys profiling ======&quot;)</span>
<span class="sd">        &gt;&gt;&gt; check_error(cudart().cudaProfilerStart())</span>
<span class="sd">        &gt;&gt;&gt; with torch.autograd.profiler.emit_nvtx():</span>
<span class="sd">        &gt;&gt;&gt;     result = perform_cuda_operations_with_streams()</span>
<span class="sd">        &gt;&gt;&gt;     print(&quot;CUDA operations completed.&quot;)</span>
<span class="sd">        &gt;&gt;&gt; check_error(torch.cuda.cudart().cudaProfilerStop())</span>
<span class="sd">        &gt;&gt;&gt; print(&quot;====== End nsys profiling ======&quot;)</span>

<span class="sd">    To run this example and save the profiling information, execute:</span>
<span class="sd">        &gt;&gt;&gt; $ nvprof --profile-from-start off --csv --print-summary -o trace_name.prof -f -- python cudart_test.py</span>

<span class="sd">    This command profiles the CUDA operations in the provided script and saves</span>
<span class="sd">    the profiling information to a file named `trace_name.prof`.</span>
<span class="sd">    The `--profile-from-start off` option ensures that profiling starts only</span>
<span class="sd">    after the `cudaProfilerStart` call in the script.</span>
<span class="sd">    The `--csv` and `--print-summary` options format the profiling output as a</span>
<span class="sd">    CSV file and print a summary, respectively.</span>
<span class="sd">    The `-o` option specifies the output file name, and the `-f` option forces the</span>
<span class="sd">    overwrite of the output file if it already exists.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_lazy_init</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">_cudart</span></div>



<span class="k">class</span><span class="w"> </span><span class="nc">cudaStatus</span><span class="p">:</span>
    <span class="n">SUCCESS</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">ERROR_NOT_READY</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">34</span>


<span class="k">class</span><span class="w"> </span><span class="nc">CudaError</span><span class="p">(</span><span class="ne">RuntimeError</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">code</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">msg</span> <span class="o">=</span> <span class="n">_cudart</span><span class="o">.</span><span class="n">cudaGetErrorString</span><span class="p">(</span><span class="n">_cudart</span><span class="o">.</span><span class="n">cudaError</span><span class="p">(</span><span class="n">code</span><span class="p">))</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">msg</span><span class="si">}</span><span class="s2"> (</span><span class="si">{</span><span class="n">code</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>


<span class="k">def</span><span class="w"> </span><span class="nf">check_error</span><span class="p">(</span><span class="n">res</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">res</span> <span class="o">!=</span> <span class="n">_cudart</span><span class="o">.</span><span class="n">cudaError</span><span class="o">.</span><span class="n">success</span><span class="p">:</span>
        <span class="k">raise</span> <span class="n">CudaError</span><span class="p">(</span><span class="n">res</span><span class="p">)</span>


<span class="k">class</span><span class="w"> </span><span class="nc">_DeviceGuard</span><span class="p">:</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">index</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">idx</span> <span class="o">=</span> <span class="n">index</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">prev_idx</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__enter__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">prev_idx</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">_exchange_device</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">idx</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__exit__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">type</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="n">value</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="n">traceback</span><span class="p">:</span> <span class="n">Any</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">idx</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">_maybe_exchange_device</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">prev_idx</span><span class="p">)</span>
        <span class="k">return</span> <span class="kc">False</span>


<div class="viewcode-block" id="device">
<a class="viewcode-back" href="../../generated/torch.cuda.device.html#torch.cuda.device">[docs]</a>
<span class="k">class</span><span class="w"> </span><span class="nc">device</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Context-manager that changes the selected device.</span>

<span class="sd">    Args:</span>
<span class="sd">        device (torch.device or int): device index to select. It&#39;s a no-op if</span>
<span class="sd">            this argument is a negative integer or ``None``.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">device</span><span class="p">:</span> <span class="n">Any</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">idx</span> <span class="o">=</span> <span class="n">_get_device_index</span><span class="p">(</span><span class="n">device</span><span class="p">,</span> <span class="n">optional</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">prev_idx</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__enter__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">prev_idx</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">_exchange_device</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">idx</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__exit__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">type</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="n">value</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="n">traceback</span><span class="p">:</span> <span class="n">Any</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">idx</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">_maybe_exchange_device</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">prev_idx</span><span class="p">)</span>
        <span class="k">return</span> <span class="kc">False</span></div>



<div class="viewcode-block" id="device_of">
<a class="viewcode-back" href="../../python-api/torch.cuda.device_of.html#torch.cuda.device_of">[docs]</a>
<span class="k">class</span><span class="w"> </span><span class="nc">device_of</span><span class="p">(</span><span class="n">device</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Context-manager that changes the current device to that of given object.</span>

<span class="sd">    You can use both tensors and storages as arguments. If a given object is</span>
<span class="sd">    not allocated on a GPU, this is a no-op.</span>

<span class="sd">    Args:</span>
<span class="sd">        obj (Tensor or Storage): object allocated on the selected device.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">obj</span><span class="p">):</span>
        <span class="n">idx</span> <span class="o">=</span> <span class="n">obj</span><span class="o">.</span><span class="n">get_device</span><span class="p">()</span> <span class="k">if</span> <span class="n">obj</span><span class="o">.</span><span class="n">is_cuda</span> <span class="k">else</span> <span class="o">-</span><span class="mi">1</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">idx</span><span class="p">)</span></div>



<div class="viewcode-block" id="set_device">
<a class="viewcode-back" href="../../python-api/torch.cuda.set_device.html#torch.cuda.set_device">[docs]</a>
<span class="k">def</span><span class="w"> </span><span class="nf">set_device</span><span class="p">(</span><span class="n">device</span><span class="p">:</span> <span class="n">_device_t</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Set the current device.</span>

<span class="sd">    Usage of this function is discouraged in favor of :any:`device`. In most</span>
<span class="sd">    cases it&#39;s better to use ``CUDA_VISIBLE_DEVICES`` environmental variable.</span>

<span class="sd">    Args:</span>
<span class="sd">        device (torch.device or int): selected device. This function is a no-op</span>
<span class="sd">            if this argument is negative.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">device</span> <span class="o">=</span> <span class="n">_get_device_index</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">device</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_cuda_setDevice</span><span class="p">(</span><span class="n">device</span><span class="p">)</span></div>



<div class="viewcode-block" id="get_device_name">
<a class="viewcode-back" href="../../generated/torch.cuda.get_device_name.html#torch.cuda.get_device_name">[docs]</a>
<span class="k">def</span><span class="w"> </span><span class="nf">get_device_name</span><span class="p">(</span><span class="n">device</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">_device_t</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Get the name of a device.</span>

<span class="sd">    Args:</span>
<span class="sd">        device (torch.device or int or str, optional): device for which to return the</span>
<span class="sd">            name. This function is a no-op if this argument is a negative</span>
<span class="sd">            integer. It uses the current device, given by :func:`~torch.cuda.current_device`,</span>
<span class="sd">            if :attr:`device` is ``None`` (default).</span>

<span class="sd">    Returns:</span>
<span class="sd">        str: the name of the device</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">get_device_properties</span><span class="p">(</span><span class="n">device</span><span class="p">)</span><span class="o">.</span><span class="n">name</span></div>



<div class="viewcode-block" id="get_device_capability">
<a class="viewcode-back" href="../../python-api/torch.cuda.get_device_capability.html#torch.cuda.get_device_capability">[docs]</a>
<span class="k">def</span><span class="w"> </span><span class="nf">get_device_capability</span><span class="p">(</span><span class="n">device</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">_device_t</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Get the cuda capability of a device.</span>

<span class="sd">    Args:</span>
<span class="sd">        device (torch.device or int or str, optional): device for which to return the</span>
<span class="sd">            device capability. This function is a no-op if this argument is</span>
<span class="sd">            a negative integer. It uses the current device, given by</span>
<span class="sd">            :func:`~torch.cuda.current_device`, if :attr:`device` is ``None``</span>
<span class="sd">            (default).</span>

<span class="sd">    Returns:</span>
<span class="sd">        tuple(int, int): the major and minor cuda capability of the device</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">prop</span> <span class="o">=</span> <span class="n">get_device_properties</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">prop</span><span class="o">.</span><span class="n">major</span><span class="p">,</span> <span class="n">prop</span><span class="o">.</span><span class="n">minor</span></div>



<div class="viewcode-block" id="get_device_properties">
<a class="viewcode-back" href="../../python-api/torch.cuda.get_device_properties.html#torch.cuda.get_device_properties">[docs]</a>
<span class="k">def</span><span class="w"> </span><span class="nf">get_device_properties</span><span class="p">(</span><span class="n">device</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">_device_t</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">_CudaDeviceProperties</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Get the properties of a device.</span>

<span class="sd">    Args:</span>
<span class="sd">        device (torch.device or int or str, optional): device for which to return the</span>
<span class="sd">            properties of the device.  It uses the current device, given by</span>
<span class="sd">            :func:`~torch.cuda.current_device`, if :attr:`device` is ``None``</span>
<span class="sd">            (default).</span>

<span class="sd">    Returns:</span>
<span class="sd">        _CudaDeviceProperties: the properties of the device</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_lazy_init</span><span class="p">()</span>  <span class="c1"># will define _get_device_properties</span>
    <span class="n">device</span> <span class="o">=</span> <span class="n">_get_device_index</span><span class="p">(</span><span class="n">device</span><span class="p">,</span> <span class="n">optional</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">device</span> <span class="o">&lt;</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">device</span> <span class="o">&gt;=</span> <span class="n">device_count</span><span class="p">():</span>
        <span class="k">raise</span> <span class="ne">AssertionError</span><span class="p">(</span><span class="s2">&quot;Invalid device id&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">_get_device_properties</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>  <span class="c1"># type: ignore[name-defined]</span></div>



<div class="viewcode-block" id="can_device_access_peer">
<a class="viewcode-back" href="../../python-api/torch.cuda.can_device_access_peer.html#torch.cuda.can_device_access_peer">[docs]</a>
<span class="k">def</span><span class="w"> </span><span class="nf">can_device_access_peer</span><span class="p">(</span><span class="n">device</span><span class="p">:</span> <span class="n">_device_t</span><span class="p">,</span> <span class="n">peer_device</span><span class="p">:</span> <span class="n">_device_t</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Check if peer access between two devices is possible.&quot;&quot;&quot;</span>
    <span class="n">_lazy_init</span><span class="p">()</span>
    <span class="n">device</span> <span class="o">=</span> <span class="n">_get_device_index</span><span class="p">(</span><span class="n">device</span><span class="p">,</span> <span class="n">optional</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">peer_device</span> <span class="o">=</span> <span class="n">_get_device_index</span><span class="p">(</span><span class="n">peer_device</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">device</span> <span class="o">&lt;</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">device</span> <span class="o">&gt;=</span> <span class="n">device_count</span><span class="p">():</span>
        <span class="k">raise</span> <span class="ne">AssertionError</span><span class="p">(</span><span class="s2">&quot;Invalid device id&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">peer_device</span> <span class="o">&lt;</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">peer_device</span> <span class="o">&gt;=</span> <span class="n">device_count</span><span class="p">():</span>
        <span class="k">raise</span> <span class="ne">AssertionError</span><span class="p">(</span><span class="s2">&quot;Invalid peer device id&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_cuda_canDeviceAccessPeer</span><span class="p">(</span><span class="n">device</span><span class="p">,</span> <span class="n">peer_device</span><span class="p">)</span></div>



<div class="viewcode-block" id="StreamContext">
<a class="viewcode-back" href="../../python-api/torch.cuda.StreamContext.html#torch.cuda.StreamContext">[docs]</a>
<span class="k">class</span><span class="w"> </span><span class="nc">StreamContext</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Context-manager that selects a given stream.</span>

<span class="sd">    All CUDA kernels queued within its context will be enqueued on a selected</span>
<span class="sd">    stream.</span>

<span class="sd">    Args:</span>
<span class="sd">        Stream (Stream): selected stream. This manager is a no-op if it&#39;s</span>
<span class="sd">            ``None``.</span>
<span class="sd">    .. note:: Streams are per-device.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">cur_stream</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="s2">&quot;torch.cuda.Stream&quot;</span><span class="p">]</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">stream</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="s2">&quot;torch.cuda.Stream&quot;</span><span class="p">]):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">stream</span> <span class="o">=</span> <span class="n">stream</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">idx</span> <span class="o">=</span> <span class="n">_get_device_index</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">is_scripting</span><span class="p">():</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">idx</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">idx</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">src_prev_stream</span> <span class="o">=</span> <span class="p">(</span>
            <span class="kc">None</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">is_scripting</span><span class="p">()</span> <span class="k">else</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">default_stream</span><span class="p">(</span><span class="kc">None</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dst_prev_stream</span> <span class="o">=</span> <span class="p">(</span>
            <span class="kc">None</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">is_scripting</span><span class="p">()</span> <span class="k">else</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">default_stream</span><span class="p">(</span><span class="kc">None</span><span class="p">)</span>
        <span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__enter__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># Local cur_stream variable for type refinement</span>
        <span class="n">cur_stream</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">stream</span>
        <span class="c1"># Return if stream is None or CUDA device not available</span>
        <span class="k">if</span> <span class="n">cur_stream</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">idx</span> <span class="o">==</span> <span class="o">-</span><span class="mi">1</span><span class="p">:</span>
            <span class="k">return</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">src_prev_stream</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">current_stream</span><span class="p">(</span><span class="kc">None</span><span class="p">)</span>

        <span class="c1"># If the stream is not on the current device, then</span>
        <span class="c1"># set the current stream on the device</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">src_prev_stream</span><span class="o">.</span><span class="n">device</span> <span class="o">!=</span> <span class="n">cur_stream</span><span class="o">.</span><span class="n">device</span><span class="p">:</span>
            <span class="k">with</span> <span class="n">device</span><span class="p">(</span><span class="n">cur_stream</span><span class="o">.</span><span class="n">device</span><span class="p">):</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">dst_prev_stream</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">current_stream</span><span class="p">(</span><span class="n">cur_stream</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">set_stream</span><span class="p">(</span><span class="n">cur_stream</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__exit__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">type</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="n">value</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="n">traceback</span><span class="p">:</span> <span class="n">Any</span><span class="p">):</span>
        <span class="c1"># Local cur_stream variable for type refinement</span>
        <span class="n">cur_stream</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">stream</span>
        <span class="c1"># If stream is None or no CUDA device available, return</span>
        <span class="k">if</span> <span class="n">cur_stream</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">idx</span> <span class="o">==</span> <span class="o">-</span><span class="mi">1</span><span class="p">:</span>
            <span class="k">return</span>

        <span class="c1"># Reset the stream on the original device</span>
        <span class="c1"># and destination device</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">src_prev_stream</span><span class="o">.</span><span class="n">device</span> <span class="o">!=</span> <span class="n">cur_stream</span><span class="o">.</span><span class="n">device</span><span class="p">:</span>  <span class="c1"># type: ignore[union-attr]</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">set_stream</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dst_prev_stream</span><span class="p">)</span>  <span class="c1"># type: ignore[arg-type]</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">set_stream</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">src_prev_stream</span><span class="p">)</span>  <span class="c1"># type: ignore[arg-type]</span></div>



<div class="viewcode-block" id="stream">
<a class="viewcode-back" href="../../python-api/torch.cuda.Stream.html#torch.cuda.stream">[docs]</a>
<span class="k">def</span><span class="w"> </span><span class="nf">stream</span><span class="p">(</span><span class="n">stream</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="s2">&quot;torch.cuda.Stream&quot;</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">StreamContext</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Wrap around the Context-manager StreamContext that selects a given stream.</span>

<span class="sd">    Arguments:</span>
<span class="sd">        stream (Stream): selected stream. This manager is a no-op if it&#39;s</span>
<span class="sd">            ``None``.</span>
<span class="sd">    .. note::</span>
<span class="sd">        In eager mode stream is of type Stream class while in JIT it is</span>
<span class="sd">        an object of the custom class ``torch.classes.cuda.Stream``.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">StreamContext</span><span class="p">(</span><span class="n">stream</span><span class="p">)</span></div>



<span class="k">def</span><span class="w"> </span><span class="nf">_set_stream_by_id</span><span class="p">(</span><span class="n">stream_id</span><span class="p">,</span> <span class="n">device_index</span><span class="p">,</span> <span class="n">device_type</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;set stream specified by the stream id, device index and</span>
<span class="sd">        device type</span>

<span class="sd">    Args: stream_id (int): stream id in stream pool</span>
<span class="sd">          device_index (int): device index in topo</span>
<span class="sd">          device_type (int): enum device type</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_cuda_setStream</span><span class="p">(</span>
        <span class="n">stream_id</span><span class="o">=</span><span class="n">stream_id</span><span class="p">,</span>
        <span class="n">device_index</span><span class="o">=</span><span class="n">device_index</span><span class="p">,</span>
        <span class="n">device_type</span><span class="o">=</span><span class="n">device_type</span><span class="p">,</span>
    <span class="p">)</span>


<div class="viewcode-block" id="set_stream">
<a class="viewcode-back" href="../../python-api/torch.cuda.set_stream.html#torch.cuda.set_stream">[docs]</a>
<span class="k">def</span><span class="w"> </span><span class="nf">set_stream</span><span class="p">(</span><span class="n">stream</span><span class="p">:</span> <span class="n">Stream</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Set the current stream.This is a wrapper API to set the stream.</span>
<span class="sd">        Usage of this function is discouraged in favor of the ``stream``</span>
<span class="sd">        context manager.</span>

<span class="sd">    Args:</span>
<span class="sd">        stream (Stream): selected stream. This function is a no-op</span>
<span class="sd">            if this argument is ``None``.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">stream</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">return</span>
    <span class="n">_set_stream_by_id</span><span class="p">(</span>
        <span class="n">stream_id</span><span class="o">=</span><span class="n">stream</span><span class="o">.</span><span class="n">stream_id</span><span class="p">,</span>
        <span class="n">device_index</span><span class="o">=</span><span class="n">stream</span><span class="o">.</span><span class="n">device_index</span><span class="p">,</span>
        <span class="n">device_type</span><span class="o">=</span><span class="n">stream</span><span class="o">.</span><span class="n">device_type</span><span class="p">,</span>
    <span class="p">)</span></div>



<span class="k">def</span><span class="w"> </span><span class="nf">_parse_visible_devices</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">]]:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Parse CUDA_VISIBLE_DEVICES environment variable.&quot;&quot;&quot;</span>
    <span class="n">var</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s2">&quot;CUDA_VISIBLE_DEVICES&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">version</span><span class="o">.</span><span class="n">hip</span><span class="p">:</span>
        <span class="n">hip_devices</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s2">&quot;HIP_VISIBLE_DEVICES&quot;</span><span class="p">)</span>
        <span class="n">rocr_devices</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s2">&quot;ROCR_VISIBLE_DEVICES&quot;</span><span class="p">)</span>

        <span class="c1"># You must take care if both HIP and ROCR env vars are set as they have</span>
        <span class="c1"># different meanings. Both env vars accept either a list of ints or a</span>
        <span class="c1"># list of UUIDs. The ROCR env var is processed first which then reduces</span>
        <span class="c1"># the number of GPUs that HIP can select from.</span>
        <span class="k">if</span> <span class="n">rocr_devices</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">rocr_count</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">rocr_devices</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;,&quot;</span><span class="p">))</span>
            <span class="k">if</span> <span class="n">hip_devices</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="c1"># sanity check if both env vars are set</span>
                <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">hip_devices</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;,&quot;</span><span class="p">))</span> <span class="o">&gt;</span> <span class="n">rocr_count</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                        <span class="s2">&quot;HIP_VISIBLE_DEVICES contains more devices than ROCR_VISIBLE_DEVICES&quot;</span>
                    <span class="p">)</span>
                <span class="c1"># HIP_VISIBLE_DEVICES is preferred over ROCR_VISIBLE_DEVICES</span>
                <span class="n">var</span> <span class="o">=</span> <span class="n">hip_devices</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">return</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">rocr_count</span><span class="p">))</span>
        <span class="k">elif</span> <span class="n">hip_devices</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">var</span> <span class="o">=</span> <span class="n">hip_devices</span>

    <span class="k">if</span> <span class="n">var</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">return</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">64</span><span class="p">))</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_strtoul</span><span class="p">(</span><span class="n">s</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Return -1 or positive integer sequence string starts with.&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">s</span><span class="p">:</span>
            <span class="k">return</span> <span class="o">-</span><span class="mi">1</span>
        <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">c</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">s</span><span class="p">):</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="n">c</span><span class="o">.</span><span class="n">isdigit</span><span class="p">()</span> <span class="ow">or</span> <span class="p">(</span><span class="n">idx</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">c</span> <span class="ow">in</span> <span class="s2">&quot;+-&quot;</span><span class="p">)):</span>
                <span class="k">break</span>
            <span class="k">if</span> <span class="n">idx</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">s</span><span class="p">):</span>
                <span class="n">idx</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="k">return</span> <span class="nb">int</span><span class="p">(</span><span class="n">s</span><span class="p">[:</span><span class="n">idx</span><span class="p">])</span> <span class="k">if</span> <span class="n">idx</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="k">else</span> <span class="o">-</span><span class="mi">1</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">parse_list_with_prefix</span><span class="p">(</span><span class="n">lst</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">prefix</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">]:</span>
        <span class="n">rcs</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">elem</span> <span class="ow">in</span> <span class="n">lst</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;,&quot;</span><span class="p">):</span>
            <span class="c1"># Repeated id results in empty set</span>
            <span class="k">if</span> <span class="n">elem</span> <span class="ow">in</span> <span class="n">rcs</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">cast</span><span class="p">(</span><span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="p">[])</span>
            <span class="c1"># Anything other but prefix is ignored</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">elem</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="n">prefix</span><span class="p">):</span>
                <span class="k">break</span>
            <span class="n">rcs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">elem</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">rcs</span>

    <span class="k">if</span> <span class="n">var</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;GPU-&quot;</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">parse_list_with_prefix</span><span class="p">(</span><span class="n">var</span><span class="p">,</span> <span class="s2">&quot;GPU-&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">var</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;MIG-&quot;</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">parse_list_with_prefix</span><span class="p">(</span><span class="n">var</span><span class="p">,</span> <span class="s2">&quot;MIG-&quot;</span><span class="p">)</span>
    <span class="c1"># CUDA_VISIBLE_DEVICES uses something like strtoul</span>
    <span class="c1"># which makes `1gpu2,2ampere` is equivalent to `1,2`</span>
    <span class="n">rc</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">elem</span> <span class="ow">in</span> <span class="n">var</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;,&quot;</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">_strtoul</span><span class="p">(</span><span class="n">elem</span><span class="o">.</span><span class="n">strip</span><span class="p">())</span>
        <span class="c1"># Repeated ordinal results in empty set</span>
        <span class="k">if</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">rc</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">cast</span><span class="p">(</span><span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="p">[])</span>
        <span class="c1"># Negative value aborts the sequence</span>
        <span class="k">if</span> <span class="n">x</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">break</span>
        <span class="n">rc</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">rc</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_raw_device_count_amdsmi</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">_HAS_PYNVML</span><span class="p">:</span>  <span class="c1"># If amdsmi is not available</span>
        <span class="k">return</span> <span class="o">-</span><span class="mi">1</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="n">amdsmi</span><span class="o">.</span><span class="n">amdsmi_init</span><span class="p">()</span>
    <span class="k">except</span> <span class="n">amdsmi</span><span class="o">.</span><span class="n">AmdSmiException</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
        <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Can&#39;t initialize amdsmi - Error code: </span><span class="si">{</span><span class="n">e</span><span class="o">.</span><span class="n">err_code</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="o">-</span><span class="mi">1</span>
    <span class="n">socket_handles</span> <span class="o">=</span> <span class="n">amdsmi</span><span class="o">.</span><span class="n">amdsmi_get_processor_handles</span><span class="p">()</span>
    <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="n">socket_handles</span><span class="p">)</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_raw_device_count_nvml</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Return number of devices as reported by NVML or negative value if NVML discovery/initialization failed.&quot;&quot;&quot;</span>
    <span class="kn">from</span><span class="w"> </span><span class="nn">ctypes</span><span class="w"> </span><span class="kn">import</span> <span class="n">byref</span><span class="p">,</span> <span class="n">c_int</span><span class="p">,</span> <span class="n">CDLL</span>

    <span class="n">nvml_h</span> <span class="o">=</span> <span class="n">CDLL</span><span class="p">(</span><span class="s2">&quot;libnvidia-ml.so.1&quot;</span><span class="p">)</span>
    <span class="n">rc</span> <span class="o">=</span> <span class="n">nvml_h</span><span class="o">.</span><span class="n">nvmlInit</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">rc</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s2">&quot;Can&#39;t initialize NVML&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="o">-</span><span class="mi">1</span>
    <span class="n">dev_count</span> <span class="o">=</span> <span class="n">c_int</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">rc</span> <span class="o">=</span> <span class="n">nvml_h</span><span class="o">.</span><span class="n">nvmlDeviceGetCount_v2</span><span class="p">(</span><span class="n">byref</span><span class="p">(</span><span class="n">dev_count</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">rc</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s2">&quot;Can&#39;t get nvml device count&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="o">-</span><span class="mi">1</span>
    <span class="k">del</span> <span class="n">nvml_h</span>
    <span class="k">return</span> <span class="n">dev_count</span><span class="o">.</span><span class="n">value</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_raw_device_uuid_amdsmi</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">]]:</span>
    <span class="kn">from</span><span class="w"> </span><span class="nn">ctypes</span><span class="w"> </span><span class="kn">import</span> <span class="n">byref</span><span class="p">,</span> <span class="n">c_int</span><span class="p">,</span> <span class="n">c_void_p</span><span class="p">,</span> <span class="n">CDLL</span><span class="p">,</span> <span class="n">create_string_buffer</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">_HAS_PYNVML</span><span class="p">:</span>  <span class="c1"># If amdsmi is not available</span>
        <span class="k">return</span> <span class="kc">None</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="n">amdsmi</span><span class="o">.</span><span class="n">amdsmi_init</span><span class="p">()</span>
    <span class="k">except</span> <span class="n">amdsmi</span><span class="o">.</span><span class="n">AmdSmiException</span><span class="p">:</span>
        <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s2">&quot;Can&#39;t initialize amdsmi&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="kc">None</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="n">socket_handles</span> <span class="o">=</span> <span class="n">amdsmi</span><span class="o">.</span><span class="n">amdsmi_get_processor_handles</span><span class="p">()</span>
        <span class="n">dev_count</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">socket_handles</span><span class="p">)</span>
    <span class="k">except</span> <span class="n">amdsmi</span><span class="o">.</span><span class="n">AmdSmiException</span><span class="p">:</span>
        <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s2">&quot;Can&#39;t get amdsmi device count&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="kc">None</span>
    <span class="n">uuids</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">dev_count</span><span class="p">):</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">handler</span> <span class="o">=</span> <span class="n">amdsmi</span><span class="o">.</span><span class="n">amdsmi_get_processor_handles</span><span class="p">()[</span><span class="n">idx</span><span class="p">]</span>
        <span class="k">except</span> <span class="n">amdsmi</span><span class="o">.</span><span class="n">AmdSmiException</span><span class="p">:</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s2">&quot;Cannot get amd device handler&quot;</span><span class="p">)</span>
            <span class="k">return</span> <span class="kc">None</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">uuid</span> <span class="o">=</span> <span class="n">amdsmi</span><span class="o">.</span><span class="n">amdsmi_get_gpu_asic_info</span><span class="p">(</span><span class="n">handler</span><span class="p">)[</span><span class="s2">&quot;asic_serial&quot;</span><span class="p">][</span>
                <span class="mi">2</span><span class="p">:</span>
            <span class="p">]</span>  <span class="c1"># Removes 0x prefix from serial</span>
        <span class="k">except</span> <span class="n">amdsmi</span><span class="o">.</span><span class="n">AmdSmiException</span><span class="p">:</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s2">&quot;Cannot get uuid for amd device&quot;</span><span class="p">)</span>
            <span class="k">return</span> <span class="kc">None</span>
        <span class="n">uuids</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
            <span class="nb">str</span><span class="p">(</span><span class="n">uuid</span><span class="p">)</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span>
        <span class="p">)</span>  <span class="c1"># Lower-case to match expected HIP_VISIBLE_DEVICES uuid input</span>
    <span class="k">return</span> <span class="n">uuids</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_raw_device_uuid_nvml</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">]]:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Return list of device UUID as reported by NVML or None if NVM discovery/initialization failed.&quot;&quot;&quot;</span>
    <span class="kn">from</span><span class="w"> </span><span class="nn">ctypes</span><span class="w"> </span><span class="kn">import</span> <span class="n">byref</span><span class="p">,</span> <span class="n">c_int</span><span class="p">,</span> <span class="n">c_void_p</span><span class="p">,</span> <span class="n">CDLL</span><span class="p">,</span> <span class="n">create_string_buffer</span>

    <span class="n">nvml_h</span> <span class="o">=</span> <span class="n">CDLL</span><span class="p">(</span><span class="s2">&quot;libnvidia-ml.so.1&quot;</span><span class="p">)</span>
    <span class="n">rc</span> <span class="o">=</span> <span class="n">nvml_h</span><span class="o">.</span><span class="n">nvmlInit</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">rc</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s2">&quot;Can&#39;t initialize NVML&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="kc">None</span>
    <span class="n">dev_count</span> <span class="o">=</span> <span class="n">c_int</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">rc</span> <span class="o">=</span> <span class="n">nvml_h</span><span class="o">.</span><span class="n">nvmlDeviceGetCount_v2</span><span class="p">(</span><span class="n">byref</span><span class="p">(</span><span class="n">dev_count</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">rc</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s2">&quot;Can&#39;t get nvml device count&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="kc">None</span>
    <span class="n">uuids</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">dev_count</span><span class="o">.</span><span class="n">value</span><span class="p">):</span>
        <span class="n">dev_id</span> <span class="o">=</span> <span class="n">c_void_p</span><span class="p">()</span>
        <span class="n">rc</span> <span class="o">=</span> <span class="n">nvml_h</span><span class="o">.</span><span class="n">nvmlDeviceGetHandleByIndex_v2</span><span class="p">(</span><span class="n">idx</span><span class="p">,</span> <span class="n">byref</span><span class="p">(</span><span class="n">dev_id</span><span class="p">))</span>
        <span class="k">if</span> <span class="n">rc</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s2">&quot;Can&#39;t get device handle&quot;</span><span class="p">)</span>
            <span class="k">return</span> <span class="kc">None</span>
        <span class="n">buf_len</span> <span class="o">=</span> <span class="mi">96</span>
        <span class="n">buf</span> <span class="o">=</span> <span class="n">create_string_buffer</span><span class="p">(</span><span class="n">buf_len</span><span class="p">)</span>
        <span class="n">rc</span> <span class="o">=</span> <span class="n">nvml_h</span><span class="o">.</span><span class="n">nvmlDeviceGetUUID</span><span class="p">(</span><span class="n">dev_id</span><span class="p">,</span> <span class="n">buf</span><span class="p">,</span> <span class="n">buf_len</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">rc</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s2">&quot;Can&#39;t get device UUID&quot;</span><span class="p">)</span>
            <span class="k">return</span> <span class="kc">None</span>
        <span class="n">uuids</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">buf</span><span class="o">.</span><span class="n">raw</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="s2">&quot;ascii&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">strip</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\0</span><span class="s2">&quot;</span><span class="p">))</span>
    <span class="k">del</span> <span class="n">nvml_h</span>
    <span class="k">return</span> <span class="n">uuids</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_transform_uuid_to_ordinals</span><span class="p">(</span><span class="n">candidates</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="n">uuids</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">]:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Given the set of partial uuids and list of known uuids builds a set of ordinals excluding ambiguous partials IDs.&quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">uuid_to_ordinal</span><span class="p">(</span><span class="n">candidate</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">uuids</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
        <span class="n">best_match</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
        <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">uuid</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">uuids</span><span class="p">):</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">uuid</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="n">candidate</span><span class="p">):</span>
                <span class="k">continue</span>
            <span class="c1"># Ambiguous candidate</span>
            <span class="k">if</span> <span class="n">best_match</span> <span class="o">!=</span> <span class="o">-</span><span class="mi">1</span><span class="p">:</span>
                <span class="k">return</span> <span class="o">-</span><span class="mi">1</span>
            <span class="n">best_match</span> <span class="o">=</span> <span class="n">idx</span>
        <span class="k">return</span> <span class="n">best_match</span>

    <span class="n">rc</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">candidate</span> <span class="ow">in</span> <span class="n">candidates</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">version</span><span class="o">.</span><span class="n">hip</span><span class="p">:</span>
            <span class="n">candidate</span> <span class="o">=</span> <span class="n">candidate</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span>
                <span class="s2">&quot;GPU-&quot;</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">,</span> <span class="mi">1</span>
            <span class="p">)</span>  <span class="c1"># Remove GPU-prefix to match amdsmi asic serial</span>
        <span class="n">idx</span> <span class="o">=</span> <span class="n">uuid_to_ordinal</span><span class="p">(</span><span class="n">candidate</span><span class="p">,</span> <span class="n">uuids</span><span class="p">)</span>
        <span class="c1"># First invalid ordinal stops parsing</span>
        <span class="k">if</span> <span class="n">idx</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">break</span>
        <span class="c1"># Duplicates result in empty set</span>
        <span class="k">if</span> <span class="n">idx</span> <span class="ow">in</span> <span class="n">rc</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">cast</span><span class="p">(</span><span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="p">[])</span>
        <span class="n">rc</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">idx</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">rc</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_device_count_amdsmi</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
    <span class="n">visible_devices</span> <span class="o">=</span> <span class="n">_parse_visible_devices</span><span class="p">()</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">visible_devices</span><span class="p">:</span>
        <span class="k">return</span> <span class="mi">0</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">visible_devices</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="ow">is</span> <span class="nb">str</span><span class="p">:</span>
            <span class="n">uuids</span> <span class="o">=</span> <span class="n">_raw_device_uuid_amdsmi</span><span class="p">()</span>
            <span class="k">if</span> <span class="n">uuids</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">return</span> <span class="o">-</span><span class="mi">1</span>
            <span class="c1"># Create string version of visible devices to avoid mypy warnings</span>
            <span class="n">visible_device_str</span> <span class="o">=</span> <span class="n">cast</span><span class="p">(</span><span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="n">visible_devices</span><span class="p">)</span>
            <span class="n">visible_devices</span> <span class="o">=</span> <span class="n">_transform_uuid_to_ordinals</span><span class="p">(</span><span class="n">visible_device_str</span><span class="p">,</span> <span class="n">uuids</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">raw_cnt</span> <span class="o">=</span> <span class="n">_raw_device_count_amdsmi</span><span class="p">()</span>
            <span class="k">if</span> <span class="n">raw_cnt</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">raw_cnt</span>
            <span class="c1"># Trim the list up to a maximum available device</span>
            <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">val</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">visible_devices</span><span class="p">):</span>
                <span class="k">if</span> <span class="n">cast</span><span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="n">val</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="n">raw_cnt</span><span class="p">:</span>
                    <span class="k">return</span> <span class="n">idx</span>
    <span class="k">except</span> <span class="ne">OSError</span><span class="p">:</span>
        <span class="k">return</span> <span class="o">-</span><span class="mi">1</span>
    <span class="k">except</span> <span class="ne">AttributeError</span><span class="p">:</span>
        <span class="k">return</span> <span class="o">-</span><span class="mi">1</span>
    <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="n">visible_devices</span><span class="p">)</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_device_count_nvml</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Return number of devices as reported by NVML taking CUDA_VISIBLE_DEVICES into account.</span>

<span class="sd">    Negative value is returned if NVML discovery or initialization has failed.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">visible_devices</span> <span class="o">=</span> <span class="n">_parse_visible_devices</span><span class="p">()</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">visible_devices</span><span class="p">:</span>
        <span class="k">return</span> <span class="mi">0</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">visible_devices</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="ow">is</span> <span class="nb">str</span><span class="p">:</span>
            <span class="c1"># Skip MIG parsing</span>
            <span class="k">if</span> <span class="n">visible_devices</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;MIG-&quot;</span><span class="p">):</span>
                <span class="k">return</span> <span class="o">-</span><span class="mi">1</span>
            <span class="n">uuids</span> <span class="o">=</span> <span class="n">_raw_device_uuid_nvml</span><span class="p">()</span>
            <span class="k">if</span> <span class="n">uuids</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">return</span> <span class="o">-</span><span class="mi">1</span>
            <span class="n">visible_devices</span> <span class="o">=</span> <span class="n">_transform_uuid_to_ordinals</span><span class="p">(</span>
                <span class="n">cast</span><span class="p">(</span><span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="n">visible_devices</span><span class="p">),</span> <span class="n">uuids</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">raw_cnt</span> <span class="o">=</span> <span class="n">_raw_device_count_nvml</span><span class="p">()</span>
            <span class="k">if</span> <span class="n">raw_cnt</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">raw_cnt</span>
            <span class="c1"># Trim the list up to a maximum available device</span>
            <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">val</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">visible_devices</span><span class="p">):</span>
                <span class="k">if</span> <span class="n">cast</span><span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="n">val</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="n">raw_cnt</span><span class="p">:</span>
                    <span class="k">return</span> <span class="n">idx</span>
    <span class="k">except</span> <span class="ne">OSError</span><span class="p">:</span>
        <span class="k">return</span> <span class="o">-</span><span class="mi">1</span>
    <span class="k">except</span> <span class="ne">AttributeError</span><span class="p">:</span>
        <span class="k">return</span> <span class="o">-</span><span class="mi">1</span>
    <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="n">visible_devices</span><span class="p">)</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_get_nvml_device_index</span><span class="p">(</span><span class="n">device</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">Device</span><span class="p">]])</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Return the NVML index of the device, taking CUDA_VISIBLE_DEVICES into account.&quot;&quot;&quot;</span>
    <span class="n">idx</span> <span class="o">=</span> <span class="n">_get_device_index</span><span class="p">(</span><span class="n">device</span><span class="p">,</span> <span class="n">optional</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">visible_devices</span> <span class="o">=</span> <span class="n">_parse_visible_devices</span><span class="p">()</span>
    <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">visible_devices</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="ow">is</span> <span class="nb">str</span><span class="p">:</span>
        <span class="n">uuids</span> <span class="o">=</span> <span class="n">_raw_device_uuid_nvml</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">uuids</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;Can&#39;t get device UUIDs&quot;</span><span class="p">)</span>
        <span class="n">visible_devices</span> <span class="o">=</span> <span class="n">_transform_uuid_to_ordinals</span><span class="p">(</span>
            <span class="n">cast</span><span class="p">(</span><span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="n">visible_devices</span><span class="p">),</span> <span class="n">uuids</span>
        <span class="p">)</span>
    <span class="n">visible_devices</span> <span class="o">=</span> <span class="n">cast</span><span class="p">(</span><span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="n">visible_devices</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">idx</span> <span class="o">&lt;</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">idx</span> <span class="o">&gt;=</span> <span class="nb">len</span><span class="p">(</span><span class="n">visible_devices</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;device </span><span class="si">{</span><span class="n">idx</span><span class="si">}</span><span class="s2"> is not visible (CUDA_VISIBLE_DEVICES=</span><span class="si">{</span><span class="n">visible_devices</span><span class="si">}</span><span class="s2">)&quot;</span>
        <span class="p">)</span>
    <span class="k">return</span> <span class="n">visible_devices</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>


<span class="n">_cached_device_count</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>


<div class="viewcode-block" id="device_count">
<a class="viewcode-back" href="../../python-api/torch.cuda.device_count.html#torch.cuda.device_count">[docs]</a>
<span class="k">def</span><span class="w"> </span><span class="nf">device_count</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Return the number of GPUs available.&quot;&quot;&quot;</span>
    <span class="k">global</span> <span class="n">_cached_device_count</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">_is_compiled</span><span class="p">():</span>
        <span class="k">return</span> <span class="mi">0</span>
    <span class="k">if</span> <span class="n">_cached_device_count</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">_cached_device_count</span>
    <span class="c1"># bypass _device_count_nvml() if rocm (not supported)</span>
    <span class="n">nvml_count</span> <span class="o">=</span> <span class="n">_device_count_amdsmi</span><span class="p">()</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">version</span><span class="o">.</span><span class="n">hip</span> <span class="k">else</span> <span class="n">_device_count_nvml</span><span class="p">()</span>
    <span class="n">r</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_cuda_getDeviceCount</span><span class="p">()</span> <span class="k">if</span> <span class="n">nvml_count</span> <span class="o">&lt;</span> <span class="mi">0</span> <span class="k">else</span> <span class="n">nvml_count</span>
    <span class="c1"># NB: Do not cache the device count prior to CUDA initialization, because</span>
    <span class="c1"># the number of devices can change due to changes to CUDA_VISIBLE_DEVICES</span>
    <span class="c1"># setting prior to CUDA initialization.</span>
    <span class="k">if</span> <span class="n">_initialized</span><span class="p">:</span>
        <span class="n">_cached_device_count</span> <span class="o">=</span> <span class="n">r</span>
    <span class="k">return</span> <span class="n">r</span></div>



<div class="viewcode-block" id="get_arch_list">
<a class="viewcode-back" href="../../python-api/torch.cuda.get_arch_list.html#torch.cuda.get_arch_list">[docs]</a>
<span class="k">def</span><span class="w"> </span><span class="nf">get_arch_list</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">]:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Return list CUDA architectures this library was compiled for.&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">is_available</span><span class="p">():</span>
        <span class="k">return</span> <span class="p">[]</span>
    <span class="n">arch_flags</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_cuda_getArchFlags</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">arch_flags</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">return</span> <span class="p">[]</span>
    <span class="k">return</span> <span class="n">arch_flags</span><span class="o">.</span><span class="n">split</span><span class="p">()</span></div>



<div class="viewcode-block" id="get_gencode_flags">
<a class="viewcode-back" href="../../python-api/torch.cuda.get_gencode_flags.html#torch.cuda.get_gencode_flags">[docs]</a>
<span class="k">def</span><span class="w"> </span><span class="nf">get_gencode_flags</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Return NVCC gencode flags this library was compiled with.&quot;&quot;&quot;</span>
    <span class="n">arch_list</span> <span class="o">=</span> <span class="n">get_arch_list</span><span class="p">()</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">arch_list</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">return</span> <span class="s2">&quot;&quot;</span>
    <span class="n">arch_list_</span> <span class="o">=</span> <span class="p">[</span><span class="n">arch</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;_&quot;</span><span class="p">)</span> <span class="k">for</span> <span class="n">arch</span> <span class="ow">in</span> <span class="n">arch_list</span><span class="p">]</span>
    <span class="k">return</span> <span class="s2">&quot; &quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span>
        <span class="p">[</span>
            <span class="sa">f</span><span class="s2">&quot;-gencode compute=compute_</span><span class="si">{</span><span class="n">arch</span><span class="si">}</span><span class="s2">,code=</span><span class="si">{</span><span class="n">kind</span><span class="si">}</span><span class="s2">_</span><span class="si">{</span><span class="n">arch</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="k">for</span> <span class="p">(</span><span class="n">kind</span><span class="p">,</span> <span class="n">arch</span><span class="p">)</span> <span class="ow">in</span> <span class="n">arch_list_</span>
        <span class="p">]</span>
    <span class="p">)</span></div>



<div class="viewcode-block" id="current_device">
<a class="viewcode-back" href="../../generated/torch.cuda.current_device.html#torch.cuda.current_device">[docs]</a>
<span class="k">def</span><span class="w"> </span><span class="nf">current_device</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Return the index of a currently selected device.&quot;&quot;&quot;</span>
    <span class="n">_lazy_init</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_cuda_getDevice</span><span class="p">()</span></div>



<div class="viewcode-block" id="synchronize">
<a class="viewcode-back" href="../../generated/torch.cuda.synchronize.html#torch.cuda.synchronize">[docs]</a>
<span class="k">def</span><span class="w"> </span><span class="nf">synchronize</span><span class="p">(</span><span class="n">device</span><span class="p">:</span> <span class="n">_device_t</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Wait for all kernels in all streams on a CUDA device to complete.</span>

<span class="sd">    Args:</span>
<span class="sd">        device (torch.device or int, optional): device for which to synchronize.</span>
<span class="sd">            It uses the current device, given by :func:`~torch.cuda.current_device`,</span>
<span class="sd">            if :attr:`device` is ``None`` (default).</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_lazy_init</span><span class="p">()</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="n">device</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_cuda_synchronize</span><span class="p">()</span></div>



<div class="viewcode-block" id="ipc_collect">
<a class="viewcode-back" href="../../python-api/torch.cuda.ipc_collect.html#torch.cuda.ipc_collect">[docs]</a>
<span class="k">def</span><span class="w"> </span><span class="nf">ipc_collect</span><span class="p">():</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Force collects GPU memory after it has been released by CUDA IPC.</span>

<span class="sd">    .. note::</span>
<span class="sd">        Checks if any sent CUDA tensors could be cleaned from the memory. Force</span>
<span class="sd">        closes shared memory file used for reference counting if there is no</span>
<span class="sd">        active counters. Useful when the producer process stopped actively sending</span>
<span class="sd">        tensors and want to release unused memory.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_lazy_init</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_cuda_ipc_collect</span><span class="p">()</span></div>



<div class="viewcode-block" id="current_stream">
<a class="viewcode-back" href="../../python-api/torch.cuda.current_stream.html#torch.cuda.current_stream">[docs]</a>
<span class="k">def</span><span class="w"> </span><span class="nf">current_stream</span><span class="p">(</span><span class="n">device</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">_device_t</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Stream</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Return the currently selected :class:`Stream` for a given device.</span>

<span class="sd">    Args:</span>
<span class="sd">        device (torch.device or int, optional): selected device. Returns</span>
<span class="sd">            the currently selected :class:`Stream` for the current device, given</span>
<span class="sd">            by :func:`~torch.cuda.current_device`, if :attr:`device` is ``None``</span>
<span class="sd">            (default).</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_lazy_init</span><span class="p">()</span>
    <span class="n">streamdata</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_cuda_getCurrentStream</span><span class="p">(</span>
        <span class="n">_get_device_index</span><span class="p">(</span><span class="n">device</span><span class="p">,</span> <span class="n">optional</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">Stream</span><span class="p">(</span>
        <span class="n">stream_id</span><span class="o">=</span><span class="n">streamdata</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">device_index</span><span class="o">=</span><span class="n">streamdata</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">device_type</span><span class="o">=</span><span class="n">streamdata</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
    <span class="p">)</span></div>



<div class="viewcode-block" id="default_stream">
<a class="viewcode-back" href="../../python-api/torch.cuda.default_stream.html#torch.cuda.default_stream">[docs]</a>
<span class="k">def</span><span class="w"> </span><span class="nf">default_stream</span><span class="p">(</span><span class="n">device</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">_device_t</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Stream</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Return the default :class:`Stream` for a given device.</span>

<span class="sd">    Args:</span>
<span class="sd">        device (torch.device or int, optional): selected device. Returns</span>
<span class="sd">            the default :class:`Stream` for the current device, given by</span>
<span class="sd">            :func:`~torch.cuda.current_device`, if :attr:`device` is ``None``</span>
<span class="sd">            (default).</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_lazy_init</span><span class="p">()</span>
    <span class="n">streamdata</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_cuda_getDefaultStream</span><span class="p">(</span>
        <span class="n">_get_device_index</span><span class="p">(</span><span class="n">device</span><span class="p">,</span> <span class="n">optional</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">Stream</span><span class="p">(</span>
        <span class="n">stream_id</span><span class="o">=</span><span class="n">streamdata</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">device_index</span><span class="o">=</span><span class="n">streamdata</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">device_type</span><span class="o">=</span><span class="n">streamdata</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
    <span class="p">)</span></div>



<div class="viewcode-block" id="get_stream_from_external">
<a class="viewcode-back" href="../../python-api/torch.cuda.get_stream_from_external.html#torch.cuda.get_stream_from_external">[docs]</a>
<span class="k">def</span><span class="w"> </span><span class="nf">get_stream_from_external</span><span class="p">(</span>
    <span class="n">data_ptr</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">device</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">_device_t</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Stream</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Return a :class:`Stream` from an externally allocated CUDA stream.</span>

<span class="sd">    This function is used to wrap streams allocated in other libraries in order</span>
<span class="sd">    to facilitate data exchange and multi-library interactions.</span>

<span class="sd">    .. note:: This function doesn&#39;t manage the stream life-cycle, it is the user</span>
<span class="sd">       responsibility to keep the referenced stream alive while this returned</span>
<span class="sd">       stream is being used.</span>

<span class="sd">    Args:</span>
<span class="sd">        data_ptr(int): Integer representation of the `cudaStream_t` value that</span>
<span class="sd">            is allocated externally.</span>
<span class="sd">        device(torch.device or int, optional): the device where the stream</span>
<span class="sd">            was originally allocated. If device is specified incorrectly,</span>
<span class="sd">            subsequent launches using this stream may fail.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_lazy_init</span><span class="p">()</span>
    <span class="n">streamdata</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_cuda_getStreamFromExternal</span><span class="p">(</span>
        <span class="n">data_ptr</span><span class="p">,</span> <span class="n">_get_device_index</span><span class="p">(</span><span class="n">device</span><span class="p">,</span> <span class="n">optional</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">Stream</span><span class="p">(</span>
        <span class="n">stream_id</span><span class="o">=</span><span class="n">streamdata</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">device_index</span><span class="o">=</span><span class="n">streamdata</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">device_type</span><span class="o">=</span><span class="n">streamdata</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
    <span class="p">)</span></div>



<div class="viewcode-block" id="current_blas_handle">
<a class="viewcode-back" href="../../generated/torch.cuda.current_blas_handle.html#torch.cuda.current_blas_handle">[docs]</a>
<span class="k">def</span><span class="w"> </span><span class="nf">current_blas_handle</span><span class="p">():</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Return cublasHandle_t pointer to current cuBLAS handle&quot;&quot;&quot;</span>
    <span class="n">_lazy_init</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_cuda_getCurrentBlasHandle</span><span class="p">()</span></div>



<div class="viewcode-block" id="set_sync_debug_mode">
<a class="viewcode-back" href="../../python-api/torch.cuda.set_sync_debug_mode.html#torch.cuda.set_sync_debug_mode">[docs]</a>
<span class="k">def</span><span class="w"> </span><span class="nf">set_sync_debug_mode</span><span class="p">(</span><span class="n">debug_mode</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">str</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Set the debug mode for cuda synchronizing operations.</span>

<span class="sd">    Args:</span>
<span class="sd">        debug_mode(str or int): if &quot;default&quot; or 0, don&#39;t error or warn on synchronizing operations,</span>
<span class="sd">            if &quot;warn&quot; or 1, warn on synchronizing operations, if &quot;error&quot; or 2, error out synchronizing operations.</span>

<span class="sd">    Warning:</span>
<span class="sd">        This is an experimental feature, and not all synchronizing operations will trigger warning or error. In</span>
<span class="sd">        particular, operations in torch.distributed and torch.sparse namespaces are not covered yet.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_lazy_init</span><span class="p">()</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">debug_mode</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">debug_mode</span> <span class="o">==</span> <span class="s2">&quot;default&quot;</span><span class="p">:</span>
            <span class="n">debug_mode</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">elif</span> <span class="n">debug_mode</span> <span class="o">==</span> <span class="s2">&quot;warn&quot;</span><span class="p">:</span>
            <span class="n">debug_mode</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="k">elif</span> <span class="n">debug_mode</span> <span class="o">==</span> <span class="s2">&quot;error&quot;</span><span class="p">:</span>
            <span class="n">debug_mode</span> <span class="o">=</span> <span class="mi">2</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                <span class="s2">&quot;invalid value of debug_mode, expected one of `default`, `warn`, `error`&quot;</span>
            <span class="p">)</span>

    <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_cuda_set_sync_debug_mode</span><span class="p">(</span><span class="n">debug_mode</span><span class="p">)</span></div>



<div class="viewcode-block" id="get_sync_debug_mode">
<a class="viewcode-back" href="../../python-api/torch.cuda.get_sync_debug_mode.html#torch.cuda.get_sync_debug_mode">[docs]</a>
<span class="k">def</span><span class="w"> </span><span class="nf">get_sync_debug_mode</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Return current value of debug mode for cuda synchronizing operations.&quot;&quot;&quot;</span>
    <span class="n">_lazy_init</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_cuda_get_sync_debug_mode</span><span class="p">()</span></div>



<span class="k">def</span><span class="w"> </span><span class="nf">_get_pynvml_handler</span><span class="p">(</span><span class="n">device</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">Device</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">_HAS_PYNVML</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ModuleNotFoundError</span><span class="p">(</span>
            <span class="s2">&quot;pynvml does not seem to be installed or it can&#39;t be imported.&quot;</span>
        <span class="p">)</span> <span class="kn">from</span><span class="w"> </span><span class="nn">_PYNVML_ERR</span>
    <span class="kn">from</span><span class="w"> </span><span class="nn">pynvml</span><span class="w"> </span><span class="kn">import</span> <span class="n">NVMLError_DriverNotLoaded</span>

    <span class="k">try</span><span class="p">:</span>
        <span class="n">pynvml</span><span class="o">.</span><span class="n">nvmlInit</span><span class="p">()</span>
    <span class="k">except</span> <span class="n">NVMLError_DriverNotLoaded</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;cuda driver can&#39;t be loaded, is cuda enabled?&quot;</span><span class="p">)</span> <span class="kn">from</span><span class="w"> </span><span class="nn">e</span>

    <span class="n">device</span> <span class="o">=</span> <span class="n">_get_nvml_device_index</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="n">handle</span> <span class="o">=</span> <span class="n">pynvml</span><span class="o">.</span><span class="n">nvmlDeviceGetHandleByIndex</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">handle</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_get_amdsmi_handler</span><span class="p">(</span><span class="n">device</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">Device</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">_HAS_PYNVML</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ModuleNotFoundError</span><span class="p">(</span>
            <span class="s2">&quot;amdsmi does not seem to be installed or it can&#39;t be imported.&quot;</span>
        <span class="p">)</span> <span class="kn">from</span><span class="w"> </span><span class="nn">_PYNVML_ERR</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="n">amdsmi</span><span class="o">.</span><span class="n">amdsmi_init</span><span class="p">()</span>
    <span class="k">except</span> <span class="n">amdsmi</span><span class="o">.</span><span class="n">AmdSmiException</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
            <span class="s2">&quot;amdsmi driver can&#39;t be loaded, requires &gt;=ROCm5.6 installation&quot;</span>
        <span class="p">)</span> <span class="kn">from</span><span class="w"> </span><span class="nn">e</span>
    <span class="n">device</span> <span class="o">=</span> <span class="n">_get_amdsmi_device_index</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="n">handle</span> <span class="o">=</span> <span class="n">amdsmi</span><span class="o">.</span><span class="n">amdsmi_get_processor_handles</span><span class="p">()[</span><span class="n">device</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">handle</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_get_amdsmi_device_index</span><span class="p">(</span><span class="n">device</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">Device</span><span class="p">]])</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Return the amdsmi index of the device, taking visible_devices into account.&quot;&quot;&quot;</span>
    <span class="n">idx</span> <span class="o">=</span> <span class="n">_get_device_index</span><span class="p">(</span><span class="n">device</span><span class="p">,</span> <span class="n">optional</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">visible_devices</span> <span class="o">=</span> <span class="n">_parse_visible_devices</span><span class="p">()</span>
    <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">visible_devices</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="ow">is</span> <span class="nb">str</span><span class="p">:</span>
        <span class="n">uuids</span> <span class="o">=</span> <span class="n">_raw_device_uuid_amdsmi</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">uuids</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;Can&#39;t get device UUIDs&quot;</span><span class="p">)</span>
        <span class="n">visible_devices_str</span> <span class="o">=</span> <span class="n">cast</span><span class="p">(</span>
            <span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="n">visible_devices</span>
        <span class="p">)</span>  <span class="c1"># Create str variable for mypy</span>
        <span class="n">visible_devices</span> <span class="o">=</span> <span class="n">_transform_uuid_to_ordinals</span><span class="p">(</span><span class="n">visible_devices_str</span><span class="p">,</span> <span class="n">uuids</span><span class="p">)</span>
    <span class="n">idx_map</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="nb">enumerate</span><span class="p">(</span><span class="n">cast</span><span class="p">(</span><span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="n">visible_devices</span><span class="p">)))</span>
    <span class="k">if</span> <span class="n">idx</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">idx_map</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;device </span><span class="si">{</span><span class="n">idx</span><span class="si">}</span><span class="s2"> is not visible (HIP_VISIBLE_DEVICES=</span><span class="si">{</span><span class="n">visible_devices</span><span class="si">}</span><span class="s2">)&quot;</span>
        <span class="p">)</span>
    <span class="k">return</span> <span class="n">idx_map</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_get_amdsmi_device_memory_used</span><span class="p">(</span><span class="n">device</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">Device</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
    <span class="n">handle</span> <span class="o">=</span> <span class="n">_get_amdsmi_handler</span><span class="p">()</span>
    <span class="n">device</span> <span class="o">=</span> <span class="n">_get_amdsmi_device_index</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="c1"># amdsmi_get_gpu_vram_usage returns mem usage in megabytes</span>
    <span class="n">mem_mega_bytes</span> <span class="o">=</span> <span class="n">amdsmi</span><span class="o">.</span><span class="n">amdsmi_get_gpu_vram_usage</span><span class="p">(</span><span class="n">handle</span><span class="p">)[</span><span class="s2">&quot;vram_used&quot;</span><span class="p">]</span>
    <span class="n">mem_bytes</span> <span class="o">=</span> <span class="n">mem_mega_bytes</span> <span class="o">*</span> <span class="mi">1024</span> <span class="o">*</span> <span class="mi">1024</span>
    <span class="k">return</span> <span class="n">mem_bytes</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_get_amdsmi_memory_usage</span><span class="p">(</span><span class="n">device</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">Device</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
    <span class="n">handle</span> <span class="o">=</span> <span class="n">_get_amdsmi_handler</span><span class="p">()</span>
    <span class="n">device</span> <span class="o">=</span> <span class="n">_get_amdsmi_device_index</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="n">handle</span> <span class="o">=</span> <span class="n">amdsmi</span><span class="o">.</span><span class="n">amdsmi_get_processor_handles</span><span class="p">()[</span><span class="n">device</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">amdsmi</span><span class="o">.</span><span class="n">amdsmi_get_gpu_activity</span><span class="p">(</span><span class="n">handle</span><span class="p">)[</span><span class="s2">&quot;umc_activity&quot;</span><span class="p">]</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_get_amdsmi_utilization</span><span class="p">(</span><span class="n">device</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">Device</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
    <span class="n">handle</span> <span class="o">=</span> <span class="n">_get_amdsmi_handler</span><span class="p">()</span>
    <span class="n">device</span> <span class="o">=</span> <span class="n">_get_amdsmi_device_index</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="n">handle</span> <span class="o">=</span> <span class="n">amdsmi</span><span class="o">.</span><span class="n">amdsmi_get_processor_handles</span><span class="p">()[</span><span class="n">device</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">amdsmi</span><span class="o">.</span><span class="n">amdsmi_get_gpu_activity</span><span class="p">(</span><span class="n">handle</span><span class="p">)[</span><span class="s2">&quot;gfx_activity&quot;</span><span class="p">]</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_get_amdsmi_temperature</span><span class="p">(</span><span class="n">device</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">Device</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
    <span class="n">handle</span> <span class="o">=</span> <span class="n">_get_amdsmi_handler</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">amdsmi</span><span class="o">.</span><span class="n">amdsmi_get_temp_metric</span><span class="p">(</span>
        <span class="n">handle</span><span class="p">,</span>
        <span class="n">amdsmi</span><span class="o">.</span><span class="n">AmdSmiTemperatureType</span><span class="o">.</span><span class="n">JUNCTION</span><span class="p">,</span>
        <span class="n">amdsmi</span><span class="o">.</span><span class="n">AmdSmiTemperatureMetric</span><span class="o">.</span><span class="n">CURRENT</span><span class="p">,</span>
    <span class="p">)</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_get_amdsmi_power_draw</span><span class="p">(</span><span class="n">device</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">Device</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
    <span class="n">handle</span> <span class="o">=</span> <span class="n">_get_amdsmi_handler</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="n">socket_power</span> <span class="o">=</span> <span class="n">amdsmi</span><span class="o">.</span><span class="n">amdsmi_get_power_info</span><span class="p">(</span><span class="n">handle</span><span class="p">)[</span><span class="s2">&quot;average_socket_power&quot;</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">socket_power</span> <span class="o">!=</span> <span class="s2">&quot;N/A&quot;</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">socket_power</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">amdsmi</span><span class="o">.</span><span class="n">amdsmi_get_power_info</span><span class="p">(</span><span class="n">handle</span><span class="p">)[</span><span class="s2">&quot;current_socket_power&quot;</span><span class="p">]</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_get_amdsmi_clock_rate</span><span class="p">(</span><span class="n">device</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">Device</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
    <span class="n">handle</span> <span class="o">=</span> <span class="n">_get_amdsmi_handler</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="n">clock_info</span> <span class="o">=</span> <span class="n">amdsmi</span><span class="o">.</span><span class="n">amdsmi_get_clock_info</span><span class="p">(</span><span class="n">handle</span><span class="p">,</span> <span class="n">amdsmi</span><span class="o">.</span><span class="n">AmdSmiClkType</span><span class="o">.</span><span class="n">GFX</span><span class="p">)</span>
    <span class="k">if</span> <span class="s2">&quot;cur_clk&quot;</span> <span class="ow">in</span> <span class="n">clock_info</span><span class="p">:</span>  <span class="c1"># ROCm 6.2 deprecation</span>
        <span class="k">return</span> <span class="n">clock_info</span><span class="p">[</span><span class="s2">&quot;cur_clk&quot;</span><span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">clock_info</span><span class="p">[</span><span class="s2">&quot;clk&quot;</span><span class="p">]</span>


<div class="viewcode-block" id="device_memory_used">
<a class="viewcode-back" href="../../python-api/torch.cuda.device_memory_used.html#torch.cuda.device_memory_used">[docs]</a>
<span class="k">def</span><span class="w"> </span><span class="nf">device_memory_used</span><span class="p">(</span><span class="n">device</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">Device</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Return used global (device) memory in bytes as given by `nvidia-smi` or `amd-smi`.</span>

<span class="sd">    Args:</span>
<span class="sd">        device (torch.device or int, optional): selected device. Returns</span>
<span class="sd">            statistic for the current device, given by :func:`~torch.cuda.current_device`,</span>
<span class="sd">            if :attr:`device` is ``None`` (default).</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">version</span><span class="o">.</span><span class="n">hip</span><span class="p">:</span>
        <span class="n">handle</span> <span class="o">=</span> <span class="n">_get_pynvml_handler</span><span class="p">()</span>
        <span class="n">device</span> <span class="o">=</span> <span class="n">_get_nvml_device_index</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">handle</span> <span class="o">=</span> <span class="n">pynvml</span><span class="o">.</span><span class="n">nvmlDeviceGetHandleByIndex</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">pynvml</span><span class="o">.</span><span class="n">nvmlDeviceGetMemoryInfo</span><span class="p">(</span><span class="n">handle</span><span class="p">)</span><span class="o">.</span><span class="n">used</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">_get_amdsmi_device_memory_used</span><span class="p">(</span><span class="n">device</span><span class="p">)</span></div>



<div class="viewcode-block" id="memory_usage">
<a class="viewcode-back" href="../../generated/torch.cuda.memory_usage.html#torch.cuda.memory_usage">[docs]</a>
<span class="k">def</span><span class="w"> </span><span class="nf">memory_usage</span><span class="p">(</span><span class="n">device</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">Device</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Return the percent of time over the past sample period during which global (device)</span>
<span class="sd">    memory was being read or written as given by `nvidia-smi`.</span>

<span class="sd">    Args:</span>
<span class="sd">        device (torch.device or int, optional): selected device. Returns</span>
<span class="sd">            statistic for the current device, given by :func:`~torch.cuda.current_device`,</span>
<span class="sd">            if :attr:`device` is ``None`` (default).</span>

<span class="sd">    Warning: Each sample period may be between 1 second and 1/6 second,</span>
<span class="sd">    depending on the product being queried.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">version</span><span class="o">.</span><span class="n">hip</span><span class="p">:</span>
        <span class="n">handle</span> <span class="o">=</span> <span class="n">_get_pynvml_handler</span><span class="p">()</span>
        <span class="n">device</span> <span class="o">=</span> <span class="n">_get_nvml_device_index</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">handle</span> <span class="o">=</span> <span class="n">pynvml</span><span class="o">.</span><span class="n">nvmlDeviceGetHandleByIndex</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">pynvml</span><span class="o">.</span><span class="n">nvmlDeviceGetUtilizationRates</span><span class="p">(</span><span class="n">handle</span><span class="p">)</span><span class="o">.</span><span class="n">memory</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">_get_amdsmi_memory_usage</span><span class="p">(</span><span class="n">device</span><span class="p">)</span></div>



<div class="viewcode-block" id="utilization">
<a class="viewcode-back" href="../../python-api/torch.cuda.utilization.html#torch.cuda.utilization">[docs]</a>
<span class="k">def</span><span class="w"> </span><span class="nf">utilization</span><span class="p">(</span><span class="n">device</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">Device</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Return the percent of time over the past sample period during which one or</span>
<span class="sd">    more kernels was executing on the GPU as given by `nvidia-smi`.</span>

<span class="sd">    Args:</span>
<span class="sd">        device (torch.device or int, optional): selected device. Returns</span>
<span class="sd">            statistic for the current device, given by :func:`~torch.cuda.current_device`,</span>
<span class="sd">            if :attr:`device` is ``None`` (default).</span>

<span class="sd">    Warning: Each sample period may be between 1 second and 1/6 second,</span>
<span class="sd">    depending on the product being queried.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">version</span><span class="o">.</span><span class="n">hip</span><span class="p">:</span>
        <span class="n">handle</span> <span class="o">=</span> <span class="n">_get_pynvml_handler</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">device</span> <span class="o">=</span> <span class="n">_get_nvml_device_index</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">handle</span> <span class="o">=</span> <span class="n">pynvml</span><span class="o">.</span><span class="n">nvmlDeviceGetHandleByIndex</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">pynvml</span><span class="o">.</span><span class="n">nvmlDeviceGetUtilizationRates</span><span class="p">(</span><span class="n">handle</span><span class="p">)</span><span class="o">.</span><span class="n">gpu</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">_get_amdsmi_utilization</span><span class="p">(</span><span class="n">device</span><span class="p">)</span></div>



<div class="viewcode-block" id="temperature">
<a class="viewcode-back" href="../../python-api/torch.cuda.temperature.html#torch.cuda.temperature">[docs]</a>
<span class="k">def</span><span class="w"> </span><span class="nf">temperature</span><span class="p">(</span><span class="n">device</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">Device</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Return the average temperature of the GPU sensor in Degrees C (Centigrades).</span>

<span class="sd">    The average temperature is computed based on past sample period as given by `nvidia-smi`.</span>

<span class="sd">    Args:</span>
<span class="sd">        device (torch.device or int, optional): selected device. Returns</span>
<span class="sd">            statistic for the current device, given by :func:`~torch.cuda.current_device`,</span>
<span class="sd">            if :attr:`device` is ``None`` (default).</span>

<span class="sd">    Warning: Each sample period may be between 1 second and 1/6 second,</span>
<span class="sd">    depending on the product being queried.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">version</span><span class="o">.</span><span class="n">hip</span><span class="p">:</span>
        <span class="n">handle</span> <span class="o">=</span> <span class="n">_get_pynvml_handler</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="c1"># 0 refers to the temperature sensor for the GPU die.</span>
        <span class="k">return</span> <span class="n">pynvml</span><span class="o">.</span><span class="n">nvmlDeviceGetTemperature</span><span class="p">(</span><span class="n">handle</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">_get_amdsmi_temperature</span><span class="p">(</span><span class="n">device</span><span class="p">)</span></div>



<div class="viewcode-block" id="power_draw">
<a class="viewcode-back" href="../../generated/torch.cuda.power_draw.html#torch.cuda.power_draw">[docs]</a>
<span class="k">def</span><span class="w"> </span><span class="nf">power_draw</span><span class="p">(</span><span class="n">device</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">Device</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Return the average power draw of the GPU sensor in mW (MilliWatts)</span>
<span class="sd">        over the past sample period as given by `nvidia-smi` for Fermi or newer fully supported devices.</span>

<span class="sd">    Args:</span>
<span class="sd">        device (torch.device or int, optional): selected device. Returns</span>
<span class="sd">            statistic for the current device, given by :func:`~torch.cuda.current_device`,</span>
<span class="sd">            if :attr:`device` is ``None`` (default).</span>

<span class="sd">    Warning: Each sample period may be between 1 second and 1/6 second,</span>
<span class="sd">    depending on the product being queried.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">version</span><span class="o">.</span><span class="n">hip</span><span class="p">:</span>
        <span class="n">handle</span> <span class="o">=</span> <span class="n">_get_pynvml_handler</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">pynvml</span><span class="o">.</span><span class="n">nvmlDeviceGetPowerUsage</span><span class="p">(</span><span class="n">handle</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">_get_amdsmi_power_draw</span><span class="p">(</span><span class="n">device</span><span class="p">)</span></div>



<div class="viewcode-block" id="clock_rate">
<a class="viewcode-back" href="../../python-api/torch.cuda.clock_rate.html#torch.cuda.clock_rate">[docs]</a>
<span class="k">def</span><span class="w"> </span><span class="nf">clock_rate</span><span class="p">(</span><span class="n">device</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">Device</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Return the clock speed of the GPU SM in Hz Hertz over the past sample period as given by `nvidia-smi`.</span>

<span class="sd">    Args:</span>
<span class="sd">        device (torch.device or int, optional): selected device. Returns</span>
<span class="sd">            statistic for the current device, given by :func:`~torch.cuda.current_device`,</span>
<span class="sd">            if :attr:`device` is ``None`` (default).</span>

<span class="sd">    Warning: Each sample period may be between 1 second and 1/6 second,</span>
<span class="sd">    depending on the product being queried.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">version</span><span class="o">.</span><span class="n">hip</span><span class="p">:</span>
        <span class="n">handle</span> <span class="o">=</span> <span class="n">_get_pynvml_handler</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">pynvml</span><span class="o">.</span><span class="n">nvmlDeviceGetClockInfo</span><span class="p">(</span><span class="n">handle</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">_get_amdsmi_clock_rate</span><span class="p">(</span><span class="n">device</span><span class="p">)</span></div>



<span class="k">def</span><span class="w"> </span><span class="nf">_get_device</span><span class="p">(</span><span class="n">device</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Return the torch.device type object from the passed in device.</span>

<span class="sd">    Args:</span>
<span class="sd">        device (torch.device or int): selected device.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">device</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
        <span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">device</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
        <span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">device</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_get_generator</span><span class="p">(</span><span class="n">device</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">Generator</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Return the CUDA Generator object for the given device.</span>

<span class="sd">    Args:</span>
<span class="sd">        device (torch.device): selected device.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">idx</span> <span class="o">=</span> <span class="n">device</span><span class="o">.</span><span class="n">index</span>
    <span class="k">if</span> <span class="n">idx</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">idx</span> <span class="o">=</span> <span class="n">current_device</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">default_generators</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_set_rng_state_offset</span><span class="p">(</span>
    <span class="n">offset</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">device</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;cuda&quot;</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Set the random number generator state offset of the specified GPU.</span>

<span class="sd">    Args:</span>
<span class="sd">        offset (int): The desired offset</span>
<span class="sd">        device (torch.device or int, optional): The device to set the RNG state.</span>
<span class="sd">            Default: ``&#39;cuda&#39;`` (i.e., ``torch.device(&#39;cuda&#39;)``, the current CUDA device).</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">final_device</span> <span class="o">=</span> <span class="n">_get_device</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">cb</span><span class="p">():</span>
        <span class="n">default_generator</span> <span class="o">=</span> <span class="n">_get_generator</span><span class="p">(</span><span class="n">final_device</span><span class="p">)</span>
        <span class="n">default_generator</span><span class="o">.</span><span class="n">set_offset</span><span class="p">(</span><span class="n">offset</span><span class="p">)</span>

    <span class="n">_lazy_call</span><span class="p">(</span><span class="n">cb</span><span class="p">)</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_get_rng_state_offset</span><span class="p">(</span><span class="n">device</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;cuda&quot;</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Return the random number generator state offset of the specified GPU.</span>

<span class="sd">    Args:</span>
<span class="sd">        device (torch.device or int, optional): The device to return the RNG state offset of.</span>
<span class="sd">            Default: ``&#39;cuda&#39;`` (i.e., ``torch.device(&#39;cuda&#39;)``, the current CUDA device).</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This function eagerly initializes CUDA.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_lazy_init</span><span class="p">()</span>
    <span class="n">final_device</span> <span class="o">=</span> <span class="n">_get_device</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="n">default_generator</span> <span class="o">=</span> <span class="n">_get_generator</span><span class="p">(</span><span class="n">final_device</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">default_generator</span><span class="o">.</span><span class="n">get_offset</span><span class="p">()</span>


<span class="kn">from</span><span class="w"> </span><span class="nn">.memory</span><span class="w"> </span><span class="kn">import</span> <span class="o">*</span>  <span class="c1"># noqa: F403</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">.random</span><span class="w"> </span><span class="kn">import</span> <span class="o">*</span>  <span class="c1"># noqa: F403</span>


<span class="c1">################################################################################</span>
<span class="c1"># Define Storage and Tensor classes</span>
<span class="c1">################################################################################</span>


<span class="nd">@staticmethod</span>  <span class="c1"># type: ignore[misc]</span>
<span class="k">def</span><span class="w"> </span><span class="nf">_lazy_new</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="n">_lazy_init</span><span class="p">()</span>
    <span class="c1"># We may need to call lazy init again if we are a forked child</span>
    <span class="c1"># del _CudaBase.__new__</span>
    <span class="k">return</span> <span class="nb">super</span><span class="p">(</span><span class="n">_CudaBase</span><span class="p">,</span> <span class="bp">cls</span><span class="p">)</span><span class="o">.</span><span class="fm">__new__</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>


<span class="k">class</span><span class="w"> </span><span class="nc">_CudaBase</span><span class="p">:</span>
    <span class="n">is_cuda</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="n">is_sparse</span> <span class="o">=</span> <span class="kc">False</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">type</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="c1"># We could use a Protocol here to tell mypy that self has `get_device` method</span>
        <span class="c1"># but it is only available in the typing module on Python &gt;= 3.8</span>
        <span class="c1"># or on typing_extensions module on Python &gt;= 3.6</span>
        <span class="k">with</span> <span class="n">device</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">get_device</span><span class="p">()):</span>  <span class="c1"># type: ignore[attr-defined]</span>
            <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">type</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>  <span class="c1"># type: ignore[misc]</span>

    <span class="fm">__new__</span> <span class="o">=</span> <span class="n">_lazy_new</span>


<span class="kn">from</span><span class="w"> </span><span class="nn">torch.storage</span><span class="w"> </span><span class="kn">import</span> <span class="n">_LegacyStorage</span><span class="p">,</span> <span class="n">_warn_typed_storage_removal</span>


<span class="k">class</span><span class="w"> </span><span class="nc">_CudaLegacyStorage</span><span class="p">(</span><span class="n">_LegacyStorage</span><span class="p">):</span>
    <span class="nd">@classmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">from_buffer</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="n">_warn_typed_storage_removal</span><span class="p">()</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;from_buffer: Not available for CUDA storage&quot;</span><span class="p">)</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">_new_with_weak_ptr</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;_new_with_weak_ptr: Not available for CUDA storage&quot;</span><span class="p">)</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">_new_shared_filename</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">manager</span><span class="p">,</span> <span class="n">obj</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;_new_shared_filename: Not available for CUDA storage&quot;</span><span class="p">)</span>


<span class="k">class</span><span class="w"> </span><span class="nc">ByteStorage</span><span class="p">(</span><span class="n">_CudaLegacyStorage</span><span class="p">):</span>
    <span class="nd">@classproperty</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">_warn_typed_storage_removal</span><span class="p">()</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dtype</span>

    <span class="nd">@classproperty</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">uint8</span>


<span class="k">class</span><span class="w"> </span><span class="nc">DoubleStorage</span><span class="p">(</span><span class="n">_CudaLegacyStorage</span><span class="p">):</span>
    <span class="nd">@classproperty</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">_warn_typed_storage_removal</span><span class="p">()</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dtype</span>

    <span class="nd">@classproperty</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">double</span>


<span class="k">class</span><span class="w"> </span><span class="nc">FloatStorage</span><span class="p">(</span><span class="n">_CudaLegacyStorage</span><span class="p">):</span>
    <span class="nd">@classproperty</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">_warn_typed_storage_removal</span><span class="p">()</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dtype</span>

    <span class="nd">@classproperty</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">float</span>


<span class="k">class</span><span class="w"> </span><span class="nc">HalfStorage</span><span class="p">(</span><span class="n">_CudaLegacyStorage</span><span class="p">):</span>
    <span class="nd">@classproperty</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">_warn_typed_storage_removal</span><span class="p">()</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dtype</span>

    <span class="nd">@classproperty</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">half</span>


<span class="k">class</span><span class="w"> </span><span class="nc">LongStorage</span><span class="p">(</span><span class="n">_CudaLegacyStorage</span><span class="p">):</span>
    <span class="nd">@classproperty</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">_warn_typed_storage_removal</span><span class="p">()</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dtype</span>

    <span class="nd">@classproperty</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">long</span>


<span class="k">class</span><span class="w"> </span><span class="nc">IntStorage</span><span class="p">(</span><span class="n">_CudaLegacyStorage</span><span class="p">):</span>
    <span class="nd">@classproperty</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">_warn_typed_storage_removal</span><span class="p">()</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dtype</span>

    <span class="nd">@classproperty</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">int</span>


<span class="k">class</span><span class="w"> </span><span class="nc">ShortStorage</span><span class="p">(</span><span class="n">_CudaLegacyStorage</span><span class="p">):</span>
    <span class="nd">@classproperty</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">_warn_typed_storage_removal</span><span class="p">()</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dtype</span>

    <span class="nd">@classproperty</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">short</span>


<span class="k">class</span><span class="w"> </span><span class="nc">CharStorage</span><span class="p">(</span><span class="n">_CudaLegacyStorage</span><span class="p">):</span>
    <span class="nd">@classproperty</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">_warn_typed_storage_removal</span><span class="p">()</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dtype</span>

    <span class="nd">@classproperty</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">int8</span>


<span class="k">class</span><span class="w"> </span><span class="nc">BoolStorage</span><span class="p">(</span><span class="n">_CudaLegacyStorage</span><span class="p">):</span>
    <span class="nd">@classproperty</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">_warn_typed_storage_removal</span><span class="p">()</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dtype</span>

    <span class="nd">@classproperty</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">bool</span>


<span class="k">class</span><span class="w"> </span><span class="nc">BFloat16Storage</span><span class="p">(</span><span class="n">_CudaLegacyStorage</span><span class="p">):</span>
    <span class="nd">@classproperty</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">_warn_typed_storage_removal</span><span class="p">()</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dtype</span>

    <span class="nd">@classproperty</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span>


<span class="k">class</span><span class="w"> </span><span class="nc">ComplexDoubleStorage</span><span class="p">(</span><span class="n">_CudaLegacyStorage</span><span class="p">):</span>
    <span class="nd">@classproperty</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">_warn_typed_storage_removal</span><span class="p">()</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dtype</span>

    <span class="nd">@classproperty</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">cdouble</span>


<span class="k">class</span><span class="w"> </span><span class="nc">ComplexFloatStorage</span><span class="p">(</span><span class="n">_CudaLegacyStorage</span><span class="p">):</span>
    <span class="nd">@classproperty</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">_warn_typed_storage_removal</span><span class="p">()</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dtype</span>

    <span class="nd">@classproperty</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">cfloat</span>


<span class="k">del</span> <span class="n">_LegacyStorage</span>
<span class="k">del</span> <span class="n">_CudaLegacyStorage</span>

<span class="n">torch</span><span class="o">.</span><span class="n">_storage_classes</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">DoubleStorage</span><span class="p">)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">_storage_classes</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">FloatStorage</span><span class="p">)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">_storage_classes</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">LongStorage</span><span class="p">)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">_storage_classes</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">IntStorage</span><span class="p">)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">_storage_classes</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">ShortStorage</span><span class="p">)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">_storage_classes</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">CharStorage</span><span class="p">)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">_storage_classes</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">ByteStorage</span><span class="p">)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">_storage_classes</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">HalfStorage</span><span class="p">)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">_storage_classes</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">BoolStorage</span><span class="p">)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">_storage_classes</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">BFloat16Storage</span><span class="p">)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">_storage_classes</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">ComplexDoubleStorage</span><span class="p">)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">_storage_classes</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">ComplexFloatStorage</span><span class="p">)</span>


<span class="k">class</span><span class="w"> </span><span class="nc">_WrappedTritonKernel</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Just a simple wrapper to store some metadata for testing purposes.&quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">kernel</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">kernel</span> <span class="o">=</span> <span class="n">kernel</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">kernel_invoked</span> <span class="o">=</span> <span class="kc">False</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="n">res</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">kernel_invoked</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="k">return</span> <span class="n">res</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_register_triton_kernels</span><span class="p">():</span>
    <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">_running_with_deploy</span><span class="p">():</span>
        <span class="k">return</span>

    <span class="nd">@_WrappedTritonKernel</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">kernel_impl</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="kn">from</span><span class="w"> </span><span class="nn">torch.sparse._triton_ops</span><span class="w"> </span><span class="kn">import</span> <span class="n">bsr_dense_mm</span>

        <span class="k">return</span> <span class="n">bsr_dense_mm</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="n">skip_checks</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="nd">@_WrappedTritonKernel</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">addmm_kernel_impl</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="kn">from</span><span class="w"> </span><span class="nn">torch.sparse._triton_ops</span><span class="w"> </span><span class="kn">import</span> <span class="n">bsr_dense_addmm</span>

        <span class="k">return</span> <span class="n">bsr_dense_addmm</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="n">skip_checks</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="n">has_triton</span> <span class="o">=</span> <span class="n">importlib</span><span class="o">.</span><span class="n">util</span><span class="o">.</span><span class="n">find_spec</span><span class="p">(</span><span class="s2">&quot;triton&quot;</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
    <span class="k">if</span> <span class="n">has_triton</span><span class="p">:</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">_TritonLibrary</span><span class="o">.</span><span class="n">registerOp</span><span class="p">(</span>
            <span class="s2">&quot;_triton_bsr_dense_mm_out&quot;</span><span class="p">,</span>
            <span class="s2">&quot;_triton_bsr_dense_mm_out(Tensor bsr, Tensor dense, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;</span><span class="p">,</span>
            <span class="n">kernel_impl</span><span class="p">,</span>
            <span class="s2">&quot;SparseCsrCUDA&quot;</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="n">torch</span><span class="o">.</span><span class="n">_TritonLibrary</span><span class="o">.</span><span class="n">registerOp</span><span class="p">(</span>
            <span class="s2">&quot;_triton_bsr_dense_addmm_out&quot;</span><span class="p">,</span>
            <span class="p">(</span>
                <span class="s2">&quot;_triton_bsr_dense_addmm_out(Tensor input, Tensor bsr, Tensor dense,&quot;</span>
                <span class="s2">&quot; *, Scalar beta, Scalar alpha, Tensor(a!) out) -&gt; Tensor(a!)&quot;</span>
            <span class="p">),</span>
            <span class="n">addmm_kernel_impl</span><span class="p">,</span>
            <span class="s2">&quot;SparseCsrCUDA&quot;</span><span class="p">,</span>
        <span class="p">)</span>


<span class="n">_lazy_call</span><span class="p">(</span><span class="n">_register_triton_kernels</span><span class="p">)</span>


<span class="kn">from</span><span class="w"> </span><span class="nn">.</span><span class="w"> </span><span class="kn">import</span> <span class="n">amp</span><span class="p">,</span> <span class="n">jiterator</span><span class="p">,</span> <span class="n">nvtx</span><span class="p">,</span> <span class="n">profiler</span><span class="p">,</span> <span class="n">sparse</span><span class="p">,</span> <span class="n">tunable</span>


<span class="n">__all__</span> <span class="o">=</span> <span class="p">[</span>
    <span class="c1"># Typed storage and tensors</span>
    <span class="s2">&quot;BFloat16Storage&quot;</span><span class="p">,</span>
    <span class="s2">&quot;BFloat16Tensor&quot;</span><span class="p">,</span>
    <span class="s2">&quot;BoolStorage&quot;</span><span class="p">,</span>
    <span class="s2">&quot;BoolTensor&quot;</span><span class="p">,</span>
    <span class="s2">&quot;ByteStorage&quot;</span><span class="p">,</span>
    <span class="s2">&quot;ByteTensor&quot;</span><span class="p">,</span>
    <span class="s2">&quot;CharStorage&quot;</span><span class="p">,</span>
    <span class="s2">&quot;CharTensor&quot;</span><span class="p">,</span>
    <span class="s2">&quot;ComplexDoubleStorage&quot;</span><span class="p">,</span>
    <span class="s2">&quot;ComplexFloatStorage&quot;</span><span class="p">,</span>
    <span class="s2">&quot;DoubleStorage&quot;</span><span class="p">,</span>
    <span class="s2">&quot;DoubleTensor&quot;</span><span class="p">,</span>
    <span class="s2">&quot;FloatStorage&quot;</span><span class="p">,</span>
    <span class="s2">&quot;FloatTensor&quot;</span><span class="p">,</span>
    <span class="s2">&quot;HalfStorage&quot;</span><span class="p">,</span>
    <span class="s2">&quot;HalfTensor&quot;</span><span class="p">,</span>
    <span class="s2">&quot;IntStorage&quot;</span><span class="p">,</span>
    <span class="s2">&quot;IntTensor&quot;</span><span class="p">,</span>
    <span class="s2">&quot;LongStorage&quot;</span><span class="p">,</span>
    <span class="s2">&quot;LongTensor&quot;</span><span class="p">,</span>
    <span class="s2">&quot;ShortStorage&quot;</span><span class="p">,</span>
    <span class="s2">&quot;ShortTensor&quot;</span><span class="p">,</span>
    <span class="s2">&quot;CUDAGraph&quot;</span><span class="p">,</span>
    <span class="s2">&quot;CudaError&quot;</span><span class="p">,</span>
    <span class="s2">&quot;DeferredCudaCallError&quot;</span><span class="p">,</span>
    <span class="s2">&quot;Event&quot;</span><span class="p">,</span>
    <span class="s2">&quot;ExternalStream&quot;</span><span class="p">,</span>
    <span class="s2">&quot;Stream&quot;</span><span class="p">,</span>
    <span class="s2">&quot;StreamContext&quot;</span><span class="p">,</span>
    <span class="s2">&quot;amp&quot;</span><span class="p">,</span>
    <span class="s2">&quot;caching_allocator_alloc&quot;</span><span class="p">,</span>
    <span class="s2">&quot;caching_allocator_delete&quot;</span><span class="p">,</span>
    <span class="s2">&quot;caching_allocator_enable&quot;</span><span class="p">,</span>
    <span class="s2">&quot;can_device_access_peer&quot;</span><span class="p">,</span>
    <span class="s2">&quot;check_error&quot;</span><span class="p">,</span>
    <span class="s2">&quot;cudaStatus&quot;</span><span class="p">,</span>
    <span class="s2">&quot;cudart&quot;</span><span class="p">,</span>
    <span class="s2">&quot;current_blas_handle&quot;</span><span class="p">,</span>
    <span class="s2">&quot;current_device&quot;</span><span class="p">,</span>
    <span class="s2">&quot;current_stream&quot;</span><span class="p">,</span>
    <span class="s2">&quot;default_generators&quot;</span><span class="p">,</span>
    <span class="s2">&quot;default_stream&quot;</span><span class="p">,</span>
    <span class="s2">&quot;device&quot;</span><span class="p">,</span>
    <span class="s2">&quot;device_count&quot;</span><span class="p">,</span>
    <span class="s2">&quot;device_memory_used&quot;</span><span class="p">,</span>
    <span class="s2">&quot;device_of&quot;</span><span class="p">,</span>
    <span class="s2">&quot;empty_cache&quot;</span><span class="p">,</span>
    <span class="s2">&quot;get_allocator_backend&quot;</span><span class="p">,</span>
    <span class="s2">&quot;CUDAPluggableAllocator&quot;</span><span class="p">,</span>
    <span class="s2">&quot;change_current_allocator&quot;</span><span class="p">,</span>
    <span class="s2">&quot;get_arch_list&quot;</span><span class="p">,</span>
    <span class="s2">&quot;get_device_capability&quot;</span><span class="p">,</span>
    <span class="s2">&quot;get_device_name&quot;</span><span class="p">,</span>
    <span class="s2">&quot;get_device_properties&quot;</span><span class="p">,</span>
    <span class="s2">&quot;get_gencode_flags&quot;</span><span class="p">,</span>
    <span class="s2">&quot;get_per_process_memory_fraction&quot;</span><span class="p">,</span>
    <span class="s2">&quot;get_rng_state&quot;</span><span class="p">,</span>
    <span class="s2">&quot;get_rng_state_all&quot;</span><span class="p">,</span>
    <span class="s2">&quot;get_stream_from_external&quot;</span><span class="p">,</span>
    <span class="s2">&quot;get_sync_debug_mode&quot;</span><span class="p">,</span>
    <span class="s2">&quot;graph&quot;</span><span class="p">,</span>
    <span class="s2">&quot;graph_pool_handle&quot;</span><span class="p">,</span>
    <span class="s2">&quot;graphs&quot;</span><span class="p">,</span>
    <span class="s2">&quot;has_half&quot;</span><span class="p">,</span>
    <span class="s2">&quot;has_magma&quot;</span><span class="p">,</span>
    <span class="s2">&quot;init&quot;</span><span class="p">,</span>
    <span class="s2">&quot;initial_seed&quot;</span><span class="p">,</span>
    <span class="s2">&quot;ipc_collect&quot;</span><span class="p">,</span>
    <span class="s2">&quot;is_available&quot;</span><span class="p">,</span>
    <span class="s2">&quot;is_bf16_supported&quot;</span><span class="p">,</span>
    <span class="s2">&quot;is_current_stream_capturing&quot;</span><span class="p">,</span>
    <span class="s2">&quot;is_initialized&quot;</span><span class="p">,</span>
    <span class="s2">&quot;jiterator&quot;</span><span class="p">,</span>
    <span class="s2">&quot;list_gpu_processes&quot;</span><span class="p">,</span>
    <span class="s2">&quot;make_graphed_callables&quot;</span><span class="p">,</span>
    <span class="s2">&quot;manual_seed&quot;</span><span class="p">,</span>
    <span class="s2">&quot;manual_seed_all&quot;</span><span class="p">,</span>
    <span class="s2">&quot;max_memory_allocated&quot;</span><span class="p">,</span>
    <span class="s2">&quot;max_memory_cached&quot;</span><span class="p">,</span>
    <span class="s2">&quot;max_memory_reserved&quot;</span><span class="p">,</span>
    <span class="s2">&quot;mem_get_info&quot;</span><span class="p">,</span>
    <span class="s2">&quot;memory&quot;</span><span class="p">,</span>
    <span class="s2">&quot;memory_allocated&quot;</span><span class="p">,</span>
    <span class="s2">&quot;memory_cached&quot;</span><span class="p">,</span>
    <span class="s2">&quot;memory_reserved&quot;</span><span class="p">,</span>
    <span class="s2">&quot;memory_snapshot&quot;</span><span class="p">,</span>
    <span class="s2">&quot;memory_stats&quot;</span><span class="p">,</span>
    <span class="s2">&quot;memory_stats_as_nested_dict&quot;</span><span class="p">,</span>
    <span class="s2">&quot;memory_summary&quot;</span><span class="p">,</span>
    <span class="s2">&quot;memory_usage&quot;</span><span class="p">,</span>
    <span class="s2">&quot;MemPool&quot;</span><span class="p">,</span>
    <span class="s2">&quot;MemPoolContext&quot;</span><span class="p">,</span>
    <span class="s2">&quot;use_mem_pool&quot;</span><span class="p">,</span>
    <span class="s2">&quot;temperature&quot;</span><span class="p">,</span>
    <span class="s2">&quot;power_draw&quot;</span><span class="p">,</span>
    <span class="s2">&quot;clock_rate&quot;</span><span class="p">,</span>
    <span class="s2">&quot;nccl&quot;</span><span class="p">,</span>
    <span class="s2">&quot;nvtx&quot;</span><span class="p">,</span>
    <span class="s2">&quot;profiler&quot;</span><span class="p">,</span>
    <span class="s2">&quot;random&quot;</span><span class="p">,</span>
    <span class="s2">&quot;reset_accumulated_memory_stats&quot;</span><span class="p">,</span>
    <span class="s2">&quot;reset_max_memory_allocated&quot;</span><span class="p">,</span>
    <span class="s2">&quot;reset_max_memory_cached&quot;</span><span class="p">,</span>
    <span class="s2">&quot;reset_peak_memory_stats&quot;</span><span class="p">,</span>
    <span class="s2">&quot;seed&quot;</span><span class="p">,</span>
    <span class="s2">&quot;seed_all&quot;</span><span class="p">,</span>
    <span class="s2">&quot;set_device&quot;</span><span class="p">,</span>
    <span class="s2">&quot;set_per_process_memory_fraction&quot;</span><span class="p">,</span>
    <span class="s2">&quot;set_rng_state&quot;</span><span class="p">,</span>
    <span class="s2">&quot;set_rng_state_all&quot;</span><span class="p">,</span>
    <span class="s2">&quot;set_stream&quot;</span><span class="p">,</span>
    <span class="s2">&quot;set_sync_debug_mode&quot;</span><span class="p">,</span>
    <span class="s2">&quot;sparse&quot;</span><span class="p">,</span>
    <span class="s2">&quot;stream&quot;</span><span class="p">,</span>
    <span class="s2">&quot;streams&quot;</span><span class="p">,</span>
    <span class="s2">&quot;synchronize&quot;</span><span class="p">,</span>
    <span class="s2">&quot;tunable&quot;</span><span class="p">,</span>
    <span class="s2">&quot;utilization&quot;</span><span class="p">,</span>
<span class="p">]</span>
</pre></div>

                </article>
              
              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
</div>
                </footer>
              
            </div>
            
            
              
            
          </div>
          <footer class="bd-footer-content">
            
          </footer>
        
      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script defer src="../../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf"></script>
<script defer src="../../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf"></script>



  <footer class="bd-footer">
<div class="bd-footer__inner bd-page-width">
  
    <div class="footer-items__start">
      
        <div class="footer-item">

  <p class="copyright">
    
      © Copyright PyTorch Contributors.
      <br/>
    
  </p>
</div>
      
        <div class="footer-item">

  <p class="sphinx-version">
    Created using <a href="https://www.sphinx-doc.org/">Sphinx</a> 8.1.0.
    <br/>
  </p>
</div>
      
    </div>
  
  
  
    <div class="footer-items__end">
      
        <div class="footer-item">
<p class="theme-version">
  <!-- # L10n: Setting the PST URL as an argument as this does not need to be localized -->
  Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.16.1.
</p></div>
      
    </div>
  
</div>

  </footer>
  </body>
</html>