
<!DOCTYPE html>


<html lang="en" data-content_root="../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>torch._tensor &#8212; PyTorch main documentation</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  <!--
    this give us a css class that will be invisible only if js is disabled
  -->
  <noscript>
    <style>
      .pst-js-only { display: none !important; }

    </style>
  </noscript>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css?v=949a1ff5" />
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/katex.min.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/katex-math.css?v=91adb8b6" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="../../_static/css/jit.css?v=8de1ea5d" />
    <link rel="stylesheet" type="text/css" href="../../_static/css/custom.css?v=3ccf5357" />
    <link rel="stylesheet" type="text/css" href="../../_static/css/custom2.css?v=dd3e252d" />
  
  <!-- So that users can add custom icons -->
  <script src="../../_static/scripts/fontawesome.js?digest=8878045cc6db502f8baf"></script>
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf" />

    <script src="../../_static/documentation_options.js?v=a8da1a53"></script>
    <script src="../../_static/doctools.js?v=9bcbadda"></script>
    <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../_static/copybutton.js?v=940804e7"></script>
    <script src="../../_static/design-tabs.js?v=f930bc37"></script>
    <script type="module" src="https://cdn.jsdelivr.net/npm/mermaid@11.2.0/dist/mermaid.esm.min.mjs"></script>
    <script type="module" src="https://cdn.jsdelivr.net/npm/@mermaid-js/layout-elk@0.1.4/dist/mermaid-layout-elk.esm.min.mjs"></script>
    <script type="module">import mermaid from "https://cdn.jsdelivr.net/npm/mermaid@11.2.0/dist/mermaid.esm.min.mjs";import elkLayouts from "https://cdn.jsdelivr.net/npm/@mermaid-js/layout-elk@0.1.4/dist/mermaid-layout-elk.esm.min.mjs";mermaid.registerLayoutLoaders(elkLayouts);mermaid.initialize({startOnLoad:false});</script>
    <script src="https://cdn.jsdelivr.net/npm/d3@7.9.0/dist/d3.min.js"></script>
    <script type="module">
import mermaid from "https://cdn.jsdelivr.net/npm/mermaid@11.2.0/dist/mermaid.esm.min.mjs";
window.addEventListener("load", () => mermaid.run());
</script>
    <script>DOCUMENTATION_OPTIONS.pagename = '_modules/torch/_tensor';</script>
    <script src="../../_static/js/star-rating.js?v=8861fcb6"></script>
    <script src="../../_static/js/send-feedback.js?v=5646bf45"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />

  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  <meta name="docsearch:version" content="main (2.6.0 )" />
<script type="text/javascript" src="../../_static/js/send-feedback.js"></script>
<script type="text/javascript" src="../../_static/js/star-rating.js"></script>
<script src="https://code.jquery.com/jquery-3.7.1.min.js"></script>

<script async src="https://www.googletagmanager.com/gtag/js?id=G-TEST12345"></script>
    <script type="text/javascript">
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-TEST12345');
    </script>

  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
<body data-feedback-url="https://github.com/pytorch/pytorch">
  <div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>

          <li class="main-menu-item">
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Learn
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="">
                  <span class=dropdown-title>Get Started</span>
                  <p>Run PyTorch locally or get started quickly with one of the supported cloud platforms</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Tutorials</span>
                  <p>Whats new in PyTorch tutorials</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Learn the Basics</span>
                  <p>Familiarize yourself with PyTorch concepts and modules</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">PyTorch Recipes</span>
                  <p>Bite-size, ready-to-deploy PyTorch code examples</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Intro to PyTorch - YouTube Series</span>
                  <p>Master PyTorch basics with our engaging YouTube tutorial series</p>
                </a>
              </div>
            </div>
          </li>

          <li>
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Ecosystem
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Tools</span>
                  <p>Learn about the tools and frameworks in the PyTorch Ecosystem</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class=dropdown-title>Community</span>
                  <p>Join the PyTorch developer community to contribute, learn, and get your questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="" target="_blank">
                  <span class=dropdown-title>Forums</span>
                  <p>A place to discuss PyTorch code, issues, install, research</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class=dropdown-title>Developer Resources</span>
                  <p>Find resources and get questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Contributor Awards - 2024</span>
                  <p>Award winners announced at this year's PyTorch Conference</p>
                </a>
              </div>
            </div>
          </li>

          <li>
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Edge
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">About PyTorch Edge</span>
                  <p>Build innovative and privacy-aware AI experiences for edge devices</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">ExecuTorch</span>
                  <p>End-to-end solution for enabling on-device inference capabilities across mobile and edge devices</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">ExecuTorch Docs</span>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Docs
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">PyTorch</span>
                  <p>Explore the documentation for comprehensive guidance on how to use PyTorch</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">PyTorch Domains</span>
                  <p>Read the PyTorch Domains documentation to learn more about domain-specific libraries</p>
                </a>
              </div>
            </div>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Blogs & News
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">PyTorch Blog</span>
                  <p>Catch up on the latest technical news and happenings</p>
                </a>
                 <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Community Blog</span>
                  <p>Stories from the PyTorch ecosystem</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Videos</span>
                  <p>Learn about the latest PyTorch tutorials, new, and more </p>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Community Stories</span>
                  <p>Learn how our community solves real, everyday machine learning problems with PyTorch</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Events</span>
                  <p>Find events, webinars, and podcasts</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Newsletter</span>
                  <p>Stay up-to-date with the latest updates</p>
                </a>
            </div>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                About
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">PyTorch Foundation</span>
                  <p>Learn more about the PyTorch Foundation</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Governing Board</span>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Cloud Credit Program</span>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Technical Advisory Council</span>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Staff</span>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Contact Us</span>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
            <div class="no-dropdown">
              <a href="" data-cta="join">
                Become a Member
              </a>
            </div>
          </li>
          <li>
           <div class="main-menu-item">
             <a href="" class="github-icon">
             </a>
           </div>
          </li>
          <!--- TODO: This block adds the search icon to the nav bar. We will enable it later.
          <li>
            <div class="main-menu-item">
             <a href="" class="search-icon">
             </a>
            </div>
          </li>
          --->
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>
  </div>
</div>

  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <dialog id="pst-search-dialog">
    
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </dialog>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
<div class="bd-header__inner bd-page-width">
  <button class="pst-navbar-icon sidebar-toggle primary-toggle" aria-label="Site navigation">
    <span class="fa-solid fa-bars"></span>
  </button>
  
  
  <div class=" navbar-header-items__start">
    
      <div class="navbar-item">

  
    
  

<a class="navbar-brand logo" href="../../index.html">
  
  
  
  
  
  
    <p class="title logo__title">Home</p>
  
</a></div>
    
  </div>
  
  <div class=" navbar-header-items">
    
    <div class="me-auto navbar-header-items__center">
      
        <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../python-api/index.html">
    Python API
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../notes/index.html">
    Developer Notes
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-external" href="https://pytorch.org/tutorials/">
    Tutorials
  </a>
</li>

  </ul>
</nav></div>
      
    </div>
    
    
    <div class="navbar-header-items__end">
      
        <div class="navbar-item navbar-persistent--container">
          

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button>
        </div>
      
      
        <div class="navbar-item">
<div class="version-switcher__container dropdown pst-js-only">
  <button id="pst-version-switcher-button-2"
    type="button"
    class="version-switcher__button btn btn-sm dropdown-toggle"
    data-bs-toggle="dropdown"
    aria-haspopup="listbox"
    aria-controls="pst-version-switcher-list-2"
    aria-label="Version switcher list"
  >
    Choose version  <!-- this text may get changed later by javascript -->
    <span class="caret"></span>
  </button>
  <div id="pst-version-switcher-list-2"
    class="version-switcher__menu dropdown-menu list-group-flush py-0"
    role="listbox" aria-labelledby="pst-version-switcher-button-2">
    <!-- dropdown will be populated by javascript on page load -->
  </div>
</div></div>
      
        <div class="navbar-item">

<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button></div>
      
        <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://x.com/PyTorch" title="X" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-x-twitter fa-lg" aria-hidden="true"></i>
            <span class="sr-only">X</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/pytorch/pytorch" title="GitHub" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">GitHub</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://pypi.org/project/torch" title="PyPI" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-custom fa-pypi fa-lg" aria-hidden="true"></i>
            <span class="sr-only">PyPI</span></a>
        </li>
</ul></div>
      
    </div>
    
  </div>
  
  
    <div class="navbar-persistent--mobile">

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button>
    </div>
  

  
</div>

    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
        
      
      <dialog id="pst-primary-sidebar-modal"></dialog>
      <div id="pst-primary-sidebar" class="bd-sidebar-primary bd-sidebar hide-on-wide">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
      <div class="sidebar-header-items__center">
        
          
          
            <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../python-api/index.html">
    Python API
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../notes/index.html">
    Developer Notes
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-external" href="https://pytorch.org/tutorials/">
    Tutorials
  </a>
</li>

  </ul>
</nav></div>
          
        
      </div>
    
    
    
      <div class="sidebar-header-items__end">
        
          <div class="navbar-item">
<div class="version-switcher__container dropdown pst-js-only">
  <button id="pst-version-switcher-button-3"
    type="button"
    class="version-switcher__button btn btn-sm dropdown-toggle"
    data-bs-toggle="dropdown"
    aria-haspopup="listbox"
    aria-controls="pst-version-switcher-list-3"
    aria-label="Version switcher list"
  >
    Choose version  <!-- this text may get changed later by javascript -->
    <span class="caret"></span>
  </button>
  <div id="pst-version-switcher-list-3"
    class="version-switcher__menu dropdown-menu list-group-flush py-0"
    role="listbox" aria-labelledby="pst-version-switcher-button-3">
    <!-- dropdown will be populated by javascript on page load -->
  </div>
</div></div>
        
          <div class="navbar-item">

<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button></div>
        
          <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://x.com/PyTorch" title="X" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-x-twitter fa-lg" aria-hidden="true"></i>
            <span class="sr-only">X</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/pytorch/pytorch" title="GitHub" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">GitHub</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://pypi.org/project/torch" title="PyPI" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-custom fa-pypi fa-lg" aria-hidden="true"></i>
            <span class="sr-only">PyPI</span></a>
        </li>
</ul></div>
        
      </div>
    
  </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
      <div class="sidebar-primary-item">
<div id="ethical-ad-placement"
      class="flat"
      data-ea-publisher="readthedocs"
      data-ea-type="readthedocs-sidebar"
      data-ea-manual="true">
</div></div>
  </div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        
          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item">

<nav aria-label="Breadcrumb" class="d-print-none">
  <ul class="bd-breadcrumbs">
    
    <li class="breadcrumb-item breadcrumb-home">
      <a href="../../index.html" class="nav-link" aria-label="Home">
        <i class="fa-solid fa-home"></i>
      </a>
    </li>
    
    <li class="breadcrumb-item"><a href="../index.html" class="nav-link">Module code</a></li>
    
    
    <li class="breadcrumb-item"><a href="../torch.html" class="nav-link">torch</a></li>
    
    <li class="breadcrumb-item active" aria-current="page"><span class="ellipsis">torch._tensor</span></li>
  </ul>
</nav>
</div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item"><div class="rating">
    Rate this Page
    <div class="stars">
        
        <span class="star" data-value="1">★</span>
        
        <span class="star" data-value="2">★</span>
        
        <span class="star" data-value="3">★</span>
        
        <span class="star" data-value="4">★</span>
        
        <span class="star" data-value="5">★</span>
        
    </div>
</div></div>
      
    </div>
  
</div>
</div>
              
              
              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <h1>Source code for torch._tensor</h1><div class="highlight"><pre>
<span></span><span class="c1"># mypy: allow-untyped-defs</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">copyreg</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">enum</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">functools</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">warnings</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">collections</span><span class="w"> </span><span class="kn">import</span> <span class="n">OrderedDict</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">copy</span><span class="w"> </span><span class="kn">import</span> <span class="n">deepcopy</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">numbers</span><span class="w"> </span><span class="kn">import</span> <span class="n">Number</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">typing</span><span class="w"> </span><span class="kn">import</span> <span class="n">Any</span><span class="p">,</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">List</span><span class="p">,</span> <span class="n">Optional</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">,</span> <span class="n">Union</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch._C</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">_C</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch._namedtensor_internals</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">check_serializing_named_tensor</span><span class="p">,</span>
    <span class="n">is_ellipsis</span><span class="p">,</span>
    <span class="n">resolve_ellipsis</span><span class="p">,</span>
    <span class="n">single_ellipsis_index</span><span class="p">,</span>
    <span class="n">unzip_namedshape</span><span class="p">,</span>
    <span class="n">update_names</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch.overrides</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">get_default_nowrap_functions</span><span class="p">,</span>
    <span class="n">handle_torch_function</span><span class="p">,</span>
    <span class="n">has_torch_function</span><span class="p">,</span>
    <span class="n">has_torch_function_unary</span><span class="p">,</span>
    <span class="n">has_torch_function_variadic</span><span class="p">,</span>
<span class="p">)</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_handle_torch_function_and_wrap_type_error_to_not_implemented</span><span class="p">(</span><span class="n">f</span><span class="p">):</span>
    <span class="n">assigned</span> <span class="o">=</span> <span class="n">functools</span><span class="o">.</span><span class="n">WRAPPER_ASSIGNMENTS</span>

    <span class="nd">@functools</span><span class="o">.</span><span class="n">wraps</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">assigned</span><span class="o">=</span><span class="n">assigned</span><span class="p">)</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">wrapped</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="c1"># See https://github.com/pytorch/pytorch/issues/75462</span>
            <span class="k">if</span> <span class="n">has_torch_function</span><span class="p">(</span><span class="n">args</span><span class="p">):</span>
                <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span><span class="n">wrapped</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">f</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="k">except</span> <span class="ne">TypeError</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">NotImplemented</span>

    <span class="k">return</span> <span class="n">wrapped</span>


<span class="c1"># Should not be used, this is kept only for BC of loading old serialized Tensor subclasses</span>
<span class="k">def</span><span class="w"> </span><span class="nf">_rebuild_from_type</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="nb">type</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
    <span class="k">if</span> <span class="nb">type</span> <span class="ow">is</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">func</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">)</span>

    <span class="n">ret</span> <span class="o">=</span> <span class="n">func</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">)</span><span class="o">.</span><span class="n">as_subclass</span><span class="p">(</span><span class="nb">type</span><span class="p">)</span>
    <span class="n">ret</span><span class="o">.</span><span class="vm">__dict__</span> <span class="o">=</span> <span class="nb">dict</span>
    <span class="k">return</span> <span class="n">ret</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_rebuild_from_type_v2</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="n">new_type</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
    <span class="n">ret</span> <span class="o">=</span> <span class="n">func</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">ret</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">new_type</span><span class="p">:</span>
        <span class="n">ret</span> <span class="o">=</span> <span class="n">ret</span><span class="o">.</span><span class="n">as_subclass</span><span class="p">(</span><span class="n">new_type</span><span class="p">)</span>
    <span class="c1"># Tensor does define __setstate__ even though it doesn&#39;t define</span>
    <span class="c1"># __getstate__. So only use __setstate__ if it is NOT the one defined</span>
    <span class="c1"># on Tensor</span>
    <span class="k">if</span> <span class="p">(</span>
        <span class="nb">getattr</span><span class="p">(</span><span class="n">ret</span><span class="o">.</span><span class="vm">__class__</span><span class="p">,</span> <span class="s2">&quot;__setstate__&quot;</span><span class="p">,</span> <span class="n">Tensor</span><span class="o">.</span><span class="n">__setstate__</span><span class="p">)</span>
        <span class="ow">is</span> <span class="ow">not</span> <span class="n">Tensor</span><span class="o">.</span><span class="n">__setstate__</span>
    <span class="p">):</span>
        <span class="n">ret</span><span class="o">.</span><span class="n">__setstate__</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">ret</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">_utils</span><span class="o">.</span><span class="n">_set_obj_state</span><span class="p">(</span><span class="n">ret</span><span class="p">,</span> <span class="n">state</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">ret</span>


<span class="c1"># NB: If you subclass Tensor, and want to share the subclassed class</span>
<span class="c1"># across processes, you must also update torch/multiprocessing/reductions.py</span>
<span class="c1"># to define a ForkingPickler serialization mode for the class.</span>
<span class="c1">#</span>
<span class="c1"># NB: If you add a new method to Tensor, you must update</span>
<span class="c1"># torch/_C/__init__.pyi.in to add a type annotation for your method;</span>
<span class="c1"># otherwise, it will not show up in autocomplete.</span>
<span class="k">class</span><span class="w"> </span><span class="nc">Tensor</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">TensorBase</span><span class="p">):</span>
    <span class="n">_is_param</span><span class="p">:</span> <span class="nb">bool</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_clear_non_serializable_cached_data</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Clears any data cached in the tensor&#39;s ``__dict__`` that would prevent the tensor</span>
<span class="sd">        from being serialized.</span>

<span class="sd">        For example, subclasses with custom dispatched sizes / strides cache this info in</span>
<span class="sd">        non-serializable PyCapsules within the ``__dict__``, and this must be cleared out for</span>
<span class="sd">        serialization to function.</span>

<span class="sd">        Any subclass that overrides this MUST call ``super()._clear_non_serializable_cached_data().``</span>
<span class="sd">        Additional data cleared within the override must be able to be re-cached transparently</span>
<span class="sd">        to avoid breaking subclass functionality.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">has_torch_function_unary</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span>
                <span class="n">Tensor</span><span class="o">.</span><span class="n">_clear_non_serializable_cached_data</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="p">,),</span> <span class="bp">self</span>
            <span class="p">)</span>
        <span class="c1"># NB: Wrapper subclasses that implement custom-dispatched sizes / strides cache</span>
        <span class="c1"># this info via non-serializable PyCapsules.</span>
        <span class="n">CACHED_SIZES_STRIDES_KEYS</span> <span class="o">=</span> <span class="p">[</span>
            <span class="s2">&quot;_sym_sizes_capsule&quot;</span><span class="p">,</span>
            <span class="s2">&quot;_sym_sizes_capsule_len&quot;</span><span class="p">,</span>
            <span class="s2">&quot;_sym_strides_capsule&quot;</span><span class="p">,</span>
            <span class="s2">&quot;_sym_strides_capsule_len&quot;</span><span class="p">,</span>
        <span class="p">]</span>
        <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">CACHED_SIZES_STRIDES_KEYS</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="vm">__dict__</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">__deepcopy__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">memo</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">has_torch_function_unary</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span><span class="n">Tensor</span><span class="o">.</span><span class="n">__deepcopy__</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="p">,),</span> <span class="bp">self</span><span class="p">,</span> <span class="n">memo</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_leaf</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                <span class="s2">&quot;Only Tensors created explicitly by the user &quot;</span>
                <span class="s2">&quot;(graph leaves) support the deepcopy protocol at the moment.  &quot;</span>
                <span class="s2">&quot;If you were attempting to deepcopy a module, this may be because &quot;</span>
                <span class="s2">&quot;of a torch.nn.utils.weight_norm usage, &quot;</span>
                <span class="s2">&quot;see https://github.com/pytorch/pytorch/pull/103001&quot;</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="nb">id</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="ow">in</span> <span class="n">memo</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">memo</span><span class="p">[</span><span class="nb">id</span><span class="p">(</span><span class="bp">self</span><span class="p">)]</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="c1"># TODO: skipping storage copy is wrong for meta, as meta</span>
            <span class="c1"># does accurate alias tracking; however, the code below</span>
            <span class="c1"># doesn&#39;t work because of</span>
            <span class="c1"># https://github.com/pytorch/pytorch/issues/47442</span>
            <span class="c1"># Update the test in test_serialization if you remove &#39;meta&#39; from here</span>
            <span class="k">if</span> <span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">is_sparse</span>
                <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="o">.</span><span class="n">type</span>
                <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;lazy&quot;</span><span class="p">,</span> <span class="s2">&quot;xla&quot;</span><span class="p">,</span> <span class="s2">&quot;mtia&quot;</span><span class="p">,</span> <span class="s2">&quot;mps&quot;</span><span class="p">,</span> <span class="s2">&quot;maia&quot;</span><span class="p">,</span> <span class="s2">&quot;meta&quot;</span><span class="p">,</span> <span class="s2">&quot;ipu&quot;</span><span class="p">]</span>
                <span class="ow">or</span> <span class="p">(</span>
                    <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_has_storage</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
                    <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="o">.</span><span class="n">type</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_get_privateuse1_backend_name</span><span class="p">()</span>
                <span class="p">)</span>
                <span class="ow">or</span> <span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">Tensor</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">data_ptr</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span>
            <span class="p">):</span>
                <span class="n">new_tensor</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
                <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">new_tensor</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
                    <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                        <span class="s2">&quot;The default implementation of __deepcopy__() for wrapper subclasses &quot;</span>
                        <span class="s2">&quot;only works for subclass types that implement clone() and for which &quot;</span>
                        <span class="s2">&quot;cloning returns another instance of the same subclass. You should either &quot;</span>
                        <span class="s2">&quot;properly implement clone() for your subclass or override __deepcopy__() &quot;</span>
                        <span class="s2">&quot;if it is intended behavior for clone() to return an instance of a &quot;</span>
                        <span class="s2">&quot;different type.&quot;</span>
                    <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">new_storage</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_typed_storage</span><span class="p">()</span><span class="o">.</span><span class="n">_deepcopy</span><span class="p">(</span><span class="n">memo</span><span class="p">)</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_quantized</span><span class="p">:</span>
                    <span class="c1"># quantizer_params can be different type based on torch attribute</span>
                    <span class="n">quantizer_params</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span>
                        <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">qscheme</span><span class="p">,</span> <span class="nb">float</span><span class="p">,</span> <span class="nb">int</span><span class="p">],</span>
                        <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">qscheme</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">,</span> <span class="nb">int</span><span class="p">],</span>
                    <span class="p">]</span>
                    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">qscheme</span><span class="p">()</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">per_tensor_affine</span><span class="p">:</span>
                        <span class="n">quantizer_params</span> <span class="o">=</span> <span class="p">(</span>
                            <span class="bp">self</span><span class="o">.</span><span class="n">qscheme</span><span class="p">(),</span>
                            <span class="bp">self</span><span class="o">.</span><span class="n">q_scale</span><span class="p">(),</span>
                            <span class="bp">self</span><span class="o">.</span><span class="n">q_zero_point</span><span class="p">(),</span>
                        <span class="p">)</span>
                    <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">qscheme</span><span class="p">()</span> <span class="ow">in</span> <span class="p">(</span>
                        <span class="n">torch</span><span class="o">.</span><span class="n">per_channel_affine</span><span class="p">,</span>
                        <span class="n">torch</span><span class="o">.</span><span class="n">per_channel_affine_float_qparams</span><span class="p">,</span>
                    <span class="p">):</span>
                        <span class="n">quantizer_params</span> <span class="o">=</span> <span class="p">(</span>
                            <span class="bp">self</span><span class="o">.</span><span class="n">qscheme</span><span class="p">(),</span>
                            <span class="bp">self</span><span class="o">.</span><span class="n">q_per_channel_scales</span><span class="p">(),</span>
                            <span class="bp">self</span><span class="o">.</span><span class="n">q_per_channel_zero_points</span><span class="p">(),</span>
                            <span class="bp">self</span><span class="o">.</span><span class="n">q_per_channel_axis</span><span class="p">(),</span>
                        <span class="p">)</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                            <span class="sa">f</span><span class="s2">&quot;Unsupported qscheme </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">qscheme</span><span class="p">()</span><span class="si">}</span><span class="s2"> in deepcopy&quot;</span>
                        <span class="p">)</span>
                    <span class="c1"># TODO: Once we decide to break serialization FC, no longer</span>
                    <span class="c1"># need to wrap with TypedStorage</span>
                    <span class="n">new_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">_utils</span><span class="o">.</span><span class="n">_rebuild_qtensor</span><span class="p">(</span>
                        <span class="n">torch</span><span class="o">.</span><span class="n">storage</span><span class="o">.</span><span class="n">TypedStorage</span><span class="p">(</span>
                            <span class="n">wrap_storage</span><span class="o">=</span><span class="n">new_storage</span><span class="o">.</span><span class="n">_untyped_storage</span><span class="p">,</span>
                            <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
                            <span class="n">_internal</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                        <span class="p">),</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">storage_offset</span><span class="p">(),</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">size</span><span class="p">(),</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">(),</span>
                        <span class="n">quantizer_params</span><span class="p">,</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">,</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">_backward_hooks</span><span class="p">,</span>
                    <span class="p">)</span>
                    <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">new_tensor</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
                        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                            <span class="s2">&quot;The default implementation of __deepcopy__() for quantized tensors &quot;</span>
                            <span class="s2">&quot;expects the tensor returned by torch._utils._rebuild_qtensor() to &quot;</span>
                            <span class="s2">&quot;match the type of the instance being copied. If you encounter this, &quot;</span>
                            <span class="s2">&quot;please open an issue on PyTorch&#39;s GitHub.&quot;</span>
                        <span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">new_tensor</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">new_empty</span><span class="p">([])</span>
                    <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">new_tensor</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
                        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                            <span class="s2">&quot;The default implementation of __deepcopy__() for non-wrapper subclasses &quot;</span>
                            <span class="s2">&quot;only works for subclass types that implement new_empty() and for which &quot;</span>
                            <span class="s2">&quot;that function returns another instance of the same subclass. You should &quot;</span>
                            <span class="s2">&quot;either properly implement new_empty() for your subclass or override &quot;</span>
                            <span class="s2">&quot;__deepcopy__() if it is intended behavior for new_empty() to return &quot;</span>
                            <span class="s2">&quot;an instance of a different type.&quot;</span>
                        <span class="p">)</span>
                    <span class="n">new_tensor</span><span class="o">.</span><span class="n">set_</span><span class="p">(</span>
                        <span class="n">new_storage</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">storage_offset</span><span class="p">(),</span> <span class="bp">self</span><span class="o">.</span><span class="n">size</span><span class="p">(),</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">()</span>
                    <span class="p">)</span>
                    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_conj</span><span class="p">():</span>
                        <span class="n">new_tensor</span> <span class="o">=</span> <span class="n">new_tensor</span><span class="o">.</span><span class="n">conj_physical</span><span class="p">()</span>
                    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_neg</span><span class="p">():</span>
                        <span class="n">new_tensor</span> <span class="o">=</span> <span class="n">new_tensor</span><span class="o">.</span><span class="n">neg</span><span class="p">()</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">:</span>
                <span class="n">new_tensor</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">()</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">grad</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">new_tensor</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">__deepcopy__</span><span class="p">(</span><span class="n">memo</span><span class="p">)</span>

            <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">Tensor</span><span class="p">:</span>
                <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">new_tensor</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
                    <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                        <span class="s2">&quot;Type of deepcopy result does not match the type of the source tensor. &quot;</span>
                        <span class="s2">&quot;If you encounter this, please open an issue on PyTorch&#39;s GitHub.&quot;</span>
                    <span class="p">)</span>

                <span class="c1"># Plain Tensors don&#39;t have slots</span>
                <span class="n">slots_to_save</span> <span class="o">=</span> <span class="n">copyreg</span><span class="o">.</span><span class="n">_slotnames</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="p">)</span>  <span class="c1"># type: ignore[attr-defined]</span>
                <span class="k">for</span> <span class="n">slot</span> <span class="ow">in</span> <span class="n">slots_to_save</span><span class="p">:</span>
                    <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">slot</span><span class="p">):</span>
                        <span class="nb">setattr</span><span class="p">(</span><span class="n">new_tensor</span><span class="p">,</span> <span class="n">slot</span><span class="p">,</span> <span class="n">deepcopy</span><span class="p">(</span><span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">slot</span><span class="p">),</span> <span class="n">memo</span><span class="p">))</span>

            <span class="c1"># don&#39;t try to deepcopy non-serializable cached data</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_clear_non_serializable_cached_data</span><span class="p">()</span>
            <span class="n">new_tensor</span><span class="o">.</span><span class="vm">__dict__</span> <span class="o">=</span> <span class="n">deepcopy</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="vm">__dict__</span><span class="p">,</span> <span class="n">memo</span><span class="p">)</span>

            <span class="n">memo</span><span class="p">[</span><span class="nb">id</span><span class="p">(</span><span class="bp">self</span><span class="p">)]</span> <span class="o">=</span> <span class="n">new_tensor</span>
            <span class="k">return</span> <span class="n">new_tensor</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">__reduce_ex__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">proto</span><span class="p">):</span>
        <span class="n">materialize_fake_tensors</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">serialization</span><span class="o">.</span><span class="n">_serialization_tls</span><span class="o">.</span><span class="n">materialize_fake_tensors</span>
        <span class="p">)</span>
        <span class="n">state</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">_utils</span><span class="o">.</span><span class="n">_get_obj_state</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
        <span class="c1"># Ignore all state when using FakeTensor with skip_data(materialize_fake_tensors) because FakeTensor has</span>
        <span class="c1"># some state that cannot be pickled</span>
        <span class="k">if</span> <span class="p">(</span>
            <span class="c1"># TODO: remove hasattr, it&#39;s a hack to support versions of torch that</span>
            <span class="c1"># don&#39;t have _subclasses</span>
            <span class="nb">hasattr</span><span class="p">(</span><span class="n">torch</span><span class="p">,</span> <span class="s2">&quot;_subclasses&quot;</span><span class="p">)</span>
            <span class="ow">and</span> <span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="ow">is</span> <span class="n">torch</span><span class="o">.</span><span class="n">_subclasses</span><span class="o">.</span><span class="n">fake_tensor</span><span class="o">.</span><span class="n">FakeTensor</span>
            <span class="ow">and</span> <span class="n">materialize_fake_tensors</span>
        <span class="p">)</span> <span class="ow">or</span> <span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="ow">is</span> <span class="n">Tensor</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">state</span><span class="p">):</span>
            <span class="c1"># Fast path for regular tensor without Python state.</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_reduce_ex_internal</span><span class="p">(</span><span class="n">proto</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">has_torch_function_unary</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span><span class="n">Tensor</span><span class="o">.</span><span class="n">__reduce_ex__</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="p">,),</span> <span class="bp">self</span><span class="p">,</span> <span class="n">proto</span><span class="p">)</span>
        <span class="n">func</span><span class="p">,</span> <span class="n">args</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_reduce_ex_internal</span><span class="p">(</span><span class="n">proto</span><span class="p">)</span>
        <span class="c1"># sizes / strides cache needs to be cleared here because it&#39;ll just be re-cached</span>
        <span class="c1"># if cleared earlier. Note that state references the -actual- tensor dict.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_clear_non_serializable_cached_data</span><span class="p">()</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">_rebuild_from_type_v2</span><span class="p">,</span> <span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">),</span> <span class="n">args</span><span class="p">,</span> <span class="n">state</span><span class="p">))</span>

<div class="viewcode-block" id="Tensor.storage">
<a class="viewcode-back" href="../../python-api/generated/torch.Tensor.storage.html#torch.Tensor.storage">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">storage</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        storage() -&gt; torch.TypedStorage</span>

<span class="sd">        Returns the underlying :class:`TypedStorage`.</span>

<span class="sd">        .. warning::</span>

<span class="sd">            :class:`TypedStorage` is deprecated. It will be removed in the future, and</span>
<span class="sd">            :class:`UntypedStorage` will be the only storage class. To access the</span>
<span class="sd">            :class:`UntypedStorage` directly, use :attr:`Tensor.untyped_storage()`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">has_torch_function_unary</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span><span class="n">Tensor</span><span class="o">.</span><span class="n">storage</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="p">,),</span> <span class="bp">self</span><span class="p">)</span>

        <span class="n">torch</span><span class="o">.</span><span class="n">storage</span><span class="o">.</span><span class="n">_warn_typed_storage_removal</span><span class="p">(</span><span class="n">stacklevel</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_typed_storage</span><span class="p">()</span></div>


    <span class="c1"># For internal use only, to avoid raising deprecation warning</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">_typed_storage</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">untyped_storage</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">untyped_storage</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">TypedStorage</span><span class="p">(</span>
            <span class="n">wrap_storage</span><span class="o">=</span><span class="n">untyped_storage</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">_internal</span><span class="o">=</span><span class="kc">True</span>
        <span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_reduce_ex_internal</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">proto</span><span class="p">):</span>
        <span class="n">check_serializing_named_tensor</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>

        <span class="kn">from</span><span class="w"> </span><span class="nn">torch.utils.hooks</span><span class="w"> </span><span class="kn">import</span> <span class="n">warn_if_has_hooks</span>

        <span class="c1"># See Note [Don&#39;t serialize hooks]</span>
        <span class="n">warn_if_has_hooks</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
        <span class="n">backward_hooks</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="n">Any</span><span class="p">,</span> <span class="n">Any</span><span class="p">]</span> <span class="o">=</span> <span class="n">OrderedDict</span><span class="p">()</span>

        <span class="n">skip_data</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">serialization</span><span class="o">.</span><span class="n">_serialization_tls</span><span class="o">.</span><span class="n">skip_data</span>
        <span class="n">materialize_fake_tensors</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">serialization</span><span class="o">.</span><span class="n">_serialization_tls</span><span class="o">.</span><span class="n">materialize_fake_tensors</span>
        <span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="o">.</span><span class="n">type</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;xla&quot;</span><span class="p">,</span> <span class="s2">&quot;maia&quot;</span><span class="p">]</span> <span class="ow">or</span> <span class="p">(</span>
            <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_has_storage</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
            <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="o">.</span><span class="n">type</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_get_privateuse1_backend_name</span><span class="p">()</span>
        <span class="p">):</span>
            <span class="k">if</span> <span class="n">skip_data</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                    <span class="s2">&quot;Cannot serialize tensors on backends with no storage under skip_data context manager&quot;</span>
                <span class="p">)</span>
            <span class="n">cpu_tensor</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span>
            <span class="k">return</span> <span class="p">(</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">_utils</span><span class="o">.</span><span class="n">_rebuild_device_tensor_from_cpu_tensor</span><span class="p">,</span>
                <span class="p">(</span><span class="n">cpu_tensor</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="nb">str</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">),</span>
            <span class="p">)</span>
        <span class="c1"># Legacy comment that does not hold anymore.</span>
        <span class="c1"># Note: Numpy array is chosen to be the rebuild component for XLA, MTIA, MAIA Tensors.</span>
        <span class="c1"># We considered a few options:</span>
        <span class="c1"># 1. CPU tensor can&#39;t be used here.</span>
        <span class="c1">#    Otherwise in torch.load CPU storage is reconstructed with randomly</span>
        <span class="c1">#    initialized data, moved onto backend device, and then storage is updated</span>
        <span class="c1">#    to the serialized content. This works perfectly for CPU/CUDA but not these backends;</span>
        <span class="c1">#    their tensors are disconnected with storage so they don&#39;t get the update.</span>
        <span class="c1"># 2. Python list is not a good fit due to performance reason.</span>
        <span class="c1">#    `tolist()` converts every single element in the tensor into python objects</span>
        <span class="c1">#    and serialize them one by one.</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="o">.</span><span class="n">type</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;mtia&quot;</span><span class="p">]:</span>
            <span class="c1"># Convert BFloat16 tesors to Float32 before conversion to numpy, as numpy doesn&#39;t</span>
            <span class="c1"># support BFloat16. The rebuild tensor from numpy takes in the original self.dtype,</span>
            <span class="c1"># this would reconstruct the BFloat16 tensor from numpy.</span>
            <span class="k">if</span> <span class="n">skip_data</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                    <span class="s2">&quot;Cannot serialize tensors on backends with no storage under skip_data context manager&quot;</span>
                <span class="p">)</span>
            <span class="n">numpy_tensor</span> <span class="o">=</span> <span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span> <span class="o">!=</span> <span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span>
                <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
            <span class="p">)</span>
            <span class="k">return</span> <span class="p">(</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">_utils</span><span class="o">.</span><span class="n">_rebuild_device_tensor_from_numpy</span><span class="p">,</span>
                <span class="p">(</span><span class="n">numpy_tensor</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="nb">str</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">),</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="o">.</span><span class="n">type</span> <span class="o">==</span> <span class="s2">&quot;meta&quot;</span><span class="p">:</span>
            <span class="c1"># NB: This implementation BREAKS storage sharing.  Current</span>
            <span class="c1"># hypothesis is that no one cares for meta tensors.</span>
            <span class="k">if</span> <span class="n">skip_data</span><span class="p">:</span>
                <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
                    <span class="s2">&quot;Serializing tensors on the meta device under skip_data context manager is a no-op&quot;</span>
                <span class="p">)</span>
            <span class="n">arg_meta</span> <span class="o">=</span> <span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
                <span class="nb">tuple</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">size</span><span class="p">()),</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">(),</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="k">return</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">_utils</span><span class="o">.</span><span class="n">_rebuild_meta_tensor_no_storage</span><span class="p">,</span> <span class="n">arg_meta</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_quantized</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">skip_data</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                    <span class="s2">&quot;Cannot serialize qtensor under skip_data context manager, file an issue if you need this feature&quot;</span>
                <span class="p">)</span>
            <span class="c1"># quantizer_params can be different type based on torch attribute</span>
            <span class="n">quantizer_params</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span>
                <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">qscheme</span><span class="p">,</span> <span class="nb">float</span><span class="p">,</span> <span class="nb">int</span><span class="p">],</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Any</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">,</span> <span class="nb">int</span><span class="p">]</span>
            <span class="p">]</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">qscheme</span><span class="p">()</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">per_tensor_affine</span><span class="p">:</span>
                <span class="n">quantizer_params</span> <span class="o">=</span> <span class="p">(</span>
                    <span class="n">torch</span><span class="o">.</span><span class="n">per_tensor_affine</span><span class="p">,</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">q_scale</span><span class="p">(),</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">q_zero_point</span><span class="p">(),</span>
                <span class="p">)</span>
            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">qscheme</span><span class="p">()</span> <span class="ow">in</span> <span class="p">(</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">per_channel_affine</span><span class="p">,</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">per_channel_affine_float_qparams</span><span class="p">,</span>
            <span class="p">):</span>
                <span class="c1"># convert scales and zero points to tuple to avoid recursive calls</span>
                <span class="c1"># when/if we get multi-axis quantized tensors in the future, the shape</span>
                <span class="c1"># is recoverable from the main tensor shape</span>
                <span class="n">quantizer_params</span> <span class="o">=</span> <span class="p">(</span>
                    <span class="n">torch</span><span class="o">.</span><span class="n">per_channel_affine</span><span class="p">,</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">q_per_channel_scales</span><span class="p">(),</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">q_per_channel_zero_points</span><span class="p">(),</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">q_per_channel_axis</span><span class="p">(),</span>
                <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;Serialization is not supported for tensors of type </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">qscheme</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span>
                <span class="p">)</span>
            <span class="c1"># TODO: Once we decide to break serialization FC, no longer</span>
            <span class="c1"># need to wrap with TypedStorage</span>
            <span class="n">args_qtensor</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">storage</span><span class="o">.</span><span class="n">TypedStorage</span><span class="p">(</span>
                    <span class="n">wrap_storage</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_typed_storage</span><span class="p">()</span><span class="o">.</span><span class="n">_untyped_storage</span><span class="p">,</span>
                    <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
                    <span class="n">_internal</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                <span class="p">),</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">storage_offset</span><span class="p">(),</span>
                <span class="nb">tuple</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">size</span><span class="p">()),</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">(),</span>
                <span class="n">quantizer_params</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">,</span>
                <span class="n">backward_hooks</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="k">return</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">_utils</span><span class="o">.</span><span class="n">_rebuild_qtensor</span><span class="p">,</span> <span class="n">args_qtensor</span><span class="p">)</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_sparse</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">layout</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">sparse_coo</span><span class="p">:</span>
                <span class="n">args_sparse</span> <span class="o">=</span> <span class="p">(</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">layout</span><span class="p">,</span>
                    <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_indices</span><span class="p">(),</span> <span class="bp">self</span><span class="o">.</span><span class="n">_values</span><span class="p">(),</span> <span class="bp">self</span><span class="o">.</span><span class="n">size</span><span class="p">(),</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_coalesced</span><span class="p">()),</span>
                <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;sparse tensor __reduce_ex__ for layout `</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">layout</span><span class="si">}</span><span class="s2">`&quot;</span>
                <span class="p">)</span>
            <span class="k">return</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">_utils</span><span class="o">.</span><span class="n">_rebuild_sparse_tensor</span><span class="p">,</span> <span class="n">args_sparse</span><span class="p">)</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">layout</span> <span class="ow">in</span> <span class="p">{</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">sparse_csr</span><span class="p">,</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">sparse_csc</span><span class="p">,</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">sparse_bsr</span><span class="p">,</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">sparse_bsc</span><span class="p">,</span>
        <span class="p">}:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">layout</span> <span class="ow">in</span> <span class="p">{</span><span class="n">torch</span><span class="o">.</span><span class="n">sparse_csr</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">sparse_bsr</span><span class="p">}:</span>
                <span class="n">compressed_indices</span><span class="p">,</span> <span class="n">plain_indices</span> <span class="o">=</span> <span class="p">(</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">crow_indices</span><span class="p">(),</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">col_indices</span><span class="p">(),</span>
                <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">compressed_indices</span><span class="p">,</span> <span class="n">plain_indices</span> <span class="o">=</span> <span class="p">(</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">ccol_indices</span><span class="p">(),</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">row_indices</span><span class="p">(),</span>
                <span class="p">)</span>
            <span class="n">args_sparse_compressed</span> <span class="o">=</span> <span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">layout</span><span class="p">,</span>
                <span class="p">(</span>
                    <span class="n">compressed_indices</span><span class="p">,</span>
                    <span class="n">plain_indices</span><span class="p">,</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">values</span><span class="p">(),</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">size</span><span class="p">(),</span>
                <span class="p">),</span>
            <span class="p">)</span>
            <span class="k">return</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">_utils</span><span class="o">.</span><span class="n">_rebuild_sparse_tensor</span><span class="p">,</span> <span class="n">args_sparse_compressed</span><span class="p">)</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_nested</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">skip_data</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                    <span class="s2">&quot;Cannot serialize nested tensor under skip_data context manager, file an issue if you need this feature&quot;</span>
                <span class="p">)</span>
            <span class="n">args_nested</span> <span class="o">=</span> <span class="p">(</span>
                <span class="c1"># NB: values() currently returns the storage as a buffer in an unsafe way.</span>
                <span class="c1"># Ideally, we&#39;d use a private API for this instead. TODO: Switch to this if</span>
                <span class="c1"># we ever get around to adding it.</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">values</span><span class="p">(),</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_nested_tensor_size</span><span class="p">(),</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_nested_tensor_strides</span><span class="p">(),</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_nested_tensor_storage_offsets</span><span class="p">(),</span>
            <span class="p">)</span>
            <span class="k">return</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">_utils</span><span class="o">.</span><span class="n">_rebuild_nested_tensor</span><span class="p">,</span> <span class="n">args_nested</span><span class="p">)</span>
        <span class="k">elif</span> <span class="p">(</span>
            <span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>
            <span class="ow">and</span> <span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">__torch_dispatch__</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="o">.</span><span class="n">__torch_dispatch__</span>
            <span class="ow">and</span> <span class="p">(</span>
                <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">_subclasses</span><span class="o">.</span><span class="n">functional_tensor</span><span class="o">.</span><span class="n">FunctionalTensor</span><span class="p">)</span>
                <span class="ow">or</span> <span class="p">(</span>
                    <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">_subclasses</span><span class="o">.</span><span class="n">fake_tensor</span><span class="o">.</span><span class="n">FakeTensor</span><span class="p">)</span>
                    <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">data_ptr</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span>
                <span class="p">)</span>
            <span class="p">)</span>
        <span class="p">):</span>
            <span class="n">arg_wrapper_subclass</span> <span class="o">=</span> <span class="p">(</span>
                <span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">),</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
                <span class="nb">tuple</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">size</span><span class="p">()),</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">(),</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">storage_offset</span><span class="p">(),</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">layout</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="k">return</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">_utils</span><span class="o">.</span><span class="n">_rebuild_wrapper_subclass</span><span class="p">,</span> <span class="n">arg_wrapper_subclass</span><span class="p">)</span>
        <span class="k">elif</span> <span class="p">(</span>
            <span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>
            <span class="ow">and</span> <span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">__torch_dispatch__</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="o">.</span><span class="n">__torch_dispatch__</span>
            <span class="ow">and</span> <span class="p">(</span>
                <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">_subclasses</span><span class="o">.</span><span class="n">fake_tensor</span><span class="o">.</span><span class="n">FakeTensor</span><span class="p">)</span>
                <span class="ow">and</span> <span class="ow">not</span> <span class="p">(</span><span class="n">skip_data</span> <span class="ow">and</span> <span class="n">materialize_fake_tensors</span><span class="p">)</span>
            <span class="p">)</span>
        <span class="p">):</span>
            <span class="n">arg_wrapper_subclass</span> <span class="o">=</span> <span class="p">(</span>
                <span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">),</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
                <span class="nb">tuple</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">size</span><span class="p">()),</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">(),</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">storage_offset</span><span class="p">(),</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">layout</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="k">return</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">_utils</span><span class="o">.</span><span class="n">_rebuild_wrapper_subclass</span><span class="p">,</span> <span class="n">arg_wrapper_subclass</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">v3_dtypes</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">storage</span><span class="o">.</span><span class="n">_new_dtypes</span><span class="p">()</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span> <span class="ow">in</span> <span class="n">v3_dtypes</span><span class="p">:</span>
                <span class="n">rebuild_func</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">_utils</span><span class="o">.</span><span class="n">_rebuild_tensor_v3</span>
                <span class="n">storage</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">untyped_storage</span><span class="p">()</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="c1"># TODO: Once we decide to break serialization FC, no longer</span>
                <span class="c1"># need to wrap with TypedStorage</span>
                <span class="n">rebuild_func</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">_utils</span><span class="o">.</span><span class="n">_rebuild_tensor_v2</span>  <span class="c1"># type: ignore[assignment]</span>
                <span class="n">storage</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">storage</span><span class="o">.</span><span class="n">TypedStorage</span><span class="p">(</span>
                    <span class="n">wrap_storage</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_typed_storage</span><span class="p">()</span><span class="o">.</span><span class="n">_untyped_storage</span><span class="p">,</span>
                    <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
                    <span class="n">_internal</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                <span class="p">)</span>  <span class="c1"># type: ignore[assignment]</span>

            <span class="c1"># TODO: remove hasattr, it&#39;s a hack to support versions of torch that</span>
            <span class="c1"># don&#39;t have _subclasses</span>
            <span class="k">if</span> <span class="p">(</span>
                <span class="nb">hasattr</span><span class="p">(</span><span class="n">torch</span><span class="p">,</span> <span class="s2">&quot;_subclasses&quot;</span><span class="p">)</span>
                <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">_subclasses</span><span class="o">.</span><span class="n">fake_tensor</span><span class="o">.</span><span class="n">FakeTensor</span><span class="p">)</span>
                <span class="ow">and</span> <span class="n">skip_data</span>
            <span class="p">):</span>
                <span class="n">storage</span><span class="o">.</span><span class="n">_fake_device</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">device</span>

            <span class="n">args</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">storage</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">storage_offset</span><span class="p">(),</span>
                <span class="nb">tuple</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">size</span><span class="p">()),</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">(),</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">,</span>
                <span class="n">backward_hooks</span><span class="p">,</span>
            <span class="p">)</span>  <span class="c1"># previously was self._backward_hooks</span>

            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">storage</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">storage</span><span class="o">.</span><span class="n">UntypedStorage</span><span class="p">):</span>
                <span class="n">args</span> <span class="o">=</span> <span class="n">args</span> <span class="o">+</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">,)</span>  <span class="c1"># type: ignore[assignment]</span>

            <span class="n">metadata</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">_utils</span><span class="o">.</span><span class="n">get_tensor_metadata</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">metadata</span><span class="p">:</span>
                <span class="n">args</span> <span class="o">=</span> <span class="n">args</span> <span class="o">+</span> <span class="p">(</span><span class="n">metadata</span><span class="p">,)</span>  <span class="c1"># type: ignore[assignment]</span>

            <span class="k">return</span> <span class="p">(</span><span class="n">rebuild_func</span><span class="p">,</span> <span class="n">args</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">__setstate__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">has_torch_function_unary</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span><span class="n">Tensor</span><span class="o">.</span><span class="n">__setstate__</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="p">,),</span> <span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">)</span>
        <span class="c1"># Warning: this method is NOT called when you torch.load() a tensor;</span>
        <span class="c1"># that is managed by _rebuild_tensor_v2</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_leaf</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;__setstate__ can be only called on leaf Tensors&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">state</span><span class="p">)</span> <span class="o">==</span> <span class="mi">4</span><span class="p">:</span>
            <span class="c1"># legacy serialization of Tensor</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">set_</span><span class="p">(</span><span class="o">*</span><span class="n">state</span><span class="p">)</span>
            <span class="k">return</span>
        <span class="k">elif</span> <span class="nb">len</span><span class="p">(</span><span class="n">state</span><span class="p">)</span> <span class="o">==</span> <span class="mi">5</span><span class="p">:</span>
            <span class="c1"># legacy serialization of Variable</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">state</span> <span class="o">=</span> <span class="p">(</span><span class="n">state</span><span class="p">[</span><span class="mi">3</span><span class="p">],</span> <span class="n">state</span><span class="p">[</span><span class="mi">4</span><span class="p">],</span> <span class="n">state</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span>
        <span class="c1"># The setting of _backward_hooks is expected to be a no-op.</span>
        <span class="c1"># See Note [Don&#39;t serialize hooks]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_backward_hooks</span> <span class="o">=</span> <span class="n">state</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">tensor_contents</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">has_torch_function_unary</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span>
                <span class="n">Tensor</span><span class="o">.</span><span class="fm">__repr__</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="p">,),</span> <span class="bp">self</span><span class="p">,</span> <span class="n">tensor_contents</span><span class="o">=</span><span class="n">tensor_contents</span>
            <span class="p">)</span>
        <span class="c1"># All strings are unicode in Python 3.</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_tensor_str</span><span class="o">.</span><span class="n">_str</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tensor_contents</span><span class="o">=</span><span class="n">tensor_contents</span><span class="p">)</span>

<div class="viewcode-block" id="Tensor.backward">
<a class="viewcode-back" href="../../python-api/generated/torch.Tensor.backward.html#torch.Tensor.backward">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">backward</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">gradient</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">retain_graph</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">create_graph</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="kc">None</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Computes the gradient of current tensor wrt graph leaves.</span>

<span class="sd">        The graph is differentiated using the chain rule. If the tensor is</span>
<span class="sd">        non-scalar (i.e. its data has more than one element) and requires</span>
<span class="sd">        gradient, the function additionally requires specifying a ``gradient``.</span>
<span class="sd">        It should be a tensor of matching type and shape, that represents</span>
<span class="sd">        the gradient of the differentiated function w.r.t. ``self``.</span>

<span class="sd">        This function accumulates gradients in the leaves - you might need to zero</span>
<span class="sd">        ``.grad`` attributes or set them to ``None`` before calling it.</span>
<span class="sd">        See :ref:`Default gradient layouts&lt;default-grad-layouts&gt;`</span>
<span class="sd">        for details on the memory layout of accumulated gradients.</span>

<span class="sd">        .. note::</span>

<span class="sd">            If you run any forward ops, create ``gradient``, and/or call ``backward``</span>
<span class="sd">            in a user-specified CUDA stream context, see</span>
<span class="sd">            :ref:`Stream semantics of backward passes&lt;bwd-cuda-stream-semantics&gt;`.</span>

<span class="sd">        .. note::</span>

<span class="sd">            When ``inputs`` are provided and a given input is not a leaf,</span>
<span class="sd">            the current implementation will call its grad_fn (though it is not strictly needed to get this gradients).</span>
<span class="sd">            It is an implementation detail on which the user should not rely.</span>
<span class="sd">            See https://github.com/pytorch/pytorch/pull/60521#issuecomment-867061780 for more details.</span>

<span class="sd">        Args:</span>
<span class="sd">            gradient (Tensor, optional): The gradient of the function</span>
<span class="sd">                being differentiated w.r.t. ``self``.</span>
<span class="sd">                This argument can be omitted if ``self`` is a scalar.</span>
<span class="sd">            retain_graph (bool, optional): If ``False``, the graph used to compute</span>
<span class="sd">                the grads will be freed. Note that in nearly all cases setting</span>
<span class="sd">                this option to True is not needed and often can be worked around</span>
<span class="sd">                in a much more efficient way. Defaults to the value of</span>
<span class="sd">                ``create_graph``.</span>
<span class="sd">            create_graph (bool, optional): If ``True``, graph of the derivative will</span>
<span class="sd">                be constructed, allowing to compute higher order derivative</span>
<span class="sd">                products. Defaults to ``False``.</span>
<span class="sd">            inputs (sequence of Tensor, optional): Inputs w.r.t. which the gradient will be</span>
<span class="sd">                accumulated into ``.grad``. All other tensors will be ignored. If not</span>
<span class="sd">                provided, the gradient is accumulated into all the leaf Tensors that were</span>
<span class="sd">                used to compute the :attr:`tensors`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">has_torch_function_unary</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span>
                <span class="n">Tensor</span><span class="o">.</span><span class="n">backward</span><span class="p">,</span>
                <span class="p">(</span><span class="bp">self</span><span class="p">,),</span>
                <span class="bp">self</span><span class="p">,</span>
                <span class="n">gradient</span><span class="o">=</span><span class="n">gradient</span><span class="p">,</span>
                <span class="n">retain_graph</span><span class="o">=</span><span class="n">retain_graph</span><span class="p">,</span>
                <span class="n">create_graph</span><span class="o">=</span><span class="n">create_graph</span><span class="p">,</span>
                <span class="n">inputs</span><span class="o">=</span><span class="n">inputs</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">,</span> <span class="n">gradient</span><span class="p">,</span> <span class="n">retain_graph</span><span class="p">,</span> <span class="n">create_graph</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="n">inputs</span>
        <span class="p">)</span></div>


<div class="viewcode-block" id="Tensor.register_hook">
<a class="viewcode-back" href="../../python-api/generated/torch.Tensor.register_hook.html#torch.Tensor.register_hook">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">register_hook</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hook</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Registers a backward hook.</span>

<span class="sd">        The hook will be called every time a gradient with respect to the</span>
<span class="sd">        Tensor is computed. The hook should have the following signature::</span>

<span class="sd">            hook(grad) -&gt; Tensor or None</span>


<span class="sd">        The hook should not modify its argument, but it can optionally return</span>
<span class="sd">        a new gradient which will be used in place of :attr:`grad`.</span>

<span class="sd">        This function returns a handle with a method ``handle.remove()``</span>
<span class="sd">        that removes the hook from the module.</span>

<span class="sd">        .. note::</span>
<span class="sd">            See :ref:`backward-hooks-execution` for more information on how when this hook</span>
<span class="sd">            is executed, and how its execution is ordered relative to other hooks.</span>

<span class="sd">        Example::</span>

<span class="sd">            &gt;&gt;&gt; v = torch.tensor([0., 0., 0.], requires_grad=True)</span>
<span class="sd">            &gt;&gt;&gt; h = v.register_hook(lambda grad: grad * 2)  # double the gradient</span>
<span class="sd">            &gt;&gt;&gt; v.backward(torch.tensor([1., 2., 3.]))</span>
<span class="sd">            &gt;&gt;&gt; v.grad</span>

<span class="sd">             2</span>
<span class="sd">             4</span>
<span class="sd">             6</span>
<span class="sd">            [torch.FloatTensor of size (3,)]</span>

<span class="sd">            &gt;&gt;&gt; h.remove()  # removes the hook</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">has_torch_function_unary</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span><span class="n">Tensor</span><span class="o">.</span><span class="n">register_hook</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="p">,),</span> <span class="bp">self</span><span class="p">,</span> <span class="n">hook</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                <span class="s2">&quot;cannot register a hook on a tensor that doesn&#39;t require gradient&quot;</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_backward_hooks</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_backward_hooks</span> <span class="o">=</span> <span class="n">OrderedDict</span><span class="p">()</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">grad_fn</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">grad_fn</span><span class="o">.</span><span class="n">_register_hook_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>

        <span class="kn">from</span><span class="w"> </span><span class="nn">torch.utils.hooks</span><span class="w"> </span><span class="kn">import</span> <span class="n">RemovableHandle</span>

        <span class="n">handle</span> <span class="o">=</span> <span class="n">RemovableHandle</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_backward_hooks</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_backward_hooks</span><span class="p">[</span><span class="n">handle</span><span class="o">.</span><span class="n">id</span><span class="p">]</span> <span class="o">=</span> <span class="n">hook</span>
        <span class="k">return</span> <span class="n">handle</span></div>


<div class="viewcode-block" id="Tensor.register_post_accumulate_grad_hook">
<a class="viewcode-back" href="../../python-api/generated/torch.Tensor.register_post_accumulate_grad_hook.html#torch.Tensor.register_post_accumulate_grad_hook">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">register_post_accumulate_grad_hook</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hook</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Registers a backward hook that runs after grad accumulation.</span>

<span class="sd">        The hook will be called after all gradients for a tensor have been accumulated,</span>
<span class="sd">        meaning that the .grad field has been updated on that tensor. The post</span>
<span class="sd">        accumulate grad hook is ONLY applicable for leaf tensors (tensors without a</span>
<span class="sd">        .grad_fn field). Registering this hook on a non-leaf tensor will error!</span>

<span class="sd">        The hook should have the following signature::</span>

<span class="sd">            hook(param: Tensor) -&gt; None</span>

<span class="sd">        Note that, unlike other autograd hooks, this hook operates on the tensor</span>
<span class="sd">        that requires grad and not the grad itself. The hook can in-place modify</span>
<span class="sd">        and access its Tensor argument, including its .grad field.</span>

<span class="sd">        This function returns a handle with a method ``handle.remove()``</span>
<span class="sd">        that removes the hook from the module.</span>

<span class="sd">        .. note::</span>
<span class="sd">            See :ref:`backward-hooks-execution` for more information on how when this hook</span>
<span class="sd">            is executed, and how its execution is ordered relative to other hooks. Since</span>
<span class="sd">            this hook runs during the backward pass, it will run in no_grad mode (unless</span>
<span class="sd">            create_graph is True). You can use torch.enable_grad() to re-enable autograd</span>
<span class="sd">            within the hook if you need it.</span>

<span class="sd">        Example::</span>

<span class="sd">            &gt;&gt;&gt; v = torch.tensor([0., 0., 0.], requires_grad=True)</span>
<span class="sd">            &gt;&gt;&gt; lr = 0.01</span>
<span class="sd">            &gt;&gt;&gt; # simulate a simple SGD update</span>
<span class="sd">            &gt;&gt;&gt; h = v.register_post_accumulate_grad_hook(lambda p: p.add_(p.grad, alpha=-lr))</span>
<span class="sd">            &gt;&gt;&gt; v.backward(torch.tensor([1., 2., 3.]))</span>
<span class="sd">            &gt;&gt;&gt; v</span>
<span class="sd">            tensor([-0.0100, -0.0200, -0.0300], requires_grad=True)</span>

<span class="sd">            &gt;&gt;&gt; h.remove()  # removes the hook</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">has_torch_function_unary</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span>
                <span class="n">Tensor</span><span class="o">.</span><span class="n">register_post_accumulate_grad_hook</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="p">,),</span> <span class="bp">self</span><span class="p">,</span> <span class="n">hook</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                <span class="s2">&quot;cannot register a hook on a tensor that doesn&#39;t require gradient&quot;</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">grad_fn</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                <span class="s2">&quot;post accumulate grad hooks cannot be registered on non-leaf tensors&quot;</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_post_accumulate_grad_hooks</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_post_accumulate_grad_hooks</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="n">Any</span><span class="p">,</span> <span class="n">Any</span><span class="p">]</span> <span class="o">=</span> <span class="n">OrderedDict</span><span class="p">()</span>

        <span class="kn">from</span><span class="w"> </span><span class="nn">torch.utils.hooks</span><span class="w"> </span><span class="kn">import</span> <span class="n">RemovableHandle</span>

        <span class="n">handle</span> <span class="o">=</span> <span class="n">RemovableHandle</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_post_accumulate_grad_hooks</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_post_accumulate_grad_hooks</span><span class="p">[</span><span class="n">handle</span><span class="o">.</span><span class="n">id</span><span class="p">]</span> <span class="o">=</span> <span class="n">hook</span>
        <span class="k">return</span> <span class="n">handle</span></div>


    <span class="k">def</span><span class="w"> </span><span class="nf">reinforce</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">reward</span><span class="p">):</span>
        <span class="k">def</span><span class="w"> </span><span class="nf">trim</span><span class="p">(</span><span class="nb">str</span><span class="p">):</span>
            <span class="k">return</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="n">line</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span> <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="nb">str</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)])</span>

        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
            <span class="n">trim</span><span class="p">(</span>
<span class="w">                </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;reinforce() was removed.</span>
<span class="sd">            Use torch.distributions instead.</span>
<span class="sd">            See https://pytorch.org/docs/main/distributions.html</span>

<span class="sd">            Instead of:</span>

<span class="sd">            probs = policy_network(state)</span>
<span class="sd">            action = probs.multinomial()</span>
<span class="sd">            next_state, reward = env.step(action)</span>
<span class="sd">            action.reinforce(reward)</span>
<span class="sd">            action.backward()</span>

<span class="sd">            Use:</span>

<span class="sd">            probs = policy_network(state)</span>
<span class="sd">            # NOTE: categorical is equivalent to what used to be called multinomial</span>
<span class="sd">            m = torch.distributions.Categorical(probs)</span>
<span class="sd">            action = m.sample()</span>
<span class="sd">            next_state, reward = env.step(action)</span>
<span class="sd">            loss = -m.log_prob(action) * reward</span>
<span class="sd">            loss.backward()</span>
<span class="sd">        &quot;&quot;&quot;</span>
            <span class="p">)</span>
        <span class="p">)</span>

    <span class="n">detach</span> <span class="o">=</span> <span class="n">_C</span><span class="o">.</span><span class="n">_add_docstr</span><span class="p">(</span>
        <span class="n">_C</span><span class="o">.</span><span class="n">TensorBase</span><span class="o">.</span><span class="n">detach</span><span class="p">,</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns a new Tensor, detached from the current graph.</span>

<span class="sd">    The result will never require gradient.</span>

<span class="sd">    This method also affects forward mode AD gradients and the result will never</span>
<span class="sd">    have forward mode AD gradients.</span>

<span class="sd">    .. note::</span>

<span class="sd">      Returned Tensor shares the same storage with the original one.</span>
<span class="sd">      In-place modifications on either of them will be seen, and may trigger</span>
<span class="sd">      errors in correctness checks.</span>
<span class="sd">    &quot;&quot;&quot;</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">detach_</span> <span class="o">=</span> <span class="n">_C</span><span class="o">.</span><span class="n">_add_docstr</span><span class="p">(</span>
        <span class="n">_C</span><span class="o">.</span><span class="n">TensorBase</span><span class="o">.</span><span class="n">detach_</span><span class="p">,</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Detaches the Tensor from the graph that created it, making it a leaf.</span>
<span class="sd">    Views cannot be detached in-place.</span>

<span class="sd">    This method also affects forward mode AD gradients and the result will never</span>
<span class="sd">    have forward mode AD gradients.</span>
<span class="sd">    &quot;&quot;&quot;</span><span class="p">,</span>
    <span class="p">)</span>

<div class="viewcode-block" id="Tensor.is_shared">
<a class="viewcode-back" href="../../python-api/generated/torch.Tensor.is_shared.html#torch.Tensor.is_shared">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">is_shared</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Checks if tensor is in shared memory.</span>

<span class="sd">        This is always ``True`` for CUDA tensors.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">has_torch_function_unary</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span><span class="n">Tensor</span><span class="o">.</span><span class="n">is_shared</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="p">,),</span> <span class="bp">self</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_typed_storage</span><span class="p">()</span><span class="o">.</span><span class="n">_is_shared</span><span class="p">()</span></div>


<div class="viewcode-block" id="Tensor.share_memory_">
<a class="viewcode-back" href="../../python-api/generated/torch.Tensor.share_memory_.html#torch.Tensor.share_memory_">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">share_memory_</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Moves the underlying storage to shared memory.</span>

<span class="sd">        This is a no-op if the underlying storage is already in shared memory</span>
<span class="sd">        and for CUDA tensors. Tensors in shared memory cannot be resized.</span>

<span class="sd">        See :meth:`torch.UntypedStorage.share_memory_` for more details.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">has_torch_function_unary</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span><span class="n">Tensor</span><span class="o">.</span><span class="n">share_memory_</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="p">,),</span> <span class="bp">self</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_typed_storage</span><span class="p">()</span><span class="o">.</span><span class="n">_share_memory_</span><span class="p">()</span>
        <span class="k">return</span> <span class="bp">self</span></div>


<div class="viewcode-block" id="Tensor.module_load">
<a class="viewcode-back" href="../../python-api/generated/torch.Tensor.module_load.html#torch.Tensor.module_load">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">module_load</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">,</span> <span class="n">assign</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Defines how to transform ``other`` when loading it into ``self`` in :meth:`~nn.Module.load_state_dict`.</span>

<span class="sd">        Used when :func:`~torch.__future__.get_swap_module_params_on_conversion` is ``True``.</span>

<span class="sd">        It is expected that ``self`` is a parameter or buffer in an ``nn.Module`` and ``other`` is the</span>
<span class="sd">        value in the state dictionary with the corresponding key, this method defines</span>
<span class="sd">        how ``other`` is remapped before being swapped with ``self`` via</span>
<span class="sd">        :func:`~torch.utils.swap_tensors` in :meth:`~nn.Module.load_state_dict`.</span>

<span class="sd">        .. note::</span>
<span class="sd">            This method should always return a new object that is not ``self`` or ``other``.</span>
<span class="sd">            For example, the default implementation returns ``self.copy_(other).detach()``</span>
<span class="sd">            if ``assign`` is ``False`` or ``other.detach()`` if ``assign`` is ``True``.</span>

<span class="sd">        Args:</span>
<span class="sd">            other (Tensor): value in state dict with key corresponding to ``self``</span>
<span class="sd">            assign (bool): the assign argument passed to :meth:`nn.Module.load_state_dict`</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">has_torch_function_variadic</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span>
                <span class="n">Tensor</span><span class="o">.</span><span class="n">module_load</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">),</span> <span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">,</span> <span class="n">assign</span><span class="o">=</span><span class="n">assign</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="n">assign</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">other</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">copy_</span><span class="p">(</span><span class="n">other</span><span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span></div>


    <span class="k">def</span><span class="w"> </span><span class="fm">__reversed__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Reverses the tensor along dimension 0.&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">has_torch_function_unary</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span><span class="n">Tensor</span><span class="o">.</span><span class="fm">__reversed__</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="p">,),</span> <span class="bp">self</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">flip</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

<div class="viewcode-block" id="Tensor.norm">
<a class="viewcode-back" href="../../python-api/generated/torch.Tensor.norm.html#torch.Tensor.norm">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">norm</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">p</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="s2">&quot;fro&quot;</span><span class="p">,</span>
        <span class="n">dim</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">keepdim</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;See :func:`torch.norm`&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">has_torch_function_unary</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span>
                <span class="n">Tensor</span><span class="o">.</span><span class="n">norm</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="p">,),</span> <span class="bp">self</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">p</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">dim</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="n">keepdim</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span>
            <span class="p">)</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">keepdim</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span></div>


    <span class="k">def</span><span class="w"> </span><span class="nf">solve</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
        <span class="kn">from</span><span class="w"> </span><span class="nn">torch._linalg_utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">solve</span>

        <span class="k">return</span> <span class="n">solve</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">lstsq</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
        <span class="kn">from</span><span class="w"> </span><span class="nn">torch._linalg_utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">lstsq</span>

        <span class="k">return</span> <span class="n">lstsq</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">eig</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">eigenvectors</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="kn">from</span><span class="w"> </span><span class="nn">torch._linalg_utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">eig</span>

        <span class="k">return</span> <span class="n">eig</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">eigenvectors</span><span class="o">=</span><span class="n">eigenvectors</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">symeig</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">eigenvectors</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="kn">from</span><span class="w"> </span><span class="nn">torch._linalg_utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">_symeig</span>

        <span class="k">return</span> <span class="n">_symeig</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">eigenvectors</span><span class="o">=</span><span class="n">eigenvectors</span><span class="p">)</span>

<div class="viewcode-block" id="Tensor.lu">
<a class="viewcode-back" href="../../python-api/generated/torch.Tensor.lu.html#torch.Tensor.lu">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">lu</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">pivot</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">get_infos</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;See :func:`torch.lu`&quot;&quot;&quot;</span>
        <span class="c1"># If get_infos is True, then we don&#39;t need to check for errors and vice versa</span>
        <span class="k">if</span> <span class="n">has_torch_function_unary</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span>
                <span class="n">Tensor</span><span class="o">.</span><span class="n">lu</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="p">,),</span> <span class="bp">self</span><span class="p">,</span> <span class="n">pivot</span><span class="o">=</span><span class="n">pivot</span><span class="p">,</span> <span class="n">get_infos</span><span class="o">=</span><span class="n">get_infos</span>
            <span class="p">)</span>

        <span class="n">LU</span><span class="p">,</span> <span class="n">pivots</span><span class="p">,</span> <span class="n">infos</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">_lu_with_info</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">,</span> <span class="n">pivot</span><span class="o">=</span><span class="n">pivot</span><span class="p">,</span> <span class="n">check_errors</span><span class="o">=</span><span class="p">(</span><span class="ow">not</span> <span class="n">get_infos</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="n">get_infos</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">LU</span><span class="p">,</span> <span class="n">pivots</span><span class="p">,</span> <span class="n">infos</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">LU</span><span class="p">,</span> <span class="n">pivots</span></div>


<div class="viewcode-block" id="Tensor.stft">
<a class="viewcode-back" href="../../python-api/generated/torch.Tensor.stft.html#torch.Tensor.stft">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">stft</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">n_fft</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">hop_length</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">win_length</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">window</span><span class="p">:</span> <span class="s2">&quot;Optional[Tensor]&quot;</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">center</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">pad_mode</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;reflect&quot;</span><span class="p">,</span>
        <span class="n">normalized</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">onesided</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">return_complex</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;See :func:`torch.stft`</span>

<span class="sd">        .. warning::</span>
<span class="sd">          This function changed signature at version 0.4.1. Calling with</span>
<span class="sd">          the previous signature may cause error or return incorrect result.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">has_torch_function_unary</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span>
                <span class="n">Tensor</span><span class="o">.</span><span class="n">stft</span><span class="p">,</span>
                <span class="p">(</span><span class="bp">self</span><span class="p">,),</span>
                <span class="bp">self</span><span class="p">,</span>
                <span class="n">n_fft</span><span class="p">,</span>
                <span class="n">hop_length</span><span class="o">=</span><span class="n">hop_length</span><span class="p">,</span>
                <span class="n">win_length</span><span class="o">=</span><span class="n">win_length</span><span class="p">,</span>
                <span class="n">window</span><span class="o">=</span><span class="n">window</span><span class="p">,</span>
                <span class="n">center</span><span class="o">=</span><span class="n">center</span><span class="p">,</span>
                <span class="n">pad_mode</span><span class="o">=</span><span class="n">pad_mode</span><span class="p">,</span>
                <span class="n">normalized</span><span class="o">=</span><span class="n">normalized</span><span class="p">,</span>
                <span class="n">onesided</span><span class="o">=</span><span class="n">onesided</span><span class="p">,</span>
                <span class="n">return_complex</span><span class="o">=</span><span class="n">return_complex</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">stft</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">,</span>
            <span class="n">n_fft</span><span class="p">,</span>
            <span class="n">hop_length</span><span class="p">,</span>
            <span class="n">win_length</span><span class="p">,</span>
            <span class="n">window</span><span class="p">,</span>
            <span class="n">center</span><span class="p">,</span>
            <span class="n">pad_mode</span><span class="p">,</span>
            <span class="n">normalized</span><span class="p">,</span>
            <span class="n">onesided</span><span class="p">,</span>
            <span class="n">return_complex</span><span class="o">=</span><span class="n">return_complex</span><span class="p">,</span>
        <span class="p">)</span></div>


<div class="viewcode-block" id="Tensor.istft">
<a class="viewcode-back" href="../../python-api/generated/torch.Tensor.istft.html#torch.Tensor.istft">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">istft</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">n_fft</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">hop_length</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">win_length</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">window</span><span class="p">:</span> <span class="s2">&quot;Optional[Tensor]&quot;</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">center</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">normalized</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">onesided</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">length</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">return_complex</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;See :func:`torch.istft`&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">has_torch_function_unary</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span>
                <span class="n">Tensor</span><span class="o">.</span><span class="n">istft</span><span class="p">,</span>
                <span class="p">(</span><span class="bp">self</span><span class="p">,),</span>
                <span class="bp">self</span><span class="p">,</span>
                <span class="n">n_fft</span><span class="p">,</span>
                <span class="n">hop_length</span><span class="o">=</span><span class="n">hop_length</span><span class="p">,</span>
                <span class="n">win_length</span><span class="o">=</span><span class="n">win_length</span><span class="p">,</span>
                <span class="n">window</span><span class="o">=</span><span class="n">window</span><span class="p">,</span>
                <span class="n">center</span><span class="o">=</span><span class="n">center</span><span class="p">,</span>
                <span class="n">normalized</span><span class="o">=</span><span class="n">normalized</span><span class="p">,</span>
                <span class="n">onesided</span><span class="o">=</span><span class="n">onesided</span><span class="p">,</span>
                <span class="n">length</span><span class="o">=</span><span class="n">length</span><span class="p">,</span>
                <span class="n">return_complex</span><span class="o">=</span><span class="n">return_complex</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">istft</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">,</span>
            <span class="n">n_fft</span><span class="p">,</span>
            <span class="n">hop_length</span><span class="p">,</span>
            <span class="n">win_length</span><span class="p">,</span>
            <span class="n">window</span><span class="p">,</span>
            <span class="n">center</span><span class="p">,</span>
            <span class="n">normalized</span><span class="p">,</span>
            <span class="n">onesided</span><span class="p">,</span>
            <span class="n">length</span><span class="p">,</span>
            <span class="n">return_complex</span><span class="o">=</span><span class="n">return_complex</span><span class="p">,</span>
        <span class="p">)</span></div>


    <span class="k">def</span><span class="w"> </span><span class="nf">resize</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">sizes</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">has_torch_function_unary</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span><span class="n">Tensor</span><span class="o">.</span><span class="n">resize</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="p">,),</span> <span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">sizes</span><span class="p">)</span>
        <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s2">&quot;non-inplace resize is deprecated&quot;</span><span class="p">)</span>
        <span class="kn">from</span><span class="w"> </span><span class="nn">torch.autograd._functions</span><span class="w"> </span><span class="kn">import</span> <span class="n">Resize</span>

        <span class="k">return</span> <span class="n">Resize</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sizes</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">resize_as</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tensor</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">has_torch_function_variadic</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tensor</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span><span class="n">Tensor</span><span class="o">.</span><span class="n">resize_as</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tensor</span><span class="p">),</span> <span class="bp">self</span><span class="p">,</span> <span class="n">tensor</span><span class="p">)</span>
        <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s2">&quot;non-inplace resize_as is deprecated&quot;</span><span class="p">)</span>
        <span class="kn">from</span><span class="w"> </span><span class="nn">torch.autograd._functions</span><span class="w"> </span><span class="kn">import</span> <span class="n">Resize</span>

        <span class="k">return</span> <span class="n">Resize</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tensor</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>

<div class="viewcode-block" id="Tensor.split">
<a class="viewcode-back" href="../../python-api/generated/torch.Tensor.split.html#torch.Tensor.split">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">split</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">split_size</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;See :func:`torch.split`&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">has_torch_function_unary</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span>
                <span class="n">Tensor</span><span class="o">.</span><span class="n">split</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="p">,),</span> <span class="bp">self</span><span class="p">,</span> <span class="n">split_size</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">dim</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">split_size</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="n">split_size</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">split_size</span><span class="p">)</span>
            <span class="k">except</span> <span class="ne">ValueError</span><span class="p">:</span>
                <span class="k">pass</span>

        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">split_size</span><span class="p">,</span> <span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">SymInt</span><span class="p">)):</span>
            <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_VF</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">split_size</span><span class="p">,</span> <span class="n">dim</span><span class="p">)</span>  <span class="c1"># type: ignore[attr-defined]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_VF</span><span class="o">.</span><span class="n">split_with_sizes</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">split_size</span><span class="p">,</span> <span class="n">dim</span><span class="p">)</span></div>


<div class="viewcode-block" id="Tensor.unique">
<a class="viewcode-back" href="../../python-api/generated/torch.Tensor.unique.html#torch.Tensor.unique">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">unique</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">sorted</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">return_inverse</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">return_counts</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns the unique elements of the input tensor.</span>

<span class="sd">        See :func:`torch.unique`</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">has_torch_function_unary</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span>
                <span class="n">Tensor</span><span class="o">.</span><span class="n">unique</span><span class="p">,</span>
                <span class="p">(</span><span class="bp">self</span><span class="p">,),</span>
                <span class="bp">self</span><span class="p">,</span>
                <span class="nb">sorted</span><span class="o">=</span><span class="nb">sorted</span><span class="p">,</span>
                <span class="n">return_inverse</span><span class="o">=</span><span class="n">return_inverse</span><span class="p">,</span>
                <span class="n">return_counts</span><span class="o">=</span><span class="n">return_counts</span><span class="p">,</span>
                <span class="n">dim</span><span class="o">=</span><span class="n">dim</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">,</span>
            <span class="nb">sorted</span><span class="o">=</span><span class="nb">sorted</span><span class="p">,</span>
            <span class="n">return_inverse</span><span class="o">=</span><span class="n">return_inverse</span><span class="p">,</span>
            <span class="n">return_counts</span><span class="o">=</span><span class="n">return_counts</span><span class="p">,</span>
            <span class="n">dim</span><span class="o">=</span><span class="n">dim</span><span class="p">,</span>
        <span class="p">)</span></div>


<div class="viewcode-block" id="Tensor.unique_consecutive">
<a class="viewcode-back" href="../../python-api/generated/torch.Tensor.unique_consecutive.html#torch.Tensor.unique_consecutive">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">unique_consecutive</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">return_inverse</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">return_counts</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Eliminates all but the first element from every consecutive group of equivalent elements.</span>

<span class="sd">        See :func:`torch.unique_consecutive`</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">has_torch_function_unary</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span>
                <span class="n">Tensor</span><span class="o">.</span><span class="n">unique_consecutive</span><span class="p">,</span>
                <span class="p">(</span><span class="bp">self</span><span class="p">,),</span>
                <span class="bp">self</span><span class="p">,</span>
                <span class="n">return_inverse</span><span class="o">=</span><span class="n">return_inverse</span><span class="p">,</span>
                <span class="n">return_counts</span><span class="o">=</span><span class="n">return_counts</span><span class="p">,</span>
                <span class="n">dim</span><span class="o">=</span><span class="n">dim</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">unique_consecutive</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">,</span> <span class="n">return_inverse</span><span class="o">=</span><span class="n">return_inverse</span><span class="p">,</span> <span class="n">return_counts</span><span class="o">=</span><span class="n">return_counts</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">dim</span>
        <span class="p">)</span></div>


    <span class="nd">@_handle_torch_function_and_wrap_type_error_to_not_implemented</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__rsub__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">_C</span><span class="o">.</span><span class="n">_VariableFunctions</span><span class="o">.</span><span class="n">rsub</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span>

    <span class="nd">@_handle_torch_function_and_wrap_type_error_to_not_implemented</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">__rdiv__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">reciprocal</span><span class="p">()</span> <span class="o">*</span> <span class="n">other</span>

    <span class="fm">__rtruediv__</span> <span class="o">=</span> <span class="n">__rdiv__</span>
    <span class="fm">__itruediv__</span> <span class="o">=</span> <span class="n">_C</span><span class="o">.</span><span class="n">TensorBase</span><span class="o">.</span><span class="n">__idiv__</span>

    <span class="fm">__pow__</span> <span class="o">=</span> <span class="n">_handle_torch_function_and_wrap_type_error_to_not_implemented</span><span class="p">(</span>
        <span class="n">_C</span><span class="o">.</span><span class="n">TensorBase</span><span class="o">.</span><span class="n">pow</span>
    <span class="p">)</span>
    <span class="fm">__ipow__</span> <span class="o">=</span> <span class="n">_handle_torch_function_and_wrap_type_error_to_not_implemented</span><span class="p">(</span>
        <span class="n">_C</span><span class="o">.</span><span class="n">TensorBase</span><span class="o">.</span><span class="n">pow_</span>
    <span class="p">)</span>

    <span class="nd">@_handle_torch_function_and_wrap_type_error_to_not_implemented</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__rmod__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">remainder</span><span class="p">(</span><span class="n">other</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__format__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">format_spec</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">has_torch_function_unary</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span><span class="n">Tensor</span><span class="o">.</span><span class="fm">__format__</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="p">,),</span> <span class="bp">self</span><span class="p">,</span> <span class="n">format_spec</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">and</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_meta</span> <span class="ow">and</span> <span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="ow">is</span> <span class="n">Tensor</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="o">.</span><span class="fm">__format__</span><span class="p">(</span><span class="n">format_spec</span><span class="p">)</span>
        <span class="k">return</span> <span class="nb">object</span><span class="o">.</span><span class="fm">__format__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">format_spec</span><span class="p">)</span>

    <span class="nd">@_handle_torch_function_and_wrap_type_error_to_not_implemented</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__rpow__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="n">other</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span>

    <span class="nd">@_handle_torch_function_and_wrap_type_error_to_not_implemented</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__floordiv__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">floor_divide</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span>

    <span class="nd">@_handle_torch_function_and_wrap_type_error_to_not_implemented</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__rfloordiv__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">floor_divide</span><span class="p">(</span><span class="n">other</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span>

    <span class="nd">@_handle_torch_function_and_wrap_type_error_to_not_implemented</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__rlshift__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">bitwise_left_shift</span><span class="p">(</span><span class="n">other</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span>

    <span class="nd">@_handle_torch_function_and_wrap_type_error_to_not_implemented</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__rrshift__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">bitwise_right_shift</span><span class="p">(</span><span class="n">other</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span>

    <span class="nd">@_handle_torch_function_and_wrap_type_error_to_not_implemented</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__rmatmul__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">other</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span>

    <span class="fm">__pos__</span> <span class="o">=</span> <span class="n">_C</span><span class="o">.</span><span class="n">TensorBase</span><span class="o">.</span><span class="n">positive</span>
    <span class="fm">__neg__</span> <span class="o">=</span> <span class="n">_C</span><span class="o">.</span><span class="n">TensorBase</span><span class="o">.</span><span class="n">neg</span>
    <span class="fm">__abs__</span> <span class="o">=</span> <span class="n">_C</span><span class="o">.</span><span class="n">TensorBase</span><span class="o">.</span><span class="n">abs</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">has_torch_function_unary</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span><span class="n">Tensor</span><span class="o">.</span><span class="fm">__len__</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="p">,),</span> <span class="bp">self</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;len() of a 0-d tensor&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_get_tracing_state</span><span class="p">():</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
                <span class="s2">&quot;Using len to get tensor shape might cause the trace to be incorrect. &quot;</span>
                <span class="s2">&quot;Recommended usage would be tensor.shape[0]. &quot;</span>
                <span class="s2">&quot;Passing a tensor of different shape might lead to errors or silently give &quot;</span>
                <span class="s2">&quot;incorrect results.&quot;</span><span class="p">,</span>
                <span class="n">category</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">TracerWarning</span><span class="p">,</span>
                <span class="n">stacklevel</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__iter__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># NB: we use &#39;imap&#39; and not &#39;map&#39; here, so that in Python 2 we get a</span>
        <span class="c1"># generator and don&#39;t eagerly perform all the indexes.  This could</span>
        <span class="c1"># save us work, and also helps keep trace ordering deterministic</span>
        <span class="c1"># (e.g., if you zip(*hiddens), the eager map will force all the</span>
        <span class="c1"># indexes of hiddens[0] before hiddens[1], while the generator</span>
        <span class="c1"># map will interleave them.)</span>
        <span class="c1"># NB: We have intentionally skipped __torch_function__ dispatch here.</span>
        <span class="c1"># See gh-54457</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;iteration over a 0-d tensor&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_get_tracing_state</span><span class="p">():</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
                <span class="s2">&quot;Iterating over a tensor might cause the trace to be incorrect. &quot;</span>
                <span class="s2">&quot;Passing a tensor of different shape won&#39;t change the number of &quot;</span>
                <span class="s2">&quot;iterations executed (and might lead to errors or silently give &quot;</span>
                <span class="s2">&quot;incorrect results).&quot;</span><span class="p">,</span>
                <span class="n">category</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">TracerWarning</span><span class="p">,</span>
                <span class="n">stacklevel</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">return</span> <span class="nb">iter</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">unbind</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__hash__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># Do NOT handle __torch_function__ here as user&#39;s default</span>
        <span class="c1"># implementation that handle most functions will most likely do it wrong.</span>
        <span class="c1"># It can be easily overridden by defining this method on the user</span>
        <span class="c1"># subclass if needed.</span>
        <span class="k">return</span> <span class="nb">id</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__dir__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">has_torch_function_unary</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span><span class="n">Tensor</span><span class="o">.</span><span class="fm">__dir__</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="p">,),</span> <span class="bp">self</span><span class="p">)</span>
        <span class="n">tensor_methods</span> <span class="o">=</span> <span class="nb">dir</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="p">)</span>
        <span class="n">tensor_methods</span><span class="o">.</span><span class="n">remove</span><span class="p">(</span><span class="s2">&quot;volatile&quot;</span><span class="p">)</span>  <span class="c1"># deprecated</span>
        <span class="n">attrs</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="vm">__dict__</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
        <span class="n">keys</span> <span class="o">=</span> <span class="n">tensor_methods</span> <span class="o">+</span> <span class="n">attrs</span>

        <span class="c1"># property only available dense, cuda tensors</span>
        <span class="k">if</span> <span class="p">(</span><span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_cuda</span><span class="p">)</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_sparse</span><span class="p">:</span>
            <span class="n">keys</span><span class="o">.</span><span class="n">remove</span><span class="p">(</span><span class="s2">&quot;__cuda_array_interface__&quot;</span><span class="p">)</span>

        <span class="k">return</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">keys</span><span class="p">)</span>

    <span class="c1"># Numpy array interface, to support `numpy.asarray(tensor) -&gt; ndarray`</span>
    <span class="n">__array_priority__</span> <span class="o">=</span> <span class="mi">1000</span>  <span class="c1"># prefer Tensor ops over numpy ones</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">__array__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">has_torch_function_unary</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span><span class="n">Tensor</span><span class="o">.</span><span class="n">__array__</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="p">,),</span> <span class="bp">self</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">dtype</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">dtype</span><span class="p">,</span> <span class="n">copy</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="c1"># Wrap Numpy array again in a suitable tensor when done, to support e.g.</span>
    <span class="c1"># `numpy.sin(tensor) -&gt; tensor` or `numpy.greater(tensor, 0) -&gt; ByteTensor`</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">__array_wrap__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">array</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">has_torch_function_unary</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span>
                <span class="n">Tensor</span><span class="o">.</span><span class="n">__array_wrap__</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="p">,),</span> <span class="bp">self</span><span class="p">,</span> <span class="n">array</span><span class="o">=</span><span class="n">array</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="n">array</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="nb">bool</span><span class="p">:</span>
            <span class="c1"># Workaround, torch has no built-in bool tensor</span>
            <span class="n">array</span> <span class="o">=</span> <span class="n">array</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s2">&quot;uint8&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">array</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__contains__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">element</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="o">/</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Check if `element` is present in tensor</span>

<span class="sd">        Args:</span>
<span class="sd">            element (Tensor or scalar): element to be checked</span>
<span class="sd">                for presence in current tensor&quot;</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">has_torch_function_unary</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span><span class="n">Tensor</span><span class="o">.</span><span class="fm">__contains__</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="p">,),</span> <span class="bp">self</span><span class="p">,</span> <span class="n">element</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span>
            <span class="n">element</span><span class="p">,</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Number</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">SymInt</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">SymFloat</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">SymBool</span><span class="p">)</span>
        <span class="p">):</span>
            <span class="c1"># type hint doesn&#39;t understand the __contains__ result array</span>
            <span class="k">return</span> <span class="nb">bool</span><span class="p">((</span><span class="n">element</span> <span class="o">==</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">any</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>  <span class="c1"># type: ignore[union-attr]</span>

        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;Tensor.__contains__ only supports Tensor or scalar, but you passed in a </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">element</span><span class="p">)</span><span class="si">}</span><span class="s2">.&quot;</span>
        <span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">__cuda_array_interface__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Array view description for cuda tensors.</span>

<span class="sd">        See:</span>
<span class="sd">        https://numba.pydata.org/numba-doc/latest/cuda/cuda_array_interface.html</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">has_torch_function_unary</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
            <span class="c1"># TODO mypy doesn&#39;t support @property, see: https://github.com/python/mypy/issues/6185</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span>
                <span class="n">Tensor</span><span class="o">.</span><span class="n">__cuda_array_interface__</span><span class="o">.</span><span class="fm">__get__</span><span class="p">,</span>  <span class="c1"># type: ignore[attr-defined]</span>
                <span class="p">(</span><span class="bp">self</span><span class="p">,),</span>
                <span class="bp">self</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="c1"># raise AttributeError for unsupported tensors, so that</span>
        <span class="c1"># hasattr(cpu_tensor, &quot;__cuda_array_interface__&quot;) is False.</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_cuda</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">AttributeError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Can&#39;t get __cuda_array_interface__ on non-CUDA tensor type: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">type</span><span class="p">()</span><span class="si">}</span><span class="s2"> &quot;</span>
                <span class="s2">&quot;If CUDA data is required use tensor.cuda() to copy tensor to device memory.&quot;</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_sparse</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">AttributeError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Can&#39;t get __cuda_array_interface__ on sparse type: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">type</span><span class="p">()</span><span class="si">}</span><span class="s2"> &quot;</span>
                <span class="s2">&quot;Use Tensor.to_dense() to convert to a dense tensor first.&quot;</span>
            <span class="p">)</span>

        <span class="c1"># RuntimeError, matching tensor.__array__() behavior.</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                <span class="s2">&quot;Can&#39;t get __cuda_array_interface__ on Variable that requires grad. &quot;</span>
                <span class="s2">&quot;If gradients aren&#39;t required, use var.detach() to get Variable that doesn&#39;t require grad.&quot;</span>
            <span class="p">)</span>

        <span class="c1"># CUDA devices are little-endian and tensors are stored in native byte</span>
        <span class="c1"># order. 1-byte entries are endian-agnostic.</span>
        <span class="n">typestr</span> <span class="o">=</span> <span class="p">{</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">complex64</span><span class="p">:</span> <span class="s2">&quot;&lt;c8&quot;</span><span class="p">,</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">complex128</span><span class="p">:</span> <span class="s2">&quot;&lt;c16&quot;</span><span class="p">,</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">:</span> <span class="s2">&quot;&lt;f2&quot;</span><span class="p">,</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">:</span> <span class="s2">&quot;&lt;f2&quot;</span><span class="p">,</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">:</span> <span class="s2">&quot;&lt;f4&quot;</span><span class="p">,</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">float64</span><span class="p">:</span> <span class="s2">&quot;&lt;f8&quot;</span><span class="p">,</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">uint8</span><span class="p">:</span> <span class="s2">&quot;|u1&quot;</span><span class="p">,</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">int8</span><span class="p">:</span> <span class="s2">&quot;|i1&quot;</span><span class="p">,</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">uint16</span><span class="p">:</span> <span class="s2">&quot;&lt;u2&quot;</span><span class="p">,</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">int16</span><span class="p">:</span> <span class="s2">&quot;&lt;i2&quot;</span><span class="p">,</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">uint32</span><span class="p">:</span> <span class="s2">&quot;&lt;u4&quot;</span><span class="p">,</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">:</span> <span class="s2">&quot;&lt;i4&quot;</span><span class="p">,</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">uint64</span><span class="p">:</span> <span class="s2">&quot;&lt;u8&quot;</span><span class="p">,</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">int64</span><span class="p">:</span> <span class="s2">&quot;&lt;i8&quot;</span><span class="p">,</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">bool</span><span class="p">:</span> <span class="s2">&quot;|b1&quot;</span><span class="p">,</span>
        <span class="p">}[</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">]</span>

        <span class="n">itemsize</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">element_size</span><span class="p">()</span>

        <span class="n">shape</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_contiguous</span><span class="p">():</span>
            <span class="c1"># __cuda_array_interface__ v2 requires the strides to be omitted</span>
            <span class="c1"># (either not set or set to None) for C-contiguous arrays.</span>
            <span class="n">strides</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">strides</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">s</span> <span class="o">*</span> <span class="n">itemsize</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">())</span>
        <span class="n">data_ptr</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">data_ptr</span><span class="p">()</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="k">else</span> <span class="mi">0</span>
        <span class="n">data</span> <span class="o">=</span> <span class="p">(</span><span class="n">data_ptr</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>  <span class="c1"># read-only is false</span>

        <span class="k">return</span> <span class="nb">dict</span><span class="p">(</span><span class="n">typestr</span><span class="o">=</span><span class="n">typestr</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="n">shape</span><span class="p">,</span> <span class="n">strides</span><span class="o">=</span><span class="n">strides</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="p">,</span> <span class="n">version</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

<div class="viewcode-block" id="Tensor.storage_type">
<a class="viewcode-back" href="../../python-api/generated/torch.Tensor.storage_type.html#torch.Tensor.storage_type">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">storage_type</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;storage_type() -&gt; type</span>

<span class="sd">        Returns the type of the underlying storage.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">has_torch_function_unary</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span><span class="n">Tensor</span><span class="o">.</span><span class="n">storage_type</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="p">,),</span> <span class="bp">self</span><span class="p">)</span>

        <span class="n">torch</span><span class="o">.</span><span class="n">storage</span><span class="o">.</span><span class="n">_warn_typed_storage_removal</span><span class="p">()</span>

        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_typed_storage</span><span class="p">()</span><span class="o">.</span><span class="n">_get_legacy_storage_class</span><span class="p">()</span></div>


<div class="viewcode-block" id="Tensor.refine_names">
<a class="viewcode-back" href="../../python-api/named_tensor.html#torch.Tensor.refine_names">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">refine_names</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">names</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Refines the dimension names of :attr:`self` according to :attr:`names`.</span>

<span class="sd">        Refining is a special case of renaming that &quot;lifts&quot; unnamed dimensions.</span>
<span class="sd">        A ``None`` dim can be refined to have any name; a named dim can only be</span>
<span class="sd">        refined to have the same name.</span>

<span class="sd">        Because named tensors can coexist with unnamed tensors, refining names</span>
<span class="sd">        gives a nice way to write named-tensor-aware code that works with both</span>
<span class="sd">        named and unnamed tensors.</span>

<span class="sd">        :attr:`names` may contain up to one Ellipsis (``...``).</span>
<span class="sd">        The Ellipsis is expanded greedily; it is expanded in-place to fill</span>
<span class="sd">        :attr:`names` to the same length as ``self.dim()`` using names from the</span>
<span class="sd">        corresponding indices of ``self.names``.</span>

<span class="sd">        Python 2 does not support Ellipsis but one may use a string literal</span>
<span class="sd">        instead (``&#39;...&#39;``).</span>

<span class="sd">        Args:</span>
<span class="sd">            names (iterable of str): The desired names of the output tensor. May</span>
<span class="sd">                contain up to one Ellipsis.</span>

<span class="sd">        Examples::</span>

<span class="sd">            &gt;&gt;&gt; imgs = torch.randn(32, 3, 128, 128)</span>
<span class="sd">            &gt;&gt;&gt; named_imgs = imgs.refine_names(&#39;N&#39;, &#39;C&#39;, &#39;H&#39;, &#39;W&#39;)</span>
<span class="sd">            &gt;&gt;&gt; named_imgs.names</span>
<span class="sd">            (&#39;N&#39;, &#39;C&#39;, &#39;H&#39;, &#39;W&#39;)</span>

<span class="sd">            &gt;&gt;&gt; tensor = torch.randn(2, 3, 5, 7, 11)</span>
<span class="sd">            &gt;&gt;&gt; tensor = tensor.refine_names(&#39;A&#39;, ..., &#39;B&#39;, &#39;C&#39;)</span>
<span class="sd">            &gt;&gt;&gt; tensor.names</span>
<span class="sd">            (&#39;A&#39;, None, None, &#39;B&#39;, &#39;C&#39;)</span>

<span class="sd">        .. warning::</span>
<span class="sd">            The named tensor API is experimental and subject to change.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">has_torch_function_unary</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span><span class="n">Tensor</span><span class="o">.</span><span class="n">refine_names</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="p">,),</span> <span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">names</span><span class="p">)</span>
        <span class="n">names</span> <span class="o">=</span> <span class="n">resolve_ellipsis</span><span class="p">(</span><span class="n">names</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">names</span><span class="p">,</span> <span class="s2">&quot;refine_names&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">refine_names</span><span class="p">(</span><span class="n">names</span><span class="p">)</span></div>


<div class="viewcode-block" id="Tensor.align_to">
<a class="viewcode-back" href="../../python-api/named_tensor.html#torch.Tensor.align_to">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">align_to</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">names</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Permutes the dimensions of the :attr:`self` tensor to match the order</span>
<span class="sd">        specified in :attr:`names`, adding size-one dims for any new names.</span>

<span class="sd">        All of the dims of :attr:`self` must be named in order to use this method.</span>
<span class="sd">        The resulting tensor is a view on the original tensor.</span>

<span class="sd">        All dimension names of :attr:`self` must be present in :attr:`names`.</span>
<span class="sd">        :attr:`names` may contain additional names that are not in ``self.names``;</span>
<span class="sd">        the output tensor has a size-one dimension for each of those new names.</span>

<span class="sd">        :attr:`names` may contain up to one Ellipsis (``...``).</span>
<span class="sd">        The Ellipsis is expanded to be equal to all dimension names of :attr:`self`</span>
<span class="sd">        that are not mentioned in :attr:`names`, in the order that they appear</span>
<span class="sd">        in :attr:`self`.</span>

<span class="sd">        Python 2 does not support Ellipsis but one may use a string literal</span>
<span class="sd">        instead (``&#39;...&#39;``).</span>

<span class="sd">        Args:</span>
<span class="sd">            names (iterable of str): The desired dimension ordering of the</span>
<span class="sd">                output tensor. May contain up to one Ellipsis that is expanded</span>
<span class="sd">                to all unmentioned dim names of :attr:`self`.</span>

<span class="sd">        Examples::</span>

<span class="sd">            &gt;&gt;&gt; tensor = torch.randn(2, 2, 2, 2, 2, 2)</span>
<span class="sd">            &gt;&gt;&gt; named_tensor = tensor.refine_names(&#39;A&#39;, &#39;B&#39;, &#39;C&#39;, &#39;D&#39;, &#39;E&#39;, &#39;F&#39;)</span>

<span class="sd">            # Move the F and E dims to the front while keeping the rest in order</span>
<span class="sd">            &gt;&gt;&gt; named_tensor.align_to(&#39;F&#39;, &#39;E&#39;, ...)</span>

<span class="sd">        .. warning::</span>
<span class="sd">            The named tensor API is experimental and subject to change.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">has_torch_function_unary</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span><span class="n">Tensor</span><span class="o">.</span><span class="n">align_to</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="p">,),</span> <span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">names</span><span class="p">)</span>
        <span class="n">ellipsis_idx</span> <span class="o">=</span> <span class="n">single_ellipsis_index</span><span class="p">(</span><span class="n">names</span><span class="p">,</span> <span class="s2">&quot;align_to&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">ellipsis_idx</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">align_to</span><span class="p">(</span><span class="n">names</span><span class="p">)</span>
        <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">align_to</span><span class="p">(</span>
            <span class="p">[</span><span class="n">name</span> <span class="k">for</span> <span class="n">name</span> <span class="ow">in</span> <span class="n">names</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">is_ellipsis</span><span class="p">(</span><span class="n">name</span><span class="p">)],</span> <span class="n">ellipsis_idx</span>
        <span class="p">)</span></div>


<div class="viewcode-block" id="Tensor.unflatten">
<a class="viewcode-back" href="../../python-api/generated/torch.Tensor.unflatten.html#torch.Tensor.unflatten">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">unflatten</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">sizes</span><span class="p">):</span>  <span class="c1"># type: ignore[override]</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        unflatten(dim, sizes) -&gt; Tensor</span>

<span class="sd">        See :func:`torch.unflatten`.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">has_torch_function_unary</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span><span class="n">Tensor</span><span class="o">.</span><span class="n">unflatten</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="p">,),</span> <span class="bp">self</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">sizes</span><span class="p">)</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">sizes</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;unflatten: sizes must be non-empty&quot;</span><span class="p">)</span>

        <span class="n">names</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">sizes</span><span class="p">,</span> <span class="n">OrderedDict</span><span class="p">)</span> <span class="ow">or</span> <span class="p">(</span>
            <span class="nb">isinstance</span><span class="p">(</span><span class="n">sizes</span><span class="p">,</span> <span class="p">(</span><span class="nb">tuple</span><span class="p">,</span> <span class="nb">list</span><span class="p">))</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">sizes</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="p">(</span><span class="nb">tuple</span><span class="p">,</span> <span class="nb">list</span><span class="p">))</span>
        <span class="p">):</span>
            <span class="n">names</span><span class="p">,</span> <span class="n">sizes</span> <span class="o">=</span> <span class="n">unzip_namedshape</span><span class="p">(</span><span class="n">sizes</span><span class="p">)</span>
            <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">unflatten</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">sizes</span><span class="p">,</span> <span class="n">names</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">unflatten</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">sizes</span><span class="p">)</span></div>


<div class="viewcode-block" id="Tensor.rename_">
<a class="viewcode-back" href="../../python-api/named_tensor.html#torch.Tensor.rename_">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">rename_</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">names</span><span class="p">,</span> <span class="o">**</span><span class="n">rename_map</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;In-place version of :meth:`~Tensor.rename`.&quot;&quot;&quot;</span>

        <span class="k">if</span> <span class="n">has_torch_function_unary</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span>
                <span class="n">Tensor</span><span class="o">.</span><span class="n">rename_</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="p">,),</span> <span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">names</span><span class="p">,</span> <span class="o">**</span><span class="n">rename_map</span>
            <span class="p">)</span>

        <span class="c1"># Note [rename_ / rename API]</span>
        <span class="c1"># The Python API for these is different from the C++ API. In Python:</span>
        <span class="c1"># 1) tensor.rename(*names) takes a vararglist of names</span>
        <span class="c1"># 2) tensor.rename(**rename_map) takes a map of names to rename.</span>
        <span class="c1"># C++ is static, making it difficult to implement similar behavior.</span>
        <span class="k">return</span> <span class="n">update_names</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">names</span><span class="p">,</span> <span class="n">rename_map</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span></div>


<div class="viewcode-block" id="Tensor.rename">
<a class="viewcode-back" href="../../python-api/named_tensor.html#torch.Tensor.rename">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">rename</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">names</span><span class="p">,</span> <span class="o">**</span><span class="n">rename_map</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Renames dimension names of :attr:`self`.</span>

<span class="sd">        There are two main usages:</span>

<span class="sd">        ``self.rename(**rename_map)`` returns a view on tensor that has dims</span>
<span class="sd">        renamed as specified in the mapping :attr:`rename_map`.</span>

<span class="sd">        ``self.rename(*names)`` returns a view on tensor, renaming all</span>
<span class="sd">        dimensions positionally using :attr:`names`.</span>
<span class="sd">        Use ``self.rename(None)`` to drop names on a tensor.</span>

<span class="sd">        One cannot specify both positional args :attr:`names` and keyword args</span>
<span class="sd">        :attr:`rename_map`.</span>

<span class="sd">        Examples::</span>

<span class="sd">            &gt;&gt;&gt; imgs = torch.rand(2, 3, 5, 7, names=(&#39;N&#39;, &#39;C&#39;, &#39;H&#39;, &#39;W&#39;))</span>
<span class="sd">            &gt;&gt;&gt; renamed_imgs = imgs.rename(N=&#39;batch&#39;, C=&#39;channels&#39;)</span>
<span class="sd">            &gt;&gt;&gt; renamed_imgs.names</span>
<span class="sd">            (&#39;batch&#39;, &#39;channels&#39;, &#39;H&#39;, &#39;W&#39;)</span>

<span class="sd">            &gt;&gt;&gt; renamed_imgs = imgs.rename(None)</span>
<span class="sd">            &gt;&gt;&gt; renamed_imgs.names</span>
<span class="sd">            (None, None, None, None)</span>

<span class="sd">            &gt;&gt;&gt; renamed_imgs = imgs.rename(&#39;batch&#39;, &#39;channel&#39;, &#39;height&#39;, &#39;width&#39;)</span>
<span class="sd">            &gt;&gt;&gt; renamed_imgs.names</span>
<span class="sd">            (&#39;batch&#39;, &#39;channel&#39;, &#39;height&#39;, &#39;width&#39;)</span>

<span class="sd">        .. warning::</span>
<span class="sd">            The named tensor API is experimental and subject to change.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">has_torch_function_unary</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span>
                <span class="n">Tensor</span><span class="o">.</span><span class="n">rename</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="p">,),</span> <span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">names</span><span class="p">,</span> <span class="o">**</span><span class="n">rename_map</span>
            <span class="p">)</span>

        <span class="c1"># See Note [rename_ / rename API]</span>
        <span class="k">return</span> <span class="n">update_names</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">names</span><span class="p">,</span> <span class="n">rename_map</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span></div>


<div class="viewcode-block" id="Tensor.to_sparse_coo">
<a class="viewcode-back" href="../../python-api/generated/torch.Tensor.to_sparse_coo.html#torch.Tensor.to_sparse_coo">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">to_sparse_coo</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Convert a tensor to :ref:`coordinate format &lt;sparse-coo-docs&gt;`.</span>

<span class="sd">        Examples::</span>

<span class="sd">             &gt;&gt;&gt; dense = torch.randn(5, 5)</span>
<span class="sd">             &gt;&gt;&gt; sparse = dense.to_sparse_coo()</span>
<span class="sd">             &gt;&gt;&gt; sparse._nnz()</span>
<span class="sd">             25</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">to_sparse</span><span class="p">()</span></div>


<div class="viewcode-block" id="Tensor.dim_order">
<a class="viewcode-back" href="../../python-api/generated/torch.Tensor.dim_order.html#torch.Tensor.dim_order">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">dim_order</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">ambiguity_check</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">bool</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">memory_format</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        dim_order(ambiguity_check=False) -&gt; tuple</span>

<span class="sd">        Returns the uniquely determined tuple of int describing the dim order or</span>
<span class="sd">        physical layout of :attr:`self`.</span>

<span class="sd">        The dim order represents how dimensions are laid out in memory,</span>
<span class="sd">        starting from the outermost to the innermost dimension.</span>

<span class="sd">        Note that the dim order may not always be uniquely determined.</span>
<span class="sd">        If `ambiguity_check` is True, this function raises a RuntimeError when the dim order cannot be uniquely determined;</span>
<span class="sd">        If `ambiguity_check` is a list of memory formats, this function raises a RuntimeError when tensor can not be interpreted</span>
<span class="sd">        into exactly one of the given memory formats, or it cannot be uniquely determined.</span>
<span class="sd">        If `ambiguity_check` is False, it will return one of legal dim order(s) without checking its uniqueness.</span>
<span class="sd">        Otherwise, it will raise TypeError.</span>

<span class="sd">        Args:</span>
<span class="sd">            ambiguity_check (bool or List[torch.memory_format]): The check method for ambiguity of dim order.</span>

<span class="sd">            &gt;&gt;&gt; torch.empty((2, 3, 5, 7)).dim_order()</span>
<span class="sd">            (0, 1, 2, 3)</span>
<span class="sd">            &gt;&gt;&gt; torch.empty((2, 3, 5, 7)).transpose(1, 2).dim_order()</span>
<span class="sd">            (0, 2, 1, 3)</span>
<span class="sd">            &gt;&gt;&gt; torch.empty((2, 3, 5, 7), memory_format=torch.channels_last).dim_order()</span>
<span class="sd">            (0, 2, 3, 1)</span>
<span class="sd">            &gt;&gt;&gt; torch.empty((1, 2, 3, 4)).dim_order()</span>
<span class="sd">            (0, 1, 2, 3)</span>
<span class="sd">            &gt;&gt;&gt; try:</span>
<span class="sd">            ...     torch.empty((1, 2, 3, 4)).dim_order(ambiguity_check=True)</span>
<span class="sd">            ... except RuntimeError as e:</span>
<span class="sd">            ...     print(e)</span>
<span class="sd">            The tensor does not have unique dim order, or cannot map to exact one of the given memory formats.</span>
<span class="sd">            &gt;&gt;&gt; torch.empty((1, 2, 3, 4)).dim_order(</span>
<span class="sd">            ...     ambiguity_check=[torch.contiguous_format, torch.channels_last]</span>
<span class="sd">            ... )  # It can be mapped to contiguous format</span>
<span class="sd">            (0, 1, 2, 3)</span>
<span class="sd">            &gt;&gt;&gt; try:</span>
<span class="sd">            ...     torch.empty((1, 2, 3, 4)).dim_order(ambiguity_check=&quot;ILLEGAL&quot;)</span>
<span class="sd">            ... except TypeError as e:</span>
<span class="sd">            ...     print(e)</span>
<span class="sd">            The ambiguity_check argument must be a bool or a list of memory formats.</span>
<span class="sd">        .. warning::</span>
<span class="sd">            The dim_order tensor API is experimental and subject to change.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">has_torch_function_unary</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span><span class="n">Tensor</span><span class="o">.</span><span class="n">dim_order</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="p">,),</span> <span class="bp">self</span><span class="p">)</span>

        <span class="c1"># Sanity check ambiguity_check data types</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">ambiguity_check</span><span class="p">,</span> <span class="nb">bool</span><span class="p">):</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">ambiguity_check</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span>
                    <span class="s2">&quot;The ambiguity_check argument must be a bool or a list of memory formats.&quot;</span>
                <span class="p">)</span>
            <span class="k">for</span> <span class="n">memory_format</span> <span class="ow">in</span> <span class="n">ambiguity_check</span><span class="p">:</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">memory_format</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">memory_format</span><span class="p">):</span>
                    <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span>
                        <span class="s2">&quot;The ambiguity_check argument must be a bool or a list of memory formats.&quot;</span>
                    <span class="p">)</span>

        <span class="k">def</span><span class="w"> </span><span class="nf">invalid_unique_memory_format</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">valid_memory_formats</span><span class="p">):</span>
<span class="w">            </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">            Returns True if the tensor cannot be uniquely mapped to any of the given memory formats, False otherwise.</span>
<span class="sd">            &quot;&quot;&quot;</span>

            <span class="n">n_legality</span> <span class="o">=</span> <span class="mi">0</span>

            <span class="k">for</span> <span class="n">memory_format</span> <span class="ow">in</span> <span class="n">valid_memory_formats</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">tensor</span><span class="o">.</span><span class="n">is_contiguous</span><span class="p">(</span><span class="n">memory_format</span><span class="o">=</span><span class="n">memory_format</span><span class="p">):</span>
                    <span class="n">n_legality</span> <span class="o">+=</span> <span class="mi">1</span>

            <span class="k">return</span> <span class="n">n_legality</span> <span class="o">!=</span> <span class="mi">1</span>

        <span class="k">def</span><span class="w"> </span><span class="nf">has_multiple_dim_order</span><span class="p">(</span><span class="n">tensor</span><span class="p">):</span>
<span class="w">            </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">            Returns True if there&#39;re multiple legal dim orders for given tensor, False otherwise.</span>

<span class="sd">            The tensor is considered to have multiple legal dim orders if either of the following conditions is met:</span>

<span class="sd">            * Singleton Dimensions: There&#39;s at least one singleteon dimension in the tensor.</span>
<span class="sd">              Since their size is 1, they don&#39;t affect the memory offset (stride * index</span>
<span class="sd">              is zero because index is always zero). Therefore, they can be placed anywhere</span>
<span class="sd">              in the dimension order without changing how data is accessed.</span>
<span class="sd">            * Same strides: Strides reflect how the tensor is stored in memory.</span>
<span class="sd">              If any two dimensions have the same stride, swapping these dimensions won&#39;t</span>
<span class="sd">              change how data is accessed, leading to multiple correct dimension orders.</span>
<span class="sd">            &quot;&quot;&quot;</span>

            <span class="n">sizes</span> <span class="o">=</span> <span class="n">tensor</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
            <span class="n">strides</span> <span class="o">=</span> <span class="n">tensor</span><span class="o">.</span><span class="n">stride</span><span class="p">()</span>

            <span class="c1"># Check if there are any duplicate strides</span>
            <span class="n">has_duplicate_strides</span> <span class="o">=</span> <span class="nb">any</span><span class="p">(</span>
                <span class="n">earlier</span> <span class="o">==</span> <span class="n">later</span> <span class="k">for</span> <span class="n">earlier</span><span class="p">,</span> <span class="n">later</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">strides</span><span class="p">,</span> <span class="n">strides</span><span class="p">[</span><span class="mi">1</span><span class="p">:])</span>
            <span class="p">)</span>

            <span class="c1"># Check if there are any singleton dimensions</span>
            <span class="n">has_singleton_dims</span> <span class="o">=</span> <span class="nb">any</span><span class="p">(</span><span class="n">size</span> <span class="o">==</span> <span class="mi">1</span> <span class="k">for</span> <span class="n">size</span> <span class="ow">in</span> <span class="n">sizes</span><span class="p">)</span>

            <span class="k">return</span> <span class="n">has_duplicate_strides</span> <span class="ow">or</span> <span class="n">has_singleton_dims</span>

        <span class="n">valid_memory_formats</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">ambiguity_check</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">ambiguity_check</span><span class="p">,</span> <span class="nb">list</span><span class="p">)</span> <span class="k">else</span> <span class="p">[]</span>
        <span class="p">)</span>
        <span class="n">check_multiple_dim_order</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">ambiguity_check</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">ambiguity_check</span><span class="p">,</span> <span class="nb">bool</span><span class="p">)</span> <span class="k">else</span> <span class="kc">True</span>
        <span class="p">)</span>

        <span class="k">if</span> <span class="p">(</span>
            <span class="n">check_multiple_dim_order</span> <span class="ow">and</span> <span class="n">has_multiple_dim_order</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
        <span class="p">)</span> <span class="ow">and</span> <span class="n">invalid_unique_memory_format</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">valid_memory_formats</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                <span class="s2">&quot;The tensor does not have unique dim order, or cannot map to exact one of the given memory formats.&quot;</span>
            <span class="p">)</span>

        <span class="kn">import</span><span class="w"> </span><span class="nn">torch._prims_common</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">utils</span>

        <span class="k">return</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">utils</span><span class="o">.</span><span class="n">compute_elementwise_output_logical_to_physical_perm</span><span class="p">(</span><span class="bp">self</span><span class="p">))</span></div>


    <span class="k">def</span><span class="w"> </span><span class="nf">_update_names</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">names</span><span class="p">,</span> <span class="n">inplace</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">has_torch_function_unary</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span>
                <span class="n">Tensor</span><span class="o">.</span><span class="n">_update_names</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="p">,),</span> <span class="bp">self</span><span class="p">,</span> <span class="n">names</span><span class="p">,</span> <span class="n">inplace</span>
            <span class="p">)</span>

        <span class="c1"># See Note [rename_ / rename API]</span>
        <span class="k">if</span> <span class="n">inplace</span><span class="p">:</span>
            <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">rename_</span><span class="p">(</span><span class="n">names</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">rename</span><span class="p">(</span><span class="n">names</span><span class="p">)</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">__torch_function__</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">func</span><span class="p">,</span> <span class="n">types</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span><span class="p">(),</span> <span class="n">kwargs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        This __torch_function__ implementation wraps subclasses such that</span>
<span class="sd">        methods called on subclasses return a subclass instance instead of</span>
<span class="sd">        a ``torch.Tensor`` instance.</span>

<span class="sd">        One corollary to this is that you need coverage for torch.Tensor</span>
<span class="sd">        methods if implementing __torch_function__ for subclasses.</span>

<span class="sd">        We recommend always calling ``super().__torch_function__`` as the base</span>
<span class="sd">        case when doing the above.</span>

<span class="sd">        While not mandatory, we recommend making `__torch_function__` a classmethod.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">kwargs</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">kwargs</span> <span class="o">=</span> <span class="p">{}</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="nb">all</span><span class="p">(</span><span class="nb">issubclass</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">types</span><span class="p">):</span>
            <span class="k">return</span> <span class="bp">NotImplemented</span>

        <span class="k">with</span> <span class="n">_C</span><span class="o">.</span><span class="n">DisableTorchFunctionSubclass</span><span class="p">():</span>
            <span class="n">ret</span> <span class="o">=</span> <span class="n">func</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">func</span> <span class="ow">in</span> <span class="n">get_default_nowrap_functions</span><span class="p">():</span>
                <span class="k">return</span> <span class="n">ret</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">_convert</span><span class="p">(</span><span class="n">ret</span><span class="p">,</span> <span class="bp">cls</span><span class="p">)</span>

    <span class="n">__torch_dispatch__</span> <span class="o">=</span> <span class="n">_C</span><span class="o">.</span><span class="n">_disabled_torch_dispatch_impl</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">__dlpack__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">stream</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Creates a DLpack `capsule https://data-apis.org/array-api/latest/design_topics/data_interchange.html#data-interchange`_</span>
<span class="sd">        of the current tensor to be exported to other libraries.</span>

<span class="sd">        This function will be called from the `from_dlpack` method</span>
<span class="sd">        of the library that will consume the capsule. `from_dlpack` passes the current</span>
<span class="sd">        stream to this method as part of the specification.</span>

<span class="sd">        Args:</span>
<span class="sd">            stream (integer or None): An optional Python integer representing a</span>
<span class="sd">            pointer to a CUDA stream. The current stream is synchronized with</span>
<span class="sd">            this stream before the capsule is created, and since the capsule</span>
<span class="sd">            shares its storage with the tensor this make it safe to access from</span>
<span class="sd">            both streams.  If None or -1 is passed then no synchronization is performed.</span>
<span class="sd">            If 1 (on CUDA) or 0 (on ROCM) then the default stream is used for</span>
<span class="sd">            synchronization.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">has_torch_function_unary</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span><span class="n">Tensor</span><span class="o">.</span><span class="n">__dlpack__</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="p">,),</span> <span class="bp">self</span><span class="p">,</span> <span class="n">stream</span><span class="p">)</span>

        <span class="c1"># DLPack capsules can&#39;t capture all of PyTorch&#39;s semantics,</span>
        <span class="c1"># so we prohibit exporting tensors that would lose their properties like</span>
        <span class="c1"># requires_grad and having the conjugate bit set.</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                <span class="s2">&quot;Can&#39;t export tensors that require gradient, use tensor.detach()&quot;</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_conj</span><span class="p">():</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;Can&#39;t export tensors with the conjugate bit set&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">layout</span> <span class="o">!=</span> <span class="n">torch</span><span class="o">.</span><span class="n">strided</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                <span class="s2">&quot;Can&#39;t export tensors with layout other than torch.strided&quot;</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="n">stream</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="nb">type</span><span class="p">(</span><span class="n">stream</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="nb">int</span><span class="p">:</span>
            <span class="c1"># Stream pointers in CUDA/ROCm are uniquely numbered and can</span>
            <span class="c1"># be retrieved from their integer value.</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;stream must be ``int`` or ``none``&quot;</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">stream</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">stream</span> <span class="o">!=</span> <span class="o">-</span><span class="mi">1</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="o">.</span><span class="n">type</span> <span class="o">==</span> <span class="s2">&quot;cuda&quot;</span><span class="p">:</span>
                <span class="c1"># NB: This logic handles the special case values for default</span>
                <span class="c1"># streams and must be kept in sync with from_dlpack in</span>
                <span class="c1"># torch/utils/dlpack.py</span>
                <span class="k">if</span> <span class="n">stream</span> <span class="o">==</span> <span class="mi">1</span> <span class="ow">and</span> <span class="n">torch</span><span class="o">.</span><span class="n">version</span><span class="o">.</span><span class="n">hip</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="n">stream</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">default_stream</span><span class="p">()</span>
                <span class="k">elif</span> <span class="n">stream</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">torch</span><span class="o">.</span><span class="n">version</span><span class="o">.</span><span class="n">hip</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="n">stream</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">default_stream</span><span class="p">()</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">stream</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">ExternalStream</span><span class="p">(</span><span class="n">stream</span><span class="p">)</span>
                <span class="c1"># Only synchronize on different streams</span>
                <span class="n">sync_stream</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">current_stream</span><span class="p">()</span>
                <span class="k">if</span> <span class="n">stream</span> <span class="o">!=</span> <span class="n">sync_stream</span><span class="p">:</span>
                    <span class="n">event</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">Event</span><span class="p">()</span>
                    <span class="n">event</span><span class="o">.</span><span class="n">record</span><span class="p">(</span><span class="n">sync_stream</span><span class="p">)</span>
                    <span class="n">stream</span><span class="o">.</span><span class="n">wait_event</span><span class="p">(</span><span class="n">event</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="o">.</span><span class="n">type</span> <span class="o">==</span> <span class="s2">&quot;xla&quot;</span><span class="p">:</span>
            <span class="kn">import</span><span class="w"> </span><span class="nn">torch_xla</span>
            <span class="kn">import</span><span class="w"> </span><span class="nn">torch_xla.utils.dlpack</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">xla_dlpack</span>

            <span class="k">if</span> <span class="p">(</span>
                <span class="nb">len</span><span class="p">(</span><span class="n">torch_xla</span><span class="o">.</span><span class="n">real_devices</span><span class="p">())</span> <span class="o">&lt;=</span> <span class="mi">0</span>
                <span class="ow">or</span> <span class="s2">&quot;cuda&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">torch_xla</span><span class="o">.</span><span class="n">real_devices</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span>
            <span class="p">):</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                    <span class="s2">&quot;Can&#39;t export to dlpack an XLA tensor that is not on CUDA.&quot;</span>
                <span class="p">)</span>
            <span class="k">return</span> <span class="n">xla_dlpack</span><span class="o">.</span><span class="n">to_dlpack</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">to_dlpack</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">__dlpack_device__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">enum</span><span class="o">.</span><span class="n">IntEnum</span><span class="p">,</span> <span class="nb">int</span><span class="p">]:</span>
        <span class="k">if</span> <span class="n">has_torch_function_unary</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span><span class="n">Tensor</span><span class="o">.</span><span class="n">__dlpack_device__</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="p">,),</span> <span class="bp">self</span><span class="p">)</span>

        <span class="kn">from</span><span class="w"> </span><span class="nn">torch.utils.dlpack</span><span class="w"> </span><span class="kn">import</span> <span class="n">DLDeviceType</span>

        <span class="n">device</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">device</span>
        <span class="n">idx</span> <span class="o">=</span> <span class="n">device</span><span class="o">.</span><span class="n">index</span> <span class="k">if</span> <span class="n">device</span><span class="o">.</span><span class="n">index</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="mi">0</span>
        <span class="n">torch_device_type</span> <span class="o">=</span> <span class="n">device</span><span class="o">.</span><span class="n">type</span>
        <span class="k">if</span> <span class="n">torch_device_type</span> <span class="o">==</span> <span class="s2">&quot;cuda&quot;</span> <span class="ow">and</span> <span class="n">torch</span><span class="o">.</span><span class="n">version</span><span class="o">.</span><span class="n">hip</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">device_type</span> <span class="o">=</span> <span class="n">DLDeviceType</span><span class="o">.</span><span class="n">kDLROCM</span>
        <span class="k">elif</span> <span class="n">torch_device_type</span> <span class="o">==</span> <span class="s2">&quot;cpu&quot;</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_pinned</span><span class="p">():</span>
            <span class="n">device_type</span> <span class="o">=</span> <span class="n">DLDeviceType</span><span class="o">.</span><span class="n">kDLCPUPinned</span>
        <span class="k">elif</span> <span class="n">torch_device_type</span> <span class="o">==</span> <span class="s2">&quot;cuda&quot;</span><span class="p">:</span>
            <span class="n">device_type</span> <span class="o">=</span> <span class="n">DLDeviceType</span><span class="o">.</span><span class="n">kDLGPU</span>
        <span class="k">elif</span> <span class="n">torch_device_type</span> <span class="o">==</span> <span class="s2">&quot;cpu&quot;</span><span class="p">:</span>
            <span class="n">device_type</span> <span class="o">=</span> <span class="n">DLDeviceType</span><span class="o">.</span><span class="n">kDLCPU</span>
        <span class="k">elif</span> <span class="n">torch_device_type</span> <span class="o">==</span> <span class="s2">&quot;xpu&quot;</span><span class="p">:</span>
            <span class="n">device_type</span> <span class="o">=</span> <span class="n">DLDeviceType</span><span class="o">.</span><span class="n">kDLOneAPI</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="o">.</span><span class="n">type</span> <span class="o">==</span> <span class="s2">&quot;privateuse1&quot;</span><span class="p">:</span>
            <span class="n">device_type</span> <span class="o">=</span> <span class="n">DLDeviceType</span><span class="o">.</span><span class="n">kDLExtDev</span>
        <span class="k">elif</span> <span class="n">torch_device_type</span> <span class="o">==</span> <span class="s2">&quot;xla&quot;</span><span class="p">:</span>
            <span class="kn">import</span><span class="w"> </span><span class="nn">torch_xla</span>

            <span class="k">if</span> <span class="p">(</span>
                <span class="nb">len</span><span class="p">(</span><span class="n">torch_xla</span><span class="o">.</span><span class="n">real_devices</span><span class="p">())</span> <span class="o">&lt;=</span> <span class="mi">0</span>
                <span class="ow">or</span> <span class="s2">&quot;cuda&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">torch_xla</span><span class="o">.</span><span class="n">real_devices</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span>
            <span class="p">):</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Unknown device type </span><span class="si">{</span><span class="n">torch_device_type</span><span class="si">}</span><span class="s2"> for Dlpack&quot;</span><span class="p">)</span>

            <span class="n">device_type</span> <span class="o">=</span> <span class="n">DLDeviceType</span><span class="o">.</span><span class="n">kDLGPU</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Unknown device type </span><span class="si">{</span><span class="n">torch_device_type</span><span class="si">}</span><span class="s2"> for Dlpack&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">device_type</span><span class="p">,</span> <span class="n">idx</span><span class="p">)</span>

    <span class="vm">__module__</span> <span class="o">=</span> <span class="s2">&quot;torch&quot;</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_convert</span><span class="p">(</span><span class="n">ret</span><span class="p">,</span> <span class="bp">cls</span><span class="p">):</span>
    <span class="k">if</span> <span class="bp">cls</span> <span class="ow">is</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">ret</span>

    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">ret</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">)</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">ret</span><span class="p">,</span> <span class="bp">cls</span><span class="p">):</span>
        <span class="n">ret</span> <span class="o">=</span> <span class="n">ret</span><span class="o">.</span><span class="n">as_subclass</span><span class="p">(</span><span class="bp">cls</span><span class="p">)</span>

    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">ret</span><span class="p">,</span> <span class="p">(</span><span class="nb">tuple</span><span class="p">,</span> <span class="nb">list</span><span class="p">)):</span>
        <span class="c1"># Also handles things like namedtuples</span>
        <span class="n">ret</span> <span class="o">=</span> <span class="nb">type</span><span class="p">(</span><span class="n">ret</span><span class="p">)(</span><span class="n">_convert</span><span class="p">(</span><span class="n">r</span><span class="p">,</span> <span class="bp">cls</span><span class="p">)</span> <span class="k">for</span> <span class="n">r</span> <span class="ow">in</span> <span class="n">ret</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">ret</span>
</pre></div>

                </article>
              
              
              
                <footer class="bd-footer-article">
                  <div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item"><div class="feedback">
  <div class="rating">
    Rate this Page
    <div class="stars">
        
        <span class="star" data-value="1">★</span>
        
        <span class="star" data-value="2">★</span>
        
        <span class="star" data-value="3">★</span>
        
        <span class="star" data-value="4">★</span>
        
        <span class="star" data-value="5">★</span>
        
    </div>
</div>
  <div class="feedback-send">
    <button class="feedback-btn" onclick="openGitHubIssue()">Send Feedback</button>
  </div>
</div></div>
  
</div>
                </footer>
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
</div>
                </footer>
              
            </div>
            
            
              
            
          </div>
          <footer class="bd-footer-content">
            
          </footer>
        
      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script defer src="../../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf"></script>
<script defer src="../../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf"></script>


  <footer class="bd-footer">
<div class="bd-footer__inner bd-page-width">
  
    <div class="footer-items__start">
      
        <div class="footer-item">

  <p class="copyright">
    
      © Copyright PyTorch Contributors.
      <br/>
    
  </p>
</div>
      
        <div class="footer-item">

  <p class="sphinx-version">
    Created using <a href="https://www.sphinx-doc.org/">Sphinx</a> 8.1.0.
    <br/>
  </p>
</div>
      
    </div>
  
  
  
    <div class="footer-items__end">
      
        <div class="footer-item">
<p class="theme-version">
  <!-- # L10n: Setting the PST URL as an argument as this does not need to be localized -->
  Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.16.1.
</p></div>
      
    </div>
  
</div>

  </footer>
  </body>
</html>