
<!DOCTYPE html>


<html lang="en" data-content_root="../../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>torch.cuda.graphs &#8212; PyTorch main documentation</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  <!--
    this give us a css class that will be invisible only if js is disabled
  -->
  <noscript>
    <style>
      .pst-js-only { display: none !important; }

    </style>
  </noscript>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../../_static/styles/theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />
<link href="../../../_static/styles/pydata-sphinx-theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />

    <link rel="stylesheet" type="text/css" href="../../../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../../../_static/copybutton.css?v=949a1ff5" />
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/katex.min.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/katex-math.css?v=91adb8b6" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="../../../_static/css/jit.css?v=8de1ea5d" />
    <link rel="stylesheet" type="text/css" href="../../../_static/css/custom.css?v=3ccf5357" />
    <link rel="stylesheet" type="text/css" href="../../../_static/css/custom2.css?v=baa440dc" />
  
  <!-- So that users can add custom icons -->
  <script src="../../../_static/scripts/fontawesome.js?digest=8878045cc6db502f8baf"></script>
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf" />
<link rel="preload" as="script" href="../../../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf" />

    <script src="../../../_static/documentation_options.js?v=a8da1a53"></script>
    <script src="../../../_static/doctools.js?v=9bcbadda"></script>
    <script src="../../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../../_static/copybutton.js?v=940804e7"></script>
    <script src="../../../_static/design-tabs.js?v=f930bc37"></script>
    <script type="module" src="https://cdn.jsdelivr.net/npm/mermaid@11.2.0/dist/mermaid.esm.min.mjs"></script>
    <script type="module" src="https://cdn.jsdelivr.net/npm/@mermaid-js/layout-elk@0.1.4/dist/mermaid-layout-elk.esm.min.mjs"></script>
    <script type="module">import mermaid from "https://cdn.jsdelivr.net/npm/mermaid@11.2.0/dist/mermaid.esm.min.mjs";import elkLayouts from "https://cdn.jsdelivr.net/npm/@mermaid-js/layout-elk@0.1.4/dist/mermaid-layout-elk.esm.min.mjs";mermaid.registerLayoutLoaders(elkLayouts);mermaid.initialize({startOnLoad:false});</script>
    <script src="https://cdn.jsdelivr.net/npm/d3@7.9.0/dist/d3.min.js"></script>
    <script type="module">
import mermaid from "https://cdn.jsdelivr.net/npm/mermaid@11.2.0/dist/mermaid.esm.min.mjs";
window.addEventListener("load", () => mermaid.run());
</script>
    <script>DOCUMENTATION_OPTIONS.pagename = '_modules/torch/cuda/graphs';</script>
    <script src="../../../_static/js/star-rating.js?v=8861fcb6"></script>
    <script src="../../../_static/js/send-feedback.js?v=5646bf45"></script>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />

  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  <meta name="docsearch:version" content="main (2.6.0 )" />
<script src="https://code.jquery.com/jquery-3.7.1.min.js"></script>
<script type="text/javascript" src="../../../_static/js/send-feedback.js"></script>
<script type="text/javascript" src="../../../_static/js/star-rating.js"></script>
<script type="text/javascript" src="../../../_static/js/cookie-banner.js"></script>

<script async src="https://www.googletagmanager.com/gtag/js?id=G-TEST12345"></script>
    <script type="text/javascript">
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-TEST12345');
    </script>

  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
<body data-feedback-url="https://github.com/pytorch/pytorch">
  <div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>

          <li class="main-menu-item">
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Learn
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="">
                  <span class=dropdown-title>Get Started</span>
                  <p>Run PyTorch locally or get started quickly with one of the supported cloud platforms</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Tutorials</span>
                  <p>Whats new in PyTorch tutorials</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Learn the Basics</span>
                  <p>Familiarize yourself with PyTorch concepts and modules</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">PyTorch Recipes</span>
                  <p>Bite-size, ready-to-deploy PyTorch code examples</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Intro to PyTorch - YouTube Series</span>
                  <p>Master PyTorch basics with our engaging YouTube tutorial series</p>
                </a>
              </div>
            </div>
          </li>

          <li>
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Ecosystem
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Tools</span>
                  <p>Learn about the tools and frameworks in the PyTorch Ecosystem</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class=dropdown-title>Community</span>
                  <p>Join the PyTorch developer community to contribute, learn, and get your questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="" target="_blank">
                  <span class=dropdown-title>Forums</span>
                  <p>A place to discuss PyTorch code, issues, install, research</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class=dropdown-title>Developer Resources</span>
                  <p>Find resources and get questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Contributor Awards - 2024</span>
                  <p>Award winners announced at this year's PyTorch Conference</p>
                </a>
              </div>
            </div>
          </li>

          <li>
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Edge
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">About PyTorch Edge</span>
                  <p>Build innovative and privacy-aware AI experiences for edge devices</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">ExecuTorch</span>
                  <p>End-to-end solution for enabling on-device inference capabilities across mobile and edge devices</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">ExecuTorch Docs</span>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Docs
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">PyTorch</span>
                  <p>Explore the documentation for comprehensive guidance on how to use PyTorch</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">PyTorch Domains</span>
                  <p>Read the PyTorch Domains documentation to learn more about domain-specific libraries</p>
                </a>
              </div>
            </div>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Blogs & News
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">PyTorch Blog</span>
                  <p>Catch up on the latest technical news and happenings</p>
                </a>
                 <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Community Blog</span>
                  <p>Stories from the PyTorch ecosystem</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Videos</span>
                  <p>Learn about the latest PyTorch tutorials, new, and more </p>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Community Stories</span>
                  <p>Learn how our community solves real, everyday machine learning problems with PyTorch</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Events</span>
                  <p>Find events, webinars, and podcasts</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Newsletter</span>
                  <p>Stay up-to-date with the latest updates</p>
                </a>
            </div>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                About
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">PyTorch Foundation</span>
                  <p>Learn more about the PyTorch Foundation</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Governing Board</span>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Cloud Credit Program</span>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Technical Advisory Council</span>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Staff</span>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Contact Us</span>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
            <div class="no-dropdown">
              <a href="" data-cta="join">
                Become a Member
              </a>
            </div>
          </li>
          <li>
           <div class="main-menu-item">
             <a href="" class="github-icon">
             </a>
           </div>
          </li>
          <!--- TODO: This block adds the search icon to the nav bar. We will enable it later.
          <li>
            <div class="main-menu-item">
             <a href="" class="search-icon">
             </a>
            </div>
          </li>
          --->
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>
  </div>
</div>

  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <dialog id="pst-search-dialog">
    
<form class="bd-search d-flex align-items-center"
      action="../../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </dialog>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
<div class="bd-header__inner bd-page-width">
  <button class="pst-navbar-icon sidebar-toggle primary-toggle" aria-label="Site navigation">
    <span class="fa-solid fa-bars"></span>
  </button>
  
  
  <div class=" navbar-header-items__start">
    
      <div class="navbar-item">

  
    
  

<a class="navbar-brand logo" href="../../../index.html">
  
  
  
  
  
  
    <p class="title logo__title">Home</p>
  
</a></div>
    
  </div>
  
  <div class=" navbar-header-items">
    
    <div class="me-auto navbar-header-items__center">
      
        <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../python-api/index.html">
    Python API
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../notes/index.html">
    Developer Notes
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-external" href="https://pytorch.org/tutorials/">
    Tutorials
  </a>
</li>

  </ul>
</nav></div>
      
    </div>
    
    
    <div class="navbar-header-items__end">
      
        <div class="navbar-item navbar-persistent--container">
          

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button>
        </div>
      
      
        <div class="navbar-item">
<div class="version-switcher__container dropdown pst-js-only">
  <button id="pst-version-switcher-button-2"
    type="button"
    class="version-switcher__button btn btn-sm dropdown-toggle"
    data-bs-toggle="dropdown"
    aria-haspopup="listbox"
    aria-controls="pst-version-switcher-list-2"
    aria-label="Version switcher list"
  >
    Choose version  <!-- this text may get changed later by javascript -->
    <span class="caret"></span>
  </button>
  <div id="pst-version-switcher-list-2"
    class="version-switcher__menu dropdown-menu list-group-flush py-0"
    role="listbox" aria-labelledby="pst-version-switcher-button-2">
    <!-- dropdown will be populated by javascript on page load -->
  </div>
</div></div>
      
        <div class="navbar-item">

<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button></div>
      
        <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://x.com/PyTorch" title="X" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-x-twitter fa-lg" aria-hidden="true"></i>
            <span class="sr-only">X</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/pytorch/pytorch" title="GitHub" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">GitHub</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://pypi.org/project/torch" title="PyPI" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-custom fa-pypi fa-lg" aria-hidden="true"></i>
            <span class="sr-only">PyPI</span></a>
        </li>
</ul></div>
      
    </div>
    
  </div>
  
  
    <div class="navbar-persistent--mobile">

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button>
    </div>
  

  
</div>

    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
        
      
      <dialog id="pst-primary-sidebar-modal"></dialog>
      <div id="pst-primary-sidebar" class="bd-sidebar-primary bd-sidebar hide-on-wide">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
      <div class="sidebar-header-items__center">
        
          
          
            <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../python-api/index.html">
    Python API
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../notes/index.html">
    Developer Notes
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-external" href="https://pytorch.org/tutorials/">
    Tutorials
  </a>
</li>

  </ul>
</nav></div>
          
        
      </div>
    
    
    
      <div class="sidebar-header-items__end">
        
          <div class="navbar-item">
<div class="version-switcher__container dropdown pst-js-only">
  <button id="pst-version-switcher-button-3"
    type="button"
    class="version-switcher__button btn btn-sm dropdown-toggle"
    data-bs-toggle="dropdown"
    aria-haspopup="listbox"
    aria-controls="pst-version-switcher-list-3"
    aria-label="Version switcher list"
  >
    Choose version  <!-- this text may get changed later by javascript -->
    <span class="caret"></span>
  </button>
  <div id="pst-version-switcher-list-3"
    class="version-switcher__menu dropdown-menu list-group-flush py-0"
    role="listbox" aria-labelledby="pst-version-switcher-button-3">
    <!-- dropdown will be populated by javascript on page load -->
  </div>
</div></div>
        
          <div class="navbar-item">

<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button></div>
        
          <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://x.com/PyTorch" title="X" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-x-twitter fa-lg" aria-hidden="true"></i>
            <span class="sr-only">X</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/pytorch/pytorch" title="GitHub" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">GitHub</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://pypi.org/project/torch" title="PyPI" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-custom fa-pypi fa-lg" aria-hidden="true"></i>
            <span class="sr-only">PyPI</span></a>
        </li>
</ul></div>
        
      </div>
    
  </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
      <div class="sidebar-primary-item">
<div id="ethical-ad-placement"
      class="flat"
      data-ea-publisher="readthedocs"
      data-ea-type="readthedocs-sidebar"
      data-ea-manual="true">
</div></div>
  </div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        
          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item">

<nav aria-label="Breadcrumb" class="d-print-none">
  <ul class="bd-breadcrumbs">
    
    <li class="breadcrumb-item breadcrumb-home">
      <a href="../../../index.html" class="nav-link" aria-label="Home">
        <i class="fa-solid fa-home"></i>
      </a>
    </li>
    
    <li class="breadcrumb-item"><a href="../../index.html" class="nav-link">Module code</a></li>
    
    
    <li class="breadcrumb-item"><a href="../../torch.html" class="nav-link">torch</a></li>
    
    
    <li class="breadcrumb-item"><a href="../cuda.html" class="nav-link">torch.cuda</a></li>
    
    <li class="breadcrumb-item active" aria-current="page"><span class="ellipsis">torch.cuda.graphs</span></li>
  </ul>
</nav>
</div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">
<div class="rating">
    Rate this Page
    <div class="stars">
        
        <span class="star" data-value="1">★</span>
        
        <span class="star" data-value="2">★</span>
        
        <span class="star" data-value="3">★</span>
        
        <span class="star" data-value="4">★</span>
        
        <span class="star" data-value="5">★</span>
        
    </div>
</div>
</div>
      
    </div>
  
</div>
</div>
              
              
              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <h1>Source code for torch.cuda.graphs</h1><div class="highlight"><pre>
<span></span><span class="c1"># mypy: allow-untyped-defs</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">gc</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">typing</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">.._utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">_dummy_type</span>


<span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="p">,</span> <span class="s2">&quot;_CudaStreamBase&quot;</span><span class="p">):</span>
    <span class="c1"># Define dummy base classes</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="vm">__dict__</span><span class="p">[</span><span class="s2">&quot;_CUDAGraph&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">_dummy_type</span><span class="p">(</span><span class="s2">&quot;_CUDAGraph&quot;</span><span class="p">)</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="vm">__dict__</span><span class="p">[</span><span class="s2">&quot;_graph_pool_handle&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">_dummy_type</span><span class="p">(</span><span class="s2">&quot;_graph_pool_handle&quot;</span><span class="p">)</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="vm">__dict__</span><span class="p">[</span><span class="s2">&quot;_cuda_isCurrentStreamCapturing&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">_dummy_type</span><span class="p">(</span>
        <span class="s2">&quot;_cuda_isCurrentStreamCapturing&quot;</span>
    <span class="p">)</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">torch._C</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>  <span class="c1"># noqa: F401</span>
    <span class="n">_cuda_isCurrentStreamCapturing</span><span class="p">,</span>
    <span class="n">_CUDAGraph</span><span class="p">,</span>
    <span class="n">_graph_pool_handle</span><span class="p">,</span>
<span class="p">)</span>


<div class="viewcode-block" id="is_current_stream_capturing">
<a class="viewcode-back" href="../../../python-api/generated/torch.cuda.is_current_stream_capturing.html#torch.cuda.is_current_stream_capturing">[docs]</a>
<span class="k">def</span><span class="w"> </span><span class="nf">is_current_stream_capturing</span><span class="p">():</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Return True if CUDA graph capture is underway on the current CUDA stream, False otherwise.</span>

<span class="sd">    If a CUDA context does not exist on the current device, returns False without initializing the context.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">_cuda_isCurrentStreamCapturing</span><span class="p">()</span></div>



<span class="c1"># Python shim helps Sphinx process docstrings more reliably.</span>
<div class="viewcode-block" id="graph_pool_handle">
<a class="viewcode-back" href="../../../python-api/generated/torch.cuda.graph_pool_handle.html#torch.cuda.graph_pool_handle">[docs]</a>
<span class="k">def</span><span class="w"> </span><span class="nf">graph_pool_handle</span><span class="p">():</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Return an opaque token representing the id of a graph memory pool.</span>

<span class="sd">    See :ref:`Graph memory management&lt;graph-memory-management&gt;`.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This API is in beta and may change in future releases.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">_graph_pool_handle</span><span class="p">()</span></div>



<span class="c1"># Python shim helps Sphinx process docstrings more reliably.</span>
<div class="viewcode-block" id="CUDAGraph">
<a class="viewcode-back" href="../../../python-api/generated/torch.cuda.CUDAGraph.html#torch.cuda.CUDAGraph">[docs]</a>
<span class="k">class</span><span class="w"> </span><span class="nc">CUDAGraph</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_CUDAGraph</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Wrapper around a CUDA graph.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This API is in beta and may change in future releases.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__new__</span><span class="p">(</span><span class="bp">cls</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__new__</span><span class="p">(</span><span class="bp">cls</span><span class="p">)</span>

<div class="viewcode-block" id="CUDAGraph.capture_begin">
<a class="viewcode-back" href="../../../python-api/generated/torch.cuda.CUDAGraph.html#torch.cuda.CUDAGraph.capture_begin">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">capture_begin</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">pool</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">capture_error_mode</span><span class="o">=</span><span class="s2">&quot;global&quot;</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Begin capturing CUDA work on the current stream.</span>

<span class="sd">        Typically, you shouldn&#39;t call ``capture_begin`` yourself.</span>
<span class="sd">        Use :class:`~torch.cuda.graph` or :func:`~torch.cuda.make_graphed_callables`,</span>
<span class="sd">        which call ``capture_begin`` internally.</span>

<span class="sd">        Arguments:</span>
<span class="sd">            pool (optional): Token (returned by :func:`~torch.cuda.graph_pool_handle` or</span>
<span class="sd">                :meth:`other_Graph_instance.pool()&lt;torch.cuda.CUDAGraph.pool&gt;`) that hints this graph may share memory</span>
<span class="sd">                with the indicated pool.  See :ref:`Graph memory management&lt;graph-memory-management&gt;`.</span>
<span class="sd">            capture_error_mode (str, optional): specifies the cudaStreamCaptureMode for the graph capture stream.</span>
<span class="sd">                Can be &quot;global&quot;, &quot;thread_local&quot; or &quot;relaxed&quot;. During cuda graph capture, some actions, such as cudaMalloc,</span>
<span class="sd">                may be unsafe. &quot;global&quot; will error on actions in other threads, &quot;thread_local&quot; will only error for</span>
<span class="sd">                actions in the current thread, and &quot;relaxed&quot; will not error on these actions. Do NOT change this setting</span>
<span class="sd">                unless you&#39;re familiar with `cudaStreamCaptureMode &lt;https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__STREAM.html#group__CUDART__STREAM_1g9d0535d93a214cbf126835257b16ba85&gt;`_</span>
<span class="sd">        &quot;&quot;&quot;</span>  <span class="c1"># noqa: B950</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">capture_begin</span><span class="p">(</span><span class="n">pool</span><span class="o">=</span><span class="n">pool</span><span class="p">,</span> <span class="n">capture_error_mode</span><span class="o">=</span><span class="n">capture_error_mode</span><span class="p">)</span></div>


<div class="viewcode-block" id="CUDAGraph.capture_end">
<a class="viewcode-back" href="../../../python-api/generated/torch.cuda.CUDAGraph.html#torch.cuda.CUDAGraph.capture_end">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">capture_end</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;End CUDA graph capture on the current stream.</span>

<span class="sd">        After ``capture_end``, ``replay`` may be called on this instance.</span>

<span class="sd">        Typically, you shouldn&#39;t call ``capture_end`` yourself.</span>
<span class="sd">        Use :class:`~torch.cuda.graph` or :func:`~torch.cuda.make_graphed_callables`,</span>
<span class="sd">        which call ``capture_end`` internally.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">capture_end</span><span class="p">()</span></div>


<div class="viewcode-block" id="CUDAGraph.replay">
<a class="viewcode-back" href="../../../python-api/generated/torch.cuda.CUDAGraph.html#torch.cuda.CUDAGraph.replay">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">replay</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Replay the CUDA work captured by this graph.&quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">replay</span><span class="p">()</span></div>


<div class="viewcode-block" id="CUDAGraph.reset">
<a class="viewcode-back" href="../../../python-api/generated/torch.cuda.CUDAGraph.html#torch.cuda.CUDAGraph.reset">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">reset</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Delete the graph currently held by this instance.&quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span></div>


<div class="viewcode-block" id="CUDAGraph.pool">
<a class="viewcode-back" href="../../../python-api/generated/torch.cuda.CUDAGraph.html#torch.cuda.CUDAGraph.pool">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">pool</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Return an opaque token representing the id of this graph&#39;s memory pool.</span>

<span class="sd">        This id can optionally be passed to another graph&#39;s ``capture_begin``,</span>
<span class="sd">        which hints the other graph may share the same memory pool.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">pool</span><span class="p">()</span></div>


<div class="viewcode-block" id="CUDAGraph.enable_debug_mode">
<a class="viewcode-back" href="../../../python-api/generated/torch.cuda.CUDAGraph.html#torch.cuda.CUDAGraph.enable_debug_mode">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">enable_debug_mode</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Enable debugging mode for CUDAGraph.debug_dump.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">enable_debug_mode</span><span class="p">()</span></div>


<div class="viewcode-block" id="CUDAGraph.debug_dump">
<a class="viewcode-back" href="../../../python-api/generated/torch.cuda.CUDAGraph.html#torch.cuda.CUDAGraph.debug_dump">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">debug_dump</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">debug_path</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Arguments:</span>
<span class="sd">            debug_path (required): Path to dump the graph to.</span>

<span class="sd">        Calls a debugging function to dump the graph if the debugging is</span>
<span class="sd">        enabled via CUDAGraph.enable_debug_mode()</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">debug_dump</span><span class="p">(</span><span class="n">debug_path</span><span class="p">)</span></div>
</div>



<div class="viewcode-block" id="graph">
<a class="viewcode-back" href="../../../python-api/generated/torch.cuda.graph.html#torch.cuda.graph">[docs]</a>
<span class="k">class</span><span class="w"> </span><span class="nc">graph</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Context-manager that captures CUDA work into a :class:`torch.cuda.CUDAGraph` object for later replay.</span>

<span class="sd">    See :ref:`CUDA Graphs &lt;cuda-graph-semantics&gt;` for a general introduction,</span>
<span class="sd">    detailed use, and constraints.</span>

<span class="sd">    Arguments:</span>
<span class="sd">        cuda_graph (torch.cuda.CUDAGraph): Graph object used for capture.</span>
<span class="sd">        pool (optional): Opaque token (returned by a call to :func:`~torch.cuda.graph_pool_handle()` or</span>
<span class="sd">            :meth:`other_Graph_instance.pool()&lt;torch.cuda.CUDAGraph.pool&gt;`) hinting this graph&#39;s capture</span>
<span class="sd">            may share memory from the specified pool. See :ref:`Graph memory management&lt;graph-memory-management&gt;`.</span>
<span class="sd">        stream (torch.cuda.Stream, optional): If supplied, will be set as the current stream in the context.</span>
<span class="sd">            If not supplied, ``graph`` sets its own internal side stream as the current stream in the context.</span>
<span class="sd">        capture_error_mode (str, optional): specifies the cudaStreamCaptureMode for the graph capture stream.</span>
<span class="sd">            Can be &quot;global&quot;, &quot;thread_local&quot; or &quot;relaxed&quot;. During cuda graph capture, some actions, such as cudaMalloc,</span>
<span class="sd">            may be unsafe. &quot;global&quot; will error on actions in other threads, &quot;thread_local&quot; will only error for</span>
<span class="sd">            actions in the current thread, and &quot;relaxed&quot; will not error on actions. Do NOT change this setting</span>
<span class="sd">            unless you&#39;re familiar with `cudaStreamCaptureMode &lt;https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__STREAM.html#group__CUDART__STREAM_1g9d0535d93a214cbf126835257b16ba85&gt;`_</span>

<span class="sd">    .. note::</span>
<span class="sd">        For effective memory sharing, if you pass a ``pool`` used by a previous capture and the previous capture</span>
<span class="sd">        used an explicit ``stream`` argument, you should pass the same ``stream`` argument to this capture.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This API is in beta and may change in future releases.</span>

<span class="sd">    .. _cudaStreamCaptureMode:</span>
<span class="sd">        https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__STREAM.html#group__CUDART__STREAM_1g9d0535d93a214cbf126835257b16ba85</span>
<span class="sd">    &quot;&quot;&quot;</span>  <span class="c1"># noqa: B950</span>

    <span class="n">default_capture_stream</span><span class="p">:</span> <span class="n">typing</span><span class="o">.</span><span class="n">Optional</span><span class="p">[</span><span class="s2">&quot;torch.cuda.Stream&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">cuda_graph</span><span class="p">,</span>
        <span class="n">pool</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">stream</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">capture_error_mode</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;global&quot;</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="c1"># Lazy-init of default_capture_stream helps avoid circular-import errors.</span>
        <span class="c1"># Not thread safe, but graphs already have the general (explicitly documented)</span>
        <span class="c1"># restriction that only one capture may be underway at a time in the process.</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="n">default_capture_stream</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="n">default_capture_stream</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">Stream</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">pool</span> <span class="o">=</span> <span class="p">()</span> <span class="k">if</span> <span class="n">pool</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="p">(</span><span class="n">pool</span><span class="p">,)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">capture_stream</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">stream</span> <span class="k">if</span> <span class="n">stream</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="n">default_capture_stream</span>
        <span class="p">)</span>
        <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">capture_stream</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">stream_ctx</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">stream</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">capture_stream</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cuda_graph</span> <span class="o">=</span> <span class="n">cuda_graph</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">capture_error_mode</span> <span class="o">=</span> <span class="n">capture_error_mode</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__enter__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># Free as much memory as we can for the graph</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">synchronize</span><span class="p">()</span>
        <span class="n">gc</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">empty_cache</span><span class="p">()</span>

        <span class="c1"># Stackoverflow seems comfortable with this pattern</span>
        <span class="c1"># https://stackoverflow.com/questions/26635684/calling-enter-and-exit-manually#39172487</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">stream_ctx</span><span class="o">.</span><span class="fm">__enter__</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">cuda_graph</span><span class="o">.</span><span class="n">capture_begin</span><span class="p">(</span>
            <span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">pool</span><span class="p">,</span> <span class="n">capture_error_mode</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">capture_error_mode</span>
        <span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__exit__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">exc_type</span><span class="p">,</span> <span class="n">exc_value</span><span class="p">,</span> <span class="n">traceback</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cuda_graph</span><span class="o">.</span><span class="n">capture_end</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">stream_ctx</span><span class="o">.</span><span class="fm">__exit__</span><span class="p">(</span><span class="n">exc_type</span><span class="p">,</span> <span class="n">exc_value</span><span class="p">,</span> <span class="n">traceback</span><span class="p">)</span></div>

        <span class="c1"># returning None should propagate exceptions from either capture_end or stream_ctx.__exit__()</span>


<div class="viewcode-block" id="make_graphed_callables">
<a class="viewcode-back" href="../../../python-api/generated/torch.cuda.make_graphed_callables.html#torch.cuda.make_graphed_callables">[docs]</a>
<span class="k">def</span><span class="w"> </span><span class="nf">make_graphed_callables</span><span class="p">(</span>
    <span class="n">callables</span><span class="p">,</span> <span class="n">sample_args</span><span class="p">,</span> <span class="n">num_warmup_iters</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">allow_unused_input</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">pool</span><span class="o">=</span><span class="kc">None</span>
<span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Accept callables (functions or :class:`nn.Module&lt;torch.nn.Module&gt;`\ s) and returns graphed versions.</span>

<span class="sd">    Each graphed callable&#39;s forward pass runs its source callable&#39;s</span>
<span class="sd">    forward CUDA work as a CUDA graph inside a single autograd node.</span>

<span class="sd">    The graphed callable&#39;s forward pass also appends</span>
<span class="sd">    a backward node to the autograd graph. During backward, this node runs the</span>
<span class="sd">    callable&#39;s backward work as a CUDA graph.</span>

<span class="sd">    Therefore, each graphed callable should be a drop-in replacement for its source callable</span>
<span class="sd">    in an autograd-enabled training loop.</span>

<span class="sd">    See :ref:`Partial-network capture&lt;partial-network-capture&gt;` for detailed use and constraints.</span>

<span class="sd">    If you pass a tuple of several callables, their captures will use the same memory pool.</span>
<span class="sd">    See :ref:`Graph memory management&lt;graph-memory-management&gt;` for when this is appropriate.</span>

<span class="sd">    Arguments:</span>
<span class="sd">        callables (torch.nn.Module or Python function, or tuple of these): Callable or callables to graph.</span>
<span class="sd">            See :ref:`Graph memory management&lt;graph-memory-management&gt;` for when passing a tuple of callables</span>
<span class="sd">            is appropriate.  If you pass a tuple of callables, their order in the tuple must be the same order</span>
<span class="sd">            they&#39;ll run in the live workload.</span>
<span class="sd">        sample_args (tuple of Tensors, or tuple of tuples of Tensors): Samples args for each callable.</span>
<span class="sd">            If a single callable was passed, ``sample_args`` must be a single tuple of argument Tensors.</span>
<span class="sd">            If a tuple of callables was passed, ``sample_args`` must be tuple of tuples of argument Tensors.</span>
<span class="sd">        num_warmup_iters (int): The number of warmup iterations. Currently, ``DataDistributedParallel`` needs</span>
<span class="sd">            11 iterations for warm up. Default: ``3``.</span>
<span class="sd">        allow_unused_input (bool): If False, specifying inputs that were not used when computing outputs</span>
<span class="sd">            (and therefore their grad is always zero) is an error. Defaults to False.</span>
<span class="sd">        pool (optional): Token (returned by :func:`~torch.cuda.graph_pool_handle` or</span>
<span class="sd">            :meth:`other_Graph_instance.pool()&lt;torch.cuda.CUDAGraph.pool&gt;`) that hints this graph may share memory</span>
<span class="sd">            with the indicated pool.  See :ref:`Graph memory management&lt;graph-memory-management&gt;`.</span>
<span class="sd">    .. note::</span>
<span class="sd">        The ``requires_grad`` state of each Tensor in ``sample_args`` must match the state</span>
<span class="sd">        that&#39;s expected for the corresponding real input in the training loop.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This API is in beta and may change in future releases.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        ``sample_args`` for each callable must contain only Tensors. Other types are not allowed.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        Returned callables do not support higher order differentiation (e.g., double backward).</span>

<span class="sd">    .. warning::</span>
<span class="sd">        In any :class:`~torch.nn.Module` passed to :func:`~make_graphed_callables`, only parameters</span>
<span class="sd">        may be trainable. Buffers must have ``requires_grad=False``.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        After you pass a :class:`torch.nn.Module` through :func:`~make_graphed_callables`,</span>
<span class="sd">        you may not add or remove any of that Module&#39;s parameters or buffers.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        :class:`torch.nn.Module`\s passed to :func:`~torch.cuda.make_graphed_callables` must not have module hooks</span>
<span class="sd">        registered on them at the time they are passed. However, registering hooks on modules *after* passing them</span>
<span class="sd">        through :func:`~torch.cuda.make_graphed_callables` is allowed.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        When running a graphed callable, you must pass its arguments in the same order and format</span>
<span class="sd">        they appeared in that callable&#39;s ``sample_args``.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        The automatic mixed precision is supported in :func:`~torch.cuda.make_graphed_callables` only with disabled</span>
<span class="sd">        caching. The context manager `torch.cuda.amp.autocast()` must have `cache_enabled=False`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">is_autocast_enabled</span><span class="p">()</span> <span class="ow">and</span> <span class="n">torch</span><span class="o">.</span><span class="n">is_autocast_cache_enabled</span><span class="p">():</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
            <span class="s2">&quot;make_graphed_callables does not support the autocast caching. Please set `cache_enabled=False`.&quot;</span>
        <span class="p">)</span>

    <span class="n">just_one_callable</span> <span class="o">=</span> <span class="kc">False</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">callables</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">):</span>
        <span class="n">just_one_callable</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="n">callables</span> <span class="o">=</span> <span class="p">(</span><span class="n">callables</span><span class="p">,)</span>
        <span class="n">sample_args</span> <span class="o">=</span> <span class="p">(</span><span class="n">sample_args</span><span class="p">,)</span>

    <span class="n">flatten_sample_args</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">for</span> <span class="n">c</span><span class="p">,</span> <span class="n">args</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">callables</span><span class="p">,</span> <span class="n">sample_args</span><span class="p">):</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">c</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
            <span class="k">assert</span> <span class="p">(</span>
                <span class="nb">len</span><span class="p">(</span><span class="n">c</span><span class="o">.</span><span class="n">_backward_hooks</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span>
                <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">c</span><span class="o">.</span><span class="n">_forward_hooks</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span>
                <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">c</span><span class="o">.</span><span class="n">_forward_pre_hooks</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span>
            <span class="p">),</span> <span class="p">(</span>
                <span class="s2">&quot;Modules must not have hooks registered at the time they are passed. However, registering hooks &quot;</span>
                <span class="o">+</span> <span class="s2">&quot;on modules after passing them through make_graphed_callables is allowed.&quot;</span>
            <span class="p">)</span>
            <span class="k">assert</span> <span class="nb">all</span><span class="p">(</span><span class="n">b</span><span class="o">.</span><span class="n">requires_grad</span> <span class="ow">is</span> <span class="kc">False</span> <span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="n">c</span><span class="o">.</span><span class="n">buffers</span><span class="p">()),</span> <span class="p">(</span>
                <span class="s2">&quot;In any :class:`~torch.nn.Module` passed to &quot;</span>
                <span class="o">+</span> <span class="s2">&quot;:func:`~make_graphed_callables`, only parameters may be trainable. All buffers must have &quot;</span>
                <span class="o">+</span> <span class="s2">&quot;``requires_grad=False``.&quot;</span>
            <span class="p">)</span>
        <span class="n">flatten_arg</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">_pytree</span><span class="o">.</span><span class="n">arg_tree_leaves</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">)</span>
        <span class="n">flatten_sample_args</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="nb">tuple</span><span class="p">(</span><span class="n">flatten_arg</span><span class="p">))</span>
        <span class="k">assert</span> <span class="nb">all</span><span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">arg</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="k">for</span> <span class="n">arg</span> <span class="ow">in</span> <span class="n">flatten_arg</span><span class="p">),</span> <span class="p">(</span>
            <span class="s2">&quot;In the beta API, sample_args &quot;</span>
            <span class="o">+</span> <span class="s2">&quot;for each callable must contain only Tensors. Other types are not allowed.&quot;</span>
        <span class="p">)</span>

    <span class="c1"># If a callable is an nn.Module, its graph&#39;s full input surface is the args the user explicitly</span>
    <span class="c1"># passes to forward (ie, its sample_args) AND the module&#39;s parameter attributes.</span>
    <span class="n">per_callable_len_user_args</span> <span class="o">=</span> <span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="n">args</span><span class="p">)</span> <span class="k">for</span> <span class="n">args</span> <span class="ow">in</span> <span class="n">flatten_sample_args</span><span class="p">]</span>
    <span class="n">per_callable_module_params</span> <span class="o">=</span> <span class="p">[</span>
        <span class="nb">tuple</span><span class="p">(</span><span class="n">c</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">c</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">)</span> <span class="k">else</span> <span class="p">()</span>
        <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">callables</span>
    <span class="p">]</span>
    <span class="n">per_callable_static_input_surfaces</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">flatten_sample_args</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="n">per_callable_module_params</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">callables</span><span class="p">))</span>
    <span class="p">]</span>

    <span class="n">fwd_graphs</span> <span class="o">=</span> <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">CUDAGraph</span><span class="p">()</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">callables</span><span class="p">))]</span>
    <span class="n">bwd_graphs</span> <span class="o">=</span> <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">CUDAGraph</span><span class="p">()</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">callables</span><span class="p">))]</span>

    <span class="n">mempool</span> <span class="o">=</span> <span class="n">graph_pool_handle</span><span class="p">()</span> <span class="k">if</span> <span class="n">pool</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">pool</span>

    <span class="c1"># Warmup</span>
    <span class="c1"># Hopefully prevents cudnn benchmarking and other lazy-initialization cuda work</span>
    <span class="c1"># from ending up in any captures.</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">synchronize</span><span class="p">()</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">stream</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">Stream</span><span class="p">()):</span>
        <span class="k">for</span> <span class="n">func</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">static_input_surface</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span>
            <span class="n">callables</span><span class="p">,</span> <span class="n">sample_args</span><span class="p">,</span> <span class="n">per_callable_static_input_surfaces</span>
        <span class="p">):</span>
            <span class="n">grad_inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="p">,</span> <span class="n">outputs_grad</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span>
            <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_warmup_iters</span><span class="p">):</span>
                <span class="n">outputs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">_pytree</span><span class="o">.</span><span class="n">tree_leaves</span><span class="p">(</span><span class="n">func</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">))</span>
                <span class="n">outputs_grad</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">o</span> <span class="k">for</span> <span class="n">o</span> <span class="ow">in</span> <span class="n">outputs</span> <span class="k">if</span> <span class="n">o</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">)</span>
                <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">outputs_grad</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="n">grad_inputs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span>
                        <span class="n">outputs</span><span class="o">=</span><span class="n">outputs_grad</span><span class="p">,</span>
                        <span class="n">inputs</span><span class="o">=</span><span class="nb">tuple</span><span class="p">(</span>
                            <span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">static_input_surface</span> <span class="k">if</span> <span class="n">i</span><span class="o">.</span><span class="n">requires_grad</span>
                        <span class="p">),</span>
                        <span class="n">grad_outputs</span><span class="o">=</span><span class="nb">tuple</span><span class="p">(</span>
                            <span class="n">torch</span><span class="o">.</span><span class="n">empty_like</span><span class="p">(</span><span class="n">o</span><span class="p">)</span> <span class="k">for</span> <span class="n">o</span> <span class="ow">in</span> <span class="n">outputs</span> <span class="k">if</span> <span class="n">o</span><span class="o">.</span><span class="n">requires_grad</span>
                        <span class="p">),</span>
                        <span class="n">only_inputs</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                        <span class="n">allow_unused</span><span class="o">=</span><span class="n">allow_unused_input</span><span class="p">,</span>
                    <span class="p">)</span>
            <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="p">[</span><span class="n">outputs</span><span class="p">,</span> <span class="n">outputs_grad</span><span class="p">,</span> <span class="n">grad_inputs</span><span class="p">]:</span>
                <span class="k">del</span> <span class="n">v</span>

    <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">synchronize</span><span class="p">()</span>

    <span class="c1"># All captures here share a mempool. To avoid replays corrupting each other&#39;s memory,</span>
    <span class="c1"># the safest approach is to capture all passes in the same order they&#39;ll run:</span>
    <span class="c1"># fwd 1, fwd 2, ... fwd N, then bwd N, bwd N-1, ... bwd 1.</span>

    <span class="c1"># Capture forward graphs</span>
    <span class="n">per_callable_static_outputs</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">per_callable_output_unflatten_spec</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">func</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">fwd_graph</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">callables</span><span class="p">,</span> <span class="n">sample_args</span><span class="p">,</span> <span class="n">fwd_graphs</span><span class="p">):</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">graph</span><span class="p">(</span><span class="n">fwd_graph</span><span class="p">,</span> <span class="n">pool</span><span class="o">=</span><span class="n">mempool</span><span class="p">):</span>
            <span class="n">outputs</span> <span class="o">=</span> <span class="n">func</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">)</span>

        <span class="n">flatten_outputs</span><span class="p">,</span> <span class="n">spec</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">_pytree</span><span class="o">.</span><span class="n">tree_flatten</span><span class="p">(</span><span class="n">outputs</span><span class="p">)</span>
        <span class="n">per_callable_static_outputs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="nb">tuple</span><span class="p">(</span><span class="n">flatten_outputs</span><span class="p">))</span>
        <span class="n">per_callable_output_unflatten_spec</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">spec</span><span class="p">)</span>

    <span class="c1"># Capture backward graphs in reverse order</span>
    <span class="n">per_callable_static_grad_outputs</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">per_callable_static_grad_inputs</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">static_input_surface</span><span class="p">,</span> <span class="n">static_outputs</span><span class="p">,</span> <span class="n">bwd_graph</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span>
        <span class="nb">reversed</span><span class="p">(</span><span class="n">per_callable_static_input_surfaces</span><span class="p">),</span>
        <span class="nb">reversed</span><span class="p">(</span><span class="n">per_callable_static_outputs</span><span class="p">),</span>
        <span class="nb">reversed</span><span class="p">(</span><span class="n">bwd_graphs</span><span class="p">),</span>
    <span class="p">):</span>
        <span class="c1"># For now, assumes all static_outputs require grad</span>
        <span class="c1"># assert all(o.requires_grad for o in static_outputs), &quot;Outputs of graphed callables must require grad.&quot;</span>
        <span class="n">static_grad_outputs</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">empty_like</span><span class="p">(</span><span class="n">o</span><span class="p">)</span> <span class="k">if</span> <span class="n">o</span><span class="o">.</span><span class="n">requires_grad</span> <span class="k">else</span> <span class="kc">None</span> <span class="k">for</span> <span class="n">o</span> <span class="ow">in</span> <span class="n">static_outputs</span>
        <span class="p">)</span>

        <span class="n">outputs_grad</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">o</span> <span class="k">for</span> <span class="n">o</span> <span class="ow">in</span> <span class="n">static_outputs</span> <span class="k">if</span> <span class="n">o</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">)</span>
        <span class="n">grad_inputs</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">outputs_grad</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">graph</span><span class="p">(</span><span class="n">bwd_graph</span><span class="p">,</span> <span class="n">pool</span><span class="o">=</span><span class="n">mempool</span><span class="p">):</span>
                <span class="n">grad_inputs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span>
                    <span class="n">outputs</span><span class="o">=</span><span class="n">outputs_grad</span><span class="p">,</span>
                    <span class="n">inputs</span><span class="o">=</span><span class="nb">tuple</span><span class="p">(</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">static_input_surface</span> <span class="k">if</span> <span class="n">i</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">),</span>
                    <span class="n">grad_outputs</span><span class="o">=</span><span class="nb">tuple</span><span class="p">(</span><span class="n">o</span> <span class="k">for</span> <span class="n">o</span> <span class="ow">in</span> <span class="n">static_grad_outputs</span> <span class="k">if</span> <span class="n">o</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">),</span>
                    <span class="n">only_inputs</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                    <span class="n">allow_unused</span><span class="o">=</span><span class="n">allow_unused_input</span><span class="p">,</span>
                <span class="p">)</span>

        <span class="c1"># Constructs a tuple suitable for returning from Graphed.backward:</span>
        <span class="c1"># Pads out the actually-needed grads with Nones in gradient slots for inputs that don&#39;t require grad.</span>
        <span class="c1"># I couldn&#39;t think of a slick one-liner for this pattern.</span>
        <span class="n">static_grad_inputs</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">grad_idx</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">arg</span> <span class="ow">in</span> <span class="n">static_input_surface</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">arg</span><span class="o">.</span><span class="n">requires_grad</span> <span class="ow">and</span> <span class="n">grad_inputs</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">static_grad_inputs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">grad_inputs</span><span class="p">[</span><span class="n">grad_idx</span><span class="p">])</span>
                <span class="n">grad_idx</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">static_grad_inputs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="kc">None</span><span class="p">)</span>  <span class="c1"># type: ignore[arg-type]</span>
        <span class="n">static_grad_inputs</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">static_grad_inputs</span><span class="p">)</span>  <span class="c1"># type: ignore[assignment]</span>

        <span class="n">per_callable_static_grad_outputs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">static_grad_outputs</span><span class="p">)</span>
        <span class="n">per_callable_static_grad_inputs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">static_grad_inputs</span><span class="p">)</span>

    <span class="c1"># Reverses the most recent two lists</span>
    <span class="n">per_callable_static_grad_outputs</span><span class="o">.</span><span class="n">reverse</span><span class="p">()</span>
    <span class="n">per_callable_static_grad_inputs</span><span class="o">.</span><span class="n">reverse</span><span class="p">()</span>
    <span class="c1"># Now for every per_callable list, per_callable_*[i] holds the stuff for the ith callable.</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">make_graphed_autograd_function</span><span class="p">(</span>
        <span class="n">fwd_graph</span><span class="p">,</span>
        <span class="n">bwd_graph</span><span class="p">,</span>
        <span class="n">module_params</span><span class="p">,</span>
        <span class="n">len_user_args</span><span class="p">,</span>
        <span class="n">output_unflatten_spec</span><span class="p">,</span>
        <span class="n">static_input_surface</span><span class="p">,</span>
        <span class="n">static_outputs</span><span class="p">,</span>
        <span class="n">static_grad_outputs</span><span class="p">,</span>
        <span class="n">static_grad_inputs</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="k">class</span><span class="w"> </span><span class="nc">Graphed</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">Function</span><span class="p">):</span>
            <span class="nd">@staticmethod</span>
            <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="o">*</span><span class="n">inputs</span><span class="p">):</span>
                <span class="c1"># At this stage, only the user args may (potentially) be new tensors.</span>
                <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">len_user_args</span><span class="p">):</span>
                    <span class="k">if</span> <span class="n">static_input_surface</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">data_ptr</span><span class="p">()</span> <span class="o">!=</span> <span class="n">inputs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">data_ptr</span><span class="p">():</span>
                        <span class="n">static_input_surface</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">copy_</span><span class="p">(</span><span class="n">inputs</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
                <span class="n">fwd_graph</span><span class="o">.</span><span class="n">replay</span><span class="p">()</span>
                <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">static_outputs</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)</span>
                <span class="k">return</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">o</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span> <span class="k">for</span> <span class="n">o</span> <span class="ow">in</span> <span class="n">static_outputs</span><span class="p">)</span>

            <span class="nd">@staticmethod</span>
            <span class="nd">@torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">function</span><span class="o">.</span><span class="n">once_differentiable</span>
            <span class="k">def</span><span class="w"> </span><span class="nf">backward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="o">*</span><span class="n">grads</span><span class="p">):</span>
                <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">grads</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">static_grad_outputs</span><span class="p">)</span>
                <span class="k">for</span> <span class="n">g</span><span class="p">,</span> <span class="n">grad</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">static_grad_outputs</span><span class="p">,</span> <span class="n">grads</span><span class="p">):</span>
                    <span class="k">if</span> <span class="n">g</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                        <span class="c1"># don&#39;t copy if autograd gods have been kind and the</span>
                        <span class="c1"># incoming grad is already in the right place</span>
                        <span class="k">if</span> <span class="n">g</span><span class="o">.</span><span class="n">data_ptr</span><span class="p">()</span> <span class="o">!=</span> <span class="n">grad</span><span class="o">.</span><span class="n">data_ptr</span><span class="p">():</span>
                            <span class="n">g</span><span class="o">.</span><span class="n">copy_</span><span class="p">(</span><span class="n">grad</span><span class="p">)</span>
                <span class="n">bwd_graph</span><span class="o">.</span><span class="n">replay</span><span class="p">()</span>

                <span class="c1"># Input args that didn&#39;t require grad expect a None gradient.</span>
                <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">static_grad_inputs</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)</span>
                <span class="k">return</span> <span class="nb">tuple</span><span class="p">(</span>
                    <span class="n">b</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span> <span class="k">if</span> <span class="n">b</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">b</span> <span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="n">static_grad_inputs</span>
                <span class="p">)</span>

        <span class="k">def</span><span class="w"> </span><span class="nf">functionalized</span><span class="p">(</span><span class="o">*</span><span class="n">user_args</span><span class="p">):</span>
            <span class="c1"># Runs the autograd function with inputs == all inputs to the graph that might require grad</span>
            <span class="c1"># (explicit user args + module parameters)</span>
            <span class="c1"># Assumes module params didn&#39;t change since capture.</span>
            <span class="n">flatten_user_args</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">_pytree</span><span class="o">.</span><span class="n">arg_tree_leaves</span><span class="p">(</span><span class="o">*</span><span class="n">user_args</span><span class="p">)</span>
            <span class="n">out</span> <span class="o">=</span> <span class="n">Graphed</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="o">*</span><span class="p">(</span><span class="nb">tuple</span><span class="p">(</span><span class="n">flatten_user_args</span><span class="p">)</span> <span class="o">+</span> <span class="n">module_params</span><span class="p">))</span>
            <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">_pytree</span><span class="o">.</span><span class="n">tree_unflatten</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">output_unflatten_spec</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">functionalized</span>

    <span class="c1"># Put together the final graphed callables</span>
    <span class="n">ret</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">func</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">callables</span><span class="p">):</span>
        <span class="n">graphed</span> <span class="o">=</span> <span class="n">make_graphed_autograd_function</span><span class="p">(</span>
            <span class="n">fwd_graphs</span><span class="p">[</span><span class="n">i</span><span class="p">],</span>
            <span class="n">bwd_graphs</span><span class="p">[</span><span class="n">i</span><span class="p">],</span>
            <span class="n">per_callable_module_params</span><span class="p">[</span><span class="n">i</span><span class="p">],</span>
            <span class="n">per_callable_len_user_args</span><span class="p">[</span><span class="n">i</span><span class="p">],</span>
            <span class="n">per_callable_output_unflatten_spec</span><span class="p">[</span><span class="n">i</span><span class="p">],</span>
            <span class="n">per_callable_static_input_surfaces</span><span class="p">[</span><span class="n">i</span><span class="p">],</span>
            <span class="n">per_callable_static_outputs</span><span class="p">[</span><span class="n">i</span><span class="p">],</span>
            <span class="n">per_callable_static_grad_outputs</span><span class="p">[</span><span class="n">i</span><span class="p">],</span>
            <span class="n">per_callable_static_grad_inputs</span><span class="p">[</span><span class="n">i</span><span class="p">],</span>
        <span class="p">)</span>

        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>

            <span class="k">def</span><span class="w"> </span><span class="nf">make_graphed_forward</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="n">graph_training_state</span><span class="p">,</span> <span class="n">graphed</span><span class="p">,</span> <span class="n">orig_fwd</span><span class="p">):</span>
                <span class="k">def</span><span class="w"> </span><span class="nf">new_fwd</span><span class="p">(</span><span class="o">*</span><span class="n">user_args</span><span class="p">):</span>
                    <span class="c1"># If the module&#39;s training-or-eval state matches what we graphed,</span>
                    <span class="c1"># run the graph, otherwise run the original forward method</span>
                    <span class="k">if</span> <span class="n">func</span><span class="o">.</span><span class="n">training</span> <span class="o">==</span> <span class="n">graph_training_state</span><span class="p">:</span>
                        <span class="k">return</span> <span class="n">graphed</span><span class="p">(</span><span class="o">*</span><span class="n">user_args</span><span class="p">)</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="k">return</span> <span class="n">orig_fwd</span><span class="p">(</span><span class="o">*</span><span class="n">user_args</span><span class="p">)</span>

                <span class="k">return</span> <span class="n">new_fwd</span>

            <span class="n">func</span><span class="o">.</span><span class="n">forward</span> <span class="o">=</span> <span class="n">make_graphed_forward</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="n">func</span><span class="o">.</span><span class="n">training</span><span class="p">,</span> <span class="n">graphed</span><span class="p">,</span> <span class="n">func</span><span class="o">.</span><span class="n">forward</span><span class="p">)</span>  <span class="c1"># type: ignore[assignment]</span>
            <span class="n">ret</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">func</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">ret</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">graphed</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">just_one_callable</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">ret</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

    <span class="k">return</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">ret</span><span class="p">)</span></div>

</pre></div>

                </article>
              
              
              
                <footer class="bd-footer-article">
                  <div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item">
<div class="feedback">
  
<div class="rating">
    Rate this Page
    <div class="stars">
        
        <span class="star" data-value="1">★</span>
        
        <span class="star" data-value="2">★</span>
        
        <span class="star" data-value="3">★</span>
        
        <span class="star" data-value="4">★</span>
        
        <span class="star" data-value="5">★</span>
        
    </div>
</div>

  <div class="feedback-send">
    <button class="feedback-btn" onclick="openGitHubIssue()">Send Feedback</button>
  </div>
</div>
</div>
  
</div>
                </footer>
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
</div>
                </footer>
              
            </div>
            
            
              
            
          </div>
          <footer class="bd-footer-content">
            
          </footer>
        
      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script defer src="../../../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf"></script>
<script defer src="../../../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf"></script>


  <footer class="bd-footer">
    
    <div class="bd-footer__inner bd-page-width">
      
        <div class="footer-items__start">
          
            <div class="footer-item">

  <p class="copyright">
    
      © Copyright PyTorch Contributors.
      <br/>
    
  </p>
</div>
          
            <div class="footer-item">

  <p class="sphinx-version">
    Created using <a href="https://www.sphinx-doc.org/">Sphinx</a> 8.1.0.
    <br/>
  </p>
</div>
          
        </div>
      
    
    
      <div class="footer-items__end">
        
          <div class="footer-item">
<p class="theme-version">
  <!-- # L10n: Setting the PST URL as an argument as this does not need to be localized -->
  Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.16.1.
</p></div>
        
      </div>
    
   </div>
   

   <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
      <div class="container">
        <div class="row">
          <div class="col-md-4 text-center">
            <h2>Docs</h2>
            <p>Access comprehensive developer documentation for PyTorch</p>
            <a class="with-right-arrow" href="">View Docs</a>
          </div>

          <div class="col-md-4 text-center">
            <h2>Tutorials</h2>
            <p>Get in-depth tutorials for beginners and advanced developers</p>
            <a class="with-right-arrow" href="">View Tutorials</a>
          </div>

          <div class="col-md-4 text-center">
            <h2>Resources</h2>
            <p>Find development resources and get your questions answered</p>
            <a class="with-right-arrow" href="">View Resources</a>
          </div>
        </div>
      </div>
  </div>

  <footer class="site-footer">
      <div class="container footer-container">
        <div class="footer-logo-wrapper">
          <a href="" class="footer-logo"></a>
        </div>

        <div class="footer-links-wrapper">
          <div class="footer-links-col">
            <ul>
              <li class="list-title"><a href="">PyTorch</a></li>
              <li><a href="">Get Started</a></li>
              <li><a href="">Features</a></li>
              <li><a href="">Ecosystem</a></li>
              <li><a href="">Blog</a></li>
              <li><a href="">Contributing</a></li>
            </ul>
          </div>

          <div class="footer-links-col">
            <ul>
              <li class="list-title"><a href="">Resources</a></li>
              <li><a href="">Tutorials</a></li>
              <li><a href="">Docs</a></li>
              <li><a href="" target="_blank">Discuss</a></li>
              <li><a href="" target="_blank">Github Issues</a></li>
              <li><a href="" target="_blank">Brand Guidelines</a></li>
            </ul>
          </div>

          <div class="footer-links-col">
            <ul>
              <li class="list-title">Stay up to date</li>
              <li><a href="" target="_blank">Facebook</a></li>
              <li><a href="" target="_blank">Twitter</a></li>
              <li><a href="" target="_blank">YouTube</a></li>
              <li><a href="" target="_blank">LinkedIn</a></li>
            </ul>
            </div>

          <div class="footer-links-col">
            <ul>
              <li class="list-title">PyTorch Podcasts</li>
              <li><a href="" target="_blank">Spotify</a></li>
              <li><a href="" target="_blank">Apple</a></li>
              <li><a href="" target="_blank">Google</a></li>
              <li><a href="" target="_blank">Amazon</a></li>
            </ul>
           </div>
          </div>

          <div class="privacy-policy">
            <ul>
              <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/terms/" target="_blank">Terms</a></li>
              <li class="privacy-policy-links">|</li>
              <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/privacy-policy/" target="_blank">Privacy</a></li>
            </ul>
          </div>
          <div class="copyright">
          <p>© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation.
            For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see
            <a href="https://www.linuxfoundation.org/policies/">www.linuxfoundation.org/policies/</a>. The PyTorch Foundation supports the PyTorch open source
            project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC,
            please see <a href="https://www.lfprojects.org/policies/">www.lfprojects.org/policies/</a>.</p>
        </div>
       </div>

  <div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="../../../_static/img/pytorch-x.svg">
  </div>
</div>
  </footer>


  </body>
</html>