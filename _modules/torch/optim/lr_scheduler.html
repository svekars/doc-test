
<!DOCTYPE html>


<html lang="en" data-content_root="../../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>torch.optim.lr_scheduler &#8212; PyTorch main documentation</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  <!--
    this give us a css class that will be invisible only if js is disabled
  -->
  <noscript>
    <style>
      .pst-js-only { display: none !important; }

    </style>
  </noscript>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../../_static/styles/theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />
<link href="../../../_static/styles/pydata-sphinx-theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />

    <link rel="stylesheet" type="text/css" href="../../../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../../../_static/copybutton.css?v=949a1ff5" />
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/katex.min.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/katex-math.css?v=91adb8b6" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="../../../_static/css/jit.css?v=8de1ea5d" />
    <link rel="stylesheet" type="text/css" href="../../../_static/css/custom.css?v=3ccf5357" />
    <link rel="stylesheet" type="text/css" href="../../../_static/css/custom2.css?v=a9f46c5e" />
  
  <!-- So that users can add custom icons -->
  <script src="../../../_static/scripts/fontawesome.js?digest=8878045cc6db502f8baf"></script>
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf" />
<link rel="preload" as="script" href="../../../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf" />

    <script src="../../../_static/documentation_options.js?v=a8da1a53"></script>
    <script src="../../../_static/doctools.js?v=9bcbadda"></script>
    <script src="../../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../../_static/copybutton.js?v=940804e7"></script>
    <script src="../../../_static/design-tabs.js?v=f930bc37"></script>
    <script type="module" src="https://cdn.jsdelivr.net/npm/mermaid@11.2.0/dist/mermaid.esm.min.mjs"></script>
    <script type="module" src="https://cdn.jsdelivr.net/npm/@mermaid-js/layout-elk@0.1.4/dist/mermaid-layout-elk.esm.min.mjs"></script>
    <script type="module">import mermaid from "https://cdn.jsdelivr.net/npm/mermaid@11.2.0/dist/mermaid.esm.min.mjs";import elkLayouts from "https://cdn.jsdelivr.net/npm/@mermaid-js/layout-elk@0.1.4/dist/mermaid-layout-elk.esm.min.mjs";mermaid.registerLayoutLoaders(elkLayouts);mermaid.initialize({startOnLoad:false});</script>
    <script src="https://cdn.jsdelivr.net/npm/d3@7.9.0/dist/d3.min.js"></script>
    <script type="module">
import mermaid from "https://cdn.jsdelivr.net/npm/mermaid@11.2.0/dist/mermaid.esm.min.mjs";
window.addEventListener("load", () => mermaid.run());
</script>
    <script>DOCUMENTATION_OPTIONS.pagename = '_modules/torch/optim/lr_scheduler';</script>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  <meta name="docsearch:version" content="main (2.6.0 )" />
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>

          <li class="main-menu-item">
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Learn
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="">
                  <span class=dropdown-title>Get Started</span>
                  <p>Run PyTorch locally or get started quickly with one of the supported cloud platforms</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Tutorials</span>
                  <p>Whats new in PyTorch tutorials</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Learn the Basics</span>
                  <p>Familiarize yourself with PyTorch concepts and modules</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">PyTorch Recipes</span>
                  <p>Bite-size, ready-to-deploy PyTorch code examples</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Intro to PyTorch - YouTube Series</span>
                  <p>Master PyTorch basics with our engaging YouTube tutorial series</p>
                </a>
              </div>
            </div>
          </li>

          <li>
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Ecosystem
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Tools</span>
                  <p>Learn about the tools and frameworks in the PyTorch Ecosystem</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class=dropdown-title>Community</span>
                  <p>Join the PyTorch developer community to contribute, learn, and get your questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="" target="_blank">
                  <span class=dropdown-title>Forums</span>
                  <p>A place to discuss PyTorch code, issues, install, research</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class=dropdown-title>Developer Resources</span>
                  <p>Find resources and get questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Contributor Awards - 2024</span>
                  <p>Award winners announced at this year's PyTorch Conference</p>
                </a>
              </div>
            </div>
          </li>

          <li>
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Edge
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">About PyTorch Edge</span>
                  <p>Build innovative and privacy-aware AI experiences for edge devices</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">ExecuTorch</span>
                  <p>End-to-end solution for enabling on-device inference capabilities across mobile and edge devices</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">ExecuTorch Docs</span>
                </a>
              </div>
            </div>  
          </li>

          <li class="main-menu-item">
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Docs
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">PyTorch</span>
                  <p>Explore the documentation for comprehensive guidance on how to use PyTorch</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">PyTorch Domains</span>
                  <p>Read the PyTorch Domains documentation to learn more about domain-specific libraries</p>
                </a>
              </div>
            </div>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Blogs & News 
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">PyTorch Blog</span>
                  <p>Catch up on the latest technical news and happenings</p>
                </a>
                 <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Community Blog</span>
                  <p>Stories from the PyTorch ecosystem</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Videos</span>
                  <p>Learn about the latest PyTorch tutorials, new, and more </p>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Community Stories</span>
                  <p>Learn how our community solves real, everyday machine learning problems with PyTorch</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Events</span>
                  <p>Find events, webinars, and podcasts</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Newsletter</span>
                  <p>Stay up-to-date with the latest updates</p>
                </a>
            </div>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                About
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">PyTorch Foundation</span>
                  <p>Learn more about the PyTorch Foundation</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Governing Board</span>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Cloud Credit Program</span>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Technical Advisory Council</span>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Staff</span>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Contact Us</span>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
            <div class="no-dropdown">
              <a href="" data-cta="join">
                Become a Member
              </a>
            </div>
          </li>
          <li>
           <div class="main-menu-item">
             <a href="" class="github-icon">
             </a>
           </div>
          </li>
          <!--- TODO: This block adds the search icon to the nav bar. We will enable it later. 
          <li>
            <div class="main-menu-item">
             <a href="" class="search-icon">
             </a>
            </div>
          </li>
          --->
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>
  </div>
</div>

  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <dialog id="pst-search-dialog">
    
<form class="bd-search d-flex align-items-center"
      action="../../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </dialog>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
<div class="bd-header__inner bd-page-width">
  <button class="pst-navbar-icon sidebar-toggle primary-toggle" aria-label="Site navigation">
    <span class="fa-solid fa-bars"></span>
  </button>
  
  
  <div class=" navbar-header-items__start">
    
      <div class="navbar-item">
<div class="version-switcher__container dropdown pst-js-only">
  <button id="pst-version-switcher-button-2"
    type="button"
    class="version-switcher__button btn btn-sm dropdown-toggle"
    data-bs-toggle="dropdown"
    aria-haspopup="listbox"
    aria-controls="pst-version-switcher-list-2"
    aria-label="Version switcher list"
  >
    Choose version  <!-- this text may get changed later by javascript -->
    <span class="caret"></span>
  </button>
  <div id="pst-version-switcher-list-2"
    class="version-switcher__menu dropdown-menu list-group-flush py-0"
    role="listbox" aria-labelledby="pst-version-switcher-button-2">
    <!-- dropdown will be populated by javascript on page load -->
  </div>
</div></div>
    
      <div class="navbar-item">

  
    
  

<a class="navbar-brand logo" href="../../../index.html">
  
  
  
  
  
  
    <p class="title logo__title">Home</p>
  
</a></div>
    
  </div>
  
  <div class=" navbar-header-items">
    
    <div class="me-auto navbar-header-items__center">
      
        <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../python-api/index.html">
    Python API
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../notes/index.html">
    Developer Notes
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../community/index.html">
    Community
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../lang-bindings/index.html">
    Language Bindings
  </a>
</li>

  </ul>
</nav></div>
      
    </div>
    
    
    <div class="navbar-header-items__end">
      
        <div class="navbar-item navbar-persistent--container">
          

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button>
        </div>
      
      
        <div class="navbar-item">

<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button></div>
      
        <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://x.com/PyTorch" title="X" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-x-twitter fa-lg" aria-hidden="true"></i>
            <span class="sr-only">X</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/pytorch/pytorch" title="GitHub" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">GitHub</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://pypi.org/project/torch" title="PyPI" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-custom fa-pypi fa-lg" aria-hidden="true"></i>
            <span class="sr-only">PyPI</span></a>
        </li>
</ul></div>
      
    </div>
    
  </div>
  
  
    <div class="navbar-persistent--mobile">

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button>
    </div>
  

  
</div>

    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
        
      
      <dialog id="pst-primary-sidebar-modal"></dialog>
      <div id="pst-primary-sidebar" class="bd-sidebar-primary bd-sidebar hide-on-wide">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
      <div class="sidebar-header-items__center">
        
          
          
            <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../python-api/index.html">
    Python API
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../notes/index.html">
    Developer Notes
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../community/index.html">
    Community
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../lang-bindings/index.html">
    Language Bindings
  </a>
</li>

  </ul>
</nav></div>
          
        
      </div>
    
    
    
      <div class="sidebar-header-items__end">
        
          <div class="navbar-item">

<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button></div>
        
          <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://x.com/PyTorch" title="X" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-x-twitter fa-lg" aria-hidden="true"></i>
            <span class="sr-only">X</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/pytorch/pytorch" title="GitHub" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">GitHub</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://pypi.org/project/torch" title="PyPI" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-custom fa-pypi fa-lg" aria-hidden="true"></i>
            <span class="sr-only">PyPI</span></a>
        </li>
</ul></div>
        
      </div>
    
  </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
      <div class="sidebar-primary-item">
<div id="ethical-ad-placement"
      class="flat"
      data-ea-publisher="readthedocs"
      data-ea-type="readthedocs-sidebar"
      data-ea-manual="true">
</div></div>
  </div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        
          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item">

<nav aria-label="Breadcrumb" class="d-print-none">
  <ul class="bd-breadcrumbs">
    
    <li class="breadcrumb-item breadcrumb-home">
      <a href="../../../index.html" class="nav-link" aria-label="Home">
        <i class="fa-solid fa-home"></i>
      </a>
    </li>
    
    <li class="breadcrumb-item"><a href="../../index.html" class="nav-link">Module code</a></li>
    
    
    <li class="breadcrumb-item"><a href="../../torch.html" class="nav-link">torch</a></li>
    
    
    <li class="breadcrumb-item"><a href="../optim.html" class="nav-link">torch.optim</a></li>
    
    <li class="breadcrumb-item active" aria-current="page"><span class="ellipsis">torch.optim.lr_scheduler</span></li>
  </ul>
</nav>
</div>
      
    </div>
  
  
</div>
</div>
              
              
              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <h1>Source code for torch.optim.lr_scheduler</h1><div class="highlight"><pre>
<span></span><span class="c1"># mypy: allow-untyped-defs</span>
<span class="sa">r</span><span class="sd">&quot;&quot;&quot;Learning Rate Scheduler.&quot;&quot;&quot;</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">math</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">types</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">warnings</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">bisect</span><span class="w"> </span><span class="kn">import</span> <span class="n">bisect_right</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">collections</span><span class="w"> </span><span class="kn">import</span> <span class="n">Counter</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">functools</span><span class="w"> </span><span class="kn">import</span> <span class="n">partial</span><span class="p">,</span> <span class="n">wraps</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">typing</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">Any</span><span class="p">,</span>
    <span class="n">Callable</span><span class="p">,</span>
    <span class="n">cast</span><span class="p">,</span>
    <span class="n">Dict</span><span class="p">,</span>
    <span class="n">Iterable</span><span class="p">,</span>
    <span class="n">List</span><span class="p">,</span>
    <span class="n">Literal</span><span class="p">,</span>
    <span class="n">Optional</span><span class="p">,</span>
    <span class="n">Sequence</span><span class="p">,</span>
    <span class="n">SupportsFloat</span><span class="p">,</span>
    <span class="n">TypedDict</span><span class="p">,</span>
    <span class="n">Union</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">weakref</span><span class="w"> </span><span class="kn">import</span> <span class="n">ref</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">torch</span><span class="w"> </span><span class="kn">import</span> <span class="n">inf</span><span class="p">,</span> <span class="n">Tensor</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">.optimizer</span><span class="w"> </span><span class="kn">import</span> <span class="n">Optimizer</span>


<span class="n">__all__</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s2">&quot;LambdaLR&quot;</span><span class="p">,</span>
    <span class="s2">&quot;MultiplicativeLR&quot;</span><span class="p">,</span>
    <span class="s2">&quot;StepLR&quot;</span><span class="p">,</span>
    <span class="s2">&quot;MultiStepLR&quot;</span><span class="p">,</span>
    <span class="s2">&quot;ConstantLR&quot;</span><span class="p">,</span>
    <span class="s2">&quot;LinearLR&quot;</span><span class="p">,</span>
    <span class="s2">&quot;ExponentialLR&quot;</span><span class="p">,</span>
    <span class="s2">&quot;SequentialLR&quot;</span><span class="p">,</span>
    <span class="s2">&quot;CosineAnnealingLR&quot;</span><span class="p">,</span>
    <span class="s2">&quot;ChainedScheduler&quot;</span><span class="p">,</span>
    <span class="s2">&quot;ReduceLROnPlateau&quot;</span><span class="p">,</span>
    <span class="s2">&quot;CyclicLR&quot;</span><span class="p">,</span>
    <span class="s2">&quot;CosineAnnealingWarmRestarts&quot;</span><span class="p">,</span>
    <span class="s2">&quot;OneCycleLR&quot;</span><span class="p">,</span>
    <span class="s2">&quot;PolynomialLR&quot;</span><span class="p">,</span>
    <span class="s2">&quot;LRScheduler&quot;</span><span class="p">,</span>
<span class="p">]</span>

<span class="n">EPOCH_DEPRECATION_WARNING</span> <span class="o">=</span> <span class="p">(</span>
    <span class="s2">&quot;The epoch parameter in `scheduler.step()` was not necessary and is being &quot;</span>
    <span class="s2">&quot;deprecated where possible. Please use `scheduler.step()` to step the &quot;</span>
    <span class="s2">&quot;scheduler. During the deprecation, if epoch is different from None, the &quot;</span>
    <span class="s2">&quot;closed form is used instead of the new chainable form, where available. &quot;</span>
    <span class="s2">&quot;Please open an issue if you are unable to replicate your use case: &quot;</span>
    <span class="s2">&quot;https://github.com/pytorch/pytorch/issues/new/choose.&quot;</span>
<span class="p">)</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_check_verbose_deprecated_warning</span><span class="p">(</span><span class="n">verbose</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Raise a warning when verbose is not the default value.&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">verbose</span> <span class="o">!=</span> <span class="s2">&quot;deprecated&quot;</span><span class="p">:</span>
        <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
            <span class="s2">&quot;The verbose parameter is deprecated. Please use get_last_lr() &quot;</span>
            <span class="s2">&quot;to access the learning rate.&quot;</span><span class="p">,</span>
            <span class="ne">UserWarning</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">verbose</span>
    <span class="k">return</span> <span class="kc">False</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_format_param</span><span class="p">(</span><span class="n">name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">:</span> <span class="n">Optimizer</span><span class="p">,</span> <span class="n">param</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Return correctly formatted lr/momentum for each param group.&quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_copy</span><span class="p">(</span><span class="n">_param</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">_param</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">_param</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">)</span> <span class="k">else</span> <span class="n">_param</span>

    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">param</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)):</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">param</span><span class="p">)</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2"> must have the same length as optimizer.param_groups. &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2"> has </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">param</span><span class="p">)</span><span class="si">}</span><span class="s2"> values, param_groups has </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">)</span><span class="si">}</span><span class="s2">.&quot;</span>
            <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">param</span> <span class="o">=</span> <span class="p">[</span><span class="n">param</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">)</span>

    <span class="k">return</span> <span class="nb">list</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="n">_copy</span><span class="p">,</span> <span class="n">param</span><span class="p">))</span>


<div class="viewcode-block" id="LRScheduler">
<a class="viewcode-back" href="../../../python-api/generated/torch.optim.lr_scheduler.LRScheduler.html#torch.optim.lr_scheduler.LRScheduler">[docs]</a>
<span class="k">class</span><span class="w"> </span><span class="nc">LRScheduler</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Adjusts the learning rate during optimization.&quot;&quot;&quot;</span>

    <span class="n">_get_lr_called_within_step</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">optimizer</span><span class="p">:</span> <span class="n">Optimizer</span><span class="p">,</span>
        <span class="n">last_epoch</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">verbose</span><span class="o">=</span><span class="s2">&quot;deprecated&quot;</span><span class="p">,</span>
    <span class="p">):</span>  <span class="c1"># noqa: D107</span>
        <span class="c1"># Attach optimizer</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">Optimizer</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">optimizer</span><span class="p">)</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2"> is not an Optimizer&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">optimizer</span>

        <span class="c1"># Initialize epoch and base learning rates</span>
        <span class="k">if</span> <span class="n">last_epoch</span> <span class="o">==</span> <span class="o">-</span><span class="mi">1</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">group</span> <span class="ow">in</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">:</span>
                <span class="n">initial_lr</span> <span class="o">=</span> <span class="n">group</span><span class="p">[</span><span class="s2">&quot;lr&quot;</span><span class="p">]</span>
                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">initial_lr</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
                    <span class="n">initial_lr</span> <span class="o">=</span> <span class="n">initial_lr</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
                <span class="n">group</span><span class="o">.</span><span class="n">setdefault</span><span class="p">(</span><span class="s2">&quot;initial_lr&quot;</span><span class="p">,</span> <span class="n">initial_lr</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">group</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">):</span>
                <span class="k">if</span> <span class="s2">&quot;initial_lr&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">group</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">KeyError</span><span class="p">(</span>
                        <span class="s2">&quot;param &#39;initial_lr&#39; is not specified &quot;</span>
                        <span class="sa">f</span><span class="s2">&quot;in param_groups[</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">] when resuming an optimizer&quot;</span>
                    <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">base_lrs</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">group</span><span class="p">[</span><span class="s2">&quot;initial_lr&quot;</span><span class="p">]</span> <span class="k">for</span> <span class="n">group</span> <span class="ow">in</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span>
        <span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">last_epoch</span> <span class="o">=</span> <span class="n">last_epoch</span>

        <span class="c1"># Following https://github.com/pytorch/pytorch/issues/20124</span>
        <span class="c1"># We would like to ensure that `lr_scheduler.step()` is called after</span>
        <span class="c1"># `optimizer.step()`</span>
        <span class="k">def</span><span class="w"> </span><span class="nf">patch_track_step_called</span><span class="p">(</span><span class="n">opt</span><span class="p">:</span> <span class="n">Optimizer</span><span class="p">):</span>
            <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">opt</span><span class="o">.</span><span class="n">step</span><span class="p">,</span> <span class="s2">&quot;_wrapped_by_lr_sched&quot;</span><span class="p">):</span>
                <span class="c1"># we&#39;ve already patched</span>
                <span class="k">return</span> <span class="n">opt</span><span class="o">.</span><span class="n">step</span>

            <span class="k">def</span><span class="w"> </span><span class="nf">wrap_step</span><span class="p">(</span><span class="n">step_fn</span><span class="p">):</span>
                <span class="n">opt_ref</span> <span class="o">=</span> <span class="n">ref</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="p">)</span>
                <span class="n">func</span> <span class="o">=</span> <span class="n">step_fn</span><span class="o">.</span><span class="vm">__func__</span>

                <span class="nd">@wraps</span><span class="p">(</span><span class="n">func</span><span class="p">)</span>
                <span class="k">def</span><span class="w"> </span><span class="nf">wrapper</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
                    <span class="n">opt</span> <span class="o">=</span> <span class="n">opt_ref</span><span class="p">()</span>
                    <span class="n">opt</span><span class="o">.</span><span class="n">_opt_called</span> <span class="o">=</span> <span class="kc">True</span>  <span class="c1"># type: ignore[union-attr]</span>
                    <span class="k">return</span> <span class="n">func</span><span class="o">.</span><span class="fm">__get__</span><span class="p">(</span><span class="n">opt</span><span class="p">,</span> <span class="n">opt</span><span class="o">.</span><span class="vm">__class__</span><span class="p">)(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

                <span class="n">wrapper</span><span class="o">.</span><span class="n">_wrapped_by_lr_sched</span> <span class="o">=</span> <span class="kc">True</span>  <span class="c1"># type: ignore[attr-defined]</span>
                <span class="k">return</span> <span class="n">wrapper</span>

            <span class="n">opt</span><span class="o">.</span><span class="n">step</span> <span class="o">=</span> <span class="n">wrap_step</span><span class="p">(</span><span class="n">opt</span><span class="o">.</span><span class="n">step</span><span class="p">)</span>  <span class="c1"># type: ignore[method-assign]</span>

        <span class="n">patch_track_step_called</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span> <span class="o">=</span> <span class="n">_check_verbose_deprecated_warning</span><span class="p">(</span><span class="n">verbose</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_initial_step</span><span class="p">()</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_initial_step</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize step counts and perform a step.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_step_count</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

<div class="viewcode-block" id="LRScheduler.state_dict">
<a class="viewcode-back" href="../../../python-api/generated/torch.optim.lr_scheduler.LRScheduler.html#torch.optim.lr_scheduler.LRScheduler.state_dict">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">state_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Return the state of the scheduler as a :class:`dict`.</span>

<span class="sd">        It contains an entry for every variable in self.__dict__ which</span>
<span class="sd">        is not the optimizer.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="p">{</span>
            <span class="n">key</span><span class="p">:</span> <span class="n">value</span> <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="vm">__dict__</span><span class="o">.</span><span class="n">items</span><span class="p">()</span> <span class="k">if</span> <span class="n">key</span> <span class="o">!=</span> <span class="s2">&quot;optimizer&quot;</span>
        <span class="p">}</span></div>


<div class="viewcode-block" id="LRScheduler.load_state_dict">
<a class="viewcode-back" href="../../../python-api/generated/torch.optim.lr_scheduler.LRScheduler.html#torch.optim.lr_scheduler.LRScheduler.load_state_dict">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">load_state_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state_dict</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Load the scheduler&#39;s state.</span>

<span class="sd">        Args:</span>
<span class="sd">            state_dict (dict): scheduler state. Should be an object returned</span>
<span class="sd">                from a call to :meth:`state_dict`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="vm">__dict__</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">state_dict</span><span class="p">)</span></div>


<div class="viewcode-block" id="LRScheduler.get_last_lr">
<a class="viewcode-back" href="../../../python-api/generated/torch.optim.lr_scheduler.LRScheduler.html#torch.optim.lr_scheduler.LRScheduler.get_last_lr">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">get_last_lr</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">float</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Return last computed learning rate by current scheduler.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_last_lr</span></div>


<div class="viewcode-block" id="LRScheduler.get_lr">
<a class="viewcode-back" href="../../../python-api/generated/torch.optim.lr_scheduler.LRScheduler.html#torch.optim.lr_scheduler.LRScheduler.get_lr">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">get_lr</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">float</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Compute learning rate using chainable form of the scheduler.&quot;&quot;&quot;</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span></div>


<div class="viewcode-block" id="LRScheduler.print_lr">
<a class="viewcode-back" href="../../../python-api/generated/torch.optim.lr_scheduler.LRScheduler.html#torch.optim.lr_scheduler.LRScheduler.print_lr">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">print_lr</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">is_verbose</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
        <span class="n">group</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">],</span>
        <span class="n">lr</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
        <span class="n">epoch</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Display the current learning rate.</span>

<span class="sd">        .. deprecated:: 2.4</span>
<span class="sd">            ``print_lr()`` is deprecated. Please use ``get_last_lr()`` to access the</span>
<span class="sd">            learning rate.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
            <span class="s2">&quot;`LRScheduler.print_lr()` is being deprecated. To fetch the learning rate, &quot;</span>
            <span class="s2">&quot;please use `get_last_lr()` instead. For more details, &quot;</span>
            <span class="s2">&quot;see https://github.com/pytorch/pytorch/issues/99270.&quot;</span><span class="p">,</span>
            <span class="ne">UserWarning</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="n">is_verbose</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">epoch</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Adjusting learning rate of group </span><span class="si">{</span><span class="n">group</span><span class="si">}</span><span class="s2"> to </span><span class="si">{</span><span class="n">lr</span><span class="si">:</span><span class="s2">.4e</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">epoch_str</span> <span class="o">=</span> <span class="p">(</span><span class="s2">&quot;</span><span class="si">%.2f</span><span class="s2">&quot;</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">epoch</span><span class="p">,</span> <span class="nb">float</span><span class="p">)</span> <span class="k">else</span> <span class="s2">&quot;</span><span class="si">%.5d</span><span class="s2">&quot;</span><span class="p">)</span> <span class="o">%</span> <span class="n">epoch</span>
                <span class="nb">print</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;Epoch </span><span class="si">{</span><span class="n">epoch_str</span><span class="si">}</span><span class="s2">: adjusting learning rate of group </span><span class="si">{</span><span class="n">group</span><span class="si">}</span><span class="s2"> to </span><span class="si">{</span><span class="n">lr</span><span class="si">:</span><span class="s2">.4e</span><span class="si">}</span><span class="s2">.&quot;</span>
                <span class="p">)</span></div>


<div class="viewcode-block" id="LRScheduler.step">
<a class="viewcode-back" href="../../../python-api/generated/torch.optim.lr_scheduler.LRScheduler.html#torch.optim.lr_scheduler.LRScheduler.step">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">epoch</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Perform a step.&quot;&quot;&quot;</span>
        <span class="c1"># Raise a warning if old pattern is detected</span>
        <span class="c1"># https://github.com/pytorch/pytorch/issues/20124</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_step_count</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">,</span> <span class="s2">&quot;_wrapped_by_lr_sched&quot;</span><span class="p">):</span>
                <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
                    <span class="s2">&quot;Seems like `optimizer.step()` has been overridden after learning rate scheduler &quot;</span>
                    <span class="s2">&quot;initialization. Please, make sure to call `optimizer.step()` before &quot;</span>
                    <span class="s2">&quot;`lr_scheduler.step()`. See more details at &quot;</span>
                    <span class="s2">&quot;https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate&quot;</span><span class="p">,</span>
                    <span class="ne">UserWarning</span><span class="p">,</span>
                <span class="p">)</span>

            <span class="c1"># Just check if there were two first lr_scheduler.step() calls before optimizer.step()</span>
            <span class="k">elif</span> <span class="ow">not</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="p">,</span> <span class="s2">&quot;_opt_called&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">):</span>
                <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
                    <span class="s2">&quot;Detected call of `lr_scheduler.step()` before `optimizer.step()`. &quot;</span>
                    <span class="s2">&quot;In PyTorch 1.1.0 and later, you should call them in the opposite order: &quot;</span>
                    <span class="s2">&quot;`optimizer.step()` before `lr_scheduler.step()`.  Failure to do this &quot;</span>
                    <span class="s2">&quot;will result in PyTorch skipping the first value of the learning rate schedule. &quot;</span>
                    <span class="s2">&quot;See more details at &quot;</span>
                    <span class="s2">&quot;https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate&quot;</span><span class="p">,</span>
                    <span class="ne">UserWarning</span><span class="p">,</span>
                <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_step_count</span> <span class="o">+=</span> <span class="mi">1</span>

        <span class="k">with</span> <span class="n">_enable_get_lr_call</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">epoch</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">last_epoch</span> <span class="o">+=</span> <span class="mi">1</span>
                <span class="n">values</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_lr</span><span class="p">()</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="n">EPOCH_DEPRECATION_WARNING</span><span class="p">,</span> <span class="ne">UserWarning</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">last_epoch</span> <span class="o">=</span> <span class="n">epoch</span>
                <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;_get_closed_form_lr&quot;</span><span class="p">):</span>
                    <span class="n">values</span> <span class="o">=</span> <span class="n">cast</span><span class="p">(</span><span class="n">List</span><span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_closed_form_lr</span><span class="p">())</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">values</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_lr</span><span class="p">()</span>

        <span class="k">for</span> <span class="n">param_group</span><span class="p">,</span> <span class="n">lr</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">,</span> <span class="n">values</span><span class="p">):</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">param_group</span><span class="p">[</span><span class="s2">&quot;lr&quot;</span><span class="p">],</span> <span class="n">Tensor</span><span class="p">):</span>
                <span class="n">param_group</span><span class="p">[</span><span class="s2">&quot;lr&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">fill_</span><span class="p">(</span><span class="n">lr</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">param_group</span><span class="p">[</span><span class="s2">&quot;lr&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">lr</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_last_lr</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">group</span><span class="p">[</span><span class="s2">&quot;lr&quot;</span><span class="p">]</span> <span class="k">for</span> <span class="n">group</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span>
        <span class="p">]</span></div>
</div>



<span class="k">def</span><span class="w"> </span><span class="nf">_warn_get_lr_called_within_step</span><span class="p">(</span><span class="n">lr_scheduler</span><span class="p">:</span> <span class="n">LRScheduler</span><span class="p">):</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">lr_scheduler</span><span class="o">.</span><span class="n">_get_lr_called_within_step</span><span class="p">:</span>
        <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
            <span class="s2">&quot;To get the last learning rate computed by the scheduler, &quot;</span>
            <span class="s2">&quot;please use `get_last_lr()`.&quot;</span><span class="p">,</span>
            <span class="ne">UserWarning</span><span class="p">,</span>
            <span class="n">stacklevel</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
        <span class="p">)</span>


<span class="c1"># Including _LRScheduler for backwards compatibility</span>
<span class="c1"># Subclass instead of assign because we want __name__ of _LRScheduler to be _LRScheduler (assigning would make it LRScheduler).</span>
<span class="k">class</span><span class="w"> </span><span class="nc">_LRScheduler</span><span class="p">(</span><span class="n">LRScheduler</span><span class="p">):</span>
    <span class="k">pass</span>


<span class="k">class</span><span class="w"> </span><span class="nc">_enable_get_lr_call</span><span class="p">:</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">o</span><span class="p">:</span> <span class="n">LRScheduler</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">o</span> <span class="o">=</span> <span class="n">o</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__enter__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">o</span><span class="o">.</span><span class="n">_get_lr_called_within_step</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="k">return</span> <span class="bp">self</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__exit__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">type</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">traceback</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">o</span><span class="o">.</span><span class="n">_get_lr_called_within_step</span> <span class="o">=</span> <span class="kc">False</span>


<div class="viewcode-block" id="LambdaLR">
<a class="viewcode-back" href="../../../python-api/generated/torch.optim.lr_scheduler.LambdaLR.html#torch.optim.lr_scheduler.LambdaLR">[docs]</a>
<span class="k">class</span><span class="w"> </span><span class="nc">LambdaLR</span><span class="p">(</span><span class="n">LRScheduler</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Sets the initial learning rate.</span>

<span class="sd">    The learning rate of each parameter group is set to the initial lr</span>
<span class="sd">    times a given function. When last_epoch=-1, sets initial lr as lr.</span>

<span class="sd">    Args:</span>
<span class="sd">        optimizer (Optimizer): Wrapped optimizer.</span>
<span class="sd">        lr_lambda (function or list): A function which computes a multiplicative</span>
<span class="sd">            factor given an integer parameter epoch, or a list of such</span>
<span class="sd">            functions, one for each group in optimizer.param_groups.</span>
<span class="sd">        last_epoch (int): The index of last epoch. Default: -1.</span>
<span class="sd">        verbose (bool | str): If ``True``, prints a message to stdout for</span>
<span class="sd">            each update. Default: ``False``.</span>

<span class="sd">            .. deprecated:: 2.2</span>
<span class="sd">                ``verbose`` is deprecated. Please use ``get_last_lr()`` to access the</span>
<span class="sd">                learning rate.</span>

<span class="sd">    Example:</span>
<span class="sd">        &gt;&gt;&gt; # xdoctest: +SKIP</span>
<span class="sd">        &gt;&gt;&gt; # Assuming optimizer has two groups.</span>
<span class="sd">        &gt;&gt;&gt; lambda1 = lambda epoch: epoch // 30</span>
<span class="sd">        &gt;&gt;&gt; lambda2 = lambda epoch: 0.95 ** epoch</span>
<span class="sd">        &gt;&gt;&gt; scheduler = LambdaLR(optimizer, lr_lambda=[lambda1, lambda2])</span>
<span class="sd">        &gt;&gt;&gt; for epoch in range(100):</span>
<span class="sd">        &gt;&gt;&gt;     train(...)</span>
<span class="sd">        &gt;&gt;&gt;     validate(...)</span>
<span class="sd">        &gt;&gt;&gt;     scheduler.step()</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">optimizer</span><span class="p">:</span> <span class="n">Optimizer</span><span class="p">,</span>
        <span class="n">lr_lambda</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">Callable</span><span class="p">[[</span><span class="nb">int</span><span class="p">],</span> <span class="nb">float</span><span class="p">],</span> <span class="n">List</span><span class="p">[</span><span class="n">Callable</span><span class="p">[[</span><span class="nb">int</span><span class="p">],</span> <span class="nb">float</span><span class="p">]]],</span>
        <span class="n">last_epoch</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">verbose</span><span class="o">=</span><span class="s2">&quot;deprecated&quot;</span><span class="p">,</span>
    <span class="p">):</span>  <span class="c1"># noqa: D107</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">optimizer</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">lr_lambdas</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Callable</span><span class="p">[[</span><span class="nb">int</span><span class="p">],</span> <span class="nb">float</span><span class="p">]]</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">lr_lambda</span><span class="p">,</span> <span class="nb">list</span><span class="p">)</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">lr_lambda</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">lr_lambdas</span> <span class="o">=</span> <span class="p">[</span><span class="n">lr_lambda</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">lr_lambda</span><span class="p">)</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;Expected </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">)</span><span class="si">}</span><span class="s2"> lr_lambdas, but got </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">lr_lambda</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span>
                <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">lr_lambdas</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">lr_lambda</span><span class="p">)</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">last_epoch</span><span class="p">,</span> <span class="n">verbose</span><span class="p">)</span>

<div class="viewcode-block" id="LambdaLR.state_dict">
<a class="viewcode-back" href="../../../python-api/generated/torch.optim.lr_scheduler.LambdaLR.html#torch.optim.lr_scheduler.LambdaLR.state_dict">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">state_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Return the state of the scheduler as a :class:`dict`.</span>

<span class="sd">        It contains an entry for every variable in self.__dict__ which</span>
<span class="sd">        is not the optimizer.</span>
<span class="sd">        The learning rate lambda functions will only be saved if they are callable objects</span>
<span class="sd">        and not if they are functions or lambdas.</span>

<span class="sd">        When saving or loading the scheduler, please make sure to also save or load the state of the optimizer.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">state_dict</span> <span class="o">=</span> <span class="p">{</span>
            <span class="n">key</span><span class="p">:</span> <span class="n">value</span>
            <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="vm">__dict__</span><span class="o">.</span><span class="n">items</span><span class="p">()</span>
            <span class="k">if</span> <span class="n">key</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">(</span><span class="s2">&quot;optimizer&quot;</span><span class="p">,</span> <span class="s2">&quot;lr_lambdas&quot;</span><span class="p">)</span>
        <span class="p">}</span>
        <span class="n">state_dict</span><span class="p">[</span><span class="s2">&quot;lr_lambdas&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="kc">None</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">lr_lambdas</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">fn</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">lr_lambdas</span><span class="p">):</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">fn</span><span class="p">,</span> <span class="n">types</span><span class="o">.</span><span class="n">FunctionType</span><span class="p">):</span>
                <span class="n">state_dict</span><span class="p">[</span><span class="s2">&quot;lr_lambdas&quot;</span><span class="p">][</span><span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">fn</span><span class="o">.</span><span class="vm">__dict__</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>

        <span class="k">return</span> <span class="n">state_dict</span></div>


<div class="viewcode-block" id="LambdaLR.load_state_dict">
<a class="viewcode-back" href="../../../python-api/generated/torch.optim.lr_scheduler.LambdaLR.html#torch.optim.lr_scheduler.LambdaLR.load_state_dict">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">load_state_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state_dict</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Load the scheduler&#39;s state.</span>

<span class="sd">        When saving or loading the scheduler, please make sure to also save or load the state of the optimizer.</span>

<span class="sd">        Args:</span>
<span class="sd">            state_dict (dict): scheduler state. Should be an object returned</span>
<span class="sd">                from a call to :meth:`state_dict`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">lr_lambdas</span> <span class="o">=</span> <span class="n">state_dict</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;lr_lambdas&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="vm">__dict__</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">state_dict</span><span class="p">)</span>
        <span class="c1"># Restore state_dict keys in order to prevent side effects</span>
        <span class="c1"># https://github.com/pytorch/pytorch/issues/32756</span>
        <span class="n">state_dict</span><span class="p">[</span><span class="s2">&quot;lr_lambdas&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">lr_lambdas</span>

        <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">fn</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">lr_lambdas</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">fn</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">lr_lambdas</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="vm">__dict__</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">fn</span><span class="p">)</span></div>


<div class="viewcode-block" id="LambdaLR.get_lr">
<a class="viewcode-back" href="../../../python-api/generated/torch.optim.lr_scheduler.LambdaLR.html#torch.optim.lr_scheduler.LambdaLR.get_lr">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">get_lr</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Compute learning rate.&quot;&quot;&quot;</span>
        <span class="n">_warn_get_lr_called_within_step</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>

        <span class="k">return</span> <span class="p">[</span>
            <span class="n">base_lr</span> <span class="o">*</span> <span class="n">lmbda</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">last_epoch</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">lmbda</span><span class="p">,</span> <span class="n">base_lr</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">lr_lambdas</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">base_lrs</span><span class="p">)</span>
        <span class="p">]</span></div>
</div>



<div class="viewcode-block" id="MultiplicativeLR">
<a class="viewcode-back" href="../../../python-api/generated/torch.optim.lr_scheduler.MultiplicativeLR.html#torch.optim.lr_scheduler.MultiplicativeLR">[docs]</a>
<span class="k">class</span><span class="w"> </span><span class="nc">MultiplicativeLR</span><span class="p">(</span><span class="n">LRScheduler</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Multiply the learning rate of each parameter group by the factor given in the specified function.</span>

<span class="sd">    When last_epoch=-1, set initial lr as lr.</span>

<span class="sd">    Args:</span>
<span class="sd">        optimizer (Optimizer): Wrapped optimizer.</span>
<span class="sd">        lr_lambda (function or list): A function which computes a multiplicative</span>
<span class="sd">            factor given an integer parameter epoch, or a list of such</span>
<span class="sd">            functions, one for each group in optimizer.param_groups.</span>
<span class="sd">        last_epoch (int): The index of last epoch. Default: -1.</span>
<span class="sd">        verbose (bool | str): If ``True``, prints a message to stdout for</span>
<span class="sd">            each update. Default: ``False``.</span>

<span class="sd">            .. deprecated:: 2.2</span>
<span class="sd">                ``verbose`` is deprecated. Please use ``get_last_lr()`` to access the</span>
<span class="sd">                learning rate.</span>

<span class="sd">    Example:</span>
<span class="sd">        &gt;&gt;&gt; # xdoctest: +SKIP</span>
<span class="sd">        &gt;&gt;&gt; lmbda = lambda epoch: 0.95</span>
<span class="sd">        &gt;&gt;&gt; scheduler = MultiplicativeLR(optimizer, lr_lambda=lmbda)</span>
<span class="sd">        &gt;&gt;&gt; for epoch in range(100):</span>
<span class="sd">        &gt;&gt;&gt;     train(...)</span>
<span class="sd">        &gt;&gt;&gt;     validate(...)</span>
<span class="sd">        &gt;&gt;&gt;     scheduler.step()</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">optimizer</span><span class="p">:</span> <span class="n">Optimizer</span><span class="p">,</span>
        <span class="n">lr_lambda</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">Callable</span><span class="p">[[</span><span class="nb">int</span><span class="p">],</span> <span class="nb">float</span><span class="p">],</span> <span class="n">List</span><span class="p">[</span><span class="n">Callable</span><span class="p">[[</span><span class="nb">int</span><span class="p">],</span> <span class="nb">float</span><span class="p">]]],</span>
        <span class="n">last_epoch</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">verbose</span><span class="o">=</span><span class="s2">&quot;deprecated&quot;</span><span class="p">,</span>
    <span class="p">):</span>  <span class="c1"># noqa: D107</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">optimizer</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">lr_lambdas</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Callable</span><span class="p">[[</span><span class="nb">int</span><span class="p">],</span> <span class="nb">float</span><span class="p">]]</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">lr_lambda</span><span class="p">,</span> <span class="nb">list</span><span class="p">)</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">lr_lambda</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">lr_lambdas</span> <span class="o">=</span> <span class="p">[</span><span class="n">lr_lambda</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">lr_lambda</span><span class="p">)</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;Expected </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">)</span><span class="si">}</span><span class="s2"> lr_lambdas, but got </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">lr_lambda</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span>
                <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">lr_lambdas</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">lr_lambda</span><span class="p">)</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">last_epoch</span><span class="p">,</span> <span class="n">verbose</span><span class="p">)</span>

<div class="viewcode-block" id="MultiplicativeLR.state_dict">
<a class="viewcode-back" href="../../../python-api/generated/torch.optim.lr_scheduler.MultiplicativeLR.html#torch.optim.lr_scheduler.MultiplicativeLR.state_dict">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">state_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Return the state of the scheduler as a :class:`dict`.</span>

<span class="sd">        It contains an entry for every variable in self.__dict__ which</span>
<span class="sd">        is not the optimizer.</span>
<span class="sd">        The learning rate lambda functions will only be saved if they are callable objects</span>
<span class="sd">        and not if they are functions or lambdas.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">state_dict</span> <span class="o">=</span> <span class="p">{</span>
            <span class="n">key</span><span class="p">:</span> <span class="n">value</span>
            <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="vm">__dict__</span><span class="o">.</span><span class="n">items</span><span class="p">()</span>
            <span class="k">if</span> <span class="n">key</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">(</span><span class="s2">&quot;optimizer&quot;</span><span class="p">,</span> <span class="s2">&quot;lr_lambdas&quot;</span><span class="p">)</span>
        <span class="p">}</span>
        <span class="n">state_dict</span><span class="p">[</span><span class="s2">&quot;lr_lambdas&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="kc">None</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">lr_lambdas</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">fn</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">lr_lambdas</span><span class="p">):</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">fn</span><span class="p">,</span> <span class="n">types</span><span class="o">.</span><span class="n">FunctionType</span><span class="p">):</span>
                <span class="n">state_dict</span><span class="p">[</span><span class="s2">&quot;lr_lambdas&quot;</span><span class="p">][</span><span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">fn</span><span class="o">.</span><span class="vm">__dict__</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>

        <span class="k">return</span> <span class="n">state_dict</span></div>


<div class="viewcode-block" id="MultiplicativeLR.load_state_dict">
<a class="viewcode-back" href="../../../python-api/generated/torch.optim.lr_scheduler.MultiplicativeLR.html#torch.optim.lr_scheduler.MultiplicativeLR.load_state_dict">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">load_state_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state_dict</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Load the scheduler&#39;s state.</span>

<span class="sd">        Args:</span>
<span class="sd">            state_dict (dict): scheduler state. Should be an object returned</span>
<span class="sd">                from a call to :meth:`state_dict`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">lr_lambdas</span> <span class="o">=</span> <span class="n">state_dict</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;lr_lambdas&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="vm">__dict__</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">state_dict</span><span class="p">)</span>
        <span class="c1"># Restore state_dict keys in order to prevent side effects</span>
        <span class="c1"># https://github.com/pytorch/pytorch/issues/32756</span>
        <span class="n">state_dict</span><span class="p">[</span><span class="s2">&quot;lr_lambdas&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">lr_lambdas</span>

        <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">fn</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">lr_lambdas</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">fn</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">lr_lambdas</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="vm">__dict__</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">fn</span><span class="p">)</span></div>


<div class="viewcode-block" id="MultiplicativeLR.get_lr">
<a class="viewcode-back" href="../../../python-api/generated/torch.optim.lr_scheduler.MultiplicativeLR.html#torch.optim.lr_scheduler.MultiplicativeLR.get_lr">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">get_lr</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Compute the learning rate of each parameter group.&quot;&quot;&quot;</span>
        <span class="n">_warn_get_lr_called_within_step</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">last_epoch</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">return</span> <span class="p">[</span>
                <span class="n">group</span><span class="p">[</span><span class="s2">&quot;lr&quot;</span><span class="p">]</span> <span class="o">*</span> <span class="n">lmbda</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">last_epoch</span><span class="p">)</span>
                <span class="k">for</span> <span class="n">lmbda</span><span class="p">,</span> <span class="n">group</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">lr_lambdas</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">)</span>
            <span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="p">[</span><span class="n">group</span><span class="p">[</span><span class="s2">&quot;lr&quot;</span><span class="p">]</span> <span class="k">for</span> <span class="n">group</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">]</span></div>
</div>



<div class="viewcode-block" id="StepLR">
<a class="viewcode-back" href="../../../python-api/generated/torch.optim.lr_scheduler.StepLR.html#torch.optim.lr_scheduler.StepLR">[docs]</a>
<span class="k">class</span><span class="w"> </span><span class="nc">StepLR</span><span class="p">(</span><span class="n">LRScheduler</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Decays the learning rate of each parameter group by gamma every step_size epochs.</span>

<span class="sd">    Notice that such decay can happen simultaneously with other changes to the learning rate</span>
<span class="sd">    from outside this scheduler. When last_epoch=-1, sets initial lr as lr.</span>

<span class="sd">    Args:</span>
<span class="sd">        optimizer (Optimizer): Wrapped optimizer.</span>
<span class="sd">        step_size (int): Period of learning rate decay.</span>
<span class="sd">        gamma (float): Multiplicative factor of learning rate decay.</span>
<span class="sd">            Default: 0.1.</span>
<span class="sd">        last_epoch (int): The index of last epoch. Default: -1.</span>
<span class="sd">        verbose (bool | str): If ``True``, prints a message to stdout for</span>
<span class="sd">            each update. Default: ``False``.</span>

<span class="sd">            .. deprecated:: 2.2</span>
<span class="sd">                ``verbose`` is deprecated. Please use ``get_last_lr()`` to access the</span>
<span class="sd">                learning rate.</span>

<span class="sd">    Example:</span>
<span class="sd">        &gt;&gt;&gt; # xdoctest: +SKIP</span>
<span class="sd">        &gt;&gt;&gt; # Assuming optimizer uses lr = 0.05 for all groups</span>
<span class="sd">        &gt;&gt;&gt; # lr = 0.05     if epoch &lt; 30</span>
<span class="sd">        &gt;&gt;&gt; # lr = 0.005    if 30 &lt;= epoch &lt; 60</span>
<span class="sd">        &gt;&gt;&gt; # lr = 0.0005   if 60 &lt;= epoch &lt; 90</span>
<span class="sd">        &gt;&gt;&gt; # ...</span>
<span class="sd">        &gt;&gt;&gt; scheduler = StepLR(optimizer, step_size=30, gamma=0.1)</span>
<span class="sd">        &gt;&gt;&gt; for epoch in range(100):</span>
<span class="sd">        &gt;&gt;&gt;     train(...)</span>
<span class="sd">        &gt;&gt;&gt;     validate(...)</span>
<span class="sd">        &gt;&gt;&gt;     scheduler.step()</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">optimizer</span><span class="p">:</span> <span class="n">Optimizer</span><span class="p">,</span>
        <span class="n">step_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">gamma</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
        <span class="n">last_epoch</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">verbose</span><span class="o">=</span><span class="s2">&quot;deprecated&quot;</span><span class="p">,</span>
    <span class="p">):</span>  <span class="c1"># noqa: D107</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">step_size</span> <span class="o">=</span> <span class="n">step_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span> <span class="o">=</span> <span class="n">gamma</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">last_epoch</span><span class="p">,</span> <span class="n">verbose</span><span class="p">)</span>

<div class="viewcode-block" id="StepLR.get_lr">
<a class="viewcode-back" href="../../../python-api/generated/torch.optim.lr_scheduler.StepLR.html#torch.optim.lr_scheduler.StepLR.get_lr">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">get_lr</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Compute the learning rate of each parameter group.&quot;&quot;&quot;</span>
        <span class="n">_warn_get_lr_called_within_step</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>

        <span class="k">if</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">last_epoch</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="ow">or</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">last_epoch</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">step_size</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">):</span>
            <span class="k">return</span> <span class="p">[</span><span class="n">group</span><span class="p">[</span><span class="s2">&quot;lr&quot;</span><span class="p">]</span> <span class="k">for</span> <span class="n">group</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">]</span>
        <span class="k">return</span> <span class="p">[</span><span class="n">group</span><span class="p">[</span><span class="s2">&quot;lr&quot;</span><span class="p">]</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span> <span class="k">for</span> <span class="n">group</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">]</span></div>


    <span class="k">def</span><span class="w"> </span><span class="nf">_get_closed_form_lr</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">[</span>
            <span class="n">base_lr</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span> <span class="o">**</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">last_epoch</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">step_size</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">base_lr</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">base_lrs</span>
        <span class="p">]</span></div>



<div class="viewcode-block" id="MultiStepLR">
<a class="viewcode-back" href="../../../python-api/generated/torch.optim.lr_scheduler.MultiStepLR.html#torch.optim.lr_scheduler.MultiStepLR">[docs]</a>
<span class="k">class</span><span class="w"> </span><span class="nc">MultiStepLR</span><span class="p">(</span><span class="n">LRScheduler</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Decays the learning rate of each parameter group by gamma once the number of epoch reaches one of the milestones.</span>

<span class="sd">    Notice that such decay can happen simultaneously with other changes to the learning rate</span>
<span class="sd">    from outside this scheduler. When last_epoch=-1, sets initial lr as lr.</span>

<span class="sd">    Args:</span>
<span class="sd">        optimizer (Optimizer): Wrapped optimizer.</span>
<span class="sd">        milestones (list): List of epoch indices. Must be increasing.</span>
<span class="sd">        gamma (float): Multiplicative factor of learning rate decay.</span>
<span class="sd">            Default: 0.1.</span>
<span class="sd">        last_epoch (int): The index of last epoch. Default: -1.</span>
<span class="sd">        verbose (bool | str): If ``True``, prints a message to stdout for</span>
<span class="sd">            each update. Default: ``False``.</span>

<span class="sd">            .. deprecated:: 2.2</span>
<span class="sd">                ``verbose`` is deprecated. Please use ``get_last_lr()`` to access the</span>
<span class="sd">                learning rate.</span>

<span class="sd">    Example:</span>
<span class="sd">        &gt;&gt;&gt; # xdoctest: +SKIP</span>
<span class="sd">        &gt;&gt;&gt; # Assuming optimizer uses lr = 0.05 for all groups</span>
<span class="sd">        &gt;&gt;&gt; # lr = 0.05     if epoch &lt; 30</span>
<span class="sd">        &gt;&gt;&gt; # lr = 0.005    if 30 &lt;= epoch &lt; 80</span>
<span class="sd">        &gt;&gt;&gt; # lr = 0.0005   if epoch &gt;= 80</span>
<span class="sd">        &gt;&gt;&gt; scheduler = MultiStepLR(optimizer, milestones=[30,80], gamma=0.1)</span>
<span class="sd">        &gt;&gt;&gt; for epoch in range(100):</span>
<span class="sd">        &gt;&gt;&gt;     train(...)</span>
<span class="sd">        &gt;&gt;&gt;     validate(...)</span>
<span class="sd">        &gt;&gt;&gt;     scheduler.step()</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">optimizer</span><span class="p">:</span> <span class="n">Optimizer</span><span class="p">,</span>
        <span class="n">milestones</span><span class="p">:</span> <span class="n">Iterable</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
        <span class="n">gamma</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
        <span class="n">last_epoch</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">verbose</span><span class="o">=</span><span class="s2">&quot;deprecated&quot;</span><span class="p">,</span>
    <span class="p">):</span>  <span class="c1"># noqa: D107</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">milestones</span> <span class="o">=</span> <span class="n">Counter</span><span class="p">(</span><span class="n">milestones</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span> <span class="o">=</span> <span class="n">gamma</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">last_epoch</span><span class="p">,</span> <span class="n">verbose</span><span class="p">)</span>

<div class="viewcode-block" id="MultiStepLR.get_lr">
<a class="viewcode-back" href="../../../python-api/generated/torch.optim.lr_scheduler.MultiStepLR.html#torch.optim.lr_scheduler.MultiStepLR.get_lr">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">get_lr</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Compute the learning rate of each parameter group.&quot;&quot;&quot;</span>
        <span class="n">_warn_get_lr_called_within_step</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">last_epoch</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">milestones</span><span class="p">:</span>
            <span class="k">return</span> <span class="p">[</span><span class="n">group</span><span class="p">[</span><span class="s2">&quot;lr&quot;</span><span class="p">]</span> <span class="k">for</span> <span class="n">group</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">]</span>
        <span class="k">return</span> <span class="p">[</span>
            <span class="n">group</span><span class="p">[</span><span class="s2">&quot;lr&quot;</span><span class="p">]</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span> <span class="o">**</span> <span class="bp">self</span><span class="o">.</span><span class="n">milestones</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">last_epoch</span><span class="p">]</span>
            <span class="k">for</span> <span class="n">group</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span>
        <span class="p">]</span></div>


    <span class="k">def</span><span class="w"> </span><span class="nf">_get_closed_form_lr</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">milestones</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">milestones</span><span class="o">.</span><span class="n">elements</span><span class="p">())</span>
        <span class="k">return</span> <span class="p">[</span>
            <span class="n">base_lr</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span> <span class="o">**</span> <span class="n">bisect_right</span><span class="p">(</span><span class="n">milestones</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">last_epoch</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">base_lr</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">base_lrs</span>
        <span class="p">]</span></div>



<div class="viewcode-block" id="ConstantLR">
<a class="viewcode-back" href="../../../python-api/generated/torch.optim.lr_scheduler.ConstantLR.html#torch.optim.lr_scheduler.ConstantLR">[docs]</a>
<span class="k">class</span><span class="w"> </span><span class="nc">ConstantLR</span><span class="p">(</span><span class="n">LRScheduler</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Multiply the learning rate of each parameter group by a small constant factor.</span>

<span class="sd">    The multiplication is done until the number of epoch reaches a pre-defined milestone: total_iters.</span>
<span class="sd">    Notice that such multiplication of the small constant factor can</span>
<span class="sd">    happen simultaneously with other changes to the learning rate from outside this scheduler.</span>
<span class="sd">    When last_epoch=-1, sets initial lr as lr.</span>

<span class="sd">    Args:</span>
<span class="sd">        optimizer (Optimizer): Wrapped optimizer.</span>
<span class="sd">        factor (float): The number we multiply learning rate until the milestone. Default: 1./3.</span>
<span class="sd">        total_iters (int): The number of steps that the scheduler multiplies the learning rate by the factor.</span>
<span class="sd">            Default: 5.</span>
<span class="sd">        last_epoch (int): The index of the last epoch. Default: -1.</span>
<span class="sd">        verbose (bool | str): If ``True``, prints a message to stdout for</span>
<span class="sd">            each update. Default: ``False``.</span>

<span class="sd">            .. deprecated:: 2.2</span>
<span class="sd">                ``verbose`` is deprecated. Please use ``get_last_lr()`` to access the</span>
<span class="sd">                learning rate.</span>

<span class="sd">    Example:</span>
<span class="sd">        &gt;&gt;&gt; # xdoctest: +SKIP</span>
<span class="sd">        &gt;&gt;&gt; # Assuming optimizer uses lr = 0.05 for all groups</span>
<span class="sd">        &gt;&gt;&gt; # lr = 0.025   if epoch == 0</span>
<span class="sd">        &gt;&gt;&gt; # lr = 0.025   if epoch == 1</span>
<span class="sd">        &gt;&gt;&gt; # lr = 0.025   if epoch == 2</span>
<span class="sd">        &gt;&gt;&gt; # lr = 0.025   if epoch == 3</span>
<span class="sd">        &gt;&gt;&gt; # lr = 0.05    if epoch &gt;= 4</span>
<span class="sd">        &gt;&gt;&gt; scheduler = ConstantLR(optimizer, factor=0.5, total_iters=4)</span>
<span class="sd">        &gt;&gt;&gt; for epoch in range(100):</span>
<span class="sd">        &gt;&gt;&gt;     train(...)</span>
<span class="sd">        &gt;&gt;&gt;     validate(...)</span>
<span class="sd">        &gt;&gt;&gt;     scheduler.step()</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">optimizer</span><span class="p">:</span> <span class="n">Optimizer</span><span class="p">,</span>
        <span class="n">factor</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="mi">3</span><span class="p">,</span>
        <span class="n">total_iters</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">5</span><span class="p">,</span>
        <span class="n">last_epoch</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">verbose</span><span class="o">=</span><span class="s2">&quot;deprecated&quot;</span><span class="p">,</span>
    <span class="p">):</span>  <span class="c1"># noqa: D107</span>
        <span class="k">if</span> <span class="n">factor</span> <span class="o">&gt;</span> <span class="mf">1.0</span> <span class="ow">or</span> <span class="n">factor</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Constant multiplicative factor expected to be between 0 and 1.&quot;</span>
            <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">factor</span> <span class="o">=</span> <span class="n">factor</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">total_iters</span> <span class="o">=</span> <span class="n">total_iters</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">last_epoch</span><span class="p">,</span> <span class="n">verbose</span><span class="p">)</span>

<div class="viewcode-block" id="ConstantLR.get_lr">
<a class="viewcode-back" href="../../../python-api/generated/torch.optim.lr_scheduler.ConstantLR.html#torch.optim.lr_scheduler.ConstantLR.get_lr">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">get_lr</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Compute the learning rate of each parameter group.&quot;&quot;&quot;</span>
        <span class="n">_warn_get_lr_called_within_step</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">last_epoch</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">return</span> <span class="p">[</span><span class="n">group</span><span class="p">[</span><span class="s2">&quot;lr&quot;</span><span class="p">]</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">factor</span> <span class="k">for</span> <span class="n">group</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">]</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">last_epoch</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">total_iters</span><span class="p">:</span>
            <span class="k">return</span> <span class="p">[</span><span class="n">group</span><span class="p">[</span><span class="s2">&quot;lr&quot;</span><span class="p">]</span> <span class="k">for</span> <span class="n">group</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">]</span>

        <span class="k">return</span> <span class="p">[</span>
            <span class="n">group</span><span class="p">[</span><span class="s2">&quot;lr&quot;</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">factor</span><span class="p">)</span> <span class="k">for</span> <span class="n">group</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span>
        <span class="p">]</span></div>


    <span class="k">def</span><span class="w"> </span><span class="nf">_get_closed_form_lr</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">[</span>
            <span class="n">base_lr</span>
            <span class="o">*</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">factor</span> <span class="o">+</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">last_epoch</span> <span class="o">&gt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">total_iters</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">factor</span><span class="p">))</span>
            <span class="k">for</span> <span class="n">base_lr</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">base_lrs</span>
        <span class="p">]</span></div>



<div class="viewcode-block" id="LinearLR">
<a class="viewcode-back" href="../../../python-api/generated/torch.optim.lr_scheduler.LinearLR.html#torch.optim.lr_scheduler.LinearLR">[docs]</a>
<span class="k">class</span><span class="w"> </span><span class="nc">LinearLR</span><span class="p">(</span><span class="n">LRScheduler</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Decays the learning rate of each parameter group by linearly changing small multiplicative factor.</span>

<span class="sd">    The multiplication is done until the number of epoch reaches a pre-defined milestone: total_iters.</span>
<span class="sd">    Notice that such decay can happen simultaneously with other changes to the learning rate</span>
<span class="sd">    from outside this scheduler. When last_epoch=-1, sets initial lr as lr.</span>

<span class="sd">    Args:</span>
<span class="sd">        optimizer (Optimizer): Wrapped optimizer.</span>
<span class="sd">        start_factor (float): The number we multiply learning rate in the first epoch.</span>
<span class="sd">            The multiplication factor changes towards end_factor in the following epochs.</span>
<span class="sd">            Default: 1./3.</span>
<span class="sd">        end_factor (float): The number we multiply learning rate at the end of linear changing</span>
<span class="sd">            process. Default: 1.0.</span>
<span class="sd">        total_iters (int): The number of iterations that multiplicative factor reaches to 1.</span>
<span class="sd">            Default: 5.</span>
<span class="sd">        last_epoch (int): The index of the last epoch. Default: -1.</span>
<span class="sd">        verbose (bool | str): If ``True``, prints a message to stdout for</span>
<span class="sd">            each update. Default: ``False``.</span>

<span class="sd">            .. deprecated:: 2.2</span>
<span class="sd">                ``verbose`` is deprecated. Please use ``get_last_lr()`` to access the</span>
<span class="sd">                learning rate.</span>

<span class="sd">    Example:</span>
<span class="sd">        &gt;&gt;&gt; # xdoctest: +SKIP</span>
<span class="sd">        &gt;&gt;&gt; # Assuming optimizer uses lr = 0.05 for all groups</span>
<span class="sd">        &gt;&gt;&gt; # lr = 0.025    if epoch == 0</span>
<span class="sd">        &gt;&gt;&gt; # lr = 0.03125  if epoch == 1</span>
<span class="sd">        &gt;&gt;&gt; # lr = 0.0375   if epoch == 2</span>
<span class="sd">        &gt;&gt;&gt; # lr = 0.04375  if epoch == 3</span>
<span class="sd">        &gt;&gt;&gt; # lr = 0.05    if epoch &gt;= 4</span>
<span class="sd">        &gt;&gt;&gt; scheduler = LinearLR(optimizer, start_factor=0.5, total_iters=4)</span>
<span class="sd">        &gt;&gt;&gt; for epoch in range(100):</span>
<span class="sd">        &gt;&gt;&gt;     train(...)</span>
<span class="sd">        &gt;&gt;&gt;     validate(...)</span>
<span class="sd">        &gt;&gt;&gt;     scheduler.step()</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">optimizer</span><span class="p">:</span> <span class="n">Optimizer</span><span class="p">,</span>
        <span class="n">start_factor</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="mi">3</span><span class="p">,</span>
        <span class="n">end_factor</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span>
        <span class="n">total_iters</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">5</span><span class="p">,</span>
        <span class="n">last_epoch</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">verbose</span><span class="o">=</span><span class="s2">&quot;deprecated&quot;</span><span class="p">,</span>
    <span class="p">):</span>  <span class="c1"># noqa: D107</span>
        <span class="k">if</span> <span class="n">start_factor</span> <span class="o">&gt;</span> <span class="mf">1.0</span> <span class="ow">or</span> <span class="n">start_factor</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Starting multiplicative factor expected to be greater than 0 and less or equal to 1.&quot;</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="n">end_factor</span> <span class="o">&gt;</span> <span class="mf">1.0</span> <span class="ow">or</span> <span class="n">end_factor</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Ending multiplicative factor expected to be between 0 and 1.&quot;</span>
            <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">start_factor</span> <span class="o">=</span> <span class="n">start_factor</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">end_factor</span> <span class="o">=</span> <span class="n">end_factor</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">total_iters</span> <span class="o">=</span> <span class="n">total_iters</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">last_epoch</span><span class="p">,</span> <span class="n">verbose</span><span class="p">)</span>

<div class="viewcode-block" id="LinearLR.get_lr">
<a class="viewcode-back" href="../../../python-api/generated/torch.optim.lr_scheduler.LinearLR.html#torch.optim.lr_scheduler.LinearLR.get_lr">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">get_lr</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Compute the learning rate.&quot;&quot;&quot;</span>
        <span class="n">_warn_get_lr_called_within_step</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">last_epoch</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">return</span> <span class="p">[</span>
                <span class="n">group</span><span class="p">[</span><span class="s2">&quot;lr&quot;</span><span class="p">]</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">start_factor</span> <span class="k">for</span> <span class="n">group</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span>
            <span class="p">]</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">last_epoch</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">total_iters</span><span class="p">:</span>
            <span class="k">return</span> <span class="p">[</span><span class="n">group</span><span class="p">[</span><span class="s2">&quot;lr&quot;</span><span class="p">]</span> <span class="k">for</span> <span class="n">group</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">]</span>

        <span class="k">return</span> <span class="p">[</span>
            <span class="n">group</span><span class="p">[</span><span class="s2">&quot;lr&quot;</span><span class="p">]</span>
            <span class="o">*</span> <span class="p">(</span>
                <span class="mf">1.0</span>
                <span class="o">+</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">end_factor</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">start_factor</span><span class="p">)</span>
                <span class="o">/</span> <span class="p">(</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">total_iters</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">start_factor</span>
                    <span class="o">+</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">last_epoch</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">end_factor</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">start_factor</span><span class="p">)</span>
                <span class="p">)</span>
            <span class="p">)</span>
            <span class="k">for</span> <span class="n">group</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span>
        <span class="p">]</span></div>


    <span class="k">def</span><span class="w"> </span><span class="nf">_get_closed_form_lr</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">[</span>
            <span class="n">base_lr</span>
            <span class="o">*</span> <span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">start_factor</span>
                <span class="o">+</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">end_factor</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">start_factor</span><span class="p">)</span>
                <span class="o">*</span> <span class="nb">min</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">total_iters</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">last_epoch</span><span class="p">)</span>
                <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">total_iters</span>
            <span class="p">)</span>
            <span class="k">for</span> <span class="n">base_lr</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">base_lrs</span>
        <span class="p">]</span></div>



<div class="viewcode-block" id="ExponentialLR">
<a class="viewcode-back" href="../../../python-api/generated/torch.optim.lr_scheduler.ExponentialLR.html#torch.optim.lr_scheduler.ExponentialLR">[docs]</a>
<span class="k">class</span><span class="w"> </span><span class="nc">ExponentialLR</span><span class="p">(</span><span class="n">LRScheduler</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Decays the learning rate of each parameter group by gamma every epoch.</span>

<span class="sd">    When last_epoch=-1, sets initial lr as lr.</span>

<span class="sd">    Args:</span>
<span class="sd">        optimizer (Optimizer): Wrapped optimizer.</span>
<span class="sd">        gamma (float): Multiplicative factor of learning rate decay.</span>
<span class="sd">        last_epoch (int): The index of last epoch. Default: -1.</span>
<span class="sd">        verbose (bool | str): If ``True``, prints a message to stdout for</span>
<span class="sd">            each update. Default: ``False``.</span>

<span class="sd">            .. deprecated:: 2.2</span>
<span class="sd">                ``verbose`` is deprecated. Please use ``get_last_lr()`` to access the</span>
<span class="sd">                learning rate.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">optimizer</span><span class="p">:</span> <span class="n">Optimizer</span><span class="p">,</span>
        <span class="n">gamma</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
        <span class="n">last_epoch</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">verbose</span><span class="o">=</span><span class="s2">&quot;deprecated&quot;</span><span class="p">,</span>
    <span class="p">):</span>  <span class="c1"># noqa: D107</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span> <span class="o">=</span> <span class="n">gamma</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">last_epoch</span><span class="p">,</span> <span class="n">verbose</span><span class="p">)</span>

<div class="viewcode-block" id="ExponentialLR.get_lr">
<a class="viewcode-back" href="../../../python-api/generated/torch.optim.lr_scheduler.ExponentialLR.html#torch.optim.lr_scheduler.ExponentialLR.get_lr">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">get_lr</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Compute the learning rate of each parameter group.&quot;&quot;&quot;</span>
        <span class="n">_warn_get_lr_called_within_step</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">last_epoch</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">return</span> <span class="p">[</span><span class="n">group</span><span class="p">[</span><span class="s2">&quot;lr&quot;</span><span class="p">]</span> <span class="k">for</span> <span class="n">group</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">]</span>
        <span class="k">return</span> <span class="p">[</span><span class="n">group</span><span class="p">[</span><span class="s2">&quot;lr&quot;</span><span class="p">]</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span> <span class="k">for</span> <span class="n">group</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">]</span></div>


    <span class="k">def</span><span class="w"> </span><span class="nf">_get_closed_form_lr</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">[</span><span class="n">base_lr</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span><span class="o">**</span><span class="bp">self</span><span class="o">.</span><span class="n">last_epoch</span> <span class="k">for</span> <span class="n">base_lr</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">base_lrs</span><span class="p">]</span></div>



<div class="viewcode-block" id="SequentialLR">
<a class="viewcode-back" href="../../../python-api/generated/torch.optim.lr_scheduler.SequentialLR.html#torch.optim.lr_scheduler.SequentialLR">[docs]</a>
<span class="k">class</span><span class="w"> </span><span class="nc">SequentialLR</span><span class="p">(</span><span class="n">LRScheduler</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Contains a list of schedulers expected to be called sequentially during the optimization process.</span>

<span class="sd">    Specifically, the schedulers will be called according to the milestone points, which should provide exact</span>
<span class="sd">    intervals by which each scheduler should be called at a given epoch.</span>

<span class="sd">    Args:</span>
<span class="sd">        optimizer (Optimizer): Wrapped optimizer.</span>
<span class="sd">        schedulers (list): List of chained schedulers.</span>
<span class="sd">        milestones (list): List of integers that reflects milestone points.</span>
<span class="sd">        last_epoch (int): The index of last epoch. Default: -1.</span>
<span class="sd">        verbose (bool | str): Does nothing.</span>

<span class="sd">            .. deprecated:: 2.2</span>
<span class="sd">                ``verbose`` is deprecated. Please use ``get_last_lr()`` to access the</span>
<span class="sd">                learning rate.</span>

<span class="sd">    Example:</span>
<span class="sd">        &gt;&gt;&gt; # xdoctest: +SKIP</span>
<span class="sd">        &gt;&gt;&gt; # Assuming optimizer uses lr = 1. for all groups</span>
<span class="sd">        &gt;&gt;&gt; # lr = 0.1     if epoch == 0</span>
<span class="sd">        &gt;&gt;&gt; # lr = 0.1     if epoch == 1</span>
<span class="sd">        &gt;&gt;&gt; # lr = 0.9     if epoch == 2</span>
<span class="sd">        &gt;&gt;&gt; # lr = 0.81    if epoch == 3</span>
<span class="sd">        &gt;&gt;&gt; # lr = 0.729   if epoch == 4</span>
<span class="sd">        &gt;&gt;&gt; scheduler1 = ConstantLR(optimizer, factor=0.1, total_iters=2)</span>
<span class="sd">        &gt;&gt;&gt; scheduler2 = ExponentialLR(optimizer, gamma=0.9)</span>
<span class="sd">        &gt;&gt;&gt; scheduler = SequentialLR(optimizer, schedulers=[scheduler1, scheduler2], milestones=[2])</span>
<span class="sd">        &gt;&gt;&gt; for epoch in range(100):</span>
<span class="sd">        &gt;&gt;&gt;     train(...)</span>
<span class="sd">        &gt;&gt;&gt;     validate(...)</span>
<span class="sd">        &gt;&gt;&gt;     scheduler.step()</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">optimizer</span><span class="p">:</span> <span class="n">Optimizer</span><span class="p">,</span>
        <span class="n">schedulers</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">LRScheduler</span><span class="p">],</span>
        <span class="n">milestones</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
        <span class="n">last_epoch</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">verbose</span><span class="o">=</span><span class="s2">&quot;deprecated&quot;</span><span class="p">,</span>
    <span class="p">):</span>  <span class="c1"># noqa: D107</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">schedulers</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2"> expects at least one scheduler, but got no scheduler.&quot;</span>
            <span class="p">)</span>

        <span class="k">for</span> <span class="n">scheduler_idx</span><span class="p">,</span> <span class="n">scheduler</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">schedulers</span><span class="p">):</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">scheduler</span><span class="p">,</span> <span class="s2">&quot;optimizer&quot;</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2"> at index </span><span class="si">{</span><span class="n">scheduler_idx</span><span class="si">}</span><span class="s2"> should have `optimizer` as its attribute.&quot;</span>
                <span class="p">)</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">scheduler</span><span class="p">,</span> <span class="n">ReduceLROnPlateau</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2"> does not support `ReduceLROnPlateau` scheduler as it &quot;</span>
                    <span class="s2">&quot;requires additional kwargs to be specified when calling `step`, &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;but got one at index </span><span class="si">{</span><span class="n">scheduler_idx</span><span class="si">}</span><span class="s2"> in the given schedulers sequence.&quot;</span>
                <span class="p">)</span>
            <span class="k">if</span> <span class="n">optimizer</span> <span class="o">!=</span> <span class="n">scheduler</span><span class="o">.</span><span class="n">optimizer</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2"> expects all schedulers to belong to the same optimizer, but &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;got scheduler </span><span class="si">{</span><span class="n">scheduler</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2"> at index </span><span class="si">{</span><span class="n">scheduler_idx</span><span class="si">}</span><span class="s2"> has </span><span class="si">{</span><span class="n">scheduler</span><span class="o">.</span><span class="n">optimizer</span><span class="si">}</span><span class="s2">, &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;which is different from </span><span class="si">{</span><span class="n">optimizer</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">.&quot;</span>
                <span class="p">)</span>

        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">milestones</span><span class="p">)</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="n">schedulers</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Sequential Schedulers expects number of schedulers provided to be one more &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;than the number of milestone points, but got number of schedulers </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">schedulers</span><span class="p">)</span><span class="si">}</span><span class="s2"> and the &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;number of milestones to be equal to </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">milestones</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span>
        <span class="n">_check_verbose_deprecated_warning</span><span class="p">(</span><span class="n">verbose</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_schedulers</span> <span class="o">=</span> <span class="n">schedulers</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_milestones</span> <span class="o">=</span> <span class="n">milestones</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">last_epoch</span> <span class="o">=</span> <span class="n">last_epoch</span> <span class="o">+</span> <span class="mi">1</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">optimizer</span>

        <span class="c1"># Reset learning rates back to initial values</span>
        <span class="k">for</span> <span class="n">group</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">:</span>
            <span class="n">group</span><span class="p">[</span><span class="s2">&quot;lr&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">group</span><span class="p">[</span><span class="s2">&quot;initial_lr&quot;</span><span class="p">]</span>

        <span class="c1"># &quot;Undo&quot; the step performed by other schedulers</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">recursive_undo</span><span class="p">()</span>

        <span class="c1"># Perform the initial step for only the first scheduler</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_schedulers</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">_initial_step</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_last_lr</span> <span class="o">=</span> <span class="n">schedulers</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">get_last_lr</span><span class="p">()</span>

<div class="viewcode-block" id="SequentialLR.recursive_undo">
<a class="viewcode-back" href="../../../python-api/generated/torch.optim.lr_scheduler.SequentialLR.html#torch.optim.lr_scheduler.SequentialLR.recursive_undo">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">recursive_undo</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sched</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Recursively undo any step performed by the initialisation of</span>
<span class="sd">        schedulers.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">scheds</span> <span class="o">=</span> <span class="bp">self</span> <span class="k">if</span> <span class="n">sched</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">sched</span>

        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">scheds</span><span class="p">,</span> <span class="s2">&quot;_schedulers&quot;</span><span class="p">):</span>
            <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">scheds</span><span class="o">.</span><span class="n">_schedulers</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">recursive_undo</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
        <span class="k">elif</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">scheds</span><span class="p">,</span> <span class="s2">&quot;last_epoch&quot;</span><span class="p">):</span>
            <span class="n">scheds</span><span class="o">.</span><span class="n">last_epoch</span> <span class="o">-=</span> <span class="mi">1</span></div>


<div class="viewcode-block" id="SequentialLR.step">
<a class="viewcode-back" href="../../../python-api/generated/torch.optim.lr_scheduler.SequentialLR.html#torch.optim.lr_scheduler.SequentialLR.step">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">step</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>  <span class="c1"># type: ignore[override]</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Perform a step.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">last_epoch</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="n">idx</span> <span class="o">=</span> <span class="n">bisect_right</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_milestones</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">last_epoch</span><span class="p">)</span>
        <span class="n">scheduler</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_schedulers</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">idx</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">_milestones</span><span class="p">[</span><span class="n">idx</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">last_epoch</span><span class="p">:</span>
            <span class="n">scheduler</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">scheduler</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_last_lr</span> <span class="o">=</span> <span class="n">scheduler</span><span class="o">.</span><span class="n">get_last_lr</span><span class="p">()</span></div>


<div class="viewcode-block" id="SequentialLR.state_dict">
<a class="viewcode-back" href="../../../python-api/generated/torch.optim.lr_scheduler.SequentialLR.html#torch.optim.lr_scheduler.SequentialLR.state_dict">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">state_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Return the state of the scheduler as a :class:`dict`.</span>

<span class="sd">        It contains an entry for every variable in self.__dict__ which</span>
<span class="sd">        is not the optimizer.</span>
<span class="sd">        The wrapped scheduler states will also be saved.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">state_dict</span> <span class="o">=</span> <span class="p">{</span>
            <span class="n">key</span><span class="p">:</span> <span class="n">value</span>
            <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="vm">__dict__</span><span class="o">.</span><span class="n">items</span><span class="p">()</span>
            <span class="k">if</span> <span class="n">key</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">(</span><span class="s2">&quot;optimizer&quot;</span><span class="p">,</span> <span class="s2">&quot;_schedulers&quot;</span><span class="p">)</span>
        <span class="p">}</span>
        <span class="n">state_dict</span><span class="p">[</span><span class="s2">&quot;_schedulers&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="kc">None</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_schedulers</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">s</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_schedulers</span><span class="p">):</span>
            <span class="n">state_dict</span><span class="p">[</span><span class="s2">&quot;_schedulers&quot;</span><span class="p">][</span><span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">s</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()</span>

        <span class="k">return</span> <span class="n">state_dict</span></div>


<div class="viewcode-block" id="SequentialLR.load_state_dict">
<a class="viewcode-back" href="../../../python-api/generated/torch.optim.lr_scheduler.SequentialLR.html#torch.optim.lr_scheduler.SequentialLR.load_state_dict">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">load_state_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state_dict</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Load the scheduler&#39;s state.</span>

<span class="sd">        Args:</span>
<span class="sd">            state_dict (dict): scheduler state. Should be an object returned</span>
<span class="sd">                from a call to :meth:`state_dict`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">_schedulers</span> <span class="o">=</span> <span class="n">state_dict</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;_schedulers&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="vm">__dict__</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">state_dict</span><span class="p">)</span>
        <span class="c1"># Restore state_dict keys in order to prevent side effects</span>
        <span class="c1"># https://github.com/pytorch/pytorch/issues/32756</span>
        <span class="n">state_dict</span><span class="p">[</span><span class="s2">&quot;_schedulers&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">_schedulers</span>

        <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">s</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">_schedulers</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_schedulers</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">s</span><span class="p">)</span></div>
</div>



<div class="viewcode-block" id="PolynomialLR">
<a class="viewcode-back" href="../../../python-api/generated/torch.optim.lr_scheduler.PolynomialLR.html#torch.optim.lr_scheduler.PolynomialLR">[docs]</a>
<span class="k">class</span><span class="w"> </span><span class="nc">PolynomialLR</span><span class="p">(</span><span class="n">LRScheduler</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Decays the learning rate of each parameter group using a polynomial function in the given total_iters.</span>

<span class="sd">    When last_epoch=-1, sets initial lr as lr.</span>

<span class="sd">    Args:</span>
<span class="sd">        optimizer (Optimizer): Wrapped optimizer.</span>
<span class="sd">        total_iters (int): The number of steps that the scheduler decays the learning rate. Default: 5.</span>
<span class="sd">        power (float): The power of the polynomial. Default: 1.0.</span>
<span class="sd">        verbose (bool | str): If ``True``, prints a message to stdout for</span>
<span class="sd">            each update. Default: ``False``.</span>

<span class="sd">            .. deprecated:: 2.2</span>
<span class="sd">                ``verbose`` is deprecated. Please use ``get_last_lr()`` to access the</span>
<span class="sd">                learning rate.</span>

<span class="sd">    Example:</span>
<span class="sd">        &gt;&gt;&gt; # xdoctest: +SKIP(&quot;undefined vars&quot;)</span>
<span class="sd">        &gt;&gt;&gt; # Assuming optimizer uses lr = 0.001 for all groups</span>
<span class="sd">        &gt;&gt;&gt; # lr = 0.001     if epoch == 0</span>
<span class="sd">        &gt;&gt;&gt; # lr = 0.00075   if epoch == 1</span>
<span class="sd">        &gt;&gt;&gt; # lr = 0.00050   if epoch == 2</span>
<span class="sd">        &gt;&gt;&gt; # lr = 0.00025   if epoch == 3</span>
<span class="sd">        &gt;&gt;&gt; # lr = 0.0       if epoch &gt;= 4</span>
<span class="sd">        &gt;&gt;&gt; scheduler = PolynomialLR(optimizer, total_iters=4, power=1.0)</span>
<span class="sd">        &gt;&gt;&gt; for epoch in range(100):</span>
<span class="sd">        &gt;&gt;&gt;     train(...)</span>
<span class="sd">        &gt;&gt;&gt;     validate(...)</span>
<span class="sd">        &gt;&gt;&gt;     scheduler.step()</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">optimizer</span><span class="p">:</span> <span class="n">Optimizer</span><span class="p">,</span>
        <span class="n">total_iters</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">5</span><span class="p">,</span>
        <span class="n">power</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span>
        <span class="n">last_epoch</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">verbose</span><span class="o">=</span><span class="s2">&quot;deprecated&quot;</span><span class="p">,</span>
    <span class="p">):</span>  <span class="c1"># noqa: D107</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">total_iters</span> <span class="o">=</span> <span class="n">total_iters</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">power</span> <span class="o">=</span> <span class="n">power</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">last_epoch</span><span class="p">,</span> <span class="n">verbose</span><span class="p">)</span>

<div class="viewcode-block" id="PolynomialLR.get_lr">
<a class="viewcode-back" href="../../../python-api/generated/torch.optim.lr_scheduler.PolynomialLR.html#torch.optim.lr_scheduler.PolynomialLR.get_lr">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">get_lr</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Compute the learning rate.&quot;&quot;&quot;</span>
        <span class="n">_warn_get_lr_called_within_step</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">last_epoch</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">last_epoch</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">total_iters</span><span class="p">:</span>
            <span class="k">return</span> <span class="p">[</span><span class="n">group</span><span class="p">[</span><span class="s2">&quot;lr&quot;</span><span class="p">]</span> <span class="k">for</span> <span class="n">group</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">]</span>

        <span class="n">decay_factor</span> <span class="o">=</span> <span class="p">(</span>
            <span class="p">(</span><span class="mf">1.0</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">last_epoch</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">total_iters</span><span class="p">)</span>
            <span class="o">/</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">-</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">last_epoch</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">total_iters</span><span class="p">)</span>
        <span class="p">)</span> <span class="o">**</span> <span class="bp">self</span><span class="o">.</span><span class="n">power</span>
        <span class="k">return</span> <span class="p">[</span><span class="n">group</span><span class="p">[</span><span class="s2">&quot;lr&quot;</span><span class="p">]</span> <span class="o">*</span> <span class="n">decay_factor</span> <span class="k">for</span> <span class="n">group</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">]</span></div>


    <span class="k">def</span><span class="w"> </span><span class="nf">_get_closed_form_lr</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">[</span>
            <span class="p">(</span>
                <span class="n">base_lr</span>
                <span class="o">*</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">-</span> <span class="nb">min</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">total_iters</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">last_epoch</span><span class="p">)</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">total_iters</span><span class="p">)</span>
                <span class="o">**</span> <span class="bp">self</span><span class="o">.</span><span class="n">power</span>
            <span class="p">)</span>
            <span class="k">for</span> <span class="n">base_lr</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">base_lrs</span>
        <span class="p">]</span></div>



<div class="viewcode-block" id="CosineAnnealingLR">
<a class="viewcode-back" href="../../../python-api/generated/torch.optim.lr_scheduler.CosineAnnealingLR.html#torch.optim.lr_scheduler.CosineAnnealingLR">[docs]</a>
<span class="k">class</span><span class="w"> </span><span class="nc">CosineAnnealingLR</span><span class="p">(</span><span class="n">LRScheduler</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Set the learning rate of each parameter group using a cosine annealing schedule.</span>

<span class="sd">    The :math:`\eta_{max}` is set to the initial lr and</span>
<span class="sd">    :math:`T_{cur}` is the number of epochs since the last restart in SGDR:</span>

<span class="sd">    .. math::</span>
<span class="sd">        \begin{aligned}</span>
<span class="sd">            \eta_t &amp; = \eta_{min} + \frac{1}{2}(\eta_{max} - \eta_{min})\left(1</span>
<span class="sd">            + \cos\left(\frac{T_{cur}}{T_{max}}\pi\right)\right),</span>
<span class="sd">            &amp; T_{cur} \neq (2k+1)T_{max}; \\</span>
<span class="sd">            \eta_{t+1} &amp; = \eta_{t} + \frac{1}{2}(\eta_{max} - \eta_{min})</span>
<span class="sd">            \left(1 - \cos\left(\frac{1}{T_{max}}\pi\right)\right),</span>
<span class="sd">            &amp; T_{cur} = (2k+1)T_{max}.</span>
<span class="sd">        \end{aligned}</span>

<span class="sd">    When last_epoch=-1, sets initial lr as lr. Notice that because the schedule</span>
<span class="sd">    is defined recursively, the learning rate can be simultaneously modified</span>
<span class="sd">    outside this scheduler by other operators. If the learning rate is set</span>
<span class="sd">    solely by this scheduler, the learning rate at each step becomes:</span>

<span class="sd">    .. math::</span>
<span class="sd">        \eta_t = \eta_{min} + \frac{1}{2}(\eta_{max} - \eta_{min})\left(1 +</span>
<span class="sd">        \cos\left(\frac{T_{cur}}{T_{max}}\pi\right)\right)</span>

<span class="sd">    It has been proposed in</span>
<span class="sd">    `SGDR: Stochastic Gradient Descent with Warm Restarts`_. Note that this only</span>
<span class="sd">    implements the cosine annealing part of SGDR, and not the restarts.</span>

<span class="sd">    Args:</span>
<span class="sd">        optimizer (Optimizer): Wrapped optimizer.</span>
<span class="sd">        T_max (int): Maximum number of iterations.</span>
<span class="sd">        eta_min (float): Minimum learning rate. Default: 0.</span>
<span class="sd">        last_epoch (int): The index of last epoch. Default: -1.</span>
<span class="sd">        verbose (bool | str): If ``True``, prints a message to stdout for</span>
<span class="sd">            each update. Default: ``False``.</span>

<span class="sd">            .. deprecated:: 2.2</span>
<span class="sd">                ``verbose`` is deprecated. Please use ``get_last_lr()`` to access the</span>
<span class="sd">                learning rate.</span>

<span class="sd">    .. _SGDR\: Stochastic Gradient Descent with Warm Restarts:</span>
<span class="sd">        https://arxiv.org/abs/1608.03983</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">optimizer</span><span class="p">:</span> <span class="n">Optimizer</span><span class="p">,</span>
        <span class="n">T_max</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">eta_min</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span>
        <span class="n">last_epoch</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">verbose</span><span class="o">=</span><span class="s2">&quot;deprecated&quot;</span><span class="p">,</span>
    <span class="p">):</span>  <span class="c1"># noqa: D107</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">T_max</span> <span class="o">=</span> <span class="n">T_max</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">eta_min</span> <span class="o">=</span> <span class="n">eta_min</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">last_epoch</span><span class="p">,</span> <span class="n">verbose</span><span class="p">)</span>

<div class="viewcode-block" id="CosineAnnealingLR.get_lr">
<a class="viewcode-back" href="../../../python-api/generated/torch.optim.lr_scheduler.CosineAnnealingLR.html#torch.optim.lr_scheduler.CosineAnnealingLR.get_lr">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">get_lr</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Retrieve the learning rate of each parameter group.&quot;&quot;&quot;</span>
        <span class="n">_warn_get_lr_called_within_step</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">last_epoch</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">return</span> <span class="p">[</span><span class="n">group</span><span class="p">[</span><span class="s2">&quot;lr&quot;</span><span class="p">]</span> <span class="k">for</span> <span class="n">group</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">]</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">_step_count</span> <span class="o">==</span> <span class="mi">1</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">last_epoch</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">return</span> <span class="p">[</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">eta_min</span>
                <span class="o">+</span> <span class="p">(</span><span class="n">base_lr</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">eta_min</span><span class="p">)</span>
                <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">math</span><span class="o">.</span><span class="n">cos</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">last_epoch</span><span class="p">)</span> <span class="o">*</span> <span class="n">math</span><span class="o">.</span><span class="n">pi</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">T_max</span><span class="p">))</span>
                <span class="o">/</span> <span class="mi">2</span>
                <span class="k">for</span> <span class="n">base_lr</span><span class="p">,</span> <span class="n">group</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">base_lrs</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">)</span>
            <span class="p">]</span>
        <span class="k">elif</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">last_epoch</span> <span class="o">-</span> <span class="mi">1</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">T_max</span><span class="p">)</span> <span class="o">%</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">T_max</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">return</span> <span class="p">[</span>
                <span class="n">group</span><span class="p">[</span><span class="s2">&quot;lr&quot;</span><span class="p">]</span>
                <span class="o">+</span> <span class="p">(</span><span class="n">base_lr</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">eta_min</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">math</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">math</span><span class="o">.</span><span class="n">pi</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">T_max</span><span class="p">))</span> <span class="o">/</span> <span class="mi">2</span>
                <span class="k">for</span> <span class="n">base_lr</span><span class="p">,</span> <span class="n">group</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">base_lrs</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">)</span>
            <span class="p">]</span>
        <span class="k">return</span> <span class="p">[</span>
            <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">math</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">math</span><span class="o">.</span><span class="n">pi</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">last_epoch</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">T_max</span><span class="p">))</span>
            <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">math</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">math</span><span class="o">.</span><span class="n">pi</span> <span class="o">*</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">last_epoch</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">T_max</span><span class="p">))</span>
            <span class="o">*</span> <span class="p">(</span><span class="n">group</span><span class="p">[</span><span class="s2">&quot;lr&quot;</span><span class="p">]</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">eta_min</span><span class="p">)</span>
            <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">eta_min</span>
            <span class="k">for</span> <span class="n">group</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span>
        <span class="p">]</span></div>


    <span class="k">def</span><span class="w"> </span><span class="nf">_get_closed_form_lr</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">[</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">eta_min</span>
            <span class="o">+</span> <span class="p">(</span><span class="n">base_lr</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">eta_min</span><span class="p">)</span>
            <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">math</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">math</span><span class="o">.</span><span class="n">pi</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">last_epoch</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">T_max</span><span class="p">))</span>
            <span class="o">/</span> <span class="mi">2</span>
            <span class="k">for</span> <span class="n">base_lr</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">base_lrs</span>
        <span class="p">]</span></div>



<div class="viewcode-block" id="ChainedScheduler">
<a class="viewcode-back" href="../../../python-api/generated/torch.optim.lr_scheduler.ChainedScheduler.html#torch.optim.lr_scheduler.ChainedScheduler">[docs]</a>
<span class="k">class</span><span class="w"> </span><span class="nc">ChainedScheduler</span><span class="p">(</span><span class="n">LRScheduler</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Chains a list of learning rate schedulers.</span>

<span class="sd">    Takes in a sequence of chainable learning rate schedulers and calls their</span>
<span class="sd">    step() functions consecutively in just one call to step().</span>

<span class="sd">    Args:</span>
<span class="sd">        schedulers (sequence): sequence of chained schedulers.</span>
<span class="sd">        optimizer (Optimizer, optional): Wrapped optimizer. Default: None.</span>

<span class="sd">    Example:</span>
<span class="sd">        &gt;&gt;&gt; # xdoctest: +SKIP</span>
<span class="sd">        &gt;&gt;&gt; # Assuming optimizer uses lr = 1. for all groups</span>
<span class="sd">        &gt;&gt;&gt; # lr = 0.09     if epoch == 0</span>
<span class="sd">        &gt;&gt;&gt; # lr = 0.081    if epoch == 1</span>
<span class="sd">        &gt;&gt;&gt; # lr = 0.729    if epoch == 2</span>
<span class="sd">        &gt;&gt;&gt; # lr = 0.6561   if epoch == 3</span>
<span class="sd">        &gt;&gt;&gt; # lr = 0.59049  if epoch &gt;= 4</span>
<span class="sd">        &gt;&gt;&gt; scheduler1 = ConstantLR(optimizer, factor=0.1, total_iters=2)</span>
<span class="sd">        &gt;&gt;&gt; scheduler2 = ExponentialLR(optimizer, gamma=0.9)</span>
<span class="sd">        &gt;&gt;&gt; scheduler = ChainedScheduler([scheduler1, scheduler2], optimizer=optimizer)</span>
<span class="sd">        &gt;&gt;&gt; for epoch in range(100):</span>
<span class="sd">        &gt;&gt;&gt;     train(...)</span>
<span class="sd">        &gt;&gt;&gt;     validate(...)</span>
<span class="sd">        &gt;&gt;&gt;     scheduler.step()</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">schedulers</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">LRScheduler</span><span class="p">],</span> <span class="n">optimizer</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Optimizer</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="p">):</span>  <span class="c1"># noqa: D107</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">schedulers</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2"> expects at least one scheduler to be chained, but got no scheduler.&quot;</span>
            <span class="p">)</span>

        <span class="n">optimizer</span> <span class="o">=</span> <span class="n">optimizer</span> <span class="ow">or</span> <span class="n">schedulers</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">optimizer</span>
        <span class="k">for</span> <span class="n">scheduler_idx</span><span class="p">,</span> <span class="n">scheduler</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">schedulers</span><span class="p">):</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">scheduler</span><span class="p">,</span> <span class="s2">&quot;optimizer&quot;</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2"> at index </span><span class="si">{</span><span class="n">scheduler_idx</span><span class="si">}</span><span class="s2"> should have `optimizer` as its attribute.&quot;</span>
                <span class="p">)</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">scheduler</span><span class="p">,</span> <span class="n">ReduceLROnPlateau</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2"> does not support `ReduceLROnPlateau` scheduler as it &quot;</span>
                    <span class="s2">&quot;requires additional kwargs to be specified when calling `step`, &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;but got one at index </span><span class="si">{</span><span class="n">scheduler_idx</span><span class="si">}</span><span class="s2"> in the given schedulers sequence.&quot;</span>
                <span class="p">)</span>
            <span class="k">if</span> <span class="n">optimizer</span> <span class="o">!=</span> <span class="n">scheduler</span><span class="o">.</span><span class="n">optimizer</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2"> expects all schedulers to belong to the same optimizer, but &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;got scheduler </span><span class="si">{</span><span class="n">scheduler</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2"> at index </span><span class="si">{</span><span class="n">scheduler_idx</span><span class="si">}</span><span class="s2"> has </span><span class="si">{</span><span class="n">scheduler</span><span class="o">.</span><span class="n">optimizer</span><span class="si">}</span><span class="s2">, &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;which is different from </span><span class="si">{</span><span class="n">optimizer</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">.&quot;</span>
                <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_schedulers</span> <span class="o">=</span> <span class="n">schedulers</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">optimizer</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_last_lr</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">group</span><span class="p">[</span><span class="s2">&quot;lr&quot;</span><span class="p">]</span> <span class="k">for</span> <span class="n">group</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_schedulers</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span>
        <span class="p">]</span>

<div class="viewcode-block" id="ChainedScheduler.step">
<a class="viewcode-back" href="../../../python-api/generated/torch.optim.lr_scheduler.ChainedScheduler.html#torch.optim.lr_scheduler.ChainedScheduler.step">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">step</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>  <span class="c1"># type: ignore[override]</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Perform a step.&quot;&quot;&quot;</span>
        <span class="k">for</span> <span class="n">scheduler</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_schedulers</span><span class="p">:</span>
            <span class="n">scheduler</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_last_lr</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">group</span><span class="p">[</span><span class="s2">&quot;lr&quot;</span><span class="p">]</span> <span class="k">for</span> <span class="n">group</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_schedulers</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span>
        <span class="p">]</span></div>


<div class="viewcode-block" id="ChainedScheduler.state_dict">
<a class="viewcode-back" href="../../../python-api/generated/torch.optim.lr_scheduler.ChainedScheduler.html#torch.optim.lr_scheduler.ChainedScheduler.state_dict">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">state_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Return the state of the scheduler as a :class:`dict`.</span>

<span class="sd">        It contains an entry for every variable in self.__dict__ which</span>
<span class="sd">        is not the optimizer.</span>
<span class="sd">        The wrapped scheduler states will also be saved.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">state_dict</span> <span class="o">=</span> <span class="p">{</span>
            <span class="n">key</span><span class="p">:</span> <span class="n">value</span>
            <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="vm">__dict__</span><span class="o">.</span><span class="n">items</span><span class="p">()</span>
            <span class="k">if</span> <span class="n">key</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">(</span><span class="s2">&quot;optimizer&quot;</span><span class="p">,</span> <span class="s2">&quot;_schedulers&quot;</span><span class="p">)</span>
        <span class="p">}</span>
        <span class="n">state_dict</span><span class="p">[</span><span class="s2">&quot;_schedulers&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="kc">None</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_schedulers</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">s</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_schedulers</span><span class="p">):</span>
            <span class="n">state_dict</span><span class="p">[</span><span class="s2">&quot;_schedulers&quot;</span><span class="p">][</span><span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">s</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()</span>

        <span class="k">return</span> <span class="n">state_dict</span></div>


<div class="viewcode-block" id="ChainedScheduler.load_state_dict">
<a class="viewcode-back" href="../../../python-api/generated/torch.optim.lr_scheduler.ChainedScheduler.html#torch.optim.lr_scheduler.ChainedScheduler.load_state_dict">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">load_state_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state_dict</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Load the scheduler&#39;s state.</span>

<span class="sd">        Args:</span>
<span class="sd">            state_dict (dict): scheduler state. Should be an object returned</span>
<span class="sd">                from a call to :meth:`state_dict`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">_schedulers</span> <span class="o">=</span> <span class="n">state_dict</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;_schedulers&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="vm">__dict__</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">state_dict</span><span class="p">)</span>
        <span class="c1"># Restore state_dict keys in order to prevent side effects</span>
        <span class="c1"># https://github.com/pytorch/pytorch/issues/32756</span>
        <span class="n">state_dict</span><span class="p">[</span><span class="s2">&quot;_schedulers&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">_schedulers</span>

        <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">s</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">_schedulers</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_schedulers</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">s</span><span class="p">)</span></div>
</div>



<div class="viewcode-block" id="ReduceLROnPlateau">
<a class="viewcode-back" href="../../../python-api/generated/torch.optim.lr_scheduler.ReduceLROnPlateau.html#torch.optim.lr_scheduler.ReduceLROnPlateau">[docs]</a>
<span class="k">class</span><span class="w"> </span><span class="nc">ReduceLROnPlateau</span><span class="p">(</span><span class="n">LRScheduler</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Reduce learning rate when a metric has stopped improving.</span>

<span class="sd">    Models often benefit from reducing the learning rate by a factor</span>
<span class="sd">    of 2-10 once learning stagnates. This scheduler reads a metrics</span>
<span class="sd">    quantity and if no improvement is seen for a &#39;patience&#39; number</span>
<span class="sd">    of epochs, the learning rate is reduced.</span>

<span class="sd">    Args:</span>
<span class="sd">        optimizer (Optimizer): Wrapped optimizer.</span>
<span class="sd">        mode (str): One of `min`, `max`. In `min` mode, lr will</span>
<span class="sd">            be reduced when the quantity monitored has stopped</span>
<span class="sd">            decreasing; in `max` mode it will be reduced when the</span>
<span class="sd">            quantity monitored has stopped increasing. Default: &#39;min&#39;.</span>
<span class="sd">        factor (float): Factor by which the learning rate will be</span>
<span class="sd">            reduced. new_lr = lr * factor. Default: 0.1.</span>
<span class="sd">        patience (int): The number of allowed epochs with no improvement after</span>
<span class="sd">            which the learning rate will be reduced.</span>
<span class="sd">            For example, consider the case of having no patience (`patience = 0`).</span>
<span class="sd">            In the first epoch, a baseline is established and is always considered good as there&#39;s no previous baseline.</span>
<span class="sd">            In the second epoch, if the performance is worse than the baseline,</span>
<span class="sd">            we have what is considered an intolerable epoch.</span>
<span class="sd">            Since the count of intolerable epochs (1) is greater than the patience level (0),</span>
<span class="sd">            the learning rate is reduced at the end of this epoch.</span>
<span class="sd">            From the third epoch onwards, the learning rate continues to be reduced at the end of each epoch</span>
<span class="sd">            if the performance is worse than the baseline. If the performance improves or remains the same,</span>
<span class="sd">            the learning rate is not adjusted.</span>
<span class="sd">            Default: 10.</span>
<span class="sd">        threshold (float): Threshold for measuring the new optimum,</span>
<span class="sd">            to only focus on significant changes. Default: 1e-4.</span>
<span class="sd">        threshold_mode (str): One of `rel`, `abs`. In `rel` mode,</span>
<span class="sd">            dynamic_threshold = best * ( 1 + threshold ) in &#39;max&#39;</span>
<span class="sd">            mode or best * ( 1 - threshold ) in `min` mode.</span>
<span class="sd">            In `abs` mode, dynamic_threshold = best + threshold in</span>
<span class="sd">            `max` mode or best - threshold in `min` mode. Default: &#39;rel&#39;.</span>
<span class="sd">        cooldown (int): Number of epochs to wait before resuming</span>
<span class="sd">            normal operation after lr has been reduced. Default: 0.</span>
<span class="sd">        min_lr (float or list): A scalar or a list of scalars. A</span>
<span class="sd">            lower bound on the learning rate of all param groups</span>
<span class="sd">            or each group respectively. Default: 0.</span>
<span class="sd">        eps (float): Minimal decay applied to lr. If the difference</span>
<span class="sd">            between new and old lr is smaller than eps, the update is</span>
<span class="sd">            ignored. Default: 1e-8.</span>
<span class="sd">        verbose (bool | str): If ``True``, prints a message to stdout for</span>
<span class="sd">            each update. Default: ``False``.</span>

<span class="sd">            .. deprecated:: 2.2</span>
<span class="sd">                ``verbose`` is deprecated. Please use ``get_last_lr()`` to access the</span>
<span class="sd">                learning rate.</span>

<span class="sd">    Example:</span>
<span class="sd">        &gt;&gt;&gt; # xdoctest: +SKIP</span>
<span class="sd">        &gt;&gt;&gt; optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)</span>
<span class="sd">        &gt;&gt;&gt; scheduler = ReduceLROnPlateau(optimizer, &#39;min&#39;)</span>
<span class="sd">        &gt;&gt;&gt; for epoch in range(10):</span>
<span class="sd">        &gt;&gt;&gt;     train(...)</span>
<span class="sd">        &gt;&gt;&gt;     val_loss = validate(...)</span>
<span class="sd">        &gt;&gt;&gt;     # Note that step should be called after validate()</span>
<span class="sd">        &gt;&gt;&gt;     scheduler.step(val_loss)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">optimizer</span><span class="p">:</span> <span class="n">Optimizer</span><span class="p">,</span>
        <span class="n">mode</span><span class="p">:</span> <span class="n">Literal</span><span class="p">[</span><span class="s2">&quot;min&quot;</span><span class="p">,</span> <span class="s2">&quot;max&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;min&quot;</span><span class="p">,</span>
        <span class="n">factor</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
        <span class="n">patience</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span>
        <span class="n">threshold</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-4</span><span class="p">,</span>
        <span class="n">threshold_mode</span><span class="p">:</span> <span class="n">Literal</span><span class="p">[</span><span class="s2">&quot;rel&quot;</span><span class="p">,</span> <span class="s2">&quot;abs&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;rel&quot;</span><span class="p">,</span>
        <span class="n">cooldown</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
        <span class="n">min_lr</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
        <span class="n">eps</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-8</span><span class="p">,</span>
        <span class="n">verbose</span><span class="o">=</span><span class="s2">&quot;deprecated&quot;</span><span class="p">,</span>
    <span class="p">):</span>  <span class="c1"># noqa: D107</span>
        <span class="k">if</span> <span class="n">factor</span> <span class="o">&gt;=</span> <span class="mf">1.0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Factor should be &lt; 1.0.&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">factor</span> <span class="o">=</span> <span class="n">factor</span>

        <span class="c1"># Attach optimizer</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">Optimizer</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">optimizer</span><span class="p">)</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2"> is not an Optimizer&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">optimizer</span>

        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">min_lr</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)):</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">min_lr</span><span class="p">)</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;expected </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">)</span><span class="si">}</span><span class="s2"> min_lrs, got </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">min_lr</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span>
                <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">default_min_lr</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">min_lrs</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">min_lr</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">default_min_lr</span> <span class="o">=</span> <span class="n">min_lr</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">min_lrs</span> <span class="o">=</span> <span class="p">[</span><span class="n">min_lr</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">patience</span> <span class="o">=</span> <span class="n">patience</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span> <span class="o">=</span> <span class="n">_check_verbose_deprecated_warning</span><span class="p">(</span><span class="n">verbose</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cooldown</span> <span class="o">=</span> <span class="n">cooldown</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cooldown_counter</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mode</span> <span class="o">=</span> <span class="n">mode</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">threshold</span> <span class="o">=</span> <span class="n">threshold</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">threshold_mode</span> <span class="o">=</span> <span class="n">threshold_mode</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">best</span><span class="p">:</span> <span class="nb">float</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_bad_epochs</span><span class="p">:</span> <span class="nb">int</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mode_worse</span><span class="p">:</span> <span class="nb">float</span>  <span class="c1"># the worse value for the chosen mode</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">eps</span> <span class="o">=</span> <span class="n">eps</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">last_epoch</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_last_lr</span> <span class="o">=</span> <span class="p">[</span><span class="n">group</span><span class="p">[</span><span class="s2">&quot;lr&quot;</span><span class="p">]</span> <span class="k">for</span> <span class="n">group</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_init_is_better</span><span class="p">(</span>
            <span class="n">mode</span><span class="o">=</span><span class="n">mode</span><span class="p">,</span> <span class="n">threshold</span><span class="o">=</span><span class="n">threshold</span><span class="p">,</span> <span class="n">threshold_mode</span><span class="o">=</span><span class="n">threshold_mode</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_reset</span><span class="p">()</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_reset</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Reset num_bad_epochs counter and cooldown counter.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">best</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">mode_worse</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cooldown_counter</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_bad_epochs</span> <span class="o">=</span> <span class="mi">0</span>

<div class="viewcode-block" id="ReduceLROnPlateau.step">
<a class="viewcode-back" href="../../../python-api/generated/torch.optim.lr_scheduler.ReduceLROnPlateau.html#torch.optim.lr_scheduler.ReduceLROnPlateau.step">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">metrics</span><span class="p">:</span> <span class="n">SupportsFloat</span><span class="p">,</span> <span class="n">epoch</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>  <span class="c1"># type: ignore[override]</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Perform a step.&quot;&quot;&quot;</span>
        <span class="c1"># convert `metrics` to float, in case it&#39;s a zero-dim Tensor</span>
        <span class="n">current</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">metrics</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">epoch</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">epoch</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">last_epoch</span> <span class="o">+</span> <span class="mi">1</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="n">EPOCH_DEPRECATION_WARNING</span><span class="p">,</span> <span class="ne">UserWarning</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">last_epoch</span> <span class="o">=</span> <span class="n">epoch</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_better</span><span class="p">(</span><span class="n">current</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">best</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">best</span> <span class="o">=</span> <span class="n">current</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">num_bad_epochs</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">num_bad_epochs</span> <span class="o">+=</span> <span class="mi">1</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">in_cooldown</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">cooldown_counter</span> <span class="o">-=</span> <span class="mi">1</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">num_bad_epochs</span> <span class="o">=</span> <span class="mi">0</span>  <span class="c1"># ignore any bad epochs in cooldown</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_bad_epochs</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">patience</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_reduce_lr</span><span class="p">(</span><span class="n">epoch</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">cooldown_counter</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cooldown</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">num_bad_epochs</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_last_lr</span> <span class="o">=</span> <span class="p">[</span><span class="n">group</span><span class="p">[</span><span class="s2">&quot;lr&quot;</span><span class="p">]</span> <span class="k">for</span> <span class="n">group</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">]</span></div>


    <span class="k">def</span><span class="w"> </span><span class="nf">_reduce_lr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">epoch</span><span class="p">):</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">)</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">min_lrs</span><span class="p">):</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">default_min_lr</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                    <span class="s2">&quot;The number of param groups in the `optimizer` &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;(</span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">)</span><span class="si">}</span><span class="s2">) differs &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;from when `ReduceLROnPlateau` was initialized &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;(</span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">min_lrs</span><span class="p">)</span><span class="si">}</span><span class="s2">), usually due to a new &quot;</span>
                    <span class="s2">&quot;param group being added to the optimizer. Please &quot;</span>
                    <span class="s2">&quot;modify the `min_lrs` field to match the length &quot;</span>
                    <span class="s2">&quot;of the `optimizer` param groups.&quot;</span>
                <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">min_lrs</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">default_min_lr</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">param_group</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">):</span>
            <span class="n">old_lr</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">param_group</span><span class="p">[</span><span class="s2">&quot;lr&quot;</span><span class="p">])</span>
            <span class="n">new_lr</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">old_lr</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">factor</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">min_lrs</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
            <span class="k">if</span> <span class="n">old_lr</span> <span class="o">-</span> <span class="n">new_lr</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">eps</span><span class="p">:</span>
                <span class="n">param_group</span><span class="p">[</span><span class="s2">&quot;lr&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">new_lr</span>

    <span class="nd">@property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">in_cooldown</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>  <span class="c1"># noqa: D102</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">cooldown_counter</span> <span class="o">&gt;</span> <span class="mi">0</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">is_better</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">best</span><span class="p">):</span>  <span class="c1"># noqa: D102</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mode</span> <span class="o">==</span> <span class="s2">&quot;min&quot;</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">threshold_mode</span> <span class="o">==</span> <span class="s2">&quot;rel&quot;</span><span class="p">:</span>
            <span class="n">rel_epsilon</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">threshold</span>
            <span class="k">return</span> <span class="n">a</span> <span class="o">&lt;</span> <span class="n">best</span> <span class="o">*</span> <span class="n">rel_epsilon</span>

        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">mode</span> <span class="o">==</span> <span class="s2">&quot;min&quot;</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">threshold_mode</span> <span class="o">==</span> <span class="s2">&quot;abs&quot;</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">a</span> <span class="o">&lt;</span> <span class="n">best</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">threshold</span>

        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">mode</span> <span class="o">==</span> <span class="s2">&quot;max&quot;</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">threshold_mode</span> <span class="o">==</span> <span class="s2">&quot;rel&quot;</span><span class="p">:</span>
            <span class="n">rel_epsilon</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">threshold</span> <span class="o">+</span> <span class="mf">1.0</span>
            <span class="k">return</span> <span class="n">a</span> <span class="o">&gt;</span> <span class="n">best</span> <span class="o">*</span> <span class="n">rel_epsilon</span>

        <span class="k">else</span><span class="p">:</span>  <span class="c1"># mode == &#39;max&#39; and epsilon_mode == &#39;abs&#39;:</span>
            <span class="k">return</span> <span class="n">a</span> <span class="o">&gt;</span> <span class="n">best</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">threshold</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_init_is_better</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">mode</span><span class="p">,</span> <span class="n">threshold</span><span class="p">,</span> <span class="n">threshold_mode</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">mode</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">{</span><span class="s2">&quot;min&quot;</span><span class="p">,</span> <span class="s2">&quot;max&quot;</span><span class="p">}:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;mode &quot;</span> <span class="o">+</span> <span class="n">mode</span> <span class="o">+</span> <span class="s2">&quot; is unknown!&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">threshold_mode</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">{</span><span class="s2">&quot;rel&quot;</span><span class="p">,</span> <span class="s2">&quot;abs&quot;</span><span class="p">}:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;threshold mode &quot;</span> <span class="o">+</span> <span class="n">threshold_mode</span> <span class="o">+</span> <span class="s2">&quot; is unknown!&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">mode</span> <span class="o">==</span> <span class="s2">&quot;min&quot;</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">mode_worse</span> <span class="o">=</span> <span class="n">inf</span>
        <span class="k">else</span><span class="p">:</span>  <span class="c1"># mode == &#39;max&#39;:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">mode_worse</span> <span class="o">=</span> <span class="o">-</span><span class="n">inf</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">mode</span> <span class="o">=</span> <span class="n">mode</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">threshold</span> <span class="o">=</span> <span class="n">threshold</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">threshold_mode</span> <span class="o">=</span> <span class="n">threshold_mode</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">state_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>  <span class="c1"># noqa: D102</span>
        <span class="k">return</span> <span class="p">{</span>
            <span class="n">key</span><span class="p">:</span> <span class="n">value</span> <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="vm">__dict__</span><span class="o">.</span><span class="n">items</span><span class="p">()</span> <span class="k">if</span> <span class="n">key</span> <span class="o">!=</span> <span class="s2">&quot;optimizer&quot;</span>
        <span class="p">}</span>

<div class="viewcode-block" id="ReduceLROnPlateau.load_state_dict">
<a class="viewcode-back" href="../../../python-api/generated/torch.optim.lr_scheduler.ReduceLROnPlateau.html#torch.optim.lr_scheduler.ReduceLROnPlateau.load_state_dict">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">load_state_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state_dict</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Load the scheduler&#39;s state.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="vm">__dict__</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">state_dict</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_init_is_better</span><span class="p">(</span>
            <span class="n">mode</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">mode</span><span class="p">,</span> <span class="n">threshold</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">threshold</span><span class="p">,</span> <span class="n">threshold_mode</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">threshold_mode</span>
        <span class="p">)</span></div>
</div>



<div class="viewcode-block" id="CyclicLR">
<a class="viewcode-back" href="../../../python-api/generated/torch.optim.lr_scheduler.CyclicLR.html#torch.optim.lr_scheduler.CyclicLR">[docs]</a>
<span class="k">class</span><span class="w"> </span><span class="nc">CyclicLR</span><span class="p">(</span><span class="n">LRScheduler</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Sets the learning rate of each parameter group according to cyclical learning rate policy (CLR).</span>

<span class="sd">    The policy cycles the learning rate between two boundaries with a constant frequency,</span>
<span class="sd">    as detailed in the paper `Cyclical Learning Rates for Training Neural Networks`_.</span>
<span class="sd">    The distance between the two boundaries can be scaled on a per-iteration</span>
<span class="sd">    or per-cycle basis.</span>

<span class="sd">    Cyclical learning rate policy changes the learning rate after every batch.</span>
<span class="sd">    `step` should be called after a batch has been used for training.</span>

<span class="sd">    This class has three built-in policies, as put forth in the paper:</span>

<span class="sd">    * &quot;triangular&quot;: A basic triangular cycle without amplitude scaling.</span>
<span class="sd">    * &quot;triangular2&quot;: A basic triangular cycle that scales initial amplitude by half each cycle.</span>
<span class="sd">    * &quot;exp_range&quot;: A cycle that scales initial amplitude by :math:`\text{gamma}^{\text{cycle iterations}}`</span>
<span class="sd">      at each cycle iteration.</span>

<span class="sd">    This implementation was adapted from the github repo: `bckenstler/CLR`_</span>

<span class="sd">    Args:</span>
<span class="sd">        optimizer (Optimizer): Wrapped optimizer.</span>
<span class="sd">        base_lr (float or list): Initial learning rate which is the</span>
<span class="sd">            lower boundary in the cycle for each parameter group.</span>
<span class="sd">        max_lr (float or list): Upper learning rate boundaries in the cycle</span>
<span class="sd">            for each parameter group. Functionally,</span>
<span class="sd">            it defines the cycle amplitude (max_lr - base_lr).</span>
<span class="sd">            The lr at any cycle is the sum of base_lr</span>
<span class="sd">            and some scaling of the amplitude; therefore</span>
<span class="sd">            max_lr may not actually be reached depending on</span>
<span class="sd">            scaling function.</span>
<span class="sd">        step_size_up (int): Number of training iterations in the</span>
<span class="sd">            increasing half of a cycle. Default: 2000</span>
<span class="sd">        step_size_down (int): Number of training iterations in the</span>
<span class="sd">            decreasing half of a cycle. If step_size_down is None,</span>
<span class="sd">            it is set to step_size_up. Default: None</span>
<span class="sd">        mode (str): One of {triangular, triangular2, exp_range}.</span>
<span class="sd">            Values correspond to policies detailed above.</span>
<span class="sd">            If scale_fn is not None, this argument is ignored.</span>
<span class="sd">            Default: &#39;triangular&#39;</span>
<span class="sd">        gamma (float): Constant in &#39;exp_range&#39; scaling function:</span>
<span class="sd">            gamma**(cycle iterations)</span>
<span class="sd">            Default: 1.0</span>
<span class="sd">        scale_fn (function): Custom scaling policy defined by a single</span>
<span class="sd">            argument lambda function, where</span>
<span class="sd">            0 &lt;= scale_fn(x) &lt;= 1 for all x &gt;= 0.</span>
<span class="sd">            If specified, then &#39;mode&#39; is ignored.</span>
<span class="sd">            Default: None</span>
<span class="sd">        scale_mode (str): {&#39;cycle&#39;, &#39;iterations&#39;}.</span>
<span class="sd">            Defines whether scale_fn is evaluated on</span>
<span class="sd">            cycle number or cycle iterations (training</span>
<span class="sd">            iterations since start of cycle).</span>
<span class="sd">            Default: &#39;cycle&#39;</span>
<span class="sd">        cycle_momentum (bool): If ``True``, momentum is cycled inversely</span>
<span class="sd">            to learning rate between &#39;base_momentum&#39; and &#39;max_momentum&#39;.</span>
<span class="sd">            Default: True</span>
<span class="sd">        base_momentum (float or list): Lower momentum boundaries in the cycle</span>
<span class="sd">            for each parameter group. Note that momentum is cycled inversely</span>
<span class="sd">            to learning rate; at the peak of a cycle, momentum is</span>
<span class="sd">            &#39;base_momentum&#39; and learning rate is &#39;max_lr&#39;.</span>
<span class="sd">            Default: 0.8</span>
<span class="sd">        max_momentum (float or list): Upper momentum boundaries in the cycle</span>
<span class="sd">            for each parameter group. Functionally,</span>
<span class="sd">            it defines the cycle amplitude (max_momentum - base_momentum).</span>
<span class="sd">            The momentum at any cycle is the difference of max_momentum</span>
<span class="sd">            and some scaling of the amplitude; therefore</span>
<span class="sd">            base_momentum may not actually be reached depending on</span>
<span class="sd">            scaling function. Note that momentum is cycled inversely</span>
<span class="sd">            to learning rate; at the start of a cycle, momentum is &#39;max_momentum&#39;</span>
<span class="sd">            and learning rate is &#39;base_lr&#39;</span>
<span class="sd">            Default: 0.9</span>
<span class="sd">        last_epoch (int): The index of the last batch. This parameter is used when</span>
<span class="sd">            resuming a training job. Since `step()` should be invoked after each</span>
<span class="sd">            batch instead of after each epoch, this number represents the total</span>
<span class="sd">            number of *batches* computed, not the total number of epochs computed.</span>
<span class="sd">            When last_epoch=-1, the schedule is started from the beginning.</span>
<span class="sd">            Default: -1</span>
<span class="sd">        verbose (bool | str): If ``True``, prints a message to stdout for</span>
<span class="sd">            each update. Default: ``False``.</span>

<span class="sd">            .. deprecated:: 2.2</span>
<span class="sd">                ``verbose`` is deprecated. Please use ``get_last_lr()`` to access the</span>
<span class="sd">                learning rate.</span>

<span class="sd">    Example:</span>
<span class="sd">        &gt;&gt;&gt; # xdoctest: +SKIP</span>
<span class="sd">        &gt;&gt;&gt; optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)</span>
<span class="sd">        &gt;&gt;&gt; scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=0.01, max_lr=0.1)</span>
<span class="sd">        &gt;&gt;&gt; data_loader = torch.utils.data.DataLoader(...)</span>
<span class="sd">        &gt;&gt;&gt; for epoch in range(10):</span>
<span class="sd">        &gt;&gt;&gt;     for batch in data_loader:</span>
<span class="sd">        &gt;&gt;&gt;         train_batch(...)</span>
<span class="sd">        &gt;&gt;&gt;         scheduler.step()</span>


<span class="sd">    .. _Cyclical Learning Rates for Training Neural Networks: https://arxiv.org/abs/1506.01186</span>
<span class="sd">    .. _bckenstler/CLR: https://github.com/bckenstler/CLR</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">optimizer</span><span class="p">:</span> <span class="n">Optimizer</span><span class="p">,</span>
        <span class="n">base_lr</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">float</span><span class="p">]],</span>
        <span class="n">max_lr</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">float</span><span class="p">]],</span>
        <span class="n">step_size_up</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2000</span><span class="p">,</span>
        <span class="n">step_size_down</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">mode</span><span class="p">:</span> <span class="n">Literal</span><span class="p">[</span><span class="s2">&quot;triangular&quot;</span><span class="p">,</span> <span class="s2">&quot;triangular2&quot;</span><span class="p">,</span> <span class="s2">&quot;exp_range&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;triangular&quot;</span><span class="p">,</span>
        <span class="n">gamma</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span>
        <span class="n">scale_fn</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Callable</span><span class="p">[[</span><span class="nb">float</span><span class="p">],</span> <span class="nb">float</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">scale_mode</span><span class="p">:</span> <span class="n">Literal</span><span class="p">[</span><span class="s2">&quot;cycle&quot;</span><span class="p">,</span> <span class="s2">&quot;iterations&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;cycle&quot;</span><span class="p">,</span>
        <span class="n">cycle_momentum</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">base_momentum</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.8</span><span class="p">,</span>
        <span class="n">max_momentum</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.9</span><span class="p">,</span>
        <span class="n">last_epoch</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">verbose</span><span class="o">=</span><span class="s2">&quot;deprecated&quot;</span><span class="p">,</span>
    <span class="p">):</span>  <span class="c1"># noqa: D107</span>
        <span class="c1"># Attach optimizer</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">Optimizer</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">optimizer</span><span class="p">)</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2"> is not an Optimizer&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">optimizer</span>

        <span class="n">base_lrs</span> <span class="o">=</span> <span class="n">_format_param</span><span class="p">(</span><span class="s2">&quot;base_lr&quot;</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">base_lr</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">last_epoch</span> <span class="o">==</span> <span class="o">-</span><span class="mi">1</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">lr</span><span class="p">,</span> <span class="n">group</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">base_lrs</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">):</span>
                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">group</span><span class="p">[</span><span class="s2">&quot;lr&quot;</span><span class="p">],</span> <span class="n">Tensor</span><span class="p">):</span>
                    <span class="n">lr_val</span> <span class="o">=</span> <span class="n">lr</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">lr</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">)</span> <span class="k">else</span> <span class="n">lr</span>
                    <span class="n">group</span><span class="p">[</span><span class="s2">&quot;lr&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">fill_</span><span class="p">(</span><span class="n">lr_val</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">group</span><span class="p">[</span><span class="s2">&quot;lr&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">lr</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">max_lrs</span> <span class="o">=</span> <span class="n">_format_param</span><span class="p">(</span><span class="s2">&quot;max_lr&quot;</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">max_lr</span><span class="p">)</span>

        <span class="n">step_size_up</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">step_size_up</span><span class="p">)</span>
        <span class="n">step_size_down</span> <span class="o">=</span> <span class="p">(</span>
            <span class="nb">float</span><span class="p">(</span><span class="n">step_size_down</span><span class="p">)</span> <span class="k">if</span> <span class="n">step_size_down</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">step_size_up</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">total_size</span> <span class="o">=</span> <span class="n">step_size_up</span> <span class="o">+</span> <span class="n">step_size_down</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">step_ratio</span> <span class="o">=</span> <span class="n">step_size_up</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">total_size</span>

        <span class="k">if</span> <span class="n">mode</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;triangular&quot;</span><span class="p">,</span> <span class="s2">&quot;triangular2&quot;</span><span class="p">,</span> <span class="s2">&quot;exp_range&quot;</span><span class="p">]</span> <span class="ow">and</span> <span class="n">scale_fn</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;mode is invalid and scale_fn is None&quot;</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">mode</span> <span class="o">=</span> <span class="n">mode</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span> <span class="o">=</span> <span class="n">gamma</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_scale_fn_ref</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[[</span><span class="nb">float</span><span class="p">],</span> <span class="nb">float</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_scale_fn_custom</span> <span class="o">=</span> <span class="n">scale_fn</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">scale_mode</span> <span class="o">=</span> <span class="n">scale_mode</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_init_scale_fn</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">cycle_momentum</span> <span class="o">=</span> <span class="n">cycle_momentum</span>
        <span class="k">if</span> <span class="n">cycle_momentum</span><span class="p">:</span>
            <span class="k">if</span> <span class="p">(</span>
                <span class="s2">&quot;momentum&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">defaults</span>
                <span class="ow">and</span> <span class="s2">&quot;betas&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">defaults</span>
            <span class="p">):</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s2">&quot;optimizer must support momentum or beta1 with `cycle_momentum` option enabled&quot;</span>
                <span class="p">)</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">use_beta1</span> <span class="o">=</span> <span class="s2">&quot;betas&quot;</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">defaults</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">base_momentums</span> <span class="o">=</span> <span class="n">_format_param</span><span class="p">(</span>
                <span class="s2">&quot;base_momentum&quot;</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">base_momentum</span>
            <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">max_momentums</span> <span class="o">=</span> <span class="n">_format_param</span><span class="p">(</span><span class="s2">&quot;max_momentum&quot;</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">max_momentum</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">last_epoch</span> <span class="o">==</span> <span class="o">-</span><span class="mi">1</span><span class="p">:</span>
                <span class="k">for</span> <span class="n">m_momentum</span><span class="p">,</span> <span class="n">b_momentum</span><span class="p">,</span> <span class="n">group</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">max_momentums</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">base_momentums</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span>
                <span class="p">):</span>
                    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_beta1</span><span class="p">:</span>
                        <span class="n">group</span><span class="p">[</span><span class="s2">&quot;betas&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">m_momentum</span><span class="p">,</span> <span class="o">*</span><span class="n">group</span><span class="p">[</span><span class="s2">&quot;betas&quot;</span><span class="p">][</span><span class="mi">1</span><span class="p">:])</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="n">group</span><span class="p">[</span><span class="s2">&quot;momentum&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">m_momentum</span>
                    <span class="n">group</span><span class="p">[</span><span class="s2">&quot;max_momentum&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">m_momentum</span>
                    <span class="n">group</span><span class="p">[</span><span class="s2">&quot;base_momentum&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">b_momentum</span>

        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">last_epoch</span><span class="p">,</span> <span class="n">verbose</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">base_lrs</span> <span class="o">=</span> <span class="n">base_lrs</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_init_scale_fn</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_scale_fn_custom</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mode</span> <span class="o">==</span> <span class="s2">&quot;triangular&quot;</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_scale_fn_ref</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_triangular_scale_fn</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">scale_mode</span> <span class="o">=</span> <span class="s2">&quot;cycle&quot;</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">mode</span> <span class="o">==</span> <span class="s2">&quot;triangular2&quot;</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_scale_fn_ref</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_triangular2_scale_fn</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">scale_mode</span> <span class="o">=</span> <span class="s2">&quot;cycle&quot;</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">mode</span> <span class="o">==</span> <span class="s2">&quot;exp_range&quot;</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_scale_fn_ref</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_exp_range_scale_fn</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">scale_mode</span> <span class="o">=</span> <span class="s2">&quot;iterations&quot;</span>

<div class="viewcode-block" id="CyclicLR.scale_fn">
<a class="viewcode-back" href="../../../python-api/generated/torch.optim.lr_scheduler.CyclicLR.html#torch.optim.lr_scheduler.CyclicLR.scale_fn">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">scale_fn</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Get the scaling policy.&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_scale_fn_custom</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_scale_fn_custom</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_scale_fn_ref</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># static method</span></div>


    <span class="nd">@staticmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">_triangular_scale_fn</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="nb">float</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
        <span class="k">return</span> <span class="mf">1.0</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">_triangular2_scale_fn</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="nb">float</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
        <span class="k">return</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mf">2.0</span> <span class="o">**</span> <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">_exp_range_scale_fn</span><span class="p">(</span><span class="n">gamma</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="nb">float</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">gamma</span><span class="o">**</span><span class="n">x</span>

<div class="viewcode-block" id="CyclicLR.get_lr">
<a class="viewcode-back" href="../../../python-api/generated/torch.optim.lr_scheduler.CyclicLR.html#torch.optim.lr_scheduler.CyclicLR.get_lr">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">get_lr</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Calculate the learning rate at batch index.</span>

<span class="sd">        This function treats `self.last_epoch` as the last batch index.</span>

<span class="sd">        If `self.cycle_momentum` is ``True``, this function has a side effect of</span>
<span class="sd">        updating the optimizer&#39;s momentum.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">_warn_get_lr_called_within_step</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>

        <span class="n">cycle</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">floor</span><span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">last_epoch</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">total_size</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">last_epoch</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">total_size</span> <span class="o">-</span> <span class="n">cycle</span>
        <span class="k">if</span> <span class="n">x</span> <span class="o">&lt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">step_ratio</span><span class="p">:</span>
            <span class="n">scale_factor</span> <span class="o">=</span> <span class="n">x</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">step_ratio</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">scale_factor</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">step_ratio</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>

        <span class="n">lrs</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">base_lr</span><span class="p">,</span> <span class="n">max_lr</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">base_lrs</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_lrs</span><span class="p">):</span>
            <span class="n">base_height</span> <span class="o">=</span> <span class="p">(</span><span class="n">max_lr</span> <span class="o">-</span> <span class="n">base_lr</span><span class="p">)</span> <span class="o">*</span> <span class="n">scale_factor</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">scale_mode</span> <span class="o">==</span> <span class="s2">&quot;cycle&quot;</span><span class="p">:</span>
                <span class="n">lr</span> <span class="o">=</span> <span class="n">base_lr</span> <span class="o">+</span> <span class="n">base_height</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">scale_fn</span><span class="p">(</span><span class="n">cycle</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">lr</span> <span class="o">=</span> <span class="n">base_lr</span> <span class="o">+</span> <span class="n">base_height</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">scale_fn</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">last_epoch</span><span class="p">)</span>
            <span class="n">lrs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">lr</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">cycle_momentum</span><span class="p">:</span>
            <span class="n">momentums</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">for</span> <span class="n">base_momentum</span><span class="p">,</span> <span class="n">max_momentum</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">base_momentums</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_momentums</span>
            <span class="p">):</span>
                <span class="n">base_height</span> <span class="o">=</span> <span class="p">(</span><span class="n">max_momentum</span> <span class="o">-</span> <span class="n">base_momentum</span><span class="p">)</span> <span class="o">*</span> <span class="n">scale_factor</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">scale_mode</span> <span class="o">==</span> <span class="s2">&quot;cycle&quot;</span><span class="p">:</span>
                    <span class="n">momentum</span> <span class="o">=</span> <span class="n">max_momentum</span> <span class="o">-</span> <span class="n">base_height</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">scale_fn</span><span class="p">(</span><span class="n">cycle</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">momentum</span> <span class="o">=</span> <span class="n">max_momentum</span> <span class="o">-</span> <span class="n">base_height</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">scale_fn</span><span class="p">(</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">last_epoch</span>
                    <span class="p">)</span>
                <span class="n">momentums</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">momentum</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">param_group</span><span class="p">,</span> <span class="n">momentum</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">,</span> <span class="n">momentums</span><span class="p">):</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_beta1</span><span class="p">:</span>
                    <span class="n">param_group</span><span class="p">[</span><span class="s2">&quot;betas&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">momentum</span><span class="p">,</span> <span class="o">*</span><span class="n">param_group</span><span class="p">[</span><span class="s2">&quot;betas&quot;</span><span class="p">][</span><span class="mi">1</span><span class="p">:])</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">param_group</span><span class="p">[</span><span class="s2">&quot;momentum&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">momentum</span>

        <span class="k">return</span> <span class="n">lrs</span></div>


    <span class="k">def</span><span class="w"> </span><span class="nf">state_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>  <span class="c1"># noqa: D102</span>
        <span class="n">state</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()</span>
        <span class="c1"># We are dropping the `_scale_fn_ref` attribute because it is a</span>
        <span class="c1"># `weakref.WeakMethod` and can&#39;t be pickled.</span>
        <span class="n">state</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;_scale_fn_ref&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">fn</span> <span class="o">=</span> <span class="n">state</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;_scale_fn_custom&quot;</span><span class="p">)</span>
        <span class="n">state</span><span class="p">[</span><span class="s2">&quot;_scale_fn_custom&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="n">fn</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">fn</span><span class="p">,</span> <span class="n">types</span><span class="o">.</span><span class="n">FunctionType</span><span class="p">):</span>
            <span class="c1"># The _scale_fn_custom will only be saved if it is a callable object</span>
            <span class="c1"># and not if it is a function or lambda.</span>
            <span class="n">state</span><span class="p">[</span><span class="s2">&quot;_scale_fn_custom&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">fn</span><span class="o">.</span><span class="vm">__dict__</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>

        <span class="k">return</span> <span class="n">state</span>

<div class="viewcode-block" id="CyclicLR.load_state_dict">
<a class="viewcode-back" href="../../../python-api/generated/torch.optim.lr_scheduler.CyclicLR.html#torch.optim.lr_scheduler.CyclicLR.load_state_dict">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">load_state_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state_dict</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Load the scheduler&#39;s state.&quot;&quot;&quot;</span>
        <span class="n">fn</span> <span class="o">=</span> <span class="n">state_dict</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;_scale_fn_custom&quot;</span><span class="p">)</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">state_dict</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">fn</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_scale_fn_custom</span><span class="o">.</span><span class="vm">__dict__</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">fn</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_init_scale_fn</span><span class="p">()</span></div>
</div>



<div class="viewcode-block" id="CosineAnnealingWarmRestarts">
<a class="viewcode-back" href="../../../python-api/generated/torch.optim.lr_scheduler.CosineAnnealingWarmRestarts.html#torch.optim.lr_scheduler.CosineAnnealingWarmRestarts">[docs]</a>
<span class="k">class</span><span class="w"> </span><span class="nc">CosineAnnealingWarmRestarts</span><span class="p">(</span><span class="n">LRScheduler</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Set the learning rate of each parameter group using a cosine annealing schedule.</span>

<span class="sd">    The :math:`\eta_{max}` is set to the initial lr, :math:`T_{cur}`</span>
<span class="sd">    is the number of epochs since the last restart and :math:`T_{i}` is the number</span>
<span class="sd">    of epochs between two warm restarts in SGDR:</span>

<span class="sd">    .. math::</span>
<span class="sd">        \eta_t = \eta_{min} + \frac{1}{2}(\eta_{max} - \eta_{min})\left(1 +</span>
<span class="sd">        \cos\left(\frac{T_{cur}}{T_{i}}\pi\right)\right)</span>

<span class="sd">    When :math:`T_{cur}=T_{i}`, set :math:`\eta_t = \eta_{min}`.</span>
<span class="sd">    When :math:`T_{cur}=0` after restart, set :math:`\eta_t=\eta_{max}`.</span>

<span class="sd">    It has been proposed in</span>
<span class="sd">    `SGDR: Stochastic Gradient Descent with Warm Restarts`_.</span>

<span class="sd">    Args:</span>
<span class="sd">        optimizer (Optimizer): Wrapped optimizer.</span>
<span class="sd">        T_0 (int): Number of iterations until the first restart.</span>
<span class="sd">        T_mult (int, optional): A factor by which :math:`T_{i}` increases after a restart. Default: 1.</span>
<span class="sd">        eta_min (float, optional): Minimum learning rate. Default: 0.</span>
<span class="sd">        last_epoch (int, optional): The index of the last epoch. Default: -1.</span>
<span class="sd">        verbose (bool | str): If ``True``, prints a message to stdout for</span>
<span class="sd">            each update. Default: ``False``.</span>

<span class="sd">            .. deprecated:: 2.2</span>
<span class="sd">                ``verbose`` is deprecated. Please use ``get_last_lr()`` to access the</span>
<span class="sd">                learning rate.</span>

<span class="sd">    .. _SGDR\: Stochastic Gradient Descent with Warm Restarts:</span>
<span class="sd">        https://arxiv.org/abs/1608.03983</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">optimizer</span><span class="p">:</span> <span class="n">Optimizer</span><span class="p">,</span>
        <span class="n">T_0</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">T_mult</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">eta_min</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span>
        <span class="n">last_epoch</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">verbose</span><span class="o">=</span><span class="s2">&quot;deprecated&quot;</span><span class="p">,</span>
    <span class="p">):</span>  <span class="c1"># noqa: D107</span>
        <span class="k">if</span> <span class="n">T_0</span> <span class="o">&lt;=</span> <span class="mi">0</span> <span class="ow">or</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">T_0</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Expected positive integer T_0, but got </span><span class="si">{</span><span class="n">T_0</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">T_mult</span> <span class="o">&lt;</span> <span class="mi">1</span> <span class="ow">or</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">T_mult</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Expected integer T_mult &gt;= 1, but got </span><span class="si">{</span><span class="n">T_mult</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">eta_min</span><span class="p">,</span> <span class="p">(</span><span class="nb">float</span><span class="p">,</span> <span class="nb">int</span><span class="p">)):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Expected float or int eta_min, but got </span><span class="si">{</span><span class="n">eta_min</span><span class="si">}</span><span class="s2"> of type </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">eta_min</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">T_0</span> <span class="o">=</span> <span class="n">T_0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">T_i</span> <span class="o">=</span> <span class="n">T_0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">T_mult</span> <span class="o">=</span> <span class="n">T_mult</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">eta_min</span> <span class="o">=</span> <span class="n">eta_min</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">T_cur</span> <span class="o">=</span> <span class="n">last_epoch</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">last_epoch</span><span class="p">,</span> <span class="n">verbose</span><span class="p">)</span>

<div class="viewcode-block" id="CosineAnnealingWarmRestarts.get_lr">
<a class="viewcode-back" href="../../../python-api/generated/torch.optim.lr_scheduler.CosineAnnealingWarmRestarts.html#torch.optim.lr_scheduler.CosineAnnealingWarmRestarts.get_lr">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">get_lr</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Compute the initial learning rate.&quot;&quot;&quot;</span>
        <span class="n">_warn_get_lr_called_within_step</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>

        <span class="k">return</span> <span class="p">[</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">eta_min</span>
            <span class="o">+</span> <span class="p">(</span><span class="n">base_lr</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">eta_min</span><span class="p">)</span>
            <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">math</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">math</span><span class="o">.</span><span class="n">pi</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">T_cur</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">T_i</span><span class="p">))</span>
            <span class="o">/</span> <span class="mi">2</span>
            <span class="k">for</span> <span class="n">base_lr</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">base_lrs</span>
        <span class="p">]</span></div>


<div class="viewcode-block" id="CosineAnnealingWarmRestarts.step">
<a class="viewcode-back" href="../../../python-api/generated/torch.optim.lr_scheduler.CosineAnnealingWarmRestarts.html#torch.optim.lr_scheduler.CosineAnnealingWarmRestarts.step">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">epoch</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Step could be called after every batch update.</span>

<span class="sd">        Example:</span>
<span class="sd">            &gt;&gt;&gt; # xdoctest: +SKIP(&quot;Undefined vars&quot;)</span>
<span class="sd">            &gt;&gt;&gt; scheduler = CosineAnnealingWarmRestarts(optimizer, T_0, T_mult)</span>
<span class="sd">            &gt;&gt;&gt; iters = len(dataloader)</span>
<span class="sd">            &gt;&gt;&gt; for epoch in range(20):</span>
<span class="sd">            &gt;&gt;&gt;     for i, sample in enumerate(dataloader):</span>
<span class="sd">            &gt;&gt;&gt;         inputs, labels = sample[&#39;inputs&#39;], sample[&#39;labels&#39;]</span>
<span class="sd">            &gt;&gt;&gt;         optimizer.zero_grad()</span>
<span class="sd">            &gt;&gt;&gt;         outputs = net(inputs)</span>
<span class="sd">            &gt;&gt;&gt;         loss = criterion(outputs, labels)</span>
<span class="sd">            &gt;&gt;&gt;         loss.backward()</span>
<span class="sd">            &gt;&gt;&gt;         optimizer.step()</span>
<span class="sd">            &gt;&gt;&gt;         scheduler.step(epoch + i / iters)</span>

<span class="sd">        This function can be called in an interleaved way.</span>

<span class="sd">        Example:</span>
<span class="sd">            &gt;&gt;&gt; # xdoctest: +SKIP(&quot;Undefined vars&quot;)</span>
<span class="sd">            &gt;&gt;&gt; scheduler = CosineAnnealingWarmRestarts(optimizer, T_0, T_mult)</span>
<span class="sd">            &gt;&gt;&gt; for epoch in range(20):</span>
<span class="sd">            &gt;&gt;&gt;     scheduler.step()</span>
<span class="sd">            &gt;&gt;&gt; scheduler.step(26)</span>
<span class="sd">            &gt;&gt;&gt; scheduler.step() # scheduler.step(27), instead of scheduler(20)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">epoch</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">last_epoch</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">epoch</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="k">if</span> <span class="n">epoch</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">epoch</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">last_epoch</span> <span class="o">+</span> <span class="mi">1</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">T_cur</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">T_cur</span> <span class="o">+</span> <span class="mi">1</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">T_cur</span> <span class="o">&gt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">T_i</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">T_cur</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">T_cur</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">T_i</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">T_i</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">T_i</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">T_mult</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">epoch</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Expected non-negative epoch, but got </span><span class="si">{</span><span class="n">epoch</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">epoch</span> <span class="o">&gt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">T_0</span><span class="p">:</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">T_mult</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">T_cur</span> <span class="o">=</span> <span class="n">epoch</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">T_0</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">n</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span>
                        <span class="n">math</span><span class="o">.</span><span class="n">log</span><span class="p">(</span>
                            <span class="p">(</span><span class="n">epoch</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">T_0</span> <span class="o">*</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">T_mult</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">T_mult</span>
                        <span class="p">)</span>
                    <span class="p">)</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">T_cur</span> <span class="o">=</span> <span class="n">epoch</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">T_0</span> <span class="o">*</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">T_mult</span><span class="o">**</span><span class="n">n</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">T_mult</span> <span class="o">-</span> <span class="mi">1</span>
                    <span class="p">)</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">T_i</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">T_0</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">T_mult</span> <span class="o">**</span> <span class="p">(</span><span class="n">n</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">T_i</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">T_0</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">T_cur</span> <span class="o">=</span> <span class="n">epoch</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">last_epoch</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">floor</span><span class="p">(</span><span class="n">epoch</span><span class="p">)</span>

        <span class="k">with</span> <span class="n">_enable_get_lr_call</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
            <span class="k">for</span> <span class="n">param_group</span><span class="p">,</span> <span class="n">lr</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_lr</span><span class="p">()):</span>
                <span class="n">param_group</span><span class="p">[</span><span class="s2">&quot;lr&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">lr</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_last_lr</span> <span class="o">=</span> <span class="p">[</span><span class="n">group</span><span class="p">[</span><span class="s2">&quot;lr&quot;</span><span class="p">]</span> <span class="k">for</span> <span class="n">group</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">]</span></div>
</div>



<span class="k">class</span><span class="w"> </span><span class="nc">_SchedulePhase</span><span class="p">(</span><span class="n">TypedDict</span><span class="p">):</span>
    <span class="n">end_step</span><span class="p">:</span> <span class="nb">float</span>
    <span class="n">start_lr</span><span class="p">:</span> <span class="nb">str</span>
    <span class="n">end_lr</span><span class="p">:</span> <span class="nb">str</span>
    <span class="n">start_momentum</span><span class="p">:</span> <span class="nb">str</span>
    <span class="n">end_momentum</span><span class="p">:</span> <span class="nb">str</span>


<div class="viewcode-block" id="OneCycleLR">
<a class="viewcode-back" href="../../../python-api/generated/torch.optim.lr_scheduler.OneCycleLR.html#torch.optim.lr_scheduler.OneCycleLR">[docs]</a>
<span class="k">class</span><span class="w"> </span><span class="nc">OneCycleLR</span><span class="p">(</span><span class="n">LRScheduler</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Sets the learning rate of each parameter group according to the 1cycle learning rate policy.</span>

<span class="sd">    The 1cycle policy anneals the learning rate from an initial learning rate to some maximum</span>
<span class="sd">    learning rate and then from that maximum learning rate to some minimum learning rate much</span>
<span class="sd">    lower than the initial learning rate.</span>
<span class="sd">    This policy was initially described in the paper `Super-Convergence:</span>
<span class="sd">    Very Fast Training of Neural Networks Using Large Learning Rates`_.</span>

<span class="sd">    The 1cycle learning rate policy changes the learning rate after every batch.</span>
<span class="sd">    `step` should be called after a batch has been used for training.</span>

<span class="sd">    This scheduler is not chainable.</span>

<span class="sd">    Note also that the total number of steps in the cycle can be determined in one</span>
<span class="sd">    of two ways (listed in order of precedence):</span>

<span class="sd">    #. A value for total_steps is explicitly provided.</span>
<span class="sd">    #. A number of epochs (epochs) and a number of steps per epoch</span>
<span class="sd">       (steps_per_epoch) are provided.</span>
<span class="sd">       In this case, the number of total steps is inferred by</span>
<span class="sd">       total_steps = epochs * steps_per_epoch</span>

<span class="sd">    You must either provide a value for total_steps or provide a value for both</span>
<span class="sd">    epochs and steps_per_epoch.</span>

<span class="sd">    The default behaviour of this scheduler follows the fastai implementation of 1cycle, which</span>
<span class="sd">    claims that &quot;unpublished work has shown even better results by using only two phases&quot;. To</span>
<span class="sd">    mimic the behaviour of the original paper instead, set ``three_phase=True``.</span>

<span class="sd">    Args:</span>
<span class="sd">        optimizer (Optimizer): Wrapped optimizer.</span>
<span class="sd">        max_lr (float or list): Upper learning rate boundaries in the cycle</span>
<span class="sd">            for each parameter group.</span>
<span class="sd">        total_steps (int): The total number of steps in the cycle. Note that</span>
<span class="sd">            if a value is not provided here, then it must be inferred by providing</span>
<span class="sd">            a value for epochs and steps_per_epoch.</span>
<span class="sd">            Default: None</span>
<span class="sd">        epochs (int): The number of epochs to train for. This is used along</span>
<span class="sd">            with steps_per_epoch in order to infer the total number of steps in the cycle</span>
<span class="sd">            if a value for total_steps is not provided.</span>
<span class="sd">            Default: None</span>
<span class="sd">        steps_per_epoch (int): The number of steps per epoch to train for. This is</span>
<span class="sd">            used along with epochs in order to infer the total number of steps in the</span>
<span class="sd">            cycle if a value for total_steps is not provided.</span>
<span class="sd">            Default: None</span>
<span class="sd">        pct_start (float): The percentage of the cycle (in number of steps) spent</span>
<span class="sd">            increasing the learning rate.</span>
<span class="sd">            Default: 0.3</span>
<span class="sd">        anneal_strategy (str): {&#39;cos&#39;, &#39;linear&#39;}</span>
<span class="sd">            Specifies the annealing strategy: &quot;cos&quot; for cosine annealing, &quot;linear&quot; for</span>
<span class="sd">            linear annealing.</span>
<span class="sd">            Default: &#39;cos&#39;</span>
<span class="sd">        cycle_momentum (bool): If ``True``, momentum is cycled inversely</span>
<span class="sd">            to learning rate between &#39;base_momentum&#39; and &#39;max_momentum&#39;.</span>
<span class="sd">            Default: True</span>
<span class="sd">        base_momentum (float or list): Lower momentum boundaries in the cycle</span>
<span class="sd">            for each parameter group. Note that momentum is cycled inversely</span>
<span class="sd">            to learning rate; at the peak of a cycle, momentum is</span>
<span class="sd">            &#39;base_momentum&#39; and learning rate is &#39;max_lr&#39;.</span>
<span class="sd">            Default: 0.85</span>
<span class="sd">        max_momentum (float or list): Upper momentum boundaries in the cycle</span>
<span class="sd">            for each parameter group. Functionally,</span>
<span class="sd">            it defines the cycle amplitude (max_momentum - base_momentum).</span>
<span class="sd">            Note that momentum is cycled inversely</span>
<span class="sd">            to learning rate; at the start of a cycle, momentum is &#39;max_momentum&#39;</span>
<span class="sd">            and learning rate is &#39;base_lr&#39;</span>
<span class="sd">            Default: 0.95</span>
<span class="sd">        div_factor (float): Determines the initial learning rate via</span>
<span class="sd">            initial_lr = max_lr/div_factor</span>
<span class="sd">            Default: 25</span>
<span class="sd">        final_div_factor (float): Determines the minimum learning rate via</span>
<span class="sd">            min_lr = initial_lr/final_div_factor</span>
<span class="sd">            Default: 1e4</span>
<span class="sd">        three_phase (bool): If ``True``, use a third phase of the schedule to annihilate the</span>
<span class="sd">            learning rate according to &#39;final_div_factor&#39; instead of modifying the second</span>
<span class="sd">            phase (the first two phases will be symmetrical about the step indicated by</span>
<span class="sd">            &#39;pct_start&#39;).</span>
<span class="sd">        last_epoch (int): The index of the last batch. This parameter is used when</span>
<span class="sd">            resuming a training job. Since `step()` should be invoked after each</span>
<span class="sd">            batch instead of after each epoch, this number represents the total</span>
<span class="sd">            number of *batches* computed, not the total number of epochs computed.</span>
<span class="sd">            When last_epoch=-1, the schedule is started from the beginning.</span>
<span class="sd">            Default: -1</span>
<span class="sd">        verbose (bool | str): If ``True``, prints a message to stdout for</span>
<span class="sd">            each update. Default: ``False``.</span>

<span class="sd">            .. deprecated:: 2.2</span>
<span class="sd">                ``verbose`` is deprecated. Please use ``get_last_lr()`` to access the</span>
<span class="sd">                learning rate.</span>

<span class="sd">    Example:</span>
<span class="sd">        &gt;&gt;&gt; # xdoctest: +SKIP</span>
<span class="sd">        &gt;&gt;&gt; data_loader = torch.utils.data.DataLoader(...)</span>
<span class="sd">        &gt;&gt;&gt; optimizer = torch.optim.SGD(model.parameters(), lr=1e-4, momentum=0.9)</span>
<span class="sd">        &gt;&gt;&gt; scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.01, steps_per_epoch=len(data_loader), epochs=10)</span>
<span class="sd">        &gt;&gt;&gt; for epoch in range(10):</span>
<span class="sd">        &gt;&gt;&gt;     for batch in data_loader:</span>
<span class="sd">        &gt;&gt;&gt;         train_batch(...)</span>
<span class="sd">        &gt;&gt;&gt;         optimizer.step()</span>
<span class="sd">        &gt;&gt;&gt;         scheduler.step()</span>


<span class="sd">    .. _Super-Convergence\: Very Fast Training of Neural Networks Using Large Learning Rates:</span>
<span class="sd">        https://arxiv.org/abs/1708.07120</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">optimizer</span><span class="p">:</span> <span class="n">Optimizer</span><span class="p">,</span>
        <span class="n">max_lr</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">float</span><span class="p">]],</span>
        <span class="n">total_steps</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">epochs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">steps_per_epoch</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">pct_start</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.3</span><span class="p">,</span>
        <span class="n">anneal_strategy</span><span class="p">:</span> <span class="n">Literal</span><span class="p">[</span><span class="s2">&quot;cos&quot;</span><span class="p">,</span> <span class="s2">&quot;linear&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;cos&quot;</span><span class="p">,</span>
        <span class="n">cycle_momentum</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">base_momentum</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">float</span><span class="p">]]</span> <span class="o">=</span> <span class="mf">0.85</span><span class="p">,</span>
        <span class="n">max_momentum</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">float</span><span class="p">]]</span> <span class="o">=</span> <span class="mf">0.95</span><span class="p">,</span>
        <span class="n">div_factor</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">25.0</span><span class="p">,</span>
        <span class="n">final_div_factor</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e4</span><span class="p">,</span>
        <span class="n">three_phase</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">last_epoch</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">verbose</span><span class="o">=</span><span class="s2">&quot;deprecated&quot;</span><span class="p">,</span>
    <span class="p">):</span>  <span class="c1"># noqa: D107</span>
        <span class="c1"># Validate optimizer</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">Optimizer</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">optimizer</span><span class="p">)</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2"> is not an Optimizer&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">optimizer</span>

        <span class="c1"># Validate total_steps</span>
        <span class="k">if</span> <span class="n">total_steps</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">total_steps</span> <span class="o">&lt;=</span> <span class="mi">0</span> <span class="ow">or</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">total_steps</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;Expected positive integer total_steps, but got </span><span class="si">{</span><span class="n">total_steps</span><span class="si">}</span><span class="s2">&quot;</span>
                <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">total_steps</span> <span class="o">=</span> <span class="n">total_steps</span>
        <span class="k">elif</span> <span class="n">epochs</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">steps_per_epoch</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">epochs</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="ow">or</span> <span class="n">epochs</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Expected positive integer epochs, but got </span><span class="si">{</span><span class="n">epochs</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">steps_per_epoch</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="ow">or</span> <span class="n">steps_per_epoch</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;Expected positive integer steps_per_epoch, but got </span><span class="si">{</span><span class="n">steps_per_epoch</span><span class="si">}</span><span class="s2">&quot;</span>
                <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">total_steps</span> <span class="o">=</span> <span class="n">epochs</span> <span class="o">*</span> <span class="n">steps_per_epoch</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;You must define either total_steps OR (epochs AND steps_per_epoch)&quot;</span>
            <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_schedule_phases</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">_SchedulePhase</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">three_phase</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_schedule_phases</span> <span class="o">=</span> <span class="p">[</span>
                <span class="p">{</span>
                    <span class="s2">&quot;end_step&quot;</span><span class="p">:</span> <span class="nb">float</span><span class="p">(</span><span class="n">pct_start</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">total_steps</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span>
                    <span class="s2">&quot;start_lr&quot;</span><span class="p">:</span> <span class="s2">&quot;initial_lr&quot;</span><span class="p">,</span>
                    <span class="s2">&quot;end_lr&quot;</span><span class="p">:</span> <span class="s2">&quot;max_lr&quot;</span><span class="p">,</span>
                    <span class="s2">&quot;start_momentum&quot;</span><span class="p">:</span> <span class="s2">&quot;max_momentum&quot;</span><span class="p">,</span>
                    <span class="s2">&quot;end_momentum&quot;</span><span class="p">:</span> <span class="s2">&quot;base_momentum&quot;</span><span class="p">,</span>
                <span class="p">},</span>
                <span class="p">{</span>
                    <span class="s2">&quot;end_step&quot;</span><span class="p">:</span> <span class="nb">float</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">pct_start</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">total_steps</span><span class="p">)</span> <span class="o">-</span> <span class="mi">2</span><span class="p">,</span>
                    <span class="s2">&quot;start_lr&quot;</span><span class="p">:</span> <span class="s2">&quot;max_lr&quot;</span><span class="p">,</span>
                    <span class="s2">&quot;end_lr&quot;</span><span class="p">:</span> <span class="s2">&quot;initial_lr&quot;</span><span class="p">,</span>
                    <span class="s2">&quot;start_momentum&quot;</span><span class="p">:</span> <span class="s2">&quot;base_momentum&quot;</span><span class="p">,</span>
                    <span class="s2">&quot;end_momentum&quot;</span><span class="p">:</span> <span class="s2">&quot;max_momentum&quot;</span><span class="p">,</span>
                <span class="p">},</span>
                <span class="p">{</span>
                    <span class="s2">&quot;end_step&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">total_steps</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span>
                    <span class="s2">&quot;start_lr&quot;</span><span class="p">:</span> <span class="s2">&quot;initial_lr&quot;</span><span class="p">,</span>
                    <span class="s2">&quot;end_lr&quot;</span><span class="p">:</span> <span class="s2">&quot;min_lr&quot;</span><span class="p">,</span>
                    <span class="s2">&quot;start_momentum&quot;</span><span class="p">:</span> <span class="s2">&quot;max_momentum&quot;</span><span class="p">,</span>
                    <span class="s2">&quot;end_momentum&quot;</span><span class="p">:</span> <span class="s2">&quot;max_momentum&quot;</span><span class="p">,</span>
                <span class="p">},</span>
            <span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_schedule_phases</span> <span class="o">=</span> <span class="p">[</span>
                <span class="p">{</span>
                    <span class="s2">&quot;end_step&quot;</span><span class="p">:</span> <span class="nb">float</span><span class="p">(</span><span class="n">pct_start</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">total_steps</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span>
                    <span class="s2">&quot;start_lr&quot;</span><span class="p">:</span> <span class="s2">&quot;initial_lr&quot;</span><span class="p">,</span>
                    <span class="s2">&quot;end_lr&quot;</span><span class="p">:</span> <span class="s2">&quot;max_lr&quot;</span><span class="p">,</span>
                    <span class="s2">&quot;start_momentum&quot;</span><span class="p">:</span> <span class="s2">&quot;max_momentum&quot;</span><span class="p">,</span>
                    <span class="s2">&quot;end_momentum&quot;</span><span class="p">:</span> <span class="s2">&quot;base_momentum&quot;</span><span class="p">,</span>
                <span class="p">},</span>
                <span class="p">{</span>
                    <span class="s2">&quot;end_step&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">total_steps</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span>
                    <span class="s2">&quot;start_lr&quot;</span><span class="p">:</span> <span class="s2">&quot;max_lr&quot;</span><span class="p">,</span>
                    <span class="s2">&quot;end_lr&quot;</span><span class="p">:</span> <span class="s2">&quot;min_lr&quot;</span><span class="p">,</span>
                    <span class="s2">&quot;start_momentum&quot;</span><span class="p">:</span> <span class="s2">&quot;base_momentum&quot;</span><span class="p">,</span>
                    <span class="s2">&quot;end_momentum&quot;</span><span class="p">:</span> <span class="s2">&quot;max_momentum&quot;</span><span class="p">,</span>
                <span class="p">},</span>
            <span class="p">]</span>

        <span class="c1"># Validate pct_start</span>
        <span class="k">if</span> <span class="n">pct_start</span> <span class="o">&lt;</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">pct_start</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="ow">or</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">pct_start</span><span class="p">,</span> <span class="nb">float</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Expected float between 0 and 1 pct_start, but got </span><span class="si">{</span><span class="n">pct_start</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span>

        <span class="c1"># Validate anneal_strategy</span>
        <span class="k">if</span> <span class="n">anneal_strategy</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;cos&quot;</span><span class="p">,</span> <span class="s2">&quot;linear&quot;</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;anneal_strategy must be one of &#39;cos&#39; or &#39;linear&#39;, instead got </span><span class="si">{</span><span class="n">anneal_strategy</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_anneal_func_type</span> <span class="o">=</span> <span class="n">anneal_strategy</span>

        <span class="c1"># Initialize learning rate variables</span>
        <span class="n">max_lrs</span> <span class="o">=</span> <span class="n">_format_param</span><span class="p">(</span><span class="s2">&quot;max_lr&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">max_lr</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">last_epoch</span> <span class="o">==</span> <span class="o">-</span><span class="mi">1</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">group</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">):</span>
                <span class="n">group</span><span class="p">[</span><span class="s2">&quot;initial_lr&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">max_lrs</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">/</span> <span class="n">div_factor</span>
                <span class="n">group</span><span class="p">[</span><span class="s2">&quot;max_lr&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">max_lrs</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
                <span class="n">group</span><span class="p">[</span><span class="s2">&quot;min_lr&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">group</span><span class="p">[</span><span class="s2">&quot;initial_lr&quot;</span><span class="p">]</span> <span class="o">/</span> <span class="n">final_div_factor</span>

        <span class="c1"># Initialize momentum variables</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cycle_momentum</span> <span class="o">=</span> <span class="n">cycle_momentum</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">cycle_momentum</span><span class="p">:</span>
            <span class="k">if</span> <span class="p">(</span>
                <span class="s2">&quot;momentum&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">defaults</span>
                <span class="ow">and</span> <span class="s2">&quot;betas&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">defaults</span>
            <span class="p">):</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s2">&quot;optimizer must support momentum or beta1 with `cycle_momentum` option enabled&quot;</span>
                <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">use_beta1</span> <span class="o">=</span> <span class="s2">&quot;betas&quot;</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">defaults</span>
            <span class="n">max_momentums</span> <span class="o">=</span> <span class="n">_format_param</span><span class="p">(</span><span class="s2">&quot;max_momentum&quot;</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">max_momentum</span><span class="p">)</span>
            <span class="n">base_momentums</span> <span class="o">=</span> <span class="n">_format_param</span><span class="p">(</span><span class="s2">&quot;base_momentum&quot;</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">base_momentum</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">last_epoch</span> <span class="o">==</span> <span class="o">-</span><span class="mi">1</span><span class="p">:</span>
                <span class="k">for</span> <span class="n">m_momentum</span><span class="p">,</span> <span class="n">b_momentum</span><span class="p">,</span> <span class="n">group</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span>
                    <span class="n">max_momentums</span><span class="p">,</span> <span class="n">base_momentums</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span>
                <span class="p">):</span>
                    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_beta1</span><span class="p">:</span>
                        <span class="n">group</span><span class="p">[</span><span class="s2">&quot;betas&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">m_momentum</span><span class="p">,</span> <span class="o">*</span><span class="n">group</span><span class="p">[</span><span class="s2">&quot;betas&quot;</span><span class="p">][</span><span class="mi">1</span><span class="p">:])</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="n">group</span><span class="p">[</span><span class="s2">&quot;momentum&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">m_momentum</span>
                    <span class="n">group</span><span class="p">[</span><span class="s2">&quot;max_momentum&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">m_momentum</span>
                    <span class="n">group</span><span class="p">[</span><span class="s2">&quot;base_momentum&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">b_momentum</span>

        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">last_epoch</span><span class="p">,</span> <span class="n">verbose</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_anneal_func</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;_anneal_func_type&quot;</span><span class="p">):</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_anneal_func_type</span> <span class="o">==</span> <span class="s2">&quot;cos&quot;</span><span class="p">:</span>
                <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_annealing_cos</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">_anneal_func_type</span> <span class="o">==</span> <span class="s2">&quot;linear&quot;</span><span class="p">:</span>
                <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_annealing_linear</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Unknown _anneal_func_type: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">_anneal_func_type</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># For BC</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">anneal_func</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>  <span class="c1"># type: ignore[attr-defined]</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">_annealing_cos</span><span class="p">(</span><span class="n">start</span><span class="p">,</span> <span class="n">end</span><span class="p">,</span> <span class="n">pct</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Cosine anneal from `start` to `end` as pct goes from 0.0 to 1.0.&quot;&quot;&quot;</span>
        <span class="n">cos_out</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">math</span><span class="o">.</span><span class="n">pi</span> <span class="o">*</span> <span class="n">pct</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span>
        <span class="k">return</span> <span class="n">end</span> <span class="o">+</span> <span class="p">(</span><span class="n">start</span> <span class="o">-</span> <span class="n">end</span><span class="p">)</span> <span class="o">/</span> <span class="mf">2.0</span> <span class="o">*</span> <span class="n">cos_out</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">_annealing_linear</span><span class="p">(</span><span class="n">start</span><span class="p">,</span> <span class="n">end</span><span class="p">,</span> <span class="n">pct</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Linearly anneal from `start` to `end` as pct goes from 0.0 to 1.0.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">end</span> <span class="o">-</span> <span class="n">start</span><span class="p">)</span> <span class="o">*</span> <span class="n">pct</span> <span class="o">+</span> <span class="n">start</span>

<div class="viewcode-block" id="OneCycleLR.get_lr">
<a class="viewcode-back" href="../../../python-api/generated/torch.optim.lr_scheduler.OneCycleLR.html#torch.optim.lr_scheduler.OneCycleLR.get_lr">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">get_lr</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Compute the learning rate of each parameter group.&quot;&quot;&quot;</span>
        <span class="n">_warn_get_lr_called_within_step</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>

        <span class="n">lrs</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">step_num</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">last_epoch</span>

        <span class="k">if</span> <span class="n">step_num</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">total_steps</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Tried to step </span><span class="si">{</span><span class="n">step_num</span><span class="si">}</span><span class="s2"> times. The specified number of total steps is </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">total_steps</span><span class="si">}</span><span class="s2">&quot;</span>  <span class="c1"># noqa: UP032</span>
            <span class="p">)</span>

        <span class="k">for</span> <span class="n">group</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">:</span>
            <span class="n">start_step</span> <span class="o">=</span> <span class="mf">0.0</span>
            <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">phase</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_schedule_phases</span><span class="p">):</span>
                <span class="n">end_step</span> <span class="o">=</span> <span class="n">phase</span><span class="p">[</span><span class="s2">&quot;end_step&quot;</span><span class="p">]</span>
                <span class="k">if</span> <span class="n">step_num</span> <span class="o">&lt;=</span> <span class="n">end_step</span> <span class="ow">or</span> <span class="n">i</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_schedule_phases</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">:</span>
                    <span class="n">pct</span> <span class="o">=</span> <span class="p">(</span><span class="n">step_num</span> <span class="o">-</span> <span class="n">start_step</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">end_step</span> <span class="o">-</span> <span class="n">start_step</span><span class="p">)</span>
                    <span class="n">computed_lr</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_anneal_func</span><span class="p">(</span>
                        <span class="n">group</span><span class="p">[</span><span class="n">phase</span><span class="p">[</span><span class="s2">&quot;start_lr&quot;</span><span class="p">]],</span> <span class="n">group</span><span class="p">[</span><span class="n">phase</span><span class="p">[</span><span class="s2">&quot;end_lr&quot;</span><span class="p">]],</span> <span class="n">pct</span>
                    <span class="p">)</span>
                    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">cycle_momentum</span><span class="p">:</span>
                        <span class="n">computed_momentum</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_anneal_func</span><span class="p">(</span>
                            <span class="n">group</span><span class="p">[</span><span class="n">phase</span><span class="p">[</span><span class="s2">&quot;start_momentum&quot;</span><span class="p">]],</span>
                            <span class="n">group</span><span class="p">[</span><span class="n">phase</span><span class="p">[</span><span class="s2">&quot;end_momentum&quot;</span><span class="p">]],</span>
                            <span class="n">pct</span><span class="p">,</span>
                        <span class="p">)</span>
                    <span class="k">break</span>
                <span class="n">start_step</span> <span class="o">=</span> <span class="n">phase</span><span class="p">[</span><span class="s2">&quot;end_step&quot;</span><span class="p">]</span>

            <span class="n">lrs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">computed_lr</span><span class="p">)</span>  <span class="c1"># type: ignore[possibly-undefined]</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">cycle_momentum</span><span class="p">:</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_beta1</span><span class="p">:</span>
                    <span class="n">group</span><span class="p">[</span><span class="s2">&quot;betas&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">computed_momentum</span><span class="p">,</span> <span class="o">*</span><span class="n">group</span><span class="p">[</span><span class="s2">&quot;betas&quot;</span><span class="p">][</span><span class="mi">1</span><span class="p">:])</span>  <span class="c1"># type: ignore[possibly-undefined]</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">group</span><span class="p">[</span>
                        <span class="s2">&quot;momentum&quot;</span>
                    <span class="p">]</span> <span class="o">=</span> <span class="n">computed_momentum</span>  <span class="c1"># type: ignore[possibly-undefined]</span>

        <span class="k">return</span> <span class="n">lrs</span></div>
</div>

</pre></div>

                </article>
              
              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
</div>
                </footer>
              
            </div>
            
            
              
            
          </div>
          <footer class="bd-footer-content">
            
          </footer>
        
      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script defer src="../../../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf"></script>
<script defer src="../../../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf"></script>


  <footer class="bd-footer">
<div class="bd-footer__inner bd-page-width">
  
    <div class="footer-items__start">
      
        <div class="footer-item">

  <p class="copyright">
    
       Copyright PyTorch Contributors.
      <br/>
    
  </p>
</div>
      
        <div class="footer-item">

  <p class="sphinx-version">
    Created using <a href="https://www.sphinx-doc.org/">Sphinx</a> 8.1.0.
    <br/>
  </p>
</div>
      
    </div>
  
  
  
    <div class="footer-items__end">
      
        <div class="footer-item">
<p class="theme-version">
  <!-- # L10n: Setting the PST URL as an argument as this does not need to be localized -->
  Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.16.1.
</p></div>
      
    </div>
  
</div>

  </footer>
  </body>
</html>