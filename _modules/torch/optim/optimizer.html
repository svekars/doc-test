
<!DOCTYPE html>


<html lang="en" data-content_root="../../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>torch.optim.optimizer &#8212; PyTorch main documentation</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  <!--
    this give us a css class that will be invisible only if js is disabled
  -->
  <noscript>
    <style>
      .pst-js-only { display: none !important; }

    </style>
  </noscript>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../../_static/styles/theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />
<link href="../../../_static/styles/pydata-sphinx-theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />

    <link rel="stylesheet" type="text/css" href="../../../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../../../_static/copybutton.css?v=949a1ff5" />
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/katex.min.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/katex-math.css?v=91adb8b6" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="../../../_static/css/jit.css?v=8de1ea5d" />
    <link rel="stylesheet" type="text/css" href="../../../_static/css/custom.css?v=3ccf5357" />
    <link rel="stylesheet" type="text/css" href="../../../_static/css/custom2.css?v=baa440dc" />
  
  <!-- So that users can add custom icons -->
  <script src="../../../_static/scripts/fontawesome.js?digest=8878045cc6db502f8baf"></script>
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf" />
<link rel="preload" as="script" href="../../../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf" />

    <script src="../../../_static/documentation_options.js?v=a8da1a53"></script>
    <script src="../../../_static/doctools.js?v=9bcbadda"></script>
    <script src="../../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../../_static/copybutton.js?v=940804e7"></script>
    <script src="../../../_static/design-tabs.js?v=f930bc37"></script>
    <script type="module" src="https://cdn.jsdelivr.net/npm/mermaid@11.2.0/dist/mermaid.esm.min.mjs"></script>
    <script type="module" src="https://cdn.jsdelivr.net/npm/@mermaid-js/layout-elk@0.1.4/dist/mermaid-layout-elk.esm.min.mjs"></script>
    <script type="module">import mermaid from "https://cdn.jsdelivr.net/npm/mermaid@11.2.0/dist/mermaid.esm.min.mjs";import elkLayouts from "https://cdn.jsdelivr.net/npm/@mermaid-js/layout-elk@0.1.4/dist/mermaid-layout-elk.esm.min.mjs";mermaid.registerLayoutLoaders(elkLayouts);mermaid.initialize({startOnLoad:false});</script>
    <script src="https://cdn.jsdelivr.net/npm/d3@7.9.0/dist/d3.min.js"></script>
    <script type="module">
import mermaid from "https://cdn.jsdelivr.net/npm/mermaid@11.2.0/dist/mermaid.esm.min.mjs";
window.addEventListener("load", () => mermaid.run());
</script>
    <script>DOCUMENTATION_OPTIONS.pagename = '_modules/torch/optim/optimizer';</script>
    <script src="../../../_static/js/star-rating.js?v=8861fcb6"></script>
    <script src="../../../_static/js/send-feedback.js?v=5646bf45"></script>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />

  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  <meta name="docsearch:version" content="main (2.6.0 )" />
<script src="https://code.jquery.com/jquery-3.7.1.min.js"></script>
<script type="text/javascript" src="../../../_static/js/send-feedback.js"></script>
<script type="text/javascript" src="../../../_static/js/star-rating.js"></script>
<script type="text/javascript" src="../../../_static/js/cookie-banner.js"></script>

<script async src="https://www.googletagmanager.com/gtag/js?id=G-TEST12345"></script>
    <script type="text/javascript">
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-TEST12345');
    </script>

  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
<body data-feedback-url="https://github.com/pytorch/pytorch">
  <div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>

          <li class="main-menu-item">
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Learn
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="">
                  <span class=dropdown-title>Get Started</span>
                  <p>Run PyTorch locally or get started quickly with one of the supported cloud platforms</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Tutorials</span>
                  <p>Whats new in PyTorch tutorials</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Learn the Basics</span>
                  <p>Familiarize yourself with PyTorch concepts and modules</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">PyTorch Recipes</span>
                  <p>Bite-size, ready-to-deploy PyTorch code examples</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Intro to PyTorch - YouTube Series</span>
                  <p>Master PyTorch basics with our engaging YouTube tutorial series</p>
                </a>
              </div>
            </div>
          </li>

          <li>
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Ecosystem
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Tools</span>
                  <p>Learn about the tools and frameworks in the PyTorch Ecosystem</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class=dropdown-title>Community</span>
                  <p>Join the PyTorch developer community to contribute, learn, and get your questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="" target="_blank">
                  <span class=dropdown-title>Forums</span>
                  <p>A place to discuss PyTorch code, issues, install, research</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class=dropdown-title>Developer Resources</span>
                  <p>Find resources and get questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Contributor Awards - 2024</span>
                  <p>Award winners announced at this year's PyTorch Conference</p>
                </a>
              </div>
            </div>
          </li>

          <li>
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Edge
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">About PyTorch Edge</span>
                  <p>Build innovative and privacy-aware AI experiences for edge devices</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">ExecuTorch</span>
                  <p>End-to-end solution for enabling on-device inference capabilities across mobile and edge devices</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">ExecuTorch Docs</span>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Docs
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">PyTorch</span>
                  <p>Explore the documentation for comprehensive guidance on how to use PyTorch</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">PyTorch Domains</span>
                  <p>Read the PyTorch Domains documentation to learn more about domain-specific libraries</p>
                </a>
              </div>
            </div>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Blogs & News
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">PyTorch Blog</span>
                  <p>Catch up on the latest technical news and happenings</p>
                </a>
                 <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Community Blog</span>
                  <p>Stories from the PyTorch ecosystem</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Videos</span>
                  <p>Learn about the latest PyTorch tutorials, new, and more </p>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Community Stories</span>
                  <p>Learn how our community solves real, everyday machine learning problems with PyTorch</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Events</span>
                  <p>Find events, webinars, and podcasts</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Newsletter</span>
                  <p>Stay up-to-date with the latest updates</p>
                </a>
            </div>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                About
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">PyTorch Foundation</span>
                  <p>Learn more about the PyTorch Foundation</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Governing Board</span>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Cloud Credit Program</span>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Technical Advisory Council</span>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Staff</span>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Contact Us</span>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
            <div class="no-dropdown">
              <a href="" data-cta="join">
                Become a Member
              </a>
            </div>
          </li>
          <li>
           <div class="main-menu-item">
             <a href="" class="github-icon">
             </a>
           </div>
          </li>
          <!--- TODO: This block adds the search icon to the nav bar. We will enable it later.
          <li>
            <div class="main-menu-item">
             <a href="" class="search-icon">
             </a>
            </div>
          </li>
          --->
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>
  </div>
</div>

  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <dialog id="pst-search-dialog">
    
<form class="bd-search d-flex align-items-center"
      action="../../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </dialog>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
<div class="bd-header__inner bd-page-width">
  <button class="pst-navbar-icon sidebar-toggle primary-toggle" aria-label="Site navigation">
    <span class="fa-solid fa-bars"></span>
  </button>
  
  
  <div class=" navbar-header-items__start">
    
      <div class="navbar-item">

  
    
  

<a class="navbar-brand logo" href="../../../index.html">
  
  
  
  
  
  
    <p class="title logo__title">Home</p>
  
</a></div>
    
  </div>
  
  <div class=" navbar-header-items">
    
    <div class="me-auto navbar-header-items__center">
      
        <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../python-api/index.html">
    Python API
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../notes/index.html">
    Developer Notes
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-external" href="https://pytorch.org/tutorials/">
    Tutorials
  </a>
</li>

  </ul>
</nav></div>
      
    </div>
    
    
    <div class="navbar-header-items__end">
      
        <div class="navbar-item navbar-persistent--container">
          

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button>
        </div>
      
      
        <div class="navbar-item">
<div class="version-switcher__container dropdown pst-js-only">
  <button id="pst-version-switcher-button-2"
    type="button"
    class="version-switcher__button btn btn-sm dropdown-toggle"
    data-bs-toggle="dropdown"
    aria-haspopup="listbox"
    aria-controls="pst-version-switcher-list-2"
    aria-label="Version switcher list"
  >
    Choose version  <!-- this text may get changed later by javascript -->
    <span class="caret"></span>
  </button>
  <div id="pst-version-switcher-list-2"
    class="version-switcher__menu dropdown-menu list-group-flush py-0"
    role="listbox" aria-labelledby="pst-version-switcher-button-2">
    <!-- dropdown will be populated by javascript on page load -->
  </div>
</div></div>
      
        <div class="navbar-item">

<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button></div>
      
        <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://x.com/PyTorch" title="X" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-x-twitter fa-lg" aria-hidden="true"></i>
            <span class="sr-only">X</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/pytorch/pytorch" title="GitHub" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">GitHub</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://pypi.org/project/torch" title="PyPI" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-custom fa-pypi fa-lg" aria-hidden="true"></i>
            <span class="sr-only">PyPI</span></a>
        </li>
</ul></div>
      
    </div>
    
  </div>
  
  
    <div class="navbar-persistent--mobile">

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button>
    </div>
  

  
</div>

    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
        
      
      <dialog id="pst-primary-sidebar-modal"></dialog>
      <div id="pst-primary-sidebar" class="bd-sidebar-primary bd-sidebar hide-on-wide">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
      <div class="sidebar-header-items__center">
        
          
          
            <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../python-api/index.html">
    Python API
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../notes/index.html">
    Developer Notes
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-external" href="https://pytorch.org/tutorials/">
    Tutorials
  </a>
</li>

  </ul>
</nav></div>
          
        
      </div>
    
    
    
      <div class="sidebar-header-items__end">
        
          <div class="navbar-item">
<div class="version-switcher__container dropdown pst-js-only">
  <button id="pst-version-switcher-button-3"
    type="button"
    class="version-switcher__button btn btn-sm dropdown-toggle"
    data-bs-toggle="dropdown"
    aria-haspopup="listbox"
    aria-controls="pst-version-switcher-list-3"
    aria-label="Version switcher list"
  >
    Choose version  <!-- this text may get changed later by javascript -->
    <span class="caret"></span>
  </button>
  <div id="pst-version-switcher-list-3"
    class="version-switcher__menu dropdown-menu list-group-flush py-0"
    role="listbox" aria-labelledby="pst-version-switcher-button-3">
    <!-- dropdown will be populated by javascript on page load -->
  </div>
</div></div>
        
          <div class="navbar-item">

<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button></div>
        
          <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://x.com/PyTorch" title="X" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-x-twitter fa-lg" aria-hidden="true"></i>
            <span class="sr-only">X</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/pytorch/pytorch" title="GitHub" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">GitHub</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://pypi.org/project/torch" title="PyPI" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-custom fa-pypi fa-lg" aria-hidden="true"></i>
            <span class="sr-only">PyPI</span></a>
        </li>
</ul></div>
        
      </div>
    
  </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
      <div class="sidebar-primary-item">
<div id="ethical-ad-placement"
      class="flat"
      data-ea-publisher="readthedocs"
      data-ea-type="readthedocs-sidebar"
      data-ea-manual="true">
</div></div>
  </div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        
          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item">

<nav aria-label="Breadcrumb" class="d-print-none">
  <ul class="bd-breadcrumbs">
    
    <li class="breadcrumb-item breadcrumb-home">
      <a href="../../../index.html" class="nav-link" aria-label="Home">
        <i class="fa-solid fa-home"></i>
      </a>
    </li>
    
    <li class="breadcrumb-item"><a href="../../index.html" class="nav-link">Module code</a></li>
    
    
    <li class="breadcrumb-item"><a href="../../torch.html" class="nav-link">torch</a></li>
    
    
    <li class="breadcrumb-item"><a href="../optim.html" class="nav-link">torch.optim</a></li>
    
    <li class="breadcrumb-item active" aria-current="page"><span class="ellipsis">torch.optim.optimizer</span></li>
  </ul>
</nav>
</div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">
<div class="rating">
    Rate this Page
    <div class="stars">
        
        <span class="star" data-value="1">★</span>
        
        <span class="star" data-value="2">★</span>
        
        <span class="star" data-value="3">★</span>
        
        <span class="star" data-value="4">★</span>
        
        <span class="star" data-value="5">★</span>
        
    </div>
</div>
</div>
      
    </div>
  
</div>
</div>
              
              
              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <h1>Source code for torch.optim.optimizer</h1><div class="highlight"><pre>
<span></span><span class="c1"># mypy: allow-untyped-decorators</span>
<span class="c1"># mypy: allow-untyped-defs</span>
<span class="sd">&quot;&quot;&quot;Base optimizer.&quot;&quot;&quot;</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">functools</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">warnings</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">collections</span><span class="w"> </span><span class="kn">import</span> <span class="n">defaultdict</span><span class="p">,</span> <span class="n">OrderedDict</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">copy</span><span class="w"> </span><span class="kn">import</span> <span class="n">deepcopy</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">itertools</span><span class="w"> </span><span class="kn">import</span> <span class="n">chain</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">typing</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">Any</span><span class="p">,</span>
    <span class="n">Callable</span><span class="p">,</span>
    <span class="n">cast</span><span class="p">,</span>
    <span class="n">DefaultDict</span><span class="p">,</span>
    <span class="n">Dict</span><span class="p">,</span>
    <span class="n">Hashable</span><span class="p">,</span>
    <span class="n">Iterable</span><span class="p">,</span>
    <span class="n">List</span><span class="p">,</span>
    <span class="n">Optional</span><span class="p">,</span>
    <span class="n">overload</span><span class="p">,</span>
    <span class="n">Set</span><span class="p">,</span>
    <span class="n">Tuple</span><span class="p">,</span>
    <span class="n">TypeVar</span><span class="p">,</span>
    <span class="n">Union</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">typing_extensions</span><span class="w"> </span><span class="kn">import</span> <span class="n">ParamSpec</span><span class="p">,</span> <span class="n">Self</span><span class="p">,</span> <span class="n">TypeAlias</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.utils.hooks</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">hooks</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch.utils._foreach_utils</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">_get_foreach_kernels_supported_devices</span><span class="p">,</span>
    <span class="n">_get_fused_kernels_supported_devices</span><span class="p">,</span>
    <span class="n">_group_tensors_by_device_and_dtype</span><span class="p">,</span>
    <span class="n">Indices</span><span class="p">,</span>
    <span class="n">TensorListList</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch.utils.hooks</span><span class="w"> </span><span class="kn">import</span> <span class="n">RemovableHandle</span>


<span class="n">Args</span><span class="p">:</span> <span class="n">TypeAlias</span> <span class="o">=</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Any</span><span class="p">,</span> <span class="o">...</span><span class="p">]</span>
<span class="n">Kwargs</span><span class="p">:</span> <span class="n">TypeAlias</span> <span class="o">=</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]</span>
<span class="n">StateDict</span><span class="p">:</span> <span class="n">TypeAlias</span> <span class="o">=</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]</span>
<span class="n">DeviceDict</span> <span class="o">=</span> <span class="n">Dict</span><span class="p">[</span><span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">],</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span>
<span class="n">DeviceDtypeDict</span> <span class="o">=</span> <span class="n">Dict</span><span class="p">[</span><span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span><span class="p">]],</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span>


<span class="n">GlobalOptimizerPreHook</span><span class="p">:</span> <span class="n">TypeAlias</span> <span class="o">=</span> <span class="n">Callable</span><span class="p">[</span>
    <span class="p">[</span><span class="s2">&quot;Optimizer&quot;</span><span class="p">,</span> <span class="n">Args</span><span class="p">,</span> <span class="n">Kwargs</span><span class="p">],</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">Args</span><span class="p">,</span> <span class="n">Kwargs</span><span class="p">]]</span>
<span class="p">]</span>
<span class="n">GlobalOptimizerPostHook</span><span class="p">:</span> <span class="n">TypeAlias</span> <span class="o">=</span> <span class="n">Callable</span><span class="p">[[</span><span class="s2">&quot;Optimizer&quot;</span><span class="p">,</span> <span class="n">Args</span><span class="p">,</span> <span class="n">Kwargs</span><span class="p">],</span> <span class="kc">None</span><span class="p">]</span>

<span class="n">__all__</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s2">&quot;Optimizer&quot;</span><span class="p">,</span>
    <span class="s2">&quot;register_optimizer_step_pre_hook&quot;</span><span class="p">,</span>
    <span class="s2">&quot;register_optimizer_step_post_hook&quot;</span><span class="p">,</span>
<span class="p">]</span>
<span class="n">_global_optimizer_pre_hooks</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">GlobalOptimizerPreHook</span><span class="p">]</span> <span class="o">=</span> <span class="n">OrderedDict</span><span class="p">()</span>
<span class="n">_global_optimizer_post_hooks</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">GlobalOptimizerPostHook</span><span class="p">]</span> <span class="o">=</span> <span class="n">OrderedDict</span><span class="p">()</span>
<span class="n">_foreach_supported_types</span> <span class="o">=</span> <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">parameter</span><span class="o">.</span><span class="n">Parameter</span><span class="p">]</span>


<span class="k">class</span><span class="w"> </span><span class="nc">_RequiredParameter</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Singleton class representing a required parameter for an Optimizer.&quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
        <span class="k">return</span> <span class="s2">&quot;&lt;required parameter&gt;&quot;</span>


<span class="n">required</span> <span class="o">=</span> <span class="n">_RequiredParameter</span><span class="p">()</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_use_grad_for_differentiable</span><span class="p">(</span><span class="n">func</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">_use_grad</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="kn">import</span><span class="w"> </span><span class="nn">torch._dynamo</span>

        <span class="n">prev_grad</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">is_grad_enabled</span><span class="p">()</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="c1"># Note on graph break below:</span>
            <span class="c1"># we need to graph break to ensure that aot respects the no_grad annotation.</span>
            <span class="c1"># This is important for perf because without this, functionalization will generate an epilogue</span>
            <span class="c1"># which updates the mutated parameters of the optimizer which is *not* visible to inductor, as a result,</span>
            <span class="c1"># inductor will allocate for every parameter in the model, which is horrible.</span>
            <span class="c1"># With this, aot correctly sees that this is an inference graph, and functionalization will generate</span>
            <span class="c1"># an epilogue which is appended to the graph, which *is* visible to inductor, as a result, inductor sees that</span>
            <span class="c1"># step is in place and is able to avoid the extra allocation.</span>
            <span class="c1"># In the future, we will either 1) continue to graph break on backward, so this graph break does not matter</span>
            <span class="c1"># or 2) have a fully fused forward and backward graph, which will have no_grad by default, and we can remove this</span>
            <span class="c1"># graph break to allow the fully fused fwd-bwd-optimizer graph to be compiled.</span>
            <span class="c1"># see https://github.com/pytorch/pytorch/issues/104053</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">set_grad_enabled</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">defaults</span><span class="p">[</span><span class="s2">&quot;differentiable&quot;</span><span class="p">])</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">_dynamo</span><span class="o">.</span><span class="n">graph_break</span><span class="p">()</span>
            <span class="n">ret</span> <span class="o">=</span> <span class="n">func</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="k">finally</span><span class="p">:</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">_dynamo</span><span class="o">.</span><span class="n">graph_break</span><span class="p">()</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">set_grad_enabled</span><span class="p">(</span><span class="n">prev_grad</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">ret</span>

    <span class="n">functools</span><span class="o">.</span><span class="n">update_wrapper</span><span class="p">(</span><span class="n">_use_grad</span><span class="p">,</span> <span class="n">func</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">_use_grad</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_get_value</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="c1"># item is significantly faster than a cpu tensor in eager mode</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">is_scripting</span><span class="p">()</span> <span class="ow">and</span> <span class="n">torch</span><span class="o">.</span><span class="n">compiler</span><span class="o">.</span><span class="n">is_compiling</span><span class="p">():</span>
        <span class="k">return</span> <span class="n">x</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">x</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="k">else</span> <span class="n">x</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_stack_if_compiling</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">is_scripting</span><span class="p">()</span> <span class="ow">and</span> <span class="n">torch</span><span class="o">.</span><span class="n">compiler</span><span class="o">.</span><span class="n">is_compiling</span><span class="p">():</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">x</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_disable_dynamo_if_unsupported</span><span class="p">(</span><span class="n">single_tensor_fn</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="c1"># workaround for torchscript BC</span>
    <span class="c1"># it requires all called functions to be in the</span>
    <span class="c1"># global environment at the site at which the</span>
    <span class="c1"># maybe_fallback closure is created</span>
    <span class="k">if</span> <span class="n">single_tensor_fn</span><span class="p">:</span>
        <span class="nb">globals</span><span class="p">()[</span><span class="n">single_tensor_fn</span><span class="o">.</span><span class="vm">__name__</span><span class="p">]</span> <span class="o">=</span> <span class="n">single_tensor_fn</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">wrapper</span><span class="p">(</span><span class="n">func</span><span class="p">):</span>
        <span class="kn">import</span><span class="w"> </span><span class="nn">inspect</span>

        <span class="n">disabled_func</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">_disable_dynamo</span><span class="p">(</span><span class="n">func</span><span class="p">)</span>
        <span class="n">ps</span> <span class="o">=</span> <span class="n">inspect</span><span class="o">.</span><span class="n">signature</span><span class="p">(</span><span class="n">func</span><span class="p">)</span><span class="o">.</span><span class="n">parameters</span>
        <span class="n">has_state_steps</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">state_steps_ind</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">ps</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="s2">&quot;state_steps&quot;</span><span class="p">)</span>
        <span class="k">except</span> <span class="ne">ValueError</span><span class="p">:</span>
            <span class="n">has_state_steps</span> <span class="o">=</span> <span class="kc">False</span>

        <span class="c1"># Today, there are cases where we stack state steps</span>
        <span class="c1"># and pass them as the value arg of foreach ops.</span>
        <span class="c1"># Having state steps on cuda as the value arg is not supported in eager,</span>
        <span class="c1"># but this only occurs in the rare case that the user explicitly deletes</span>
        <span class="c1"># the capturable flag. If capturable=True, this is not a problem.</span>
        <span class="nd">@functools</span><span class="o">.</span><span class="n">wraps</span><span class="p">(</span><span class="n">func</span><span class="p">)</span>
        <span class="k">def</span><span class="w"> </span><span class="nf">maybe_fallback</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">compiler</span><span class="o">.</span><span class="n">is_compiling</span><span class="p">()</span> <span class="ow">and</span> <span class="p">(</span>
                <span class="ow">not</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;capturable&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
                <span class="ow">and</span> <span class="n">has_state_steps</span>
                <span class="ow">and</span> <span class="p">(</span><span class="n">args</span><span class="p">[</span><span class="n">state_steps_ind</span><span class="p">]</span> <span class="ow">and</span> <span class="n">args</span><span class="p">[</span><span class="n">state_steps_ind</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">is_cuda</span><span class="p">)</span>
                <span class="ow">or</span> <span class="p">(</span>
                    <span class="s2">&quot;state_steps&quot;</span> <span class="ow">in</span> <span class="n">kwargs</span>
                    <span class="ow">and</span> <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;state_steps&quot;</span><span class="p">]</span>
                    <span class="ow">and</span> <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;state_steps&quot;</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">is_cuda</span>
                <span class="p">)</span>
            <span class="p">):</span>
                <span class="k">return</span> <span class="n">disabled_func</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">func</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">maybe_fallback</span>

    <span class="k">return</span> <span class="n">wrapper</span>


<span class="c1"># For any optimizer with a faster implementation, we attempt to default to the</span>
<span class="c1"># fastest + stablest whenever possible. For foreach, the requirements are to have</span>
<span class="c1"># native params all on CUDA. For fused, there&#39;s currently the additional requirement</span>
<span class="c1"># that the tensors&#39; dtypes must be floating point. Neither alternative supports</span>
<span class="c1"># torch.jit.script nor differentiable, so we fall back to the single tensor</span>
<span class="c1"># implementation in those cases.</span>
<span class="k">def</span><span class="w"> </span><span class="nf">_default_to_fused_or_foreach</span><span class="p">(</span>
    <span class="n">params</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span> <span class="n">differentiable</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span> <span class="n">use_fused</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">bool</span><span class="p">,</span> <span class="nb">bool</span><span class="p">]:</span>
    <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">is_scripting</span><span class="p">()</span> <span class="ow">or</span> <span class="n">differentiable</span><span class="p">:</span>
        <span class="k">return</span> <span class="kc">False</span><span class="p">,</span> <span class="kc">False</span>

    <span class="n">fused_supported_devices</span> <span class="o">=</span> <span class="n">_get_fused_kernels_supported_devices</span><span class="p">()</span>
    <span class="n">foreach_supported_devices</span> <span class="o">=</span> <span class="n">_get_foreach_kernels_supported_devices</span><span class="p">()</span>
    <span class="n">fused</span> <span class="o">=</span> <span class="n">use_fused</span> <span class="ow">and</span> <span class="nb">all</span><span class="p">(</span>
        <span class="n">p</span> <span class="ow">is</span> <span class="kc">None</span>
        <span class="ow">or</span> <span class="p">(</span>
            <span class="nb">type</span><span class="p">(</span><span class="n">p</span><span class="p">)</span> <span class="ow">in</span> <span class="n">_foreach_supported_types</span>
            <span class="ow">and</span> <span class="n">p</span><span class="o">.</span><span class="n">device</span><span class="o">.</span><span class="n">type</span> <span class="ow">in</span> <span class="n">fused_supported_devices</span>
            <span class="ow">and</span> <span class="n">torch</span><span class="o">.</span><span class="n">is_floating_point</span><span class="p">(</span><span class="n">p</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">params</span>
    <span class="p">)</span>
    <span class="n">foreach</span> <span class="o">=</span> <span class="ow">not</span> <span class="n">fused</span> <span class="ow">and</span> <span class="nb">all</span><span class="p">(</span>
        <span class="n">p</span> <span class="ow">is</span> <span class="kc">None</span>
        <span class="ow">or</span> <span class="p">(</span>
            <span class="nb">type</span><span class="p">(</span><span class="n">p</span><span class="p">)</span> <span class="ow">in</span> <span class="n">_foreach_supported_types</span>
            <span class="ow">and</span> <span class="n">p</span><span class="o">.</span><span class="n">device</span><span class="o">.</span><span class="n">type</span> <span class="ow">in</span> <span class="n">foreach_supported_devices</span>
        <span class="p">)</span>
        <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">params</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">fused</span><span class="p">,</span> <span class="n">foreach</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_device_dtype_check_for_fused</span><span class="p">(</span>
    <span class="n">p</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">cuda_unsupported</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">fused_supported_devices</span> <span class="o">=</span> <span class="n">_get_fused_kernels_supported_devices</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">cuda_unsupported</span><span class="p">:</span>
        <span class="n">fused_supported_devices</span><span class="o">.</span><span class="n">remove</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">device</span><span class="o">.</span><span class="n">type</span> <span class="ow">in</span> <span class="n">fused_supported_devices</span> <span class="ow">and</span> <span class="n">torch</span><span class="o">.</span><span class="n">is_floating_point</span><span class="p">(</span><span class="n">p</span><span class="p">)):</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
            <span class="s2">&quot;`fused=True` requires all the params to be floating point Tensors of &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;supported devices: </span><span class="si">{</span><span class="n">fused_supported_devices</span><span class="si">}</span><span class="s2"> but </span><span class="si">{</span><span class="n">p</span><span class="o">.</span><span class="n">dtype</span><span class="si">}</span><span class="s2"> and </span><span class="si">{</span><span class="n">p</span><span class="o">.</span><span class="n">device</span><span class="o">.</span><span class="n">type</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="p">)</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_view_as_real</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="o">*</span><span class="n">state_and_grads</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">p</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">params</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">is_complex</span><span class="p">(</span><span class="n">p</span><span class="p">):</span>
            <span class="n">params</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">view_as_real</span><span class="p">(</span><span class="n">params</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
            <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">state_and_grads</span><span class="p">:</span>
                <span class="n">s</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">view_as_real</span><span class="p">(</span><span class="n">s</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_get_scalar_dtype</span><span class="p">(</span><span class="n">is_fused</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">is_fused</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">float32</span>
    <span class="k">return</span> <span class="p">(</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">float64</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">get_default_dtype</span><span class="p">()</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">float64</span> <span class="k">else</span> <span class="n">torch</span><span class="o">.</span><span class="n">float32</span>
    <span class="p">)</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_get_capturable_supported_devices</span><span class="p">(</span><span class="n">supports_xla</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Return the device type list that supports capturable optimizer.&quot;&quot;&quot;</span>
    <span class="n">capturable_supported_devices</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;cuda&quot;</span><span class="p">,</span> <span class="s2">&quot;xpu&quot;</span><span class="p">,</span> <span class="s2">&quot;hpu&quot;</span><span class="p">]</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">is_scripting</span><span class="p">():</span>
        <span class="n">capturable_supported_devices</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_get_privateuse1_backend_name</span><span class="p">())</span>
    <span class="k">if</span> <span class="n">supports_xla</span><span class="p">:</span>
        <span class="n">capturable_supported_devices</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">&quot;xla&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">capturable_supported_devices</span>


<span class="c1"># Common doc strings among optimizers</span>
<span class="n">_params_doc</span> <span class="o">=</span> <span class="sa">r</span><span class="s2">&quot;&quot;&quot;params (iterable): iterable of parameters or named_parameters to optimize</span>
<span class="s2">            or iterable of dicts defining parameter groups. When using named_parameters,</span>
<span class="s2">            all parameters in all groups should be named&quot;&quot;&quot;</span>

<span class="n">_foreach_doc</span> <span class="o">=</span> <span class="sa">r</span><span class="s2">&quot;&quot;&quot;foreach (bool, optional): whether foreach implementation of optimizer</span>
<span class="s2">            is used. If unspecified by the user (so foreach is None), we will try to use</span>
<span class="s2">            foreach over the for-loop implementation on CUDA, since it is usually</span>
<span class="s2">            significantly more performant. Note that the foreach implementation uses</span>
<span class="s2">            ~ sizeof(params) more peak memory than the for-loop version due to the intermediates</span>
<span class="s2">            being a tensorlist vs just one tensor. If memory is prohibitive, batch fewer</span>
<span class="s2">            parameters through the optimizer at a time or switch this flag to False (default: None)&quot;&quot;&quot;</span>

<span class="n">_fused_doc</span> <span class="o">=</span> <span class="sa">r</span><span class="s2">&quot;&quot;&quot;fused (bool, optional): whether the fused implementation is used.</span>
<span class="s2">            Currently, `torch.float64`, `torch.float32`, `torch.float16`, and `torch.bfloat16`</span>
<span class="s2">            are supported. (default: None)</span>

<span class="s2">    .. note:: The foreach and fused implementations are typically faster than the for-loop,</span>
<span class="s2">              single-tensor implementation, with fused being theoretically fastest with both</span>
<span class="s2">              vertical and horizontal fusion. As such, if the user has not specified either</span>
<span class="s2">              flag (i.e., when foreach = fused = None), we will attempt defaulting to the foreach</span>
<span class="s2">              implementation when the tensors are all on CUDA. Why not fused? Since the fused</span>
<span class="s2">              implementation is relatively new, we want to give it sufficient bake-in time.</span>
<span class="s2">              To specify fused, pass True for fused. To force running the for-loop</span>
<span class="s2">              implementation, pass False for either foreach or fused. &quot;&quot;&quot;</span>

<span class="n">_capturable_doc</span> <span class="o">=</span> <span class="sa">r</span><span class="s2">&quot;&quot;&quot;capturable (bool, optional): whether this instance is safe to</span>
<span class="s2">            capture in a CUDA graph. Passing True can impair ungraphed performance,</span>
<span class="s2">            so if you don&#39;t intend to graph capture this instance, leave it False</span>
<span class="s2">            (default: False)&quot;&quot;&quot;</span>

<span class="n">_differentiable_doc</span> <span class="o">=</span> <span class="sa">r</span><span class="s2">&quot;&quot;&quot;differentiable (bool, optional): whether autograd should</span>
<span class="s2">            occur through the optimizer step in training. Otherwise, the step()</span>
<span class="s2">            function runs in a torch.no_grad() context. Setting to True can impair</span>
<span class="s2">            performance, so leave it False if you don&#39;t intend to run autograd</span>
<span class="s2">            through this instance (default: False)&quot;&quot;&quot;</span>

<span class="n">_maximize_doc</span> <span class="o">=</span> <span class="sa">r</span><span class="s2">&quot;&quot;&quot;maximize (bool, optional): maximize the objective with respect to the</span>
<span class="s2">            params, instead of minimizing (default: False)&quot;&quot;&quot;</span>


<span class="k">def</span><span class="w"> </span><span class="nf">register_optimizer_step_pre_hook</span><span class="p">(</span><span class="n">hook</span><span class="p">:</span> <span class="n">GlobalOptimizerPreHook</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">RemovableHandle</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Register a pre hook common to all optimizers.</span>

<span class="sd">    The hook should have the following signature::</span>

<span class="sd">        hook(optimizer, args, kwargs) -&gt; None or modified args and kwargs</span>

<span class="sd">    Args:</span>
<span class="sd">        hook (Callable): A user defined hook which is registered on all optimizers.</span>

<span class="sd">    Returns:</span>
<span class="sd">        :class:`torch.utils.hooks.RemovableHandle`:</span>
<span class="sd">            a handle that can be used to remove the added hook by calling</span>
<span class="sd">            ``handle.remove()``</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">handle</span> <span class="o">=</span> <span class="n">hooks</span><span class="o">.</span><span class="n">RemovableHandle</span><span class="p">(</span><span class="n">_global_optimizer_pre_hooks</span><span class="p">)</span>
    <span class="n">_global_optimizer_pre_hooks</span><span class="p">[</span><span class="n">handle</span><span class="o">.</span><span class="n">id</span><span class="p">]</span> <span class="o">=</span> <span class="n">hook</span>
    <span class="k">return</span> <span class="n">handle</span>


<span class="k">def</span><span class="w"> </span><span class="nf">register_optimizer_step_post_hook</span><span class="p">(</span><span class="n">hook</span><span class="p">:</span> <span class="n">GlobalOptimizerPostHook</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">RemovableHandle</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Register a post hook common to all optimizers.</span>

<span class="sd">    The hook should have the following signature::</span>

<span class="sd">        hook(optimizer, args, kwargs) -&gt; None</span>

<span class="sd">    Args:</span>
<span class="sd">        hook (Callable): A user defined hook which is registered on all optimizers.</span>

<span class="sd">    Returns:</span>
<span class="sd">        :class:`torch.utils.hooks.RemovableHandle`:</span>
<span class="sd">            a handle that can be used to remove the added hook by calling</span>
<span class="sd">            ``handle.remove()``</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">handle</span> <span class="o">=</span> <span class="n">hooks</span><span class="o">.</span><span class="n">RemovableHandle</span><span class="p">(</span><span class="n">_global_optimizer_post_hooks</span><span class="p">)</span>
    <span class="n">_global_optimizer_post_hooks</span><span class="p">[</span><span class="n">handle</span><span class="o">.</span><span class="n">id</span><span class="p">]</span> <span class="o">=</span> <span class="n">hook</span>
    <span class="k">return</span> <span class="n">handle</span>


<span class="n">ParamsT</span><span class="p">:</span> <span class="n">TypeAlias</span> <span class="o">=</span> <span class="n">Union</span><span class="p">[</span>
    <span class="n">Iterable</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span> <span class="n">Iterable</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]],</span> <span class="n">Iterable</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]</span>
<span class="p">]</span>

<span class="n">_P</span> <span class="o">=</span> <span class="n">ParamSpec</span><span class="p">(</span><span class="s2">&quot;_P&quot;</span><span class="p">)</span>
<span class="n">R</span> <span class="o">=</span> <span class="n">TypeVar</span><span class="p">(</span><span class="s2">&quot;R&quot;</span><span class="p">)</span>
<span class="n">T</span> <span class="o">=</span> <span class="n">TypeVar</span><span class="p">(</span><span class="s2">&quot;T&quot;</span><span class="p">)</span>


<div class="viewcode-block" id="Optimizer">
<a class="viewcode-back" href="../../../python-api/optim.html#torch.optim.Optimizer">[docs]</a>
<span class="k">class</span><span class="w"> </span><span class="nc">Optimizer</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Base class for all optimizers.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        Parameters need to be specified as collections that have a deterministic</span>
<span class="sd">        ordering that is consistent between runs. Examples of objects that don&#39;t</span>
<span class="sd">        satisfy those properties are sets and iterators over values of dictionaries.</span>

<span class="sd">    Args:</span>
<span class="sd">        params (iterable): an iterable of :class:`torch.Tensor` s or</span>
<span class="sd">            :class:`dict` s. Specifies what Tensors should be optimized.</span>
<span class="sd">        defaults: (dict): a dict containing default values of optimization</span>
<span class="sd">            options (used when a parameter group doesn&#39;t specify them).</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">OptimizerPreHook</span><span class="p">:</span> <span class="n">TypeAlias</span> <span class="o">=</span> <span class="n">Callable</span><span class="p">[[</span><span class="n">Self</span><span class="p">,</span> <span class="n">Args</span><span class="p">,</span> <span class="n">Kwargs</span><span class="p">],</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">Args</span><span class="p">,</span> <span class="n">Kwargs</span><span class="p">]]]</span>  <span class="c1"># type: ignore[misc]</span>
    <span class="n">OptimizerPostHook</span><span class="p">:</span> <span class="n">TypeAlias</span> <span class="o">=</span> <span class="n">Callable</span><span class="p">[[</span><span class="n">Self</span><span class="p">,</span> <span class="n">Args</span><span class="p">,</span> <span class="n">Kwargs</span><span class="p">],</span> <span class="kc">None</span><span class="p">]</span>  <span class="c1"># type: ignore[misc]</span>

    <span class="n">_optimizer_step_pre_hooks</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">OptimizerPreHook</span><span class="p">]</span>
    <span class="n">_optimizer_step_post_hooks</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">OptimizerPostHook</span><span class="p">]</span>
    <span class="n">_optimizer_state_dict_pre_hooks</span><span class="p">:</span> <span class="s1">&#39;OrderedDict[int, Callable[[&quot;Optimizer&quot;], None]]&#39;</span>
    <span class="n">_optimizer_state_dict_post_hooks</span><span class="p">:</span> <span class="s1">&#39;OrderedDict[int, Callable[[&quot;Optimizer&quot;, StateDict], Optional[StateDict]]]&#39;</span>
    <span class="n">_optimizer_load_state_dict_pre_hooks</span><span class="p">:</span> <span class="s1">&#39;OrderedDict[int, Callable[[&quot;Optimizer&quot;, StateDict], Optional[StateDict]]]&#39;</span>
    <span class="n">_optimizer_load_state_dict_post_hooks</span><span class="p">:</span> <span class="s1">&#39;OrderedDict[int, Callable[[&quot;Optimizer&quot;], None]]&#39;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">:</span> <span class="n">ParamsT</span><span class="p">,</span> <span class="n">defaults</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>  <span class="c1"># noqa: D107</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_log_api_usage_once</span><span class="p">(</span><span class="s2">&quot;python.optimizer&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">defaults</span> <span class="o">=</span> <span class="n">defaults</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_optimizer_step_pre_hooks</span> <span class="o">=</span> <span class="n">OrderedDict</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_optimizer_step_post_hooks</span> <span class="o">=</span> <span class="n">OrderedDict</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_optimizer_state_dict_pre_hooks</span> <span class="o">=</span> <span class="n">OrderedDict</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_optimizer_state_dict_post_hooks</span> <span class="o">=</span> <span class="n">OrderedDict</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_optimizer_load_state_dict_pre_hooks</span> <span class="o">=</span> <span class="n">OrderedDict</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_optimizer_load_state_dict_post_hooks</span> <span class="o">=</span> <span class="n">OrderedDict</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_patch_step_function</span><span class="p">()</span>

        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span>
                <span class="s2">&quot;params argument given to the optimizer should be &quot;</span>
                <span class="s2">&quot;an iterable of Tensors or dicts, but got &quot;</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">typename</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
            <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="p">:</span> <span class="n">DefaultDict</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Any</span><span class="p">]</span> <span class="o">=</span> <span class="n">defaultdict</span><span class="p">(</span><span class="nb">dict</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">param_groups</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="n">param_groups</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">param_groups</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;optimizer got an empty parameter list&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">param_groups</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="nb">dict</span><span class="p">):</span>
            <span class="n">param_groups</span> <span class="o">=</span> <span class="p">[{</span><span class="s2">&quot;params&quot;</span><span class="p">:</span> <span class="n">param_groups</span><span class="p">}]</span>

        <span class="k">for</span> <span class="n">param_group</span> <span class="ow">in</span> <span class="n">param_groups</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">add_param_group</span><span class="p">(</span><span class="n">cast</span><span class="p">(</span><span class="nb">dict</span><span class="p">,</span> <span class="n">param_group</span><span class="p">))</span>

        <span class="c1"># Allows _cuda_graph_capture_health_check to rig a poor man&#39;s TORCH_WARN_ONCE in python,</span>
        <span class="c1"># which I don&#39;t think exists</span>
        <span class="c1"># https://github.com/pytorch/pytorch/issues/72948</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_warned_capturable_if_run_uncaptured</span> <span class="o">=</span> <span class="kc">True</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">__getstate__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>  <span class="c1"># noqa: D105</span>
        <span class="k">return</span> <span class="p">{</span>
            <span class="s2">&quot;defaults&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">defaults</span><span class="p">,</span>
            <span class="s2">&quot;state&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="p">,</span>
            <span class="s2">&quot;param_groups&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">param_groups</span><span class="p">,</span>
        <span class="p">}</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">__setstate__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>  <span class="c1"># noqa: D105</span>
        <span class="bp">self</span><span class="o">.</span><span class="vm">__dict__</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
        <span class="k">if</span> <span class="s2">&quot;_optimizer_step_pre_hooks&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="vm">__dict__</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_optimizer_step_pre_hooks</span> <span class="o">=</span> <span class="n">OrderedDict</span><span class="p">()</span>
        <span class="k">if</span> <span class="s2">&quot;_optimizer_step_post_hooks&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="vm">__dict__</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_optimizer_step_post_hooks</span> <span class="o">=</span> <span class="n">OrderedDict</span><span class="p">()</span>
        <span class="k">if</span> <span class="s2">&quot;_optimizer_state_dict_pre_hooks&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="vm">__dict__</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_optimizer_state_dict_pre_hooks</span> <span class="o">=</span> <span class="n">OrderedDict</span><span class="p">()</span>
        <span class="k">if</span> <span class="s2">&quot;_optimizer_state_dict_post_hooks&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="vm">__dict__</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_optimizer_state_dict_post_hooks</span> <span class="o">=</span> <span class="n">OrderedDict</span><span class="p">()</span>
        <span class="k">if</span> <span class="s2">&quot;_optimizer_load_state_dict_pre_hooks&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="vm">__dict__</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_optimizer_load_state_dict_pre_hooks</span> <span class="o">=</span> <span class="n">OrderedDict</span><span class="p">()</span>
        <span class="k">if</span> <span class="s2">&quot;_optimizer_load_state_dict_post_hooks&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="vm">__dict__</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_optimizer_load_state_dict_post_hooks</span> <span class="o">=</span> <span class="n">OrderedDict</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_patch_step_function</span><span class="p">()</span>  <span class="c1"># To support multiprocessing pickle/unpickle</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">defaults</span><span class="o">.</span><span class="n">setdefault</span><span class="p">(</span><span class="s2">&quot;differentiable&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>  <span class="c1"># noqa: D105</span>
        <span class="n">format_string</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span> <span class="o">+</span> <span class="s2">&quot; (&quot;</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">group</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">param_groups</span><span class="p">):</span>
            <span class="n">format_string</span> <span class="o">+=</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span>
            <span class="n">format_string</span> <span class="o">+=</span> <span class="sa">f</span><span class="s2">&quot;Parameter Group </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="se">\n</span><span class="s2">&quot;</span>
            <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">group</span><span class="o">.</span><span class="n">keys</span><span class="p">()):</span>
                <span class="k">if</span> <span class="n">key</span> <span class="o">!=</span> <span class="s2">&quot;params&quot;</span><span class="p">:</span>
                    <span class="n">format_string</span> <span class="o">+=</span> <span class="sa">f</span><span class="s2">&quot;    </span><span class="si">{</span><span class="n">key</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">group</span><span class="p">[</span><span class="n">key</span><span class="p">]</span><span class="si">}</span><span class="se">\n</span><span class="s2">&quot;</span>
        <span class="n">format_string</span> <span class="o">+=</span> <span class="s2">&quot;)&quot;</span>
        <span class="k">return</span> <span class="n">format_string</span>

    <span class="c1"># Currently needed by Adam and AdamW</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">_cuda_graph_capture_health_check</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="c1"># Note [torch.compile x capturable]</span>
        <span class="c1"># If we are compiling, we try to take the capturable path automatically by</span>
        <span class="c1"># setting the flag to True during tracing. Due to this, we skip all the checks</span>
        <span class="c1"># normally required for determining whether we can use CUDA graphs and</span>
        <span class="c1"># shunt the responsibility to torch.inductor. This saves time during tracing</span>
        <span class="c1"># since the checks are slow without sacrificing UX since inductor will warn</span>
        <span class="c1"># later if CUDA graphs cannot be enabled, e.g.,</span>
        <span class="c1"># https://github.com/pytorch/pytorch/blob/d3ba8901d8640eb16f88b2bfef9df7fa383d4b47/torch/_inductor/compile_fx.py#L390.</span>
        <span class="c1"># Thus, when compiling, inductor will determine if cudagraphs</span>
        <span class="c1"># can be enabled based on whether there is input mutation or CPU tensors.</span>
        <span class="k">if</span> <span class="p">(</span>
            <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">compiler</span><span class="o">.</span><span class="n">is_compiling</span><span class="p">()</span>
            <span class="ow">and</span> <span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_built</span><span class="p">()</span>
            <span class="ow">and</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span>
        <span class="p">):</span>
            <span class="n">capturing</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_current_stream_capturing</span><span class="p">()</span>

            <span class="k">if</span> <span class="n">capturing</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">all</span><span class="p">(</span>
                <span class="n">group</span><span class="p">[</span><span class="s2">&quot;capturable&quot;</span><span class="p">]</span> <span class="k">for</span> <span class="n">group</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">param_groups</span>
            <span class="p">):</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                    <span class="s2">&quot;Attempting CUDA graph capture of step() for an instance of &quot;</span>
                    <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span>
                    <span class="o">+</span> <span class="s2">&quot; but param_groups&#39; capturable is False.&quot;</span>
                <span class="p">)</span>

            <span class="k">if</span> <span class="p">(</span>
                <span class="p">(</span><span class="ow">not</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;_warned_capturable_if_run_uncaptured&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">))</span>
                <span class="ow">and</span> <span class="nb">all</span><span class="p">(</span><span class="n">group</span><span class="p">[</span><span class="s2">&quot;capturable&quot;</span><span class="p">]</span> <span class="k">for</span> <span class="n">group</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">param_groups</span><span class="p">)</span>
                <span class="ow">and</span> <span class="p">(</span><span class="ow">not</span> <span class="n">capturing</span><span class="p">)</span>
            <span class="p">):</span>
                <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
                    <span class="s2">&quot;This instance was constructed with capturable=True or some of all the param_groups came with capturable=True, &quot;</span>
                    <span class="s2">&quot;but step() is running without CUDA graph capture. If you never intend to graph-capture this &quot;</span>
                    <span class="s2">&quot;instance, capturable=True can impair performance, and you should set capturable=False.&quot;</span>
                <span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_warned_capturable_if_run_uncaptured</span> <span class="o">=</span> <span class="kc">True</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_optimizer_step_code</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Entry point for `torch.profile.profiler`.</span>

<span class="sd">        When python tracing is enabled the profiler will hook into this</span>
<span class="sd">        function at the CPython level to inspect the optimizer&#39;s parameters and</span>
<span class="sd">        param groups. It is called it after `step()` since many optimizers</span>
<span class="sd">        lazily initialize state.</span>

<span class="sd">        This is a workaround due to lack of a proper step hook on the optimizer,</span>
<span class="sd">        and will be removed if it exists.</span>
<span class="sd">        &quot;&quot;&quot;</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">profile_hook_step</span><span class="p">(</span><span class="n">func</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[</span><span class="n">_P</span><span class="p">,</span> <span class="n">R</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Callable</span><span class="p">[</span><span class="n">_P</span><span class="p">,</span> <span class="n">R</span><span class="p">]:</span>  <span class="c1"># noqa: D102</span>
        <span class="nd">@functools</span><span class="o">.</span><span class="n">wraps</span><span class="p">(</span><span class="n">func</span><span class="p">)</span>
        <span class="k">def</span><span class="w"> </span><span class="nf">wrapper</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">:</span> <span class="n">_P</span><span class="o">.</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">_P</span><span class="o">.</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">R</span><span class="p">:</span>
            <span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">_</span> <span class="o">=</span> <span class="n">args</span>
            <span class="bp">self</span> <span class="o">=</span> <span class="n">cast</span><span class="p">(</span><span class="n">Optimizer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span>
            <span class="n">profile_name</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;Optimizer.step#</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">.step&quot;</span>
            <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">profiler</span><span class="o">.</span><span class="n">record_function</span><span class="p">(</span><span class="n">profile_name</span><span class="p">):</span>
                <span class="c1"># call optimizer step pre hooks</span>
                <span class="k">for</span> <span class="n">pre_hook</span> <span class="ow">in</span> <span class="n">chain</span><span class="p">(</span>
                    <span class="n">_global_optimizer_pre_hooks</span><span class="o">.</span><span class="n">values</span><span class="p">(),</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_optimizer_step_pre_hooks</span><span class="o">.</span><span class="n">values</span><span class="p">(),</span>
                <span class="p">):</span>
                    <span class="n">result</span> <span class="o">=</span> <span class="n">pre_hook</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">)</span>
                    <span class="k">if</span> <span class="n">result</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">result</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">result</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
                            <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span> <span class="o">=</span> <span class="n">result</span>  <span class="c1"># type: ignore[assignment]</span>
                        <span class="k">else</span><span class="p">:</span>
                            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                                <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">func</span><span class="si">}</span><span class="s2"> must return None or a tuple of (new_args, new_kwargs), but got </span><span class="si">{</span><span class="n">result</span><span class="si">}</span><span class="s2">.&quot;</span>
                            <span class="p">)</span>

                <span class="n">out</span> <span class="o">=</span> <span class="n">func</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_optimizer_step_code</span><span class="p">()</span>

                <span class="c1"># call optimizer step post hooks</span>
                <span class="k">for</span> <span class="n">post_hook</span> <span class="ow">in</span> <span class="n">chain</span><span class="p">(</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_optimizer_step_post_hooks</span><span class="o">.</span><span class="n">values</span><span class="p">(),</span>
                    <span class="n">_global_optimizer_post_hooks</span><span class="o">.</span><span class="n">values</span><span class="p">(),</span>
                <span class="p">):</span>
                    <span class="n">post_hook</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">)</span>

                <span class="k">return</span> <span class="n">out</span>

        <span class="k">return</span> <span class="n">wrapper</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">_group_tensors_by_device_and_dtype</span><span class="p">(</span>
        <span class="n">tensorlistlist</span><span class="p">:</span> <span class="n">TensorListList</span><span class="p">,</span>
        <span class="n">with_indices</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span>
        <span class="n">Dict</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">],</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">TensorListList</span><span class="p">,</span> <span class="n">Indices</span><span class="p">]],</span>
        <span class="n">Dict</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span><span class="p">],</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">TensorListList</span><span class="p">,</span> <span class="n">Indices</span><span class="p">]],</span>
    <span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Group a list of lists of tensors by device and dtype.</span>

<span class="sd">        Skips this step if we are compiling since this will occur during inductor lowering.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">compiler</span><span class="o">.</span><span class="n">is_compiling</span><span class="p">():</span>
            <span class="k">return</span> <span class="p">{(</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">):</span> <span class="p">(</span><span class="n">tensorlistlist</span><span class="p">,</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">tensorlistlist</span><span class="p">[</span><span class="mi">0</span><span class="p">]))))}</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">_group_tensors_by_device_and_dtype</span><span class="p">(</span><span class="n">tensorlistlist</span><span class="p">,</span> <span class="n">with_indices</span><span class="p">)</span>  <span class="c1"># type: ignore[return-value, arg-type]</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_patch_step_function</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_zero_grad_profile_name</span> <span class="o">=</span> <span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;Optimizer.zero_grad#</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">.zero_grad&quot;</span>
        <span class="p">)</span>
        <span class="n">hooked</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="n">step</span><span class="p">,</span> <span class="s2">&quot;hooked&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">hooked</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="n">step</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">profile_hook_step</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="n">step</span><span class="p">)</span>  <span class="c1"># type: ignore[assignment]</span>
            <span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="n">step</span><span class="o">.</span><span class="n">hooked</span> <span class="o">=</span> <span class="kc">True</span>  <span class="c1"># type: ignore[attr-defined]</span>

<div class="viewcode-block" id="Optimizer.register_step_pre_hook">
<a class="viewcode-back" href="../../../python-api/generated/torch.optim.Optimizer.register_step_pre_hook.html#torch.optim.Optimizer.register_step_pre_hook">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">register_step_pre_hook</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hook</span><span class="p">:</span> <span class="n">OptimizerPreHook</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">RemovableHandle</span><span class="p">:</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Register an optimizer step pre hook which will be called before optimizer step.</span>

<span class="sd">        It should have the following signature::</span>

<span class="sd">            hook(optimizer, args, kwargs) -&gt; None or modified args and kwargs</span>

<span class="sd">        The ``optimizer`` argument is the optimizer instance being used. If</span>
<span class="sd">        args and kwargs are modified by the pre-hook, then the transformed</span>
<span class="sd">        values are returned as a tuple containing the new_args and new_kwargs.</span>

<span class="sd">        Args:</span>
<span class="sd">            hook (Callable): The user defined hook to be registered.</span>

<span class="sd">        Returns:</span>
<span class="sd">            :class:`torch.utils.hooks.RemovableHandle`:</span>
<span class="sd">                a handle that can be used to remove the added hook by calling</span>
<span class="sd">                ``handle.remove()``</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">handle</span> <span class="o">=</span> <span class="n">hooks</span><span class="o">.</span><span class="n">RemovableHandle</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_optimizer_step_pre_hooks</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_optimizer_step_pre_hooks</span><span class="p">[</span><span class="n">handle</span><span class="o">.</span><span class="n">id</span><span class="p">]</span> <span class="o">=</span> <span class="n">hook</span>
        <span class="k">return</span> <span class="n">handle</span></div>


<div class="viewcode-block" id="Optimizer.register_step_post_hook">
<a class="viewcode-back" href="../../../python-api/generated/torch.optim.Optimizer.register_step_post_hook.html#torch.optim.Optimizer.register_step_post_hook">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">register_step_post_hook</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hook</span><span class="p">:</span> <span class="n">OptimizerPostHook</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">RemovableHandle</span><span class="p">:</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Register an optimizer step post hook which will be called after optimizer step.</span>

<span class="sd">        It should have the following signature::</span>

<span class="sd">            hook(optimizer, args, kwargs) -&gt; None</span>

<span class="sd">        The ``optimizer`` argument is the optimizer instance being used.</span>

<span class="sd">        Args:</span>
<span class="sd">            hook (Callable): The user defined hook to be registered.</span>

<span class="sd">        Returns:</span>
<span class="sd">            :class:`torch.utils.hooks.RemovableHandle`:</span>
<span class="sd">                a handle that can be used to remove the added hook by calling</span>
<span class="sd">                ``handle.remove()``</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">handle</span> <span class="o">=</span> <span class="n">hooks</span><span class="o">.</span><span class="n">RemovableHandle</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_optimizer_step_post_hooks</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_optimizer_step_post_hooks</span><span class="p">[</span><span class="n">handle</span><span class="o">.</span><span class="n">id</span><span class="p">]</span> <span class="o">=</span> <span class="n">hook</span>
        <span class="k">return</span> <span class="n">handle</span></div>


<div class="viewcode-block" id="Optimizer.register_state_dict_pre_hook">
<a class="viewcode-back" href="../../../python-api/generated/torch.optim.Optimizer.register_state_dict_pre_hook.html#torch.optim.Optimizer.register_state_dict_pre_hook">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">register_state_dict_pre_hook</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">hook</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[[</span><span class="s2">&quot;Optimizer&quot;</span><span class="p">],</span> <span class="kc">None</span><span class="p">],</span> <span class="n">prepend</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">RemovableHandle</span><span class="p">:</span>  <span class="c1"># noqa: D101</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Register a state dict pre-hook which will be called before :meth:`~torch.optim.Optimizer.state_dict` is called.</span>

<span class="sd">        It should have the following signature::</span>

<span class="sd">            hook(optimizer) -&gt; None</span>

<span class="sd">        The ``optimizer`` argument is the optimizer instance being used.</span>
<span class="sd">        The hook will be called with argument ``self`` before calling ``state_dict`` on ``self``.</span>
<span class="sd">        The registered hook can be used to perform pre-processing before the ``state_dict``</span>
<span class="sd">        call is made.</span>

<span class="sd">        Args:</span>
<span class="sd">            hook (Callable): The user defined hook to be registered.</span>
<span class="sd">            prepend (bool): If True, the provided pre ``hook`` will be fired before</span>
<span class="sd">                all the already registered pre-hooks on ``state_dict``. Otherwise,</span>
<span class="sd">                the provided ``hook`` will be fired after all the already registered</span>
<span class="sd">                pre-hooks. (default: False)</span>

<span class="sd">        Returns:</span>
<span class="sd">            :class:`torch.utils.hooks.RemoveableHandle`:</span>
<span class="sd">                a handle that can be used to remove the added hook by calling</span>
<span class="sd">                ``handle.remove()``</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">handle</span> <span class="o">=</span> <span class="n">hooks</span><span class="o">.</span><span class="n">RemovableHandle</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_optimizer_state_dict_pre_hooks</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_optimizer_state_dict_pre_hooks</span><span class="p">[</span><span class="n">handle</span><span class="o">.</span><span class="n">id</span><span class="p">]</span> <span class="o">=</span> <span class="n">hook</span>
        <span class="k">if</span> <span class="n">prepend</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_optimizer_state_dict_pre_hooks</span><span class="o">.</span><span class="n">move_to_end</span><span class="p">(</span><span class="n">handle</span><span class="o">.</span><span class="n">id</span><span class="p">,</span> <span class="n">last</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">handle</span></div>


<div class="viewcode-block" id="Optimizer.register_state_dict_post_hook">
<a class="viewcode-back" href="../../../python-api/generated/torch.optim.Optimizer.register_state_dict_post_hook.html#torch.optim.Optimizer.register_state_dict_post_hook">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">register_state_dict_post_hook</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">hook</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[[</span><span class="s2">&quot;Optimizer&quot;</span><span class="p">,</span> <span class="n">StateDict</span><span class="p">],</span> <span class="n">Optional</span><span class="p">[</span><span class="n">StateDict</span><span class="p">]],</span>
        <span class="n">prepend</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">RemovableHandle</span><span class="p">:</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Register a state dict post-hook which will be called after :meth:`~torch.optim.Optimizer.state_dict` is called.</span>

<span class="sd">        It should have the following signature::</span>

<span class="sd">            hook(optimizer, state_dict) -&gt; state_dict or None</span>

<span class="sd">        The hook will be called with arguments ``self`` and ``state_dict`` after generating</span>
<span class="sd">        a ``state_dict`` on ``self``. The hook may modify the state_dict inplace or optionally</span>
<span class="sd">        return a new one. The registered hook can be used to perform post-processing</span>
<span class="sd">        on the ``state_dict`` before it is returned.</span>

<span class="sd">        Args:</span>
<span class="sd">            hook (Callable): The user defined hook to be registered.</span>
<span class="sd">            prepend (bool): If True, the provided post ``hook`` will be fired before</span>
<span class="sd">                all the already registered post-hooks on ``state_dict``. Otherwise,</span>
<span class="sd">                the provided ``hook`` will be fired after all the already registered</span>
<span class="sd">                post-hooks. (default: False)</span>

<span class="sd">        Returns:</span>
<span class="sd">            :class:`torch.utils.hooks.RemoveableHandle`:</span>
<span class="sd">                a handle that can be used to remove the added hook by calling</span>
<span class="sd">                ``handle.remove()``</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">handle</span> <span class="o">=</span> <span class="n">hooks</span><span class="o">.</span><span class="n">RemovableHandle</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_optimizer_state_dict_post_hooks</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_optimizer_state_dict_post_hooks</span><span class="p">[</span><span class="n">handle</span><span class="o">.</span><span class="n">id</span><span class="p">]</span> <span class="o">=</span> <span class="n">hook</span>
        <span class="k">if</span> <span class="n">prepend</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_optimizer_state_dict_post_hooks</span><span class="o">.</span><span class="n">move_to_end</span><span class="p">(</span><span class="n">handle</span><span class="o">.</span><span class="n">id</span><span class="p">,</span> <span class="n">last</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">handle</span></div>


<div class="viewcode-block" id="Optimizer.state_dict">
<a class="viewcode-back" href="../../../python-api/generated/torch.optim.Optimizer.state_dict.html#torch.optim.Optimizer.state_dict">[docs]</a>
    <span class="nd">@torch</span><span class="o">.</span><span class="n">_disable_dynamo</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">state_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">StateDict</span><span class="p">:</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Return the state of the optimizer as a :class:`dict`.</span>

<span class="sd">        It contains two entries:</span>

<span class="sd">        * ``state``: a Dict holding current optimization state. Its content</span>
<span class="sd">            differs between optimizer classes, but some common characteristics</span>
<span class="sd">            hold. For example, state is saved per parameter, and the parameter</span>
<span class="sd">            itself is NOT saved. ``state`` is a Dictionary mapping parameter ids</span>
<span class="sd">            to a Dict with state corresponding to each parameter.</span>
<span class="sd">        * ``param_groups``: a List containing all parameter groups where each</span>
<span class="sd">            parameter group is a Dict. Each parameter group contains metadata</span>
<span class="sd">            specific to the optimizer, such as learning rate and weight decay,</span>
<span class="sd">            as well as a List of parameter IDs of the parameters in the group.</span>
<span class="sd">            If a param group was initialized with ``named_parameters()`` the names</span>
<span class="sd">            content will also be saved in the state dict.</span>

<span class="sd">        NOTE: The parameter IDs may look like indices but they are just IDs</span>
<span class="sd">        associating state with param_group. When loading from a state_dict,</span>
<span class="sd">        the optimizer will zip the param_group ``params`` (int IDs) and the</span>
<span class="sd">        optimizer ``param_groups`` (actual ``nn.Parameter`` s) in order to</span>
<span class="sd">        match state WITHOUT additional verification.</span>

<span class="sd">        A returned state dict might look something like:</span>

<span class="sd">        .. code-block:: text</span>

<span class="sd">            {</span>
<span class="sd">                &#39;state&#39;: {</span>
<span class="sd">                    0: {&#39;momentum_buffer&#39;: tensor(...), ...},</span>
<span class="sd">                    1: {&#39;momentum_buffer&#39;: tensor(...), ...},</span>
<span class="sd">                    2: {&#39;momentum_buffer&#39;: tensor(...), ...},</span>
<span class="sd">                    3: {&#39;momentum_buffer&#39;: tensor(...), ...}</span>
<span class="sd">                },</span>
<span class="sd">                &#39;param_groups&#39;: [</span>
<span class="sd">                    {</span>
<span class="sd">                        &#39;lr&#39;: 0.01,</span>
<span class="sd">                        &#39;weight_decay&#39;: 0,</span>
<span class="sd">                        ...</span>
<span class="sd">                        &#39;params&#39;: [0]</span>
<span class="sd">                        &#39;param_names&#39; [&#39;param0&#39;]  (optional)</span>
<span class="sd">                    },</span>
<span class="sd">                    {</span>
<span class="sd">                        &#39;lr&#39;: 0.001,</span>
<span class="sd">                        &#39;weight_decay&#39;: 0.5,</span>
<span class="sd">                        ...</span>
<span class="sd">                        &#39;params&#39;: [1, 2, 3]</span>
<span class="sd">                        &#39;param_names&#39;: [&#39;param1&#39;, &#39;layer.weight&#39;, &#39;layer.bias&#39;] (optional)</span>
<span class="sd">                    }</span>
<span class="sd">                ]</span>
<span class="sd">            }</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">for</span> <span class="n">pre_hook</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_optimizer_state_dict_pre_hooks</span><span class="o">.</span><span class="n">values</span><span class="p">():</span>
            <span class="n">pre_hook</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>

        <span class="c1"># Save order indices instead of Tensors</span>
        <span class="n">param_mappings</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="n">start_index</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="k">def</span><span class="w"> </span><span class="nf">pack_group</span><span class="p">(</span><span class="n">group</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
            <span class="k">nonlocal</span> <span class="n">start_index</span>
            <span class="n">packed</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">v</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">group</span><span class="o">.</span><span class="n">items</span><span class="p">()</span> <span class="k">if</span> <span class="n">k</span> <span class="o">!=</span> <span class="s2">&quot;params&quot;</span><span class="p">}</span>
            <span class="n">param_mappings</span><span class="o">.</span><span class="n">update</span><span class="p">(</span>
                <span class="p">{</span>
                    <span class="nb">id</span><span class="p">(</span><span class="n">p</span><span class="p">):</span> <span class="n">i</span>
                    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">p</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">group</span><span class="p">[</span><span class="s2">&quot;params&quot;</span><span class="p">],</span> <span class="n">start_index</span><span class="p">)</span>
                    <span class="k">if</span> <span class="nb">id</span><span class="p">(</span><span class="n">p</span><span class="p">)</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">param_mappings</span>
                <span class="p">}</span>
            <span class="p">)</span>
            <span class="n">packed</span><span class="p">[</span><span class="s2">&quot;params&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="n">param_mappings</span><span class="p">[</span><span class="nb">id</span><span class="p">(</span><span class="n">p</span><span class="p">)]</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">group</span><span class="p">[</span><span class="s2">&quot;params&quot;</span><span class="p">]]</span>
            <span class="n">start_index</span> <span class="o">+=</span> <span class="nb">len</span><span class="p">(</span><span class="n">packed</span><span class="p">[</span><span class="s2">&quot;params&quot;</span><span class="p">])</span>
            <span class="k">return</span> <span class="n">packed</span>

        <span class="n">param_groups</span> <span class="o">=</span> <span class="p">[</span><span class="n">pack_group</span><span class="p">(</span><span class="n">g</span><span class="p">)</span> <span class="k">for</span> <span class="n">g</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">param_groups</span><span class="p">]</span>
        <span class="c1"># Remap state to use order indices as keys</span>
        <span class="n">packed_state</span> <span class="o">=</span> <span class="p">{</span>
            <span class="p">(</span><span class="n">param_mappings</span><span class="p">[</span><span class="nb">id</span><span class="p">(</span><span class="n">k</span><span class="p">)]</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="k">else</span> <span class="n">k</span><span class="p">):</span> <span class="n">v</span>
            <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">items</span><span class="p">()</span>
        <span class="p">}</span>

        <span class="n">state_dict</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">&quot;state&quot;</span><span class="p">:</span> <span class="n">packed_state</span><span class="p">,</span>
            <span class="s2">&quot;param_groups&quot;</span><span class="p">:</span> <span class="n">param_groups</span><span class="p">,</span>
        <span class="p">}</span>

        <span class="k">for</span> <span class="n">post_hook</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_optimizer_state_dict_post_hooks</span><span class="o">.</span><span class="n">values</span><span class="p">():</span>
            <span class="n">hook_result</span> <span class="o">=</span> <span class="n">post_hook</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state_dict</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">hook_result</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">state_dict</span> <span class="o">=</span> <span class="n">hook_result</span>
        <span class="k">return</span> <span class="n">state_dict</span></div>


    <span class="nd">@staticmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">_process_value_according_to_param_policy</span><span class="p">(</span>
        <span class="n">param</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">value</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">param_id</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">param_groups</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="n">Any</span><span class="p">,</span> <span class="n">Any</span><span class="p">]],</span>
        <span class="n">key</span><span class="p">:</span> <span class="n">Hashable</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="c1"># Floating-point types are a bit special here. They are the only ones</span>
        <span class="c1"># that are assumed to always match the type of params.</span>
        <span class="c1"># Make sure state[&#39;step&#39;] is not casted https://github.com/pytorch/pytorch/issues/74424</span>
        <span class="c1"># UNLESS fused or capturable, see note [special device hosting for step]</span>
        <span class="n">fused</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="n">capturable</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="k">assert</span> <span class="n">param_groups</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="k">for</span> <span class="n">pg</span> <span class="ow">in</span> <span class="n">param_groups</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">param_id</span> <span class="ow">in</span> <span class="n">pg</span><span class="p">[</span><span class="s2">&quot;params&quot;</span><span class="p">]:</span>
                <span class="n">fused</span> <span class="o">=</span> <span class="n">pg</span><span class="p">[</span><span class="s2">&quot;fused&quot;</span><span class="p">]</span> <span class="k">if</span> <span class="s2">&quot;fused&quot;</span> <span class="ow">in</span> <span class="n">pg</span> <span class="k">else</span> <span class="kc">False</span>
                <span class="n">capturable</span> <span class="o">=</span> <span class="n">pg</span><span class="p">[</span><span class="s2">&quot;capturable&quot;</span><span class="p">]</span> <span class="k">if</span> <span class="s2">&quot;capturable&quot;</span> <span class="ow">in</span> <span class="n">pg</span> <span class="k">else</span> <span class="kc">False</span>
                <span class="k">break</span>
        <span class="k">if</span> <span class="n">key</span> <span class="o">==</span> <span class="s2">&quot;step&quot;</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">capturable</span> <span class="ow">or</span> <span class="n">fused</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">value</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">param</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">value</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">param</span><span class="o">.</span><span class="n">is_floating_point</span><span class="p">():</span>
                <span class="k">return</span> <span class="n">value</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">param</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">param</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">value</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">param</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

<div class="viewcode-block" id="Optimizer.register_load_state_dict_pre_hook">
<a class="viewcode-back" href="../../../python-api/generated/torch.optim.Optimizer.register_load_state_dict_pre_hook.html#torch.optim.Optimizer.register_load_state_dict_pre_hook">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">register_load_state_dict_pre_hook</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">hook</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[[</span><span class="s2">&quot;Optimizer&quot;</span><span class="p">,</span> <span class="n">StateDict</span><span class="p">],</span> <span class="n">Optional</span><span class="p">[</span><span class="n">StateDict</span><span class="p">]],</span>
        <span class="n">prepend</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">RemovableHandle</span><span class="p">:</span>  <span class="c1"># noqa: D205 D400</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Register a load_state_dict pre-hook which will be called before</span>
<span class="sd">        :meth:`~torch.optim.Optimizer.load_state_dict` is called. It should have the</span>
<span class="sd">        following signature::</span>

<span class="sd">            hook(optimizer, state_dict) -&gt; state_dict or None</span>

<span class="sd">        The ``optimizer`` argument is the optimizer instance being used and the</span>
<span class="sd">        ``state_dict`` argument is a shallow copy of the ``state_dict`` the user</span>
<span class="sd">        passed in to ``load_state_dict``. The hook may modify the state_dict inplace</span>
<span class="sd">        or optionally return a new one. If a state_dict is returned, it will be used</span>
<span class="sd">        to be loaded into the optimizer.</span>

<span class="sd">        The hook will be called with argument ``self`` and ``state_dict`` before</span>
<span class="sd">        calling ``load_state_dict`` on ``self``. The registered hook can be used to</span>
<span class="sd">        perform pre-processing before the ``load_state_dict`` call is made.</span>

<span class="sd">        Args:</span>
<span class="sd">            hook (Callable): The user defined hook to be registered.</span>
<span class="sd">            prepend (bool): If True, the provided pre ``hook`` will be fired before</span>
<span class="sd">                all the already registered pre-hooks on ``load_state_dict``. Otherwise,</span>
<span class="sd">                the provided ``hook`` will be fired after all the already registered</span>
<span class="sd">                pre-hooks. (default: False)</span>

<span class="sd">        Returns:</span>
<span class="sd">            :class:`torch.utils.hooks.RemoveableHandle`:</span>
<span class="sd">                a handle that can be used to remove the added hook by calling</span>
<span class="sd">                ``handle.remove()``</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">handle</span> <span class="o">=</span> <span class="n">hooks</span><span class="o">.</span><span class="n">RemovableHandle</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_optimizer_load_state_dict_pre_hooks</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_optimizer_load_state_dict_pre_hooks</span><span class="p">[</span><span class="n">handle</span><span class="o">.</span><span class="n">id</span><span class="p">]</span> <span class="o">=</span> <span class="n">hook</span>
        <span class="k">if</span> <span class="n">prepend</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_optimizer_load_state_dict_pre_hooks</span><span class="o">.</span><span class="n">move_to_end</span><span class="p">(</span><span class="n">handle</span><span class="o">.</span><span class="n">id</span><span class="p">,</span> <span class="n">last</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">handle</span></div>


<div class="viewcode-block" id="Optimizer.register_load_state_dict_post_hook">
<a class="viewcode-back" href="../../../python-api/generated/torch.optim.Optimizer.register_load_state_dict_post_hook.html#torch.optim.Optimizer.register_load_state_dict_post_hook">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">register_load_state_dict_post_hook</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">hook</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[[</span><span class="s2">&quot;Optimizer&quot;</span><span class="p">],</span> <span class="kc">None</span><span class="p">],</span> <span class="n">prepend</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">RemovableHandle</span><span class="p">:</span>  <span class="c1"># noqa: D205 D400</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Register a load_state_dict post-hook which will be called after</span>
<span class="sd">        :meth:`~torch.optim.Optimizer.load_state_dict` is called. It should have the</span>
<span class="sd">        following signature::</span>

<span class="sd">            hook(optimizer) -&gt; None</span>

<span class="sd">        The ``optimizer`` argument is the optimizer instance being used.</span>

<span class="sd">        The hook will be called with argument ``self`` after calling</span>
<span class="sd">        ``load_state_dict`` on ``self``. The registered hook can be used to</span>
<span class="sd">        perform post-processing after ``load_state_dict`` has loaded the</span>
<span class="sd">        ``state_dict``.</span>

<span class="sd">        Args:</span>
<span class="sd">            hook (Callable): The user defined hook to be registered.</span>
<span class="sd">            prepend (bool): If True, the provided post ``hook`` will be fired before</span>
<span class="sd">                all the already registered post-hooks on ``load_state_dict``. Otherwise,</span>
<span class="sd">                the provided ``hook`` will be fired after all the already registered</span>
<span class="sd">                post-hooks. (default: False)</span>

<span class="sd">        Returns:</span>
<span class="sd">            :class:`torch.utils.hooks.RemoveableHandle`:</span>
<span class="sd">                a handle that can be used to remove the added hook by calling</span>
<span class="sd">                ``handle.remove()``</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">handle</span> <span class="o">=</span> <span class="n">hooks</span><span class="o">.</span><span class="n">RemovableHandle</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_optimizer_load_state_dict_post_hooks</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_optimizer_load_state_dict_post_hooks</span><span class="p">[</span><span class="n">handle</span><span class="o">.</span><span class="n">id</span><span class="p">]</span> <span class="o">=</span> <span class="n">hook</span>
        <span class="k">if</span> <span class="n">prepend</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_optimizer_load_state_dict_post_hooks</span><span class="o">.</span><span class="n">move_to_end</span><span class="p">(</span><span class="n">handle</span><span class="o">.</span><span class="n">id</span><span class="p">,</span> <span class="n">last</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>  <span class="c1"># type: ignore[attr-defined]</span>
        <span class="k">return</span> <span class="n">handle</span></div>


<div class="viewcode-block" id="Optimizer.load_state_dict">
<a class="viewcode-back" href="../../../python-api/generated/torch.optim.Optimizer.load_state_dict.html#torch.optim.Optimizer.load_state_dict">[docs]</a>
    <span class="nd">@torch</span><span class="o">.</span><span class="n">_disable_dynamo</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">load_state_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state_dict</span><span class="p">:</span> <span class="n">StateDict</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Load the optimizer state.</span>

<span class="sd">        Args:</span>
<span class="sd">            state_dict (dict): optimizer state. Should be an object returned</span>
<span class="sd">                from a call to :meth:`state_dict`.</span>

<span class="sd">        .. note::</span>
<span class="sd">            The names of the parameters (if they exist under the &quot;param_names&quot; key of each param group</span>
<span class="sd">            in :meth:`state_dict`) will not affect the loading process.</span>
<span class="sd">            To use the parameters&#39; names for custom cases (such as when the parameters in the loaded state dict</span>
<span class="sd">            differ from those initialized in the optimizer),</span>
<span class="sd">            a custom ``register_load_state_dict_pre_hook`` should be implemented to adapt the loaded dict</span>
<span class="sd">            accordingly.</span>
<span class="sd">            If ``param_names`` exist in loaded state dict ``param_groups`` they will be saved and override</span>
<span class="sd">            the current names, if present, in the optimizer state. If they do not exist in loaded state dict,</span>
<span class="sd">            the optimizer ``param_names`` will remain unchanged.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># shallow copy, to be consistent with module API</span>
        <span class="n">state_dict</span> <span class="o">=</span> <span class="n">state_dict</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>

        <span class="k">for</span> <span class="n">pre_hook</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_optimizer_load_state_dict_pre_hooks</span><span class="o">.</span><span class="n">values</span><span class="p">():</span>
            <span class="n">hook_result</span> <span class="o">=</span> <span class="n">pre_hook</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state_dict</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">hook_result</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">state_dict</span> <span class="o">=</span> <span class="n">hook_result</span>

        <span class="c1"># Validate the state_dict</span>
        <span class="n">groups</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">param_groups</span>

        <span class="c1"># Deepcopy as we write into saved_groups later to update state</span>
        <span class="n">saved_groups</span> <span class="o">=</span> <span class="n">deepcopy</span><span class="p">(</span><span class="n">state_dict</span><span class="p">[</span><span class="s2">&quot;param_groups&quot;</span><span class="p">])</span>

        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">groups</span><span class="p">)</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="n">saved_groups</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;loaded state dict has a different number of &quot;</span> <span class="s2">&quot;parameter groups&quot;</span>
            <span class="p">)</span>
        <span class="n">param_lens</span> <span class="o">=</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">g</span><span class="p">[</span><span class="s2">&quot;params&quot;</span><span class="p">])</span> <span class="k">for</span> <span class="n">g</span> <span class="ow">in</span> <span class="n">groups</span><span class="p">)</span>
        <span class="n">saved_lens</span> <span class="o">=</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">g</span><span class="p">[</span><span class="s2">&quot;params&quot;</span><span class="p">])</span> <span class="k">for</span> <span class="n">g</span> <span class="ow">in</span> <span class="n">saved_groups</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">any</span><span class="p">(</span><span class="n">p_len</span> <span class="o">!=</span> <span class="n">s_len</span> <span class="k">for</span> <span class="n">p_len</span><span class="p">,</span> <span class="n">s_len</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">param_lens</span><span class="p">,</span> <span class="n">saved_lens</span><span class="p">)):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;loaded state dict contains a parameter group &quot;</span>
                <span class="s2">&quot;that doesn&#39;t match the size of optimizer&#39;s group&quot;</span>
            <span class="p">)</span>

        <span class="c1"># Update the state</span>
        <span class="n">id_map</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span>
            <span class="nb">zip</span><span class="p">(</span>
                <span class="n">chain</span><span class="o">.</span><span class="n">from_iterable</span><span class="p">(</span><span class="n">g</span><span class="p">[</span><span class="s2">&quot;params&quot;</span><span class="p">]</span> <span class="k">for</span> <span class="n">g</span> <span class="ow">in</span> <span class="n">saved_groups</span><span class="p">),</span>
                <span class="n">chain</span><span class="o">.</span><span class="n">from_iterable</span><span class="p">(</span><span class="n">g</span><span class="p">[</span><span class="s2">&quot;params&quot;</span><span class="p">]</span> <span class="k">for</span> <span class="n">g</span> <span class="ow">in</span> <span class="n">groups</span><span class="p">),</span>
            <span class="p">)</span>
        <span class="p">)</span>

        <span class="k">def</span><span class="w"> </span><span class="nf">_cast</span><span class="p">(</span><span class="n">param</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">param_id</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">param_groups</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">            </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Make a deep copy of value, casting all tensors to device of param.&quot;&quot;&quot;</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
                <span class="k">return</span> <span class="n">Optimizer</span><span class="o">.</span><span class="n">_process_value_according_to_param_policy</span><span class="p">(</span>
                    <span class="n">param</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">param_id</span><span class="p">,</span> <span class="n">param_groups</span><span class="p">,</span> <span class="n">key</span>
                <span class="p">)</span>
            <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
                <span class="k">return</span> <span class="p">{</span>
                    <span class="n">k</span><span class="p">:</span> <span class="n">_cast</span><span class="p">(</span>
                        <span class="n">param</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">param_id</span><span class="o">=</span><span class="n">param_id</span><span class="p">,</span> <span class="n">param_groups</span><span class="o">=</span><span class="n">param_groups</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="n">k</span>
                    <span class="p">)</span>
                    <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">value</span><span class="o">.</span><span class="n">items</span><span class="p">()</span>
                <span class="p">}</span>
            <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">Iterable</span><span class="p">):</span>
                <span class="k">return</span> <span class="nb">type</span><span class="p">(</span><span class="n">value</span><span class="p">)(</span><span class="n">_cast</span><span class="p">(</span><span class="n">param</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">param_id</span><span class="o">=</span><span class="n">param_id</span><span class="p">,</span> <span class="n">param_groups</span><span class="o">=</span><span class="n">param_groups</span><span class="p">)</span> <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">value</span><span class="p">)</span>  <span class="c1"># type: ignore[call-arg]</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">value</span>

        <span class="c1"># Copy state assigned to params (and cast tensors to appropriate types).</span>
        <span class="c1"># State that is not assigned to params is copied as is (needed for</span>
        <span class="c1"># backward compatibility).</span>
        <span class="n">state</span><span class="p">:</span> <span class="n">DefaultDict</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="n">Any</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]</span> <span class="o">=</span> <span class="n">defaultdict</span><span class="p">(</span><span class="nb">dict</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="p">[</span><span class="s2">&quot;state&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="k">if</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">id_map</span><span class="p">:</span>
                <span class="n">param</span> <span class="o">=</span> <span class="n">id_map</span><span class="p">[</span><span class="n">k</span><span class="p">]</span>
                <span class="n">state</span><span class="p">[</span><span class="n">param</span><span class="p">]</span> <span class="o">=</span> <span class="n">_cast</span><span class="p">(</span>
                    <span class="n">param</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">param_id</span><span class="o">=</span><span class="n">k</span><span class="p">,</span> <span class="n">param_groups</span><span class="o">=</span><span class="n">state_dict</span><span class="p">[</span><span class="s2">&quot;param_groups&quot;</span><span class="p">]</span>
                <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">state</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">v</span>

        <span class="c1"># Update parameter groups, setting their &#39;params&#39; value</span>
        <span class="k">def</span><span class="w"> </span><span class="nf">update_group</span><span class="p">(</span>
            <span class="n">group</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">],</span> <span class="n">new_group</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]</span>
        <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
            <span class="n">new_group</span><span class="p">[</span><span class="s2">&quot;params&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">group</span><span class="p">[</span><span class="s2">&quot;params&quot;</span><span class="p">]</span>
            <span class="k">if</span> <span class="s2">&quot;param_names&quot;</span> <span class="ow">in</span> <span class="n">group</span> <span class="ow">and</span> <span class="s2">&quot;param_names&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">new_group</span><span class="p">:</span>
                <span class="n">new_group</span><span class="p">[</span><span class="s2">&quot;param_names&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">group</span><span class="p">[</span><span class="s2">&quot;param_names&quot;</span><span class="p">]</span>
            <span class="k">return</span> <span class="n">new_group</span>

        <span class="n">param_groups</span> <span class="o">=</span> <span class="p">[</span><span class="n">update_group</span><span class="p">(</span><span class="n">g</span><span class="p">,</span> <span class="n">ng</span><span class="p">)</span> <span class="k">for</span> <span class="n">g</span><span class="p">,</span> <span class="n">ng</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">groups</span><span class="p">,</span> <span class="n">saved_groups</span><span class="p">)]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">__setstate__</span><span class="p">({</span><span class="s2">&quot;state&quot;</span><span class="p">:</span> <span class="n">state</span><span class="p">,</span> <span class="s2">&quot;param_groups&quot;</span><span class="p">:</span> <span class="n">param_groups</span><span class="p">})</span>

        <span class="k">for</span> <span class="n">post_hook</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_optimizer_load_state_dict_post_hooks</span><span class="o">.</span><span class="n">values</span><span class="p">():</span>
            <span class="n">post_hook</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span></div>


<div class="viewcode-block" id="Optimizer.zero_grad">
<a class="viewcode-back" href="../../../python-api/generated/torch.optim.Optimizer.zero_grad.html#torch.optim.Optimizer.zero_grad">[docs]</a>
    <span class="nd">@torch</span><span class="o">.</span><span class="n">_disable_dynamo</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">zero_grad</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">set_to_none</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Reset the gradients of all optimized :class:`torch.Tensor` s.</span>

<span class="sd">        Args:</span>
<span class="sd">            set_to_none (bool): instead of setting to zero, set the grads to None.</span>
<span class="sd">                This will in general have lower memory footprint, and can modestly improve performance.</span>
<span class="sd">                However, it changes certain behaviors. For example:</span>
<span class="sd">                1. When the user tries to access a gradient and perform manual ops on it,</span>
<span class="sd">                a None attribute or a Tensor full of 0s will behave differently.</span>
<span class="sd">                2. If the user requests ``zero_grad(set_to_none=True)`` followed by a backward pass, ``.grad``\ s</span>
<span class="sd">                are guaranteed to be None for params that did not receive a gradient.</span>
<span class="sd">                3. ``torch.optim`` optimizers have a different behavior if the gradient is 0 or None</span>
<span class="sd">                (in one case it does the step with a gradient of 0 and in the other it skips</span>
<span class="sd">                the step altogether).</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">foreach</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">defaults</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;foreach&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">defaults</span><span class="o">.</span><span class="n">get</span><span class="p">(</span>
            <span class="s2">&quot;fused&quot;</span><span class="p">,</span> <span class="kc">False</span>
        <span class="p">)</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;_zero_grad_profile_name&quot;</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_patch_step_function</span><span class="p">()</span>

        <span class="n">per_device_and_dtype_grads</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span>
            <span class="n">DefaultDict</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="n">DefaultDict</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]]</span>
        <span class="p">]</span>
        <span class="k">if</span> <span class="n">foreach</span><span class="p">:</span>
            <span class="n">per_device_and_dtype_grads</span> <span class="o">=</span> <span class="n">defaultdict</span><span class="p">(</span><span class="k">lambda</span><span class="p">:</span> <span class="n">defaultdict</span><span class="p">(</span><span class="nb">list</span><span class="p">))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">per_device_and_dtype_grads</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">profiler</span><span class="o">.</span><span class="n">record_function</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_zero_grad_profile_name</span><span class="p">):</span>
            <span class="k">for</span> <span class="n">group</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">param_groups</span><span class="p">:</span>
                <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">group</span><span class="p">[</span><span class="s2">&quot;params&quot;</span><span class="p">]:</span>
                    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                        <span class="k">if</span> <span class="n">set_to_none</span><span class="p">:</span>
                            <span class="n">p</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="kc">None</span>
                        <span class="k">else</span><span class="p">:</span>
                            <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">grad_fn</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                                <span class="n">p</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">detach_</span><span class="p">()</span>
                            <span class="k">else</span><span class="p">:</span>
                                <span class="n">p</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
                            <span class="k">if</span> <span class="ow">not</span> <span class="n">foreach</span> <span class="ow">or</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">is_sparse</span><span class="p">:</span>
                                <span class="n">p</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">zero_</span><span class="p">()</span>
                            <span class="k">else</span><span class="p">:</span>
                                <span class="k">assert</span> <span class="n">per_device_and_dtype_grads</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
                                <span class="n">per_device_and_dtype_grads</span><span class="p">[</span><span class="n">p</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">device</span><span class="p">][</span>
                                    <span class="n">p</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">dtype</span>
                                <span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">foreach</span><span class="p">:</span>
                <span class="k">assert</span> <span class="n">per_device_and_dtype_grads</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
                <span class="k">for</span> <span class="n">per_dtype_grads</span> <span class="ow">in</span> <span class="n">per_device_and_dtype_grads</span><span class="o">.</span><span class="n">values</span><span class="p">():</span>
                    <span class="k">for</span> <span class="n">grads</span> <span class="ow">in</span> <span class="n">per_dtype_grads</span><span class="o">.</span><span class="n">values</span><span class="p">():</span>
                        <span class="n">torch</span><span class="o">.</span><span class="n">_foreach_zero_</span><span class="p">(</span><span class="n">grads</span><span class="p">)</span></div>


    <span class="nd">@overload</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">closure</span><span class="p">:</span> <span class="kc">None</span> <span class="o">=</span> <span class="o">...</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="o">...</span>

    <span class="nd">@overload</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">closure</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[[],</span> <span class="nb">float</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
        <span class="o">...</span>

<div class="viewcode-block" id="Optimizer.step">
<a class="viewcode-back" href="../../../python-api/generated/torch.optim.Optimizer.step.html#torch.optim.Optimizer.step">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">closure</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Callable</span><span class="p">[[],</span> <span class="nb">float</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]:</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Perform a single optimization step to update parameter.</span>

<span class="sd">        Args:</span>
<span class="sd">            closure (Callable): A closure that reevaluates the model and</span>
<span class="sd">                returns the loss. Optional for most optimizers.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span></div>


<div class="viewcode-block" id="Optimizer.add_param_group">
<a class="viewcode-back" href="../../../python-api/generated/torch.optim.Optimizer.add_param_group.html#torch.optim.Optimizer.add_param_group">[docs]</a>
    <span class="nd">@torch</span><span class="o">.</span><span class="n">_disable_dynamo</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">add_param_group</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">param_group</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Add a param group to the :class:`Optimizer` s `param_groups`.</span>

<span class="sd">        This can be useful when fine tuning a pre-trained network as frozen layers can be made</span>
<span class="sd">        trainable and added to the :class:`Optimizer` as training progresses.</span>

<span class="sd">        Args:</span>
<span class="sd">            param_group (dict): Specifies what Tensors should be optimized along with group</span>
<span class="sd">                specific optimization options.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">param_group</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;param_group must be a dict, but got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">param_group</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

        <span class="n">params</span> <span class="o">=</span> <span class="n">param_group</span><span class="p">[</span><span class="s2">&quot;params&quot;</span><span class="p">]</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
            <span class="n">param_group</span><span class="p">[</span><span class="s2">&quot;params&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="n">params</span><span class="p">]</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="nb">set</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span>
                <span class="s2">&quot;optimizer parameters need to be organized in ordered collections, but &quot;</span>
                <span class="s2">&quot;the ordering of tensors in sets will change between runs. Please use a list instead.&quot;</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">param_group</span><span class="p">[</span><span class="s2">&quot;params&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>

        <span class="n">extracted_param_tensors</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">extracted_param_names</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">param_group</span><span class="p">[</span><span class="s2">&quot;params&quot;</span><span class="p">]:</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">param</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">):</span>
                <span class="n">param_name</span> <span class="o">=</span> <span class="n">param</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
                <span class="n">extracted_param_names</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">param_name</span><span class="p">)</span>
                <span class="n">extracted_param_tensors</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">param</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">extracted_param_tensors</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">param</span><span class="p">)</span>

        <span class="n">param_group</span><span class="p">[</span><span class="s2">&quot;params&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">extracted_param_tensors</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">extracted_param_names</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">extracted_param_names</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">extracted_param_tensors</span><span class="p">):</span>
                <span class="n">param_group</span><span class="p">[</span><span class="s2">&quot;param_names&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">extracted_param_names</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s2">&quot;all optimizer params should be with/without names. Some param names are missing&quot;</span>
                <span class="p">)</span>

        <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">param_group</span><span class="p">[</span><span class="s2">&quot;params&quot;</span><span class="p">]:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">param</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span>
                    <span class="s2">&quot;optimizer can only optimize Tensors, &quot;</span>
                    <span class="s2">&quot;but one of the params is &quot;</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">typename</span><span class="p">(</span><span class="n">param</span><span class="p">)</span>
                <span class="p">)</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">defaults</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;differentiable&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span> <span class="ow">and</span> <span class="ow">not</span> <span class="p">(</span>
                <span class="n">param</span><span class="o">.</span><span class="n">is_leaf</span> <span class="ow">or</span> <span class="n">param</span><span class="o">.</span><span class="n">retains_grad</span>
            <span class="p">):</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;can&#39;t optimize a non-leaf Tensor&quot;</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">default</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">defaults</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="k">if</span> <span class="n">default</span> <span class="ow">is</span> <span class="n">required</span> <span class="ow">and</span> <span class="n">name</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">param_group</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;parameter group didn&#39;t specify a value of required optimization parameter </span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2">&quot;</span>
                <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">param_group</span><span class="o">.</span><span class="n">setdefault</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">default</span><span class="p">)</span>

        <span class="n">params</span> <span class="o">=</span> <span class="n">param_group</span><span class="p">[</span><span class="s2">&quot;params&quot;</span><span class="p">]</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">params</span><span class="p">)</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">params</span><span class="p">)):</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
                <span class="s2">&quot;optimizer contains a parameter group with duplicate parameters; &quot;</span>
                <span class="s2">&quot;in future, this will cause an error; &quot;</span>
                <span class="s2">&quot;see github.com/pytorch/pytorch/issues/40967 for more information&quot;</span><span class="p">,</span>
                <span class="n">stacklevel</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="n">param_set</span><span class="p">:</span> <span class="n">Set</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">group</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">param_groups</span><span class="p">:</span>
            <span class="n">param_set</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">group</span><span class="p">[</span><span class="s2">&quot;params&quot;</span><span class="p">]))</span>
            <span class="k">if</span> <span class="p">(</span><span class="s2">&quot;param_names&quot;</span> <span class="ow">in</span> <span class="n">param_group</span><span class="p">)</span> <span class="o">!=</span> <span class="p">(</span><span class="s2">&quot;param_names&quot;</span> <span class="ow">in</span> <span class="n">group</span><span class="p">):</span>
                <span class="n">current_group_txt</span> <span class="o">=</span> <span class="p">(</span>
                    <span class="s2">&quot;with names&quot;</span> <span class="k">if</span> <span class="s2">&quot;param_names&quot;</span> <span class="ow">in</span> <span class="n">param_group</span> <span class="k">else</span> <span class="s2">&quot;without names&quot;</span>
                <span class="p">)</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s2">&quot;all optimizer param groups should be with/without names. &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;cannot add param group </span><span class="si">{</span><span class="n">current_group_txt</span><span class="si">}</span><span class="s2"> to the optimizer&quot;</span>
                <span class="p">)</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">param_set</span><span class="o">.</span><span class="n">isdisjoint</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">param_group</span><span class="p">[</span><span class="s2">&quot;params&quot;</span><span class="p">])):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;some parameters appear in more than one parameter group&quot;</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">param_groups</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">param_group</span><span class="p">)</span></div>
</div>

</pre></div>

                </article>
              
              
              
                <footer class="bd-footer-article">
                  <div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item">
<div class="feedback">
  
<div class="rating">
    Rate this Page
    <div class="stars">
        
        <span class="star" data-value="1">★</span>
        
        <span class="star" data-value="2">★</span>
        
        <span class="star" data-value="3">★</span>
        
        <span class="star" data-value="4">★</span>
        
        <span class="star" data-value="5">★</span>
        
    </div>
</div>

  <div class="feedback-send">
    <button class="feedback-btn" onclick="openGitHubIssue()">Send Feedback</button>
  </div>
</div>
</div>
  
</div>
                </footer>
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
</div>
                </footer>
              
            </div>
            
            
              
            
          </div>
          <footer class="bd-footer-content">
            
          </footer>
        
      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script defer src="../../../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf"></script>
<script defer src="../../../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf"></script>


  <footer class="bd-footer">
    
    <div class="bd-footer__inner bd-page-width">
      
        <div class="footer-items__start">
          
            <div class="footer-item">

  <p class="copyright">
    
      © Copyright PyTorch Contributors.
      <br/>
    
  </p>
</div>
          
            <div class="footer-item">

  <p class="sphinx-version">
    Created using <a href="https://www.sphinx-doc.org/">Sphinx</a> 8.1.0.
    <br/>
  </p>
</div>
          
        </div>
      
    
    
      <div class="footer-items__end">
        
          <div class="footer-item">
<p class="theme-version">
  <!-- # L10n: Setting the PST URL as an argument as this does not need to be localized -->
  Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.16.1.
</p></div>
        
      </div>
    
   </div>
   

   <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
      <div class="container">
        <div class="row">
          <div class="col-md-4 text-center">
            <h2>Docs</h2>
            <p>Access comprehensive developer documentation for PyTorch</p>
            <a class="with-right-arrow" href="">View Docs</a>
          </div>

          <div class="col-md-4 text-center">
            <h2>Tutorials</h2>
            <p>Get in-depth tutorials for beginners and advanced developers</p>
            <a class="with-right-arrow" href="">View Tutorials</a>
          </div>

          <div class="col-md-4 text-center">
            <h2>Resources</h2>
            <p>Find development resources and get your questions answered</p>
            <a class="with-right-arrow" href="">View Resources</a>
          </div>
        </div>
      </div>
  </div>

  <footer class="site-footer">
      <div class="container footer-container">
        <div class="footer-logo-wrapper">
          <a href="" class="footer-logo"></a>
        </div>

        <div class="footer-links-wrapper">
          <div class="footer-links-col">
            <ul>
              <li class="list-title"><a href="">PyTorch</a></li>
              <li><a href="">Get Started</a></li>
              <li><a href="">Features</a></li>
              <li><a href="">Ecosystem</a></li>
              <li><a href="">Blog</a></li>
              <li><a href="">Contributing</a></li>
            </ul>
          </div>

          <div class="footer-links-col">
            <ul>
              <li class="list-title"><a href="">Resources</a></li>
              <li><a href="">Tutorials</a></li>
              <li><a href="">Docs</a></li>
              <li><a href="" target="_blank">Discuss</a></li>
              <li><a href="" target="_blank">Github Issues</a></li>
              <li><a href="" target="_blank">Brand Guidelines</a></li>
            </ul>
          </div>

          <div class="footer-links-col">
            <ul>
              <li class="list-title">Stay up to date</li>
              <li><a href="" target="_blank">Facebook</a></li>
              <li><a href="" target="_blank">Twitter</a></li>
              <li><a href="" target="_blank">YouTube</a></li>
              <li><a href="" target="_blank">LinkedIn</a></li>
            </ul>
            </div>

          <div class="footer-links-col">
            <ul>
              <li class="list-title">PyTorch Podcasts</li>
              <li><a href="" target="_blank">Spotify</a></li>
              <li><a href="" target="_blank">Apple</a></li>
              <li><a href="" target="_blank">Google</a></li>
              <li><a href="" target="_blank">Amazon</a></li>
            </ul>
           </div>
          </div>

          <div class="privacy-policy">
            <ul>
              <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/terms/" target="_blank">Terms</a></li>
              <li class="privacy-policy-links">|</li>
              <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/privacy-policy/" target="_blank">Privacy</a></li>
            </ul>
          </div>
          <div class="copyright">
          <p>© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation.
            For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see
            <a href="https://www.linuxfoundation.org/policies/">www.linuxfoundation.org/policies/</a>. The PyTorch Foundation supports the PyTorch open source
            project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC,
            please see <a href="https://www.lfprojects.org/policies/">www.lfprojects.org/policies/</a>.</p>
        </div>
       </div>

  <div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="../../../_static/img/pytorch-x.svg">
  </div>
</div>
  </footer>


  </body>
</html>