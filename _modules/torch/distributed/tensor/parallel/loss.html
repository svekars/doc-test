
<!DOCTYPE html>


<html lang="en" data-content_root="../../../../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>torch.distributed.tensor.parallel.loss &#8212; PyTorch main documentation</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  <!--
    this give us a css class that will be invisible only if js is disabled
  -->
  <noscript>
    <style>
      .pst-js-only { display: none !important; }

    </style>
  </noscript>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../../../../_static/styles/theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />
<link href="../../../../../_static/styles/pydata-sphinx-theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />

    <link rel="stylesheet" type="text/css" href="../../../../../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../../../../../_static/copybutton.css?v=949a1ff5" />
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/katex.min.css" />
    <link rel="stylesheet" type="text/css" href="../../../../../_static/katex-math.css?v=91adb8b6" />
    <link rel="stylesheet" type="text/css" href="../../../../../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="../../../../../_static/css/jit.css?v=8de1ea5d" />
    <link rel="stylesheet" type="text/css" href="../../../../../_static/css/custom.css?v=3ccf5357" />
    <link rel="stylesheet" type="text/css" href="../../../../../_static/css/custom2.css?v=baa440dc" />
  
  <!-- So that users can add custom icons -->
  <script src="../../../../../_static/scripts/fontawesome.js?digest=8878045cc6db502f8baf"></script>
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../../../../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf" />
<link rel="preload" as="script" href="../../../../../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf" />

    <script src="../../../../../_static/documentation_options.js?v=a8da1a53"></script>
    <script src="../../../../../_static/doctools.js?v=9bcbadda"></script>
    <script src="../../../../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../../../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../../../../_static/copybutton.js?v=940804e7"></script>
    <script src="../../../../../_static/design-tabs.js?v=f930bc37"></script>
    <script type="module" src="https://cdn.jsdelivr.net/npm/mermaid@11.2.0/dist/mermaid.esm.min.mjs"></script>
    <script type="module" src="https://cdn.jsdelivr.net/npm/@mermaid-js/layout-elk@0.1.4/dist/mermaid-layout-elk.esm.min.mjs"></script>
    <script type="module">import mermaid from "https://cdn.jsdelivr.net/npm/mermaid@11.2.0/dist/mermaid.esm.min.mjs";import elkLayouts from "https://cdn.jsdelivr.net/npm/@mermaid-js/layout-elk@0.1.4/dist/mermaid-layout-elk.esm.min.mjs";mermaid.registerLayoutLoaders(elkLayouts);mermaid.initialize({startOnLoad:false});</script>
    <script src="https://cdn.jsdelivr.net/npm/d3@7.9.0/dist/d3.min.js"></script>
    <script type="module">
import mermaid from "https://cdn.jsdelivr.net/npm/mermaid@11.2.0/dist/mermaid.esm.min.mjs";
window.addEventListener("load", () => mermaid.run());
</script>
    <script>DOCUMENTATION_OPTIONS.pagename = '_modules/torch/distributed/tensor/parallel/loss';</script>
    <script src="../../../../../_static/js/star-rating.js?v=8861fcb6"></script>
    <script src="../../../../../_static/js/send-feedback.js?v=5646bf45"></script>
    <link rel="index" title="Index" href="../../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../../search.html" />

  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  <meta name="docsearch:version" content="main (2.6.0 )" />
<script src="https://code.jquery.com/jquery-3.7.1.min.js"></script>
<script type="text/javascript" src="../../../../../_static/js/send-feedback.js"></script>
<script type="text/javascript" src="../../../../../_static/js/star-rating.js"></script>
<script type="text/javascript" src="../../../../../_static/js/cookie-banner.js"></script>

<script async src="https://www.googletagmanager.com/gtag/js?id=G-TEST12345"></script>
    <script type="text/javascript">
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-TEST12345');
    </script>

  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
<body data-feedback-url="https://github.com/pytorch/pytorch">
  <div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>

          <li class="main-menu-item">
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Learn
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="">
                  <span class=dropdown-title>Get Started</span>
                  <p>Run PyTorch locally or get started quickly with one of the supported cloud platforms</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Tutorials</span>
                  <p>Whats new in PyTorch tutorials</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Learn the Basics</span>
                  <p>Familiarize yourself with PyTorch concepts and modules</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">PyTorch Recipes</span>
                  <p>Bite-size, ready-to-deploy PyTorch code examples</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Intro to PyTorch - YouTube Series</span>
                  <p>Master PyTorch basics with our engaging YouTube tutorial series</p>
                </a>
              </div>
            </div>
          </li>

          <li>
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Ecosystem
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Tools</span>
                  <p>Learn about the tools and frameworks in the PyTorch Ecosystem</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class=dropdown-title>Community</span>
                  <p>Join the PyTorch developer community to contribute, learn, and get your questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="" target="_blank">
                  <span class=dropdown-title>Forums</span>
                  <p>A place to discuss PyTorch code, issues, install, research</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class=dropdown-title>Developer Resources</span>
                  <p>Find resources and get questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Contributor Awards - 2024</span>
                  <p>Award winners announced at this year's PyTorch Conference</p>
                </a>
              </div>
            </div>
          </li>

          <li>
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Edge
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">About PyTorch Edge</span>
                  <p>Build innovative and privacy-aware AI experiences for edge devices</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">ExecuTorch</span>
                  <p>End-to-end solution for enabling on-device inference capabilities across mobile and edge devices</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">ExecuTorch Docs</span>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Docs
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">PyTorch</span>
                  <p>Explore the documentation for comprehensive guidance on how to use PyTorch</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">PyTorch Domains</span>
                  <p>Read the PyTorch Domains documentation to learn more about domain-specific libraries</p>
                </a>
              </div>
            </div>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Blogs & News
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">PyTorch Blog</span>
                  <p>Catch up on the latest technical news and happenings</p>
                </a>
                 <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Community Blog</span>
                  <p>Stories from the PyTorch ecosystem</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Videos</span>
                  <p>Learn about the latest PyTorch tutorials, new, and more </p>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Community Stories</span>
                  <p>Learn how our community solves real, everyday machine learning problems with PyTorch</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Events</span>
                  <p>Find events, webinars, and podcasts</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Newsletter</span>
                  <p>Stay up-to-date with the latest updates</p>
                </a>
            </div>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                About
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">PyTorch Foundation</span>
                  <p>Learn more about the PyTorch Foundation</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Governing Board</span>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Cloud Credit Program</span>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Technical Advisory Council</span>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Staff</span>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Contact Us</span>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
            <div class="no-dropdown">
              <a href="" data-cta="join">
                Become a Member
              </a>
            </div>
          </li>
          <li>
           <div class="main-menu-item">
             <a href="" class="github-icon">
             </a>
           </div>
          </li>
          <!--- TODO: This block adds the search icon to the nav bar. We will enable it later.
          <li>
            <div class="main-menu-item">
             <a href="" class="search-icon">
             </a>
            </div>
          </li>
          --->
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>
  </div>
</div>

  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <dialog id="pst-search-dialog">
    
<form class="bd-search d-flex align-items-center"
      action="../../../../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </dialog>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
<div class="bd-header__inner bd-page-width">
  <button class="pst-navbar-icon sidebar-toggle primary-toggle" aria-label="Site navigation">
    <span class="fa-solid fa-bars"></span>
  </button>
  
  
  <div class=" navbar-header-items__start">
    
      <div class="navbar-item">

  
    
  

<a class="navbar-brand logo" href="../../../../../index.html">
  
  
  
  
  
  
    <p class="title logo__title">Home</p>
  
</a></div>
    
  </div>
  
  <div class=" navbar-header-items">
    
    <div class="me-auto navbar-header-items__center">
      
        <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../../../python-api/index.html">
    Python API
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../../../notes/index.html">
    Developer Notes
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-external" href="https://pytorch.org/tutorials/">
    Tutorials
  </a>
</li>

  </ul>
</nav></div>
      
    </div>
    
    
    <div class="navbar-header-items__end">
      
        <div class="navbar-item navbar-persistent--container">
          

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button>
        </div>
      
      
        <div class="navbar-item">
<div class="version-switcher__container dropdown pst-js-only">
  <button id="pst-version-switcher-button-2"
    type="button"
    class="version-switcher__button btn btn-sm dropdown-toggle"
    data-bs-toggle="dropdown"
    aria-haspopup="listbox"
    aria-controls="pst-version-switcher-list-2"
    aria-label="Version switcher list"
  >
    Choose version  <!-- this text may get changed later by javascript -->
    <span class="caret"></span>
  </button>
  <div id="pst-version-switcher-list-2"
    class="version-switcher__menu dropdown-menu list-group-flush py-0"
    role="listbox" aria-labelledby="pst-version-switcher-button-2">
    <!-- dropdown will be populated by javascript on page load -->
  </div>
</div></div>
      
        <div class="navbar-item">

<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button></div>
      
        <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://x.com/PyTorch" title="X" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-x-twitter fa-lg" aria-hidden="true"></i>
            <span class="sr-only">X</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/pytorch/pytorch" title="GitHub" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">GitHub</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://pypi.org/project/torch" title="PyPI" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-custom fa-pypi fa-lg" aria-hidden="true"></i>
            <span class="sr-only">PyPI</span></a>
        </li>
</ul></div>
      
    </div>
    
  </div>
  
  
    <div class="navbar-persistent--mobile">

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button>
    </div>
  

  
</div>

    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
        
      
      <dialog id="pst-primary-sidebar-modal"></dialog>
      <div id="pst-primary-sidebar" class="bd-sidebar-primary bd-sidebar hide-on-wide">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
      <div class="sidebar-header-items__center">
        
          
          
            <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../../../python-api/index.html">
    Python API
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../../../notes/index.html">
    Developer Notes
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-external" href="https://pytorch.org/tutorials/">
    Tutorials
  </a>
</li>

  </ul>
</nav></div>
          
        
      </div>
    
    
    
      <div class="sidebar-header-items__end">
        
          <div class="navbar-item">
<div class="version-switcher__container dropdown pst-js-only">
  <button id="pst-version-switcher-button-3"
    type="button"
    class="version-switcher__button btn btn-sm dropdown-toggle"
    data-bs-toggle="dropdown"
    aria-haspopup="listbox"
    aria-controls="pst-version-switcher-list-3"
    aria-label="Version switcher list"
  >
    Choose version  <!-- this text may get changed later by javascript -->
    <span class="caret"></span>
  </button>
  <div id="pst-version-switcher-list-3"
    class="version-switcher__menu dropdown-menu list-group-flush py-0"
    role="listbox" aria-labelledby="pst-version-switcher-button-3">
    <!-- dropdown will be populated by javascript on page load -->
  </div>
</div></div>
        
          <div class="navbar-item">

<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button></div>
        
          <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://x.com/PyTorch" title="X" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-x-twitter fa-lg" aria-hidden="true"></i>
            <span class="sr-only">X</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/pytorch/pytorch" title="GitHub" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">GitHub</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://pypi.org/project/torch" title="PyPI" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-custom fa-pypi fa-lg" aria-hidden="true"></i>
            <span class="sr-only">PyPI</span></a>
        </li>
</ul></div>
        
      </div>
    
  </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
      <div class="sidebar-primary-item">
<div id="ethical-ad-placement"
      class="flat"
      data-ea-publisher="readthedocs"
      data-ea-type="readthedocs-sidebar"
      data-ea-manual="true">
</div></div>
  </div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        
          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item">

<nav aria-label="Breadcrumb" class="d-print-none">
  <ul class="bd-breadcrumbs">
    
    <li class="breadcrumb-item breadcrumb-home">
      <a href="../../../../../index.html" class="nav-link" aria-label="Home">
        <i class="fa-solid fa-home"></i>
      </a>
    </li>
    
    <li class="breadcrumb-item"><a href="../../../../index.html" class="nav-link">Module code</a></li>
    
    
    <li class="breadcrumb-item"><a href="../../../../torch.html" class="nav-link">torch</a></li>
    
    
    <li class="breadcrumb-item"><a href="../../../distributed.html" class="nav-link">torch.distributed</a></li>
    
    
    <li class="breadcrumb-item"><a href="../../tensor.html" class="nav-link">torch.distributed.tensor</a></li>
    
    <li class="breadcrumb-item active" aria-current="page"><span class="ellipsis">torch.distributed.tensor.parallel.loss</span></li>
  </ul>
</nav>
</div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">
<div class="rating">
    Rate this Page
    <div class="stars">
        
        <span class="star" data-value="1">★</span>
        
        <span class="star" data-value="2">★</span>
        
        <span class="star" data-value="3">★</span>
        
        <span class="star" data-value="4">★</span>
        
        <span class="star" data-value="5">★</span>
        
    </div>
</div>
</div>
      
    </div>
  
</div>
</div>
              
              
              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <h1>Source code for torch.distributed.tensor.parallel.loss</h1><div class="highlight"><pre>
<span></span><span class="c1"># mypy: allow-untyped-defs</span>
<span class="c1"># Copyright (c) Meta Platforms, Inc. and affiliates</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">contextlib</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">typing</span><span class="w"> </span><span class="kn">import</span> <span class="n">cast</span><span class="p">,</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">Optional</span><span class="p">,</span> <span class="n">Tuple</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch._prims_common</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">utils</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.distributed._functional_collectives</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">funcol</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.distributed.distributed_c10d</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">c10d</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch</span><span class="w"> </span><span class="kn">import</span> <span class="n">Tensor</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch.distributed.device_mesh</span><span class="w"> </span><span class="kn">import</span> <span class="n">DeviceMesh</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch.distributed.tensor</span><span class="w"> </span><span class="kn">import</span> <span class="n">DTensor</span><span class="p">,</span> <span class="n">Replicate</span><span class="p">,</span> <span class="n">Shard</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch.distributed.tensor._dtensor_spec</span><span class="w"> </span><span class="kn">import</span> <span class="n">DTensorSpec</span><span class="p">,</span> <span class="n">TensorMeta</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch.distributed.tensor._ops._embedding_ops</span><span class="w"> </span><span class="kn">import</span> <span class="n">_MaskPartial</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch.distributed.tensor._ops._math_ops</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">_skip_dim</span><span class="p">,</span>
    <span class="n">Reduction</span><span class="p">,</span>
    <span class="n">replicate_reduction_dims</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch.distributed.tensor.placement_types</span><span class="w"> </span><span class="kn">import</span> <span class="n">Placement</span>


<span class="n">aten</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">aten</span>


<span class="n">__all__</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;loss_parallel&quot;</span><span class="p">]</span>


<div class="viewcode-block" id="loss_parallel">
<a class="viewcode-back" href="../../../../../python-api/distributed.tensor.parallel.html#torch.distributed.tensor.parallel.loss_parallel">[docs]</a>
<span class="nd">@contextlib</span><span class="o">.</span><span class="n">contextmanager</span>
<span class="k">def</span><span class="w"> </span><span class="nf">loss_parallel</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    A context manager that enables loss parallelism, where efficient parallelized loss computation</span>
<span class="sd">    can be performed when the input is sharded on the class dimension. Currently only the cross-entropy</span>
<span class="sd">    loss is supported.</span>

<span class="sd">    Within this context manager, one can use :func:`~torch.nn.functional.cross_entropy` or</span>
<span class="sd">    :class:`~torch.nn.CrossEntropyLoss` as usual, with the following assumptions on the input parameters.</span>
<span class="sd">    The corresponding ``backward()`` call, if any, also needs to happen under this context manager.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (:class:`DTensor`):</span>
<span class="sd">            Input logits. Assumed to be sharded on the class dimension.</span>
<span class="sd">        target (Union[:class:`torch.Tensor`, :class:`DTensor`]):</span>
<span class="sd">            Must be ground truth class indices (class probabilities currently not supported).</span>
<span class="sd">            Assumed to be replicated across the ``DeviceMesh``.</span>
<span class="sd">        weight (Union[:class:`torch.Tensor`, :class:`DTensor`], optional):</span>
<span class="sd">            If given, assumed to be replicated across the ``DeviceMesh``.</span>
<span class="sd">        label_smoothing:</span>
<span class="sd">            Currently not supported.</span>

<span class="sd">    Returns:</span>
<span class="sd">        A replicated :class:`DTensor`.</span>

<span class="sd">    Example:</span>
<span class="sd">        A sharded DTensor is manually created here to showcase the usage.</span>
<span class="sd">        In practice, it is usually the output of a TP module.</span>

<span class="sd">        &gt;&gt;&gt; # xdoctest: +SKIP(&quot;distributed&quot;)</span>
<span class="sd">        &gt;&gt;&gt; from torch.distributed.tensor.parallel import loss_parallel</span>
<span class="sd">        &gt;&gt;&gt; from torch.distributed.device_mesh import init_device_mesh</span>
<span class="sd">        &gt;&gt;&gt; ...</span>
<span class="sd">        &gt;&gt;&gt; device_mesh = init_device_mesh(&quot;cuda&quot;, (8,))</span>
<span class="sd">        &gt;&gt;&gt; input = torch.randn(4, 16, device=&quot;cuda&quot;, requires_grad=True)</span>
<span class="sd">        &gt;&gt;&gt; dist_input = distribute_tensor(input, device_mesh, placements=[Shard(1)])</span>
<span class="sd">        &gt;&gt;&gt; target = torch.randint(16, (4,), device=&quot;cuda&quot;)</span>
<span class="sd">        &gt;&gt;&gt; with loss_parallel():</span>
<span class="sd">        &gt;&gt;&gt;     loss = F.cross_entropy(dist_input, target, reduction=&quot;mean&quot;)</span>
<span class="sd">        &gt;&gt;&gt;     loss.backward()</span>
<span class="sd">        &gt;&gt;&gt; ...</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_enable_custom_loss_ops</span><span class="p">()</span>

    <span class="k">yield</span>

    <span class="n">_disable_custom_loss_ops</span><span class="p">()</span></div>



<span class="c1"># Currently only needs to support one dimensional DeviceMesh; in general return</span>
<span class="c1"># the mesh_dim with placements[mesh_dim].is_shard(dim)</span>
<span class="k">def</span><span class="w"> </span><span class="nf">_find_all_reduce_mesh_dim</span><span class="p">(</span><span class="n">placements</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Placement</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span> <span class="n">dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">len</span><span class="p">(</span><span class="n">placements</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s2">&quot;Currently loss_parallel() only supports input on one-dimensional DeviceMesh.&quot;</span>
        <span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">placements</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">is_shard</span><span class="p">(</span><span class="n">dim</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;loss_parallel() should be enabled only when the input tensor is sharded on dimension </span><span class="si">{</span><span class="n">dim</span><span class="si">}</span><span class="s2">.&quot;</span>
        <span class="p">)</span>
    <span class="k">return</span> <span class="mi">0</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_cast_to_dtensor</span><span class="p">(</span>
    <span class="n">tensor</span><span class="p">,</span> <span class="n">placements</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Placement</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span> <span class="n">mesh</span><span class="p">:</span> <span class="n">DeviceMesh</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">DTensor</span><span class="p">:</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">DTensor</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">tensor</span><span class="o">.</span><span class="n">placements</span> <span class="o">==</span> <span class="n">placements</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">tensor</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Expected </span><span class="si">{</span><span class="n">placements</span><span class="si">}</span><span class="s2"> but got </span><span class="si">{</span><span class="n">tensor</span><span class="o">.</span><span class="n">placements</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">DTensor</span><span class="o">.</span><span class="n">from_local</span><span class="p">(</span>
            <span class="n">tensor</span><span class="p">,</span> <span class="n">device_mesh</span><span class="o">=</span><span class="n">mesh</span><span class="p">,</span> <span class="n">placements</span><span class="o">=</span><span class="n">placements</span><span class="p">,</span> <span class="n">run_check</span><span class="o">=</span><span class="kc">False</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Unsupported type </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">tensor</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_propagate_tensor_meta</span><span class="p">(</span>
    <span class="n">op_call</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">_ops</span><span class="o">.</span><span class="n">OpOverload</span><span class="p">,</span>
    <span class="n">args</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">object</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span>
    <span class="n">kwargs</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">object</span><span class="p">],</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">TensorMeta</span><span class="p">:</span>
    <span class="n">op_info</span> <span class="o">=</span> <span class="n">DTensor</span><span class="o">.</span><span class="n">_op_dispatcher</span><span class="o">.</span><span class="n">unwrap_to_op_info</span><span class="p">(</span><span class="n">op_call</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">)</span>
    <span class="n">tensor_meta</span> <span class="o">=</span> <span class="n">DTensor</span><span class="o">.</span><span class="n">_op_dispatcher</span><span class="o">.</span><span class="n">sharding_propagator</span><span class="o">.</span><span class="n">_propagate_tensor_meta</span><span class="p">(</span>
        <span class="n">op_info</span><span class="o">.</span><span class="n">schema</span>
    <span class="p">)</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">tensor_meta</span><span class="p">,</span> <span class="n">TensorMeta</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">tensor_meta</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">tensor_meta</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">tensor_meta</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Unexpected tensor meta type: </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">tensor_meta</span><span class="p">)</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>


<span class="c1"># NOTE: The implementation follows torch._decomp.decomposition._log_softmax,</span>
<span class="c1"># with all_reduce manually inserted to perform distributed computation.</span>
<span class="k">def</span><span class="w"> </span><span class="nf">_log_softmax</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">half_to_float</span><span class="p">,</span> <span class="n">mesh</span><span class="p">,</span> <span class="n">mesh_dim</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">half_to_float</span><span class="p">:</span>
        <span class="k">assert</span> <span class="n">x</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">half</span>
    <span class="n">computation_dtype</span><span class="p">,</span> <span class="n">result_dtype</span> <span class="o">=</span> <span class="n">utils</span><span class="o">.</span><span class="n">elementwise_dtypes</span><span class="p">(</span>
        <span class="n">x</span><span class="p">,</span> <span class="n">type_promotion_kind</span><span class="o">=</span><span class="n">utils</span><span class="o">.</span><span class="n">ELEMENTWISE_TYPE_PROMOTION_KIND</span><span class="o">.</span><span class="n">DEFAULT</span>
    <span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">computation_dtype</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">shifted</span> <span class="o">=</span> <span class="n">x</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">x_max</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">amax</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">x_max</span> <span class="o">=</span> <span class="n">funcol</span><span class="o">.</span><span class="n">all_reduce</span><span class="p">(</span>
            <span class="n">x_max</span><span class="p">,</span> <span class="n">reduceOp</span><span class="o">=</span><span class="n">c10d</span><span class="o">.</span><span class="n">ReduceOp</span><span class="o">.</span><span class="n">MAX</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="p">(</span><span class="n">mesh</span><span class="p">,</span> <span class="n">mesh_dim</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="n">shifted</span> <span class="o">=</span> <span class="n">x</span> <span class="o">-</span> <span class="n">x_max</span>
    <span class="n">shifted_sumexp</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">shifted</span><span class="p">),</span> <span class="n">dim</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">shifted_sumexp</span> <span class="o">=</span> <span class="n">funcol</span><span class="o">.</span><span class="n">all_reduce</span><span class="p">(</span>
        <span class="n">shifted_sumexp</span><span class="p">,</span> <span class="n">reduceOp</span><span class="o">=</span><span class="n">c10d</span><span class="o">.</span><span class="n">ReduceOp</span><span class="o">.</span><span class="n">SUM</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="p">(</span><span class="n">mesh</span><span class="p">,</span> <span class="n">mesh_dim</span><span class="p">)</span>
    <span class="p">)</span>
    <span class="n">shifted_logsumexp</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">shifted_sumexp</span><span class="p">)</span>
    <span class="n">result</span> <span class="o">=</span> <span class="n">shifted</span> <span class="o">-</span> <span class="n">shifted_logsumexp</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">half_to_float</span><span class="p">:</span>
        <span class="n">result</span> <span class="o">=</span> <span class="n">result</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">result_dtype</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">result</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_log_softmax_handler</span><span class="p">(</span>
    <span class="n">op_call</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">_ops</span><span class="o">.</span><span class="n">OpOverload</span><span class="p">,</span>
    <span class="n">args</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">object</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span>
    <span class="n">kwargs</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">object</span><span class="p">],</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">object</span><span class="p">:</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">cast</span><span class="p">(</span><span class="n">DTensor</span><span class="p">,</span> <span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="n">dim</span> <span class="o">=</span> <span class="n">cast</span><span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="n">args</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
    <span class="n">half_to_float</span> <span class="o">=</span> <span class="n">cast</span><span class="p">(</span><span class="nb">bool</span><span class="p">,</span> <span class="n">args</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span>

    <span class="n">spec</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">_spec</span>
    <span class="n">mesh_dim</span> <span class="o">=</span> <span class="n">_find_all_reduce_mesh_dim</span><span class="p">(</span><span class="n">spec</span><span class="o">.</span><span class="n">placements</span><span class="p">,</span> <span class="n">dim</span><span class="p">)</span>

    <span class="n">output_tensor_meta</span> <span class="o">=</span> <span class="n">_propagate_tensor_meta</span><span class="p">(</span><span class="n">op_call</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">)</span>

    <span class="n">res</span> <span class="o">=</span> <span class="n">_log_softmax</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">_local_tensor</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">half_to_float</span><span class="p">,</span> <span class="n">spec</span><span class="o">.</span><span class="n">mesh</span><span class="p">,</span> <span class="n">mesh_dim</span><span class="p">)</span>

    <span class="n">res_spec</span> <span class="o">=</span> <span class="n">DTensorSpec</span><span class="p">(</span>
        <span class="n">spec</span><span class="o">.</span><span class="n">mesh</span><span class="p">,</span>
        <span class="n">spec</span><span class="o">.</span><span class="n">placements</span><span class="p">,</span>
        <span class="n">tensor_meta</span><span class="o">=</span><span class="n">output_tensor_meta</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="k">return</span> <span class="n">DTensor</span><span class="p">(</span>
        <span class="n">res</span><span class="p">,</span>
        <span class="n">res_spec</span><span class="p">,</span>
        <span class="n">requires_grad</span><span class="o">=</span><span class="n">res</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">,</span>
    <span class="p">)</span>


<span class="c1"># NOTE: As explained below at _nll_loss_and_log_softmax_backward, the</span>
<span class="c1"># _log_softmax_backward_handler does not actually do any computation.</span>
<span class="k">def</span><span class="w"> </span><span class="nf">_log_softmax_backward_handler</span><span class="p">(</span>
    <span class="n">op_call</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">_ops</span><span class="o">.</span><span class="n">OpOverload</span><span class="p">,</span>
    <span class="n">args</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">object</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span>
    <span class="n">kwargs</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">object</span><span class="p">],</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">object</span><span class="p">:</span>
    <span class="n">grad_output</span> <span class="o">=</span> <span class="n">cast</span><span class="p">(</span><span class="n">DTensor</span><span class="p">,</span> <span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="n">input_dtype</span> <span class="o">=</span> <span class="n">cast</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">args</span><span class="p">[</span><span class="mi">3</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">grad_output</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">input_dtype</span><span class="p">)</span>


<span class="c1"># NOTE: The implementation follows torch._decomp.decomposition._nll_loss_forward,</span>
<span class="c1"># with customized communication inserted to perform distributed computation.</span>
<span class="k">def</span><span class="w"> </span><span class="nf">_nll_loss_forward</span><span class="p">(</span>
    <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
    <span class="n">target</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
    <span class="n">weight</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span>
    <span class="n">local_weight</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span>
    <span class="n">reduction</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">ignore_index</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">input_shape</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">,</span>
    <span class="n">channel_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">mesh</span><span class="p">:</span> <span class="n">DeviceMesh</span><span class="p">,</span>
    <span class="n">mesh_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]:</span>
    <span class="n">n_dims</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span>
    <span class="n">channel_dim</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="k">if</span> <span class="n">n_dims</span> <span class="o">&lt;</span> <span class="mi">2</span><span class="p">:</span>
        <span class="n">channel_dim</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_weight_view</span><span class="p">(</span><span class="n">weight</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">n_dims</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">shape</span> <span class="o">=</span> <span class="p">[</span>
                <span class="mi">1</span><span class="p">,</span>
            <span class="p">]</span> <span class="o">*</span> <span class="n">n_dims</span>
            <span class="n">shape</span><span class="p">[</span><span class="n">channel_dim</span><span class="p">]</span> <span class="o">=</span> <span class="n">weight</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">w</span> <span class="o">=</span> <span class="n">weight</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">w</span> <span class="o">=</span> <span class="n">weight</span>
        <span class="k">return</span> <span class="n">w</span>

    <span class="k">if</span> <span class="n">weight</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">w</span> <span class="o">=</span> <span class="n">_weight_view</span><span class="p">(</span><span class="n">weight</span><span class="p">)</span>
        <span class="k">assert</span> <span class="n">local_weight</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="n">local_w</span> <span class="o">=</span> <span class="n">_weight_view</span><span class="p">(</span><span class="n">local_weight</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="n">local_w</span>
    <span class="n">safe_target</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">target</span> <span class="o">!=</span> <span class="n">ignore_index</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
    <span class="n">safe_target_</span> <span class="o">=</span> <span class="n">safe_target</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="n">channel_dim</span><span class="p">)</span>

    <span class="c1"># The following code block is a distributed version of</span>
    <span class="c1"># result = -torch.gather(self, channel_dim, safe_target_).squeeze(channel_dim)</span>
    <span class="n">partial_placement</span> <span class="o">=</span> <span class="n">_MaskPartial</span><span class="p">(</span><span class="n">offset_shape</span><span class="o">=</span><span class="n">input_shape</span><span class="p">,</span> <span class="n">offset_dim</span><span class="o">=</span><span class="n">channel_dim</span><span class="p">)</span>
    <span class="n">safe_target_partial_</span> <span class="o">=</span> <span class="n">partial_placement</span><span class="o">.</span><span class="n">_partition_value</span><span class="p">(</span>
        <span class="n">safe_target_</span><span class="p">,</span> <span class="n">mesh</span><span class="p">,</span> <span class="n">mesh_dim</span>
    <span class="p">)</span>
    <span class="n">result_partial</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">channel_dim</span><span class="p">,</span> <span class="n">safe_target_partial_</span><span class="p">)</span>
    <span class="c1"># an all_reduce happens here</span>
    <span class="n">result_reduced</span> <span class="o">=</span> <span class="n">partial_placement</span><span class="o">.</span><span class="n">_reduce_value</span><span class="p">(</span><span class="n">result_partial</span><span class="p">,</span> <span class="n">mesh</span><span class="p">,</span> <span class="n">mesh_dim</span><span class="p">)</span>
    <span class="n">result</span> <span class="o">=</span> <span class="o">-</span><span class="n">result_reduced</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">channel_dim</span><span class="p">)</span>

    <span class="n">result</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">target</span> <span class="o">!=</span> <span class="n">ignore_index</span><span class="p">,</span> <span class="n">result</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">reduction</span> <span class="o">==</span> <span class="n">Reduction</span><span class="o">.</span><span class="n">NONE</span><span class="o">.</span><span class="n">value</span> <span class="ow">and</span> <span class="n">n_dims</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">total_weight</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">new_full</span><span class="p">((),</span> <span class="mf">0.0</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">result</span><span class="p">,</span> <span class="n">total_weight</span>

    <span class="k">if</span> <span class="n">weight</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">new_shape</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
        <span class="n">new_shape</span><span class="p">[</span><span class="n">channel_dim</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
        <span class="n">w</span> <span class="o">=</span> <span class="n">w</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="n">new_shape</span><span class="p">)</span>
        <span class="n">wsum</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">channel_dim</span><span class="p">,</span> <span class="n">safe_target_</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">channel_dim</span><span class="p">)</span>
        <span class="n">wsum</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">target</span> <span class="o">!=</span> <span class="n">ignore_index</span><span class="p">,</span> <span class="n">wsum</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
        <span class="n">total_weight</span> <span class="o">=</span> <span class="n">wsum</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">total_weight</span> <span class="o">=</span> <span class="p">(</span><span class="n">target</span> <span class="o">!=</span> <span class="n">ignore_index</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

    <span class="c1"># NOTE: this is correct only on 1D DeviceMesh; o/w additional</span>
    <span class="c1">#       all-reduce on result and total_weight is needed</span>
    <span class="k">if</span> <span class="n">reduction</span> <span class="o">==</span> <span class="n">Reduction</span><span class="o">.</span><span class="n">SUM</span><span class="o">.</span><span class="n">value</span><span class="p">:</span>
        <span class="n">result</span> <span class="o">=</span> <span class="n">result</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
    <span class="k">elif</span> <span class="n">reduction</span> <span class="o">==</span> <span class="n">Reduction</span><span class="o">.</span><span class="n">MEAN</span><span class="o">.</span><span class="n">value</span><span class="p">:</span>
        <span class="n">result</span> <span class="o">=</span> <span class="n">result</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">/</span> <span class="n">total_weight</span>

    <span class="k">return</span> <span class="n">result</span><span class="p">,</span> <span class="n">total_weight</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_nll_loss_forward_handler</span><span class="p">(</span>
    <span class="n">op_call</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">_ops</span><span class="o">.</span><span class="n">OpOverload</span><span class="p">,</span>
    <span class="n">args</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">object</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span>
    <span class="n">kwargs</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">object</span><span class="p">],</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">object</span><span class="p">:</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">cast</span><span class="p">(</span><span class="n">DTensor</span><span class="p">,</span> <span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="n">target</span> <span class="o">=</span> <span class="n">args</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">weight</span> <span class="o">=</span> <span class="n">args</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
    <span class="n">reduction</span> <span class="o">=</span> <span class="n">cast</span><span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="n">args</span><span class="p">[</span><span class="mi">3</span><span class="p">])</span>
    <span class="n">ignore_index</span> <span class="o">=</span> <span class="n">cast</span><span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="n">args</span><span class="p">[</span><span class="mi">4</span><span class="p">])</span>

    <span class="n">channel_dim</span> <span class="o">=</span> <span class="mi">1</span> <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">&gt;=</span> <span class="mi">2</span> <span class="k">else</span> <span class="mi">0</span>
    <span class="n">spec</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">_spec</span>
    <span class="n">mesh_dim</span> <span class="o">=</span> <span class="n">_find_all_reduce_mesh_dim</span><span class="p">(</span><span class="n">spec</span><span class="o">.</span><span class="n">placements</span><span class="p">,</span> <span class="n">channel_dim</span><span class="p">)</span>

    <span class="c1"># Check user input: if target and weight are not DTensors, convert them to DTensors;</span>
    <span class="c1"># if they are DTensors, check that they have the desired placements.</span>
    <span class="n">target_placements</span> <span class="o">=</span> <span class="n">_skip_dim</span><span class="p">(</span>
        <span class="n">replicate_reduction_dims</span><span class="p">(</span><span class="n">spec</span><span class="o">.</span><span class="n">placements</span><span class="p">,</span> <span class="p">[</span><span class="n">channel_dim</span><span class="p">]),</span> <span class="n">channel_dim</span>
    <span class="p">)</span>
    <span class="n">all_replicate_placements</span> <span class="o">=</span> <span class="p">(</span><span class="n">Replicate</span><span class="p">(),)</span> <span class="o">*</span> <span class="n">spec</span><span class="o">.</span><span class="n">mesh</span><span class="o">.</span><span class="n">ndim</span>
    <span class="n">target</span> <span class="o">=</span> <span class="n">_cast_to_dtensor</span><span class="p">(</span><span class="n">target</span><span class="p">,</span> <span class="n">target_placements</span><span class="p">,</span> <span class="n">spec</span><span class="o">.</span><span class="n">mesh</span><span class="p">)</span>
    <span class="n">local_weight</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">if</span> <span class="n">weight</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">weight</span> <span class="o">=</span> <span class="n">_cast_to_dtensor</span><span class="p">(</span><span class="n">weight</span><span class="p">,</span> <span class="n">all_replicate_placements</span><span class="p">,</span> <span class="n">spec</span><span class="o">.</span><span class="n">mesh</span><span class="p">)</span>
        <span class="c1"># For local computation, both (replicated) weight and (sharded) local_weight</span>
        <span class="c1"># are needed in _nll_loss_forward(). local_weight is generated here using</span>
        <span class="c1"># DTensor API, without incurring any communication.</span>
        <span class="n">sharded_placements</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">Shard</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span> <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="n">mesh_dim</span> <span class="k">else</span> <span class="n">Replicate</span><span class="p">()</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">spec</span><span class="o">.</span><span class="n">mesh</span><span class="o">.</span><span class="n">ndim</span><span class="p">)</span>
        <span class="p">]</span>
        <span class="n">local_weight</span> <span class="o">=</span> <span class="n">weight</span><span class="o">.</span><span class="n">redistribute</span><span class="p">(</span><span class="n">spec</span><span class="o">.</span><span class="n">mesh</span><span class="p">,</span> <span class="n">sharded_placements</span><span class="p">)</span><span class="o">.</span><span class="n">_local_tensor</span>
        <span class="k">assert</span> <span class="n">local_weight</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">x</span><span class="o">.</span><span class="n">_local_tensor</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">channel_dim</span><span class="p">]</span>

    <span class="k">if</span> <span class="n">reduction</span> <span class="o">==</span> <span class="n">Reduction</span><span class="o">.</span><span class="n">NONE</span><span class="o">.</span><span class="n">value</span><span class="p">:</span>
        <span class="n">output_placements</span> <span class="o">=</span> <span class="n">target_placements</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">output_placements</span> <span class="o">=</span> <span class="n">all_replicate_placements</span>

    <span class="c1"># tensor inputs to _propagate_tensor_meta need to be DTensors</span>
    <span class="n">args</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">args</span><span class="p">)</span>
    <span class="n">args</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">args</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">target</span><span class="p">,</span> <span class="n">weight</span>
    <span class="n">output_tensor_meta</span> <span class="o">=</span> <span class="n">_propagate_tensor_meta</span><span class="p">(</span><span class="n">op_call</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">args</span><span class="p">),</span> <span class="n">kwargs</span><span class="p">)</span>

    <span class="n">result</span><span class="p">,</span> <span class="n">total_weight</span> <span class="o">=</span> <span class="n">_nll_loss_forward</span><span class="p">(</span>
        <span class="n">x</span><span class="o">.</span><span class="n">_local_tensor</span><span class="p">,</span>
        <span class="n">target</span><span class="o">.</span><span class="n">_local_tensor</span><span class="p">,</span>
        <span class="n">weight</span><span class="o">.</span><span class="n">_local_tensor</span> <span class="k">if</span> <span class="n">weight</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">local_weight</span><span class="p">,</span>
        <span class="n">reduction</span><span class="p">,</span>
        <span class="n">ignore_index</span><span class="p">,</span>
        <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span>
        <span class="n">channel_dim</span><span class="p">,</span>
        <span class="n">spec</span><span class="o">.</span><span class="n">mesh</span><span class="p">,</span>
        <span class="n">mesh_dim</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">out_spec</span> <span class="o">=</span> <span class="n">DTensorSpec</span><span class="p">(</span><span class="n">spec</span><span class="o">.</span><span class="n">mesh</span><span class="p">,</span> <span class="n">output_placements</span><span class="p">,</span> <span class="n">tensor_meta</span><span class="o">=</span><span class="n">output_tensor_meta</span><span class="p">)</span>

    <span class="k">return</span> <span class="p">(</span>
        <span class="n">DTensor</span><span class="p">(</span>
            <span class="n">result</span><span class="p">,</span>
            <span class="n">out_spec</span><span class="p">,</span>
            <span class="n">requires_grad</span><span class="o">=</span><span class="n">result</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">,</span>
        <span class="p">),</span>
        <span class="n">total_weight</span><span class="p">,</span>
    <span class="p">)</span>


<span class="c1"># NOTE: The backward computation of cross_entropy goes through two steps:</span>
<span class="c1"># backward for nll_loss and then backward for log_softmax. In loss parallel,</span>
<span class="c1"># the two steps are fused into the following function (called by _nll_loss_backward_handler)</span>
<span class="c1"># to avoid communication when target contains class indices not class probabilities.</span>
<span class="c1"># Also note that the _log_softmax_backward_handler does not perform computation.</span>
<span class="c1"># The implementation resembles _nll_loss_backward and _log_softmax_backward_data</span>
<span class="c1"># from torch._decomp.decomposition.</span>
<span class="k">def</span><span class="w"> </span><span class="nf">_nll_loss_and_log_softmax_backward</span><span class="p">(</span>
    <span class="n">grad_output</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
    <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
    <span class="n">target</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
    <span class="n">weight</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span>
    <span class="n">reduction</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">ignore_index</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">total_weight</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
    <span class="n">input_shape</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">,</span>
    <span class="n">channel_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">mesh</span><span class="p">:</span> <span class="n">DeviceMesh</span><span class="p">,</span>
    <span class="n">mesh_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
    <span class="n">channel_dim</span> <span class="o">=</span> <span class="mi">0</span> <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">&lt;</span> <span class="mi">2</span> <span class="k">else</span> <span class="mi">1</span>
    <span class="k">if</span> <span class="n">reduction</span> <span class="o">==</span> <span class="n">Reduction</span><span class="o">.</span><span class="n">MEAN</span><span class="o">.</span><span class="n">value</span><span class="p">:</span>
        <span class="n">grad_output</span> <span class="o">=</span> <span class="n">grad_output</span> <span class="o">/</span> <span class="n">total_weight</span>

    <span class="n">target</span> <span class="o">=</span> <span class="n">target</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="n">channel_dim</span><span class="p">)</span>
    <span class="n">safe_target</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">target</span> <span class="o">!=</span> <span class="n">ignore_index</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
    <span class="n">grad_input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

    <span class="c1"># The following code block is a distributed version of</span>
    <span class="c1"># grad_input = torch.scatter(grad_input, channel_dim, safe_target, -1.0)</span>
    <span class="n">partial_placement</span> <span class="o">=</span> <span class="n">_MaskPartial</span><span class="p">(</span><span class="n">offset_shape</span><span class="o">=</span><span class="n">input_shape</span><span class="p">,</span> <span class="n">offset_dim</span><span class="o">=</span><span class="n">channel_dim</span><span class="p">)</span>
    <span class="n">safe_target</span> <span class="o">=</span> <span class="n">safe_target</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">channel_dim</span><span class="p">)</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
    <span class="n">masked_safe_target</span> <span class="o">=</span> <span class="n">partial_placement</span><span class="o">.</span><span class="n">_partition_value</span><span class="p">(</span><span class="n">safe_target</span><span class="p">,</span> <span class="n">mesh</span><span class="p">,</span> <span class="n">mesh_dim</span><span class="p">)</span>
    <span class="c1"># only update grad_input to -1 if not masked</span>
    <span class="k">assert</span> <span class="n">partial_placement</span><span class="o">.</span><span class="n">mask_buffer</span><span class="o">.</span><span class="n">data</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
    <span class="n">grad_update</span> <span class="o">=</span> <span class="n">partial_placement</span><span class="o">.</span><span class="n">mask_buffer</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">grad_input</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span> <span class="o">-</span> <span class="mf">1.0</span>
    <span class="n">arange_1d</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span>
        <span class="n">masked_safe_target</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="n">masked_safe_target</span><span class="o">.</span><span class="n">device</span>
    <span class="p">)</span>
    <span class="c1"># The first two cases with x.dim() &lt;= 2 are for aten.nll_loss_backward.default;</span>
    <span class="c1"># the last case is for aten.nll_loss2d_backward.default.</span>
    <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">grad_input</span><span class="p">[</span><span class="n">masked_safe_target</span><span class="p">]</span> <span class="o">=</span> <span class="n">grad_update</span>
    <span class="k">elif</span> <span class="n">x</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
        <span class="n">grad_input</span><span class="p">[</span><span class="n">arange_1d</span><span class="p">,</span> <span class="n">masked_safe_target</span><span class="p">]</span> <span class="o">=</span> <span class="n">grad_update</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">grad_input_t</span> <span class="o">=</span> <span class="n">grad_input</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">channel_dim</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">intermidate_shape</span> <span class="o">=</span> <span class="n">grad_input_t</span><span class="o">.</span><span class="n">shape</span>
        <span class="n">grad_input_2d</span> <span class="o">=</span> <span class="n">grad_input_t</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">channel_dim</span><span class="p">])</span>
        <span class="n">grad_input_2d</span><span class="p">[</span><span class="n">arange_1d</span><span class="p">,</span> <span class="n">masked_safe_target</span><span class="p">]</span> <span class="o">=</span> <span class="n">grad_update</span>
        <span class="n">grad_input</span> <span class="o">=</span> <span class="n">grad_input_2d</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">intermidate_shape</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">channel_dim</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">grad_input</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">&gt;</span> <span class="n">grad_output</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">grad_output</span> <span class="o">=</span> <span class="n">grad_output</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="n">channel_dim</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">weight</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">new_shape</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">dim</span><span class="p">())]</span>
        <span class="n">new_shape</span><span class="p">[</span><span class="n">channel_dim</span><span class="p">]</span> <span class="o">=</span> <span class="n">weight</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">weight</span> <span class="o">=</span> <span class="n">weight</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">new_shape</span><span class="p">)</span>
        <span class="c1"># In order for fused computation to work, the following line is rewritten.</span>
        <span class="c1"># grad_output = grad_output * weight</span>
        <span class="n">new_shape</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
        <span class="n">new_shape</span><span class="p">[</span><span class="n">channel_dim</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
        <span class="n">w</span> <span class="o">=</span> <span class="n">weight</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="n">new_shape</span><span class="p">)</span>
        <span class="n">w_target</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">channel_dim</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
        <span class="n">grad_output</span> <span class="o">=</span> <span class="n">grad_output</span> <span class="o">*</span> <span class="n">w_target</span>

    <span class="n">grad_output</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">target</span> <span class="o">!=</span> <span class="n">ignore_index</span><span class="p">,</span> <span class="n">grad_output</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

    <span class="c1"># NOTE: Instead of directly returning the grad_input as grad_output for log_softmax,</span>
    <span class="c1"># here we perform backward computation for log_softmax altogether to avoid the</span>
    <span class="c1"># otherwise extra all_gather communication.</span>
    <span class="c1"># return grad_input * grad_output</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">grad_input</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">x</span><span class="p">))</span> <span class="o">*</span> <span class="n">grad_output</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_nll_loss_backward_handler</span><span class="p">(</span>
    <span class="n">op_call</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">_ops</span><span class="o">.</span><span class="n">OpOverload</span><span class="p">,</span>
    <span class="n">args</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">object</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span>
    <span class="n">kwargs</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">object</span><span class="p">],</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">object</span><span class="p">:</span>
    <span class="n">grad_output</span> <span class="o">=</span> <span class="n">cast</span><span class="p">(</span><span class="n">DTensor</span><span class="p">,</span> <span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">cast</span><span class="p">(</span><span class="n">DTensor</span><span class="p">,</span> <span class="n">args</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
    <span class="n">target</span> <span class="o">=</span> <span class="n">args</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
    <span class="n">weight</span> <span class="o">=</span> <span class="n">args</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span>
    <span class="n">reduction</span> <span class="o">=</span> <span class="n">cast</span><span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="n">args</span><span class="p">[</span><span class="mi">4</span><span class="p">])</span>
    <span class="n">ignore_index</span> <span class="o">=</span> <span class="n">cast</span><span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="n">args</span><span class="p">[</span><span class="mi">5</span><span class="p">])</span>
    <span class="n">total_weight</span> <span class="o">=</span> <span class="n">cast</span><span class="p">(</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">args</span><span class="p">[</span><span class="mi">6</span><span class="p">])</span>

    <span class="n">channel_dim</span> <span class="o">=</span> <span class="mi">1</span> <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">&gt;=</span> <span class="mi">2</span> <span class="k">else</span> <span class="mi">0</span>
    <span class="n">spec</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">_spec</span>
    <span class="n">mesh_dim</span> <span class="o">=</span> <span class="n">_find_all_reduce_mesh_dim</span><span class="p">(</span><span class="n">spec</span><span class="o">.</span><span class="n">placements</span><span class="p">,</span> <span class="n">channel_dim</span><span class="p">)</span>

    <span class="c1"># if target and weight are not DTensors, convert them to DTensors</span>
    <span class="n">target_placements</span> <span class="o">=</span> <span class="n">_skip_dim</span><span class="p">(</span>
        <span class="n">replicate_reduction_dims</span><span class="p">(</span><span class="n">spec</span><span class="o">.</span><span class="n">placements</span><span class="p">,</span> <span class="p">[</span><span class="n">channel_dim</span><span class="p">]),</span> <span class="n">channel_dim</span>
    <span class="p">)</span>
    <span class="n">all_replicate_placements</span> <span class="o">=</span> <span class="p">(</span><span class="n">Replicate</span><span class="p">(),)</span> <span class="o">*</span> <span class="n">spec</span><span class="o">.</span><span class="n">mesh</span><span class="o">.</span><span class="n">ndim</span>
    <span class="n">target</span> <span class="o">=</span> <span class="n">_cast_to_dtensor</span><span class="p">(</span><span class="n">target</span><span class="p">,</span> <span class="n">target_placements</span><span class="p">,</span> <span class="n">spec</span><span class="o">.</span><span class="n">mesh</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">weight</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">weight</span> <span class="o">=</span> <span class="n">_cast_to_dtensor</span><span class="p">(</span><span class="n">weight</span><span class="p">,</span> <span class="n">all_replicate_placements</span><span class="p">,</span> <span class="n">spec</span><span class="o">.</span><span class="n">mesh</span><span class="p">)</span>

    <span class="c1"># tensor inputs to _propagate_tensor_meta need to be DTensors</span>
    <span class="n">args</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">args</span><span class="p">)</span>
    <span class="n">args</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">args</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span> <span class="o">=</span> <span class="n">target</span><span class="p">,</span> <span class="n">weight</span>
    <span class="n">args</span><span class="p">[</span><span class="mi">6</span><span class="p">]</span> <span class="o">=</span> <span class="n">_cast_to_dtensor</span><span class="p">(</span><span class="n">total_weight</span><span class="p">,</span> <span class="n">all_replicate_placements</span><span class="p">,</span> <span class="n">spec</span><span class="o">.</span><span class="n">mesh</span><span class="p">)</span>
    <span class="n">output_tensor_meta</span> <span class="o">=</span> <span class="n">_propagate_tensor_meta</span><span class="p">(</span><span class="n">op_call</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">args</span><span class="p">),</span> <span class="n">kwargs</span><span class="p">)</span>

    <span class="n">result</span> <span class="o">=</span> <span class="n">_nll_loss_and_log_softmax_backward</span><span class="p">(</span>
        <span class="n">grad_output</span><span class="o">.</span><span class="n">_local_tensor</span><span class="p">,</span>
        <span class="n">x</span><span class="o">.</span><span class="n">_local_tensor</span><span class="p">,</span>
        <span class="n">target</span><span class="o">.</span><span class="n">_local_tensor</span><span class="p">,</span>
        <span class="n">weight</span><span class="o">.</span><span class="n">_local_tensor</span> <span class="k">if</span> <span class="n">weight</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">reduction</span><span class="p">,</span>
        <span class="n">ignore_index</span><span class="p">,</span>
        <span class="n">total_weight</span><span class="p">,</span>
        <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span>
        <span class="n">channel_dim</span><span class="p">,</span>
        <span class="n">spec</span><span class="o">.</span><span class="n">mesh</span><span class="p">,</span>
        <span class="n">mesh_dim</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="c1"># the output sharding is the same as input sharding: Shard(channel_dim) on mesh_dim</span>
    <span class="n">out_spec</span> <span class="o">=</span> <span class="n">DTensorSpec</span><span class="p">(</span>
        <span class="n">spec</span><span class="o">.</span><span class="n">mesh</span><span class="p">,</span>
        <span class="n">spec</span><span class="o">.</span><span class="n">placements</span><span class="p">,</span>
        <span class="n">tensor_meta</span><span class="o">=</span><span class="n">output_tensor_meta</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="k">return</span> <span class="n">DTensor</span><span class="p">(</span>
        <span class="n">result</span><span class="p">,</span>
        <span class="n">out_spec</span><span class="p">,</span>
        <span class="n">requires_grad</span><span class="o">=</span><span class="n">result</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">,</span>
    <span class="p">)</span>


<span class="n">customized_loss_ops</span> <span class="o">=</span> <span class="p">{</span>
    <span class="n">aten</span><span class="o">.</span><span class="n">_log_softmax</span><span class="o">.</span><span class="n">default</span><span class="p">:</span> <span class="n">_log_softmax_handler</span><span class="p">,</span>
    <span class="n">aten</span><span class="o">.</span><span class="n">_log_softmax_backward_data</span><span class="o">.</span><span class="n">default</span><span class="p">:</span> <span class="n">_log_softmax_backward_handler</span><span class="p">,</span>
    <span class="n">aten</span><span class="o">.</span><span class="n">nll_loss_forward</span><span class="o">.</span><span class="n">default</span><span class="p">:</span> <span class="n">_nll_loss_forward_handler</span><span class="p">,</span>
    <span class="n">aten</span><span class="o">.</span><span class="n">nll_loss2d_forward</span><span class="o">.</span><span class="n">default</span><span class="p">:</span> <span class="n">_nll_loss_forward_handler</span><span class="p">,</span>
    <span class="n">aten</span><span class="o">.</span><span class="n">nll_loss_backward</span><span class="o">.</span><span class="n">default</span><span class="p">:</span> <span class="n">_nll_loss_backward_handler</span><span class="p">,</span>
    <span class="n">aten</span><span class="o">.</span><span class="n">nll_loss2d_backward</span><span class="o">.</span><span class="n">default</span><span class="p">:</span> <span class="n">_nll_loss_backward_handler</span><span class="p">,</span>
<span class="p">}</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_enable_custom_loss_ops</span><span class="p">():</span>
    <span class="n">DTensor</span><span class="o">.</span><span class="n">_op_dispatcher</span><span class="o">.</span><span class="n">_custom_op_handlers</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">customized_loss_ops</span><span class="p">)</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_disable_custom_loss_ops</span><span class="p">():</span>
    <span class="k">for</span> <span class="n">custom_op</span> <span class="ow">in</span> <span class="n">customized_loss_ops</span><span class="p">:</span>
        <span class="n">DTensor</span><span class="o">.</span><span class="n">_op_dispatcher</span><span class="o">.</span><span class="n">_custom_op_handlers</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="n">custom_op</span><span class="p">)</span>
</pre></div>

                </article>
              
              
              
                <footer class="bd-footer-article">
                  <div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item">
<div class="feedback">
  
<div class="rating">
    Rate this Page
    <div class="stars">
        
        <span class="star" data-value="1">★</span>
        
        <span class="star" data-value="2">★</span>
        
        <span class="star" data-value="3">★</span>
        
        <span class="star" data-value="4">★</span>
        
        <span class="star" data-value="5">★</span>
        
    </div>
</div>

  <div class="feedback-send">
    <button class="feedback-btn" onclick="openGitHubIssue()">Send Feedback</button>
  </div>
</div>
</div>
  
</div>
                </footer>
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
</div>
                </footer>
              
            </div>
            
            
              
            
          </div>
          <footer class="bd-footer-content">
            
          </footer>
        
      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script defer src="../../../../../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf"></script>
<script defer src="../../../../../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf"></script>


  <footer class="bd-footer">
    
    <div class="bd-footer__inner bd-page-width">
      
        <div class="footer-items__start">
          
            <div class="footer-item">

  <p class="copyright">
    
      © Copyright PyTorch Contributors.
      <br/>
    
  </p>
</div>
          
            <div class="footer-item">

  <p class="sphinx-version">
    Created using <a href="https://www.sphinx-doc.org/">Sphinx</a> 8.1.0.
    <br/>
  </p>
</div>
          
        </div>
      
    
    
      <div class="footer-items__end">
        
          <div class="footer-item">
<p class="theme-version">
  <!-- # L10n: Setting the PST URL as an argument as this does not need to be localized -->
  Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.16.1.
</p></div>
        
      </div>
    
   </div>
   

   <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
      <div class="container">
        <div class="row">
          <div class="col-md-4 text-center">
            <h2>Docs</h2>
            <p>Access comprehensive developer documentation for PyTorch</p>
            <a class="with-right-arrow" href="">View Docs</a>
          </div>

          <div class="col-md-4 text-center">
            <h2>Tutorials</h2>
            <p>Get in-depth tutorials for beginners and advanced developers</p>
            <a class="with-right-arrow" href="">View Tutorials</a>
          </div>

          <div class="col-md-4 text-center">
            <h2>Resources</h2>
            <p>Find development resources and get your questions answered</p>
            <a class="with-right-arrow" href="">View Resources</a>
          </div>
        </div>
      </div>
  </div>

  <footer class="site-footer">
      <div class="container footer-container">
        <div class="footer-logo-wrapper">
          <a href="" class="footer-logo"></a>
        </div>

        <div class="footer-links-wrapper">
          <div class="footer-links-col">
            <ul>
              <li class="list-title"><a href="">PyTorch</a></li>
              <li><a href="">Get Started</a></li>
              <li><a href="">Features</a></li>
              <li><a href="">Ecosystem</a></li>
              <li><a href="">Blog</a></li>
              <li><a href="">Contributing</a></li>
            </ul>
          </div>

          <div class="footer-links-col">
            <ul>
              <li class="list-title"><a href="">Resources</a></li>
              <li><a href="">Tutorials</a></li>
              <li><a href="">Docs</a></li>
              <li><a href="" target="_blank">Discuss</a></li>
              <li><a href="" target="_blank">Github Issues</a></li>
              <li><a href="" target="_blank">Brand Guidelines</a></li>
            </ul>
          </div>

          <div class="footer-links-col">
            <ul>
              <li class="list-title">Stay up to date</li>
              <li><a href="" target="_blank">Facebook</a></li>
              <li><a href="" target="_blank">Twitter</a></li>
              <li><a href="" target="_blank">YouTube</a></li>
              <li><a href="" target="_blank">LinkedIn</a></li>
            </ul>
            </div>

          <div class="footer-links-col">
            <ul>
              <li class="list-title">PyTorch Podcasts</li>
              <li><a href="" target="_blank">Spotify</a></li>
              <li><a href="" target="_blank">Apple</a></li>
              <li><a href="" target="_blank">Google</a></li>
              <li><a href="" target="_blank">Amazon</a></li>
            </ul>
           </div>
          </div>

          <div class="privacy-policy">
            <ul>
              <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/terms/" target="_blank">Terms</a></li>
              <li class="privacy-policy-links">|</li>
              <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/privacy-policy/" target="_blank">Privacy</a></li>
            </ul>
          </div>
          <div class="copyright">
          <p>© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation.
            For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see
            <a href="https://www.linuxfoundation.org/policies/">www.linuxfoundation.org/policies/</a>. The PyTorch Foundation supports the PyTorch open source
            project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC,
            please see <a href="https://www.lfprojects.org/policies/">www.lfprojects.org/policies/</a>.</p>
        </div>
       </div>

  <div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="../../../../../_static/img/pytorch-x.svg">
  </div>
</div>
  </footer>


  </body>
</html>