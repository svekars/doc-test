
<!DOCTYPE html>


<html lang="en" data-content_root="../../../../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>torch.distributed.nn.api.remote_module &#8212; PyTorch main documentation</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  <!--
    this give us a css class that will be invisible only if js is disabled
  -->
  <noscript>
    <style>
      .pst-js-only { display: none !important; }

    </style>
  </noscript>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../../../../_static/styles/theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />
<link href="../../../../../_static/styles/pydata-sphinx-theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />

    <link rel="stylesheet" type="text/css" href="../../../../../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../../../../../_static/copybutton.css?v=949a1ff5" />
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/katex.min.css" />
    <link rel="stylesheet" type="text/css" href="../../../../../_static/katex-math.css?v=91adb8b6" />
    <link rel="stylesheet" type="text/css" href="../../../../../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="../../../../../_static/css/jit.css?v=8de1ea5d" />
    <link rel="stylesheet" type="text/css" href="../../../../../_static/css/custom.css?v=3ccf5357" />
    <link rel="stylesheet" type="text/css" href="../../../../../_static/css/custom2.css?v=baa440dc" />
  
  <!-- So that users can add custom icons -->
  <script src="../../../../../_static/scripts/fontawesome.js?digest=8878045cc6db502f8baf"></script>
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../../../../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf" />
<link rel="preload" as="script" href="../../../../../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf" />

    <script src="../../../../../_static/documentation_options.js?v=a8da1a53"></script>
    <script src="../../../../../_static/doctools.js?v=9bcbadda"></script>
    <script src="../../../../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../../../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../../../../_static/copybutton.js?v=940804e7"></script>
    <script src="../../../../../_static/design-tabs.js?v=f930bc37"></script>
    <script type="module" src="https://cdn.jsdelivr.net/npm/mermaid@11.2.0/dist/mermaid.esm.min.mjs"></script>
    <script type="module" src="https://cdn.jsdelivr.net/npm/@mermaid-js/layout-elk@0.1.4/dist/mermaid-layout-elk.esm.min.mjs"></script>
    <script type="module">import mermaid from "https://cdn.jsdelivr.net/npm/mermaid@11.2.0/dist/mermaid.esm.min.mjs";import elkLayouts from "https://cdn.jsdelivr.net/npm/@mermaid-js/layout-elk@0.1.4/dist/mermaid-layout-elk.esm.min.mjs";mermaid.registerLayoutLoaders(elkLayouts);mermaid.initialize({startOnLoad:false});</script>
    <script src="https://cdn.jsdelivr.net/npm/d3@7.9.0/dist/d3.min.js"></script>
    <script type="module">
import mermaid from "https://cdn.jsdelivr.net/npm/mermaid@11.2.0/dist/mermaid.esm.min.mjs";
window.addEventListener("load", () => mermaid.run());
</script>
    <script>DOCUMENTATION_OPTIONS.pagename = '_modules/torch/distributed/nn/api/remote_module';</script>
    <script src="../../../../../_static/js/star-rating.js?v=8861fcb6"></script>
    <script src="../../../../../_static/js/send-feedback.js?v=5646bf45"></script>
    <link rel="index" title="Index" href="../../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../../search.html" />

  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  <meta name="docsearch:version" content="main (2.6.0 )" />
<script src="https://code.jquery.com/jquery-3.7.1.min.js"></script>
<script type="text/javascript" src="../../../../../_static/js/send-feedback.js"></script>
<script type="text/javascript" src="../../../../../_static/js/star-rating.js"></script>
<script type="text/javascript" src="../../../../../_static/js/cookie-banner.js"></script>

<script async src="https://www.googletagmanager.com/gtag/js?id=G-TEST12345"></script>
    <script type="text/javascript">
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-TEST12345');
    </script>

  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
<body data-feedback-url="https://github.com/pytorch/pytorch">
  <div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>

          <li class="main-menu-item">
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Learn
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="">
                  <span class=dropdown-title>Get Started</span>
                  <p>Run PyTorch locally or get started quickly with one of the supported cloud platforms</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Tutorials</span>
                  <p>Whats new in PyTorch tutorials</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Learn the Basics</span>
                  <p>Familiarize yourself with PyTorch concepts and modules</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">PyTorch Recipes</span>
                  <p>Bite-size, ready-to-deploy PyTorch code examples</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Intro to PyTorch - YouTube Series</span>
                  <p>Master PyTorch basics with our engaging YouTube tutorial series</p>
                </a>
              </div>
            </div>
          </li>

          <li>
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Ecosystem
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Tools</span>
                  <p>Learn about the tools and frameworks in the PyTorch Ecosystem</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class=dropdown-title>Community</span>
                  <p>Join the PyTorch developer community to contribute, learn, and get your questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="" target="_blank">
                  <span class=dropdown-title>Forums</span>
                  <p>A place to discuss PyTorch code, issues, install, research</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class=dropdown-title>Developer Resources</span>
                  <p>Find resources and get questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Contributor Awards - 2024</span>
                  <p>Award winners announced at this year's PyTorch Conference</p>
                </a>
              </div>
            </div>
          </li>

          <li>
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Edge
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">About PyTorch Edge</span>
                  <p>Build innovative and privacy-aware AI experiences for edge devices</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">ExecuTorch</span>
                  <p>End-to-end solution for enabling on-device inference capabilities across mobile and edge devices</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">ExecuTorch Docs</span>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Docs
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">PyTorch</span>
                  <p>Explore the documentation for comprehensive guidance on how to use PyTorch</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">PyTorch Domains</span>
                  <p>Read the PyTorch Domains documentation to learn more about domain-specific libraries</p>
                </a>
              </div>
            </div>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Blogs & News
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">PyTorch Blog</span>
                  <p>Catch up on the latest technical news and happenings</p>
                </a>
                 <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Community Blog</span>
                  <p>Stories from the PyTorch ecosystem</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Videos</span>
                  <p>Learn about the latest PyTorch tutorials, new, and more </p>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Community Stories</span>
                  <p>Learn how our community solves real, everyday machine learning problems with PyTorch</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Events</span>
                  <p>Find events, webinars, and podcasts</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Newsletter</span>
                  <p>Stay up-to-date with the latest updates</p>
                </a>
            </div>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                About
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">PyTorch Foundation</span>
                  <p>Learn more about the PyTorch Foundation</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Governing Board</span>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Cloud Credit Program</span>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Technical Advisory Council</span>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Staff</span>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Contact Us</span>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
            <div class="no-dropdown">
              <a href="" data-cta="join">
                Become a Member
              </a>
            </div>
          </li>
          <li>
           <div class="main-menu-item">
             <a href="" class="github-icon">
             </a>
           </div>
          </li>
          <!--- TODO: This block adds the search icon to the nav bar. We will enable it later.
          <li>
            <div class="main-menu-item">
             <a href="" class="search-icon">
             </a>
            </div>
          </li>
          --->
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>
  </div>
</div>

  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <dialog id="pst-search-dialog">
    
<form class="bd-search d-flex align-items-center"
      action="../../../../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </dialog>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
<div class="bd-header__inner bd-page-width">
  <button class="pst-navbar-icon sidebar-toggle primary-toggle" aria-label="Site navigation">
    <span class="fa-solid fa-bars"></span>
  </button>
  
  
  <div class=" navbar-header-items__start">
    
      <div class="navbar-item">

  
    
  

<a class="navbar-brand logo" href="../../../../../index.html">
  
  
  
  
  
  
    <p class="title logo__title">Home</p>
  
</a></div>
    
  </div>
  
  <div class=" navbar-header-items">
    
    <div class="me-auto navbar-header-items__center">
      
        <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../../../python-api/index.html">
    Python API
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../../../notes/index.html">
    Developer Notes
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-external" href="https://pytorch.org/tutorials/">
    Tutorials
  </a>
</li>

  </ul>
</nav></div>
      
    </div>
    
    
    <div class="navbar-header-items__end">
      
        <div class="navbar-item navbar-persistent--container">
          

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button>
        </div>
      
      
        <div class="navbar-item">
<div class="version-switcher__container dropdown pst-js-only">
  <button id="pst-version-switcher-button-2"
    type="button"
    class="version-switcher__button btn btn-sm dropdown-toggle"
    data-bs-toggle="dropdown"
    aria-haspopup="listbox"
    aria-controls="pst-version-switcher-list-2"
    aria-label="Version switcher list"
  >
    Choose version  <!-- this text may get changed later by javascript -->
    <span class="caret"></span>
  </button>
  <div id="pst-version-switcher-list-2"
    class="version-switcher__menu dropdown-menu list-group-flush py-0"
    role="listbox" aria-labelledby="pst-version-switcher-button-2">
    <!-- dropdown will be populated by javascript on page load -->
  </div>
</div></div>
      
        <div class="navbar-item">

<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button></div>
      
        <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://x.com/PyTorch" title="X" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-x-twitter fa-lg" aria-hidden="true"></i>
            <span class="sr-only">X</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/pytorch/pytorch" title="GitHub" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">GitHub</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://pypi.org/project/torch" title="PyPI" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-custom fa-pypi fa-lg" aria-hidden="true"></i>
            <span class="sr-only">PyPI</span></a>
        </li>
</ul></div>
      
    </div>
    
  </div>
  
  
    <div class="navbar-persistent--mobile">

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button>
    </div>
  

  
</div>

    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
        
      
      <dialog id="pst-primary-sidebar-modal"></dialog>
      <div id="pst-primary-sidebar" class="bd-sidebar-primary bd-sidebar hide-on-wide">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
      <div class="sidebar-header-items__center">
        
          
          
            <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../../../python-api/index.html">
    Python API
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../../../notes/index.html">
    Developer Notes
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-external" href="https://pytorch.org/tutorials/">
    Tutorials
  </a>
</li>

  </ul>
</nav></div>
          
        
      </div>
    
    
    
      <div class="sidebar-header-items__end">
        
          <div class="navbar-item">
<div class="version-switcher__container dropdown pst-js-only">
  <button id="pst-version-switcher-button-3"
    type="button"
    class="version-switcher__button btn btn-sm dropdown-toggle"
    data-bs-toggle="dropdown"
    aria-haspopup="listbox"
    aria-controls="pst-version-switcher-list-3"
    aria-label="Version switcher list"
  >
    Choose version  <!-- this text may get changed later by javascript -->
    <span class="caret"></span>
  </button>
  <div id="pst-version-switcher-list-3"
    class="version-switcher__menu dropdown-menu list-group-flush py-0"
    role="listbox" aria-labelledby="pst-version-switcher-button-3">
    <!-- dropdown will be populated by javascript on page load -->
  </div>
</div></div>
        
          <div class="navbar-item">

<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button></div>
        
          <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://x.com/PyTorch" title="X" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-x-twitter fa-lg" aria-hidden="true"></i>
            <span class="sr-only">X</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/pytorch/pytorch" title="GitHub" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">GitHub</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://pypi.org/project/torch" title="PyPI" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-custom fa-pypi fa-lg" aria-hidden="true"></i>
            <span class="sr-only">PyPI</span></a>
        </li>
</ul></div>
        
      </div>
    
  </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
      <div class="sidebar-primary-item">
<div id="ethical-ad-placement"
      class="flat"
      data-ea-publisher="readthedocs"
      data-ea-type="readthedocs-sidebar"
      data-ea-manual="true">
</div></div>
  </div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        
          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item">

<nav aria-label="Breadcrumb" class="d-print-none">
  <ul class="bd-breadcrumbs">
    
    <li class="breadcrumb-item breadcrumb-home">
      <a href="../../../../../index.html" class="nav-link" aria-label="Home">
        <i class="fa-solid fa-home"></i>
      </a>
    </li>
    
    <li class="breadcrumb-item"><a href="../../../../index.html" class="nav-link">Module code</a></li>
    
    
    <li class="breadcrumb-item"><a href="../../../../torch.html" class="nav-link">torch</a></li>
    
    
    <li class="breadcrumb-item"><a href="../../../distributed.html" class="nav-link">torch.distributed</a></li>
    
    <li class="breadcrumb-item active" aria-current="page"><span class="ellipsis">torch.distributed.nn.api.remote_module</span></li>
  </ul>
</nav>
</div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">
<div class="rating">
    Rate this Page
    <div class="stars">
        
        <span class="star" data-value="1">★</span>
        
        <span class="star" data-value="2">★</span>
        
        <span class="star" data-value="3">★</span>
        
        <span class="star" data-value="4">★</span>
        
        <span class="star" data-value="5">★</span>
        
    </div>
</div>
</div>
      
    </div>
  
</div>
</div>
              
              
              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <h1>Source code for torch.distributed.nn.api.remote_module</h1><div class="highlight"><pre>
<span></span><span class="ch">#!/usr/bin/python3</span>
<span class="c1"># mypy: allow-untyped-defs</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">collections</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">io</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">sys</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">types</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">typing</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">Any</span><span class="p">,</span>
    <span class="n">Callable</span><span class="p">,</span>
    <span class="n">Dict</span><span class="p">,</span>
    <span class="n">Iterator</span><span class="p">,</span>
    <span class="n">List</span><span class="p">,</span>
    <span class="n">Mapping</span><span class="p">,</span>
    <span class="n">Optional</span><span class="p">,</span>
    <span class="n">Set</span><span class="p">,</span>
    <span class="n">Tuple</span><span class="p">,</span>
    <span class="n">Type</span><span class="p">,</span>
    <span class="n">TypeVar</span><span class="p">,</span>
    <span class="n">Union</span><span class="p">,</span>
<span class="p">)</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.distributed.rpc</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">rpc</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch</span><span class="w"> </span><span class="kn">import</span> <span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="p">,</span> <span class="n">nn</span><span class="p">,</span> <span class="n">Tensor</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch.distributed</span><span class="w"> </span><span class="kn">import</span> <span class="n">_remote_device</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch.distributed.nn.jit</span><span class="w"> </span><span class="kn">import</span> <span class="n">instantiator</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch.distributed.rpc.internal</span><span class="w"> </span><span class="kn">import</span> <span class="n">_internal_rpc_pickler</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="kn">import</span> <span class="n">Module</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch.nn.parameter</span><span class="w"> </span><span class="kn">import</span> <span class="n">Parameter</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch.utils.hooks</span><span class="w"> </span><span class="kn">import</span> <span class="n">RemovableHandle</span>


<span class="n">__all__</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;RemoteModule&quot;</span><span class="p">]</span>

<span class="n">_grad_t</span> <span class="o">=</span> <span class="n">Union</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span> <span class="n">Tensor</span><span class="p">]</span>
<span class="c1"># See https://mypy.readthedocs.io/en/latest/generics.html#generic-methods-and-generic-self for the use</span>
<span class="c1"># of `T` to annotate `self`. Many methods of `Module` return `self` and we want those return values to be</span>
<span class="c1"># the type of the subclass, not the looser type of `Module`.</span>
<span class="n">T</span> <span class="o">=</span> <span class="n">TypeVar</span><span class="p">(</span><span class="s2">&quot;T&quot;</span><span class="p">,</span> <span class="n">bound</span><span class="o">=</span><span class="s2">&quot;Module&quot;</span><span class="p">)</span>

<span class="n">_NON_SCRIPTABLE_REMOTE_MODULE_MODULE</span> <span class="o">=</span> <span class="p">(</span>
    <span class="n">instantiator</span><span class="o">.</span><span class="n">instantiate_non_scriptable_remote_module_template</span><span class="p">()</span>
<span class="p">)</span>

<span class="n">_REMOTE_MODULE_PICKLED_ATTRIBUTES</span> <span class="o">=</span> <span class="p">(</span>
    <span class="s2">&quot;on&quot;</span><span class="p">,</span>
    <span class="s2">&quot;device&quot;</span><span class="p">,</span>
    <span class="s2">&quot;is_device_map_set&quot;</span><span class="p">,</span>
    <span class="s2">&quot;is_scriptable&quot;</span><span class="p">,</span>
    <span class="s2">&quot;generated_methods&quot;</span><span class="p">,</span>
    <span class="s2">&quot;module_rref&quot;</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">_SerializedRemoteModule</span> <span class="o">=</span> <span class="n">collections</span><span class="o">.</span><span class="n">namedtuple</span><span class="p">(</span><span class="s2">&quot;_SerializedRemoteModule&quot;</span><span class="p">,</span> <span class="n">_REMOTE_MODULE_PICKLED_ATTRIBUTES</span><span class="p">)</span>  <span class="c1"># type: ignore[misc]</span>

<span class="c1"># These attributes are mostly from RemoteModule&#39;s parent class and are intentionally not pickled.</span>
<span class="c1"># A new attribute of RemoteModule should be either in _REMOTE_MODULE_PICKLED_ATTRIBUTES</span>
<span class="c1"># or _REMOTE_MODULE_ATTRIBUTES_IGNORE_FOR_PICKLING.</span>
<span class="c1"># Otherwise, it will not be pickled.</span>
<span class="n">_REMOTE_MODULE_ATTRIBUTES_IGNORE_FOR_PICKLING</span> <span class="o">=</span> <span class="p">(</span>
    <span class="s2">&quot;training&quot;</span><span class="p">,</span>
    <span class="s2">&quot;_parameters&quot;</span><span class="p">,</span>
    <span class="s2">&quot;_buffers&quot;</span><span class="p">,</span>
    <span class="s2">&quot;_non_persistent_buffers_set&quot;</span><span class="p">,</span>
    <span class="s2">&quot;_backward_hooks&quot;</span><span class="p">,</span>
    <span class="s2">&quot;_backward_pre_hooks&quot;</span><span class="p">,</span>
    <span class="s2">&quot;_is_full_backward_hook&quot;</span><span class="p">,</span>
    <span class="s2">&quot;_forward_hooks&quot;</span><span class="p">,</span>
    <span class="s2">&quot;_forward_hooks_with_kwargs&quot;</span><span class="p">,</span>
    <span class="s2">&quot;_forward_hooks_always_called&quot;</span><span class="p">,</span>
    <span class="s2">&quot;_forward_pre_hooks&quot;</span><span class="p">,</span>
    <span class="s2">&quot;_forward_pre_hooks_with_kwargs&quot;</span><span class="p">,</span>
    <span class="s2">&quot;_state_dict_hooks&quot;</span><span class="p">,</span>
    <span class="s2">&quot;_state_dict_pre_hooks&quot;</span><span class="p">,</span>
    <span class="s2">&quot;_load_state_dict_pre_hooks&quot;</span><span class="p">,</span>
    <span class="s2">&quot;_load_state_dict_post_hooks&quot;</span><span class="p">,</span>
    <span class="s2">&quot;_state_dict_pre_hooks&quot;</span><span class="p">,</span>
    <span class="s2">&quot;_modules&quot;</span><span class="p">,</span>
    <span class="c1"># The two attributes below are generated methods, not available at pickling time.</span>
    <span class="s2">&quot;forward_async&quot;</span><span class="p">,</span>
    <span class="s2">&quot;forward&quot;</span><span class="p">,</span>
<span class="p">)</span>


<span class="c1"># RPC handler.</span>
<span class="k">def</span><span class="w"> </span><span class="nf">_instantiate_template</span><span class="p">(</span><span class="n">module_interface_cls</span><span class="p">,</span> <span class="n">enable_moving_cpu_tensors_to_cuda</span><span class="p">):</span>
    <span class="n">instantiator</span><span class="o">.</span><span class="n">instantiate_scriptable_remote_module_template</span><span class="p">(</span>
        <span class="n">module_interface_cls</span><span class="p">,</span> <span class="n">enable_moving_cpu_tensors_to_cuda</span>
    <span class="p">)</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_create_module</span><span class="p">(</span><span class="n">module_cls</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">,</span> <span class="n">device</span><span class="p">):</span>
    <span class="n">module</span> <span class="o">=</span> <span class="n">module_cls</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s2">&quot;Expect `module_cls(*args, **kwargs)` returns an instance of &lt;class nn.Module&gt;, &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;but it returns an instance of </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">module</span><span class="p">)</span><span class="si">}</span><span class="s2">.&quot;</span>
        <span class="p">)</span>
    <span class="n">module</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">module</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_create_module_with_interface</span><span class="p">(</span>
    <span class="n">module_cls</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">module_interface_cls</span>
<span class="p">):</span>
    <span class="n">module</span> <span class="o">=</span> <span class="n">_create_module</span><span class="p">(</span><span class="n">module_cls</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">module_interface_cls</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">module</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">script</span><span class="p">(</span><span class="n">module</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">rpc</span><span class="o">.</span><span class="n">RRef</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">module_interface_cls</span><span class="p">)</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_param_rrefs</span><span class="p">(</span><span class="n">module_rref</span><span class="p">,</span> <span class="n">recurse</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">rpc</span><span class="o">.</span><span class="n">RRef</span><span class="p">[</span><span class="n">Parameter</span><span class="p">]]:</span>
    <span class="n">ret</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">rpc</span><span class="o">.</span><span class="n">RRef</span><span class="p">[</span><span class="n">Parameter</span><span class="p">]]</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">rpc</span><span class="o">.</span><span class="n">RRef</span><span class="p">(</span><span class="n">param</span><span class="p">)</span> <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">module_rref</span><span class="o">.</span><span class="n">local_value</span><span class="p">()</span><span class="o">.</span><span class="n">parameters</span><span class="p">(</span><span class="n">recurse</span><span class="p">)</span>
    <span class="p">]</span>
    <span class="k">return</span> <span class="n">ret</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_raise_not_supported</span><span class="p">(</span><span class="n">name</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Method ``</span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2">`` not supported for RemoteModule&quot;</span><span class="p">)</span>


<span class="k">class</span><span class="w"> </span><span class="nc">_RemoteModule</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__new__</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="c1"># Use __new__ for logging purposes.</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_log_api_usage_once</span><span class="p">(</span><span class="s2">&quot;torch.distributed.nn.api.remote_module&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__new__</span><span class="p">(</span><span class="bp">cls</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">remote_device</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="n">module_cls</span><span class="p">:</span> <span class="n">Type</span><span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">],</span>
        <span class="n">args</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">kwargs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">_module_interface_cls</span><span class="p">:</span> <span class="n">Any</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        RemoteModule instance can only be created after RPC initialization.</span>

<span class="sd">        It creates a user-specified module on a specified remote node.</span>
<span class="sd">        It behaves like a regular ``nn.Module`` except that the ``forward`` method is</span>
<span class="sd">        executed on the remote node.</span>
<span class="sd">        It takes care of autograd recording to ensure the backward pass propagates</span>
<span class="sd">        gradients back to the corresponding remote module.</span>
<span class="sd">        It can be shared across processors using `RPC framework &lt;https://pytorch.org/docs/stable/rpc.html&gt;`__,</span>
<span class="sd">        without incurring any overheads of copying the actual module,</span>
<span class="sd">        which is equivalent to an :class:`~torch.distributed.rpc.RRef`</span>
<span class="sd">        pointing to the remote module.</span>

<span class="sd">        The arguments of ``forward_async`` and ``forward`` are the same as</span>
<span class="sd">        the ``forward`` method of the module returned by the ``module_cls``.</span>

<span class="sd">        Apart from ``forward_async`` and ``forward``, no other methods are supported from nn.Module for now.</span>

<span class="sd">        Particularly, to create a hybrid model, typically the local modules should be</span>
<span class="sd">        created outside of remote modules, rather than as submodules of any remote module (by calling ``add_module``).</span>
<span class="sd">        Hybrid Example:</span>
<span class="sd">                &gt;&gt;&gt; class HybridModel(nn.Module):</span>
<span class="sd">                &gt;&gt;&gt;     def __init__(self) -&gt; None:</span>
<span class="sd">                &gt;&gt;&gt;         nn.Module.__init__(self)</span>
<span class="sd">                &gt;&gt;&gt;         self.remote_embedding = RemoteModule(...)</span>
<span class="sd">                &gt;&gt;&gt;         self.local_linear = nn.Linear(...)</span>

<span class="sd">        For example, if ``module_cls`` returns an instance of ``nn.Linear``,</span>
<span class="sd">        that has ``forward`` method signature, ``def forward(input: Tensor) -&gt; Tensor:``,</span>
<span class="sd">        the generated ``RemoteModule`` will have 2 methods in signature of</span>
<span class="sd">        ``def forward(input: Tensor) -&gt; Tensor:`` and</span>
<span class="sd">        ``def forward_async(input: Tensor) -&gt; Future[Tensor]:``.</span>

<span class="sd">        .. note::</span>
<span class="sd">            If the remote module is placed on a cuda device,</span>
<span class="sd">            any input CPU tensors will be automatically moved to the same cuda device,</span>
<span class="sd">            and GPU tensors are returned over the wire according to the device map of the remote worker on TensorPipe RPC backend.</span>

<span class="sd">        Args:</span>
<span class="sd">            remote_device (str): Device on the destination worker where we&#39;d like to place this module.</span>
<span class="sd">                The device can be a local device or a remote device specified by one of the following remote</span>
<span class="sd">                formats:</span>

<span class="sd">                    1. &quot;rank:&lt;rank&gt;/&lt;device&gt;&quot; (ex: &quot;rank:0/cuda:0&quot;).</span>
<span class="sd">                    2. &quot;&lt;worker_name&gt;/&lt;device&gt;&quot; (ex: &quot;trainer0/cuda:0&quot;).</span>

<span class="sd">                In addition, the device field can be optional and the default value is &quot;cpu&quot;.</span>
<span class="sd">            module_cls (nn.Module): For example,</span>
<span class="sd">                &gt;&gt;&gt; class MyModule(nn.Module):</span>
<span class="sd">                &gt;&gt;&gt;     def forward(input):</span>
<span class="sd">                &gt;&gt;&gt;         return input + 1</span>
<span class="sd">                &gt;&gt;&gt;</span>
<span class="sd">                &gt;&gt;&gt; module_cls = MyModule</span>
<span class="sd">            args (Sequence, optional): args to be passed to ``module_cls``.</span>
<span class="sd">            kwargs (Dict, optional): kwargs to be passed to ``module_cls``.</span>
<span class="sd">            _module_interface_cls (type, optional): The TorchScript interface type for the module</span>
<span class="sd">                to be created. The type object should be decorated by @torch.jit.interface.</span>
<span class="sd">                If not provided, the generated RemoteModule is not torchscript-able.</span>
<span class="sd">                Warning, this is an experimental API and susceptible to frequent changes.</span>

<span class="sd">        Returns:</span>
<span class="sd">            A remote module instance which wraps the :class:`~nn.Module` created by the</span>
<span class="sd">            user-provided ``module_cls``, it has a blocking ``forward`` method and an</span>
<span class="sd">            asynchronous ``forward_async`` method that returns a future of the ``forward`` call</span>
<span class="sd">            on the user-provided module on the remote side.</span>

<span class="sd">        Example::</span>
<span class="sd">            Run the following code in two different processes:</span>

<span class="sd">            &gt;&gt;&gt; # xdoctest: +SKIP(&quot;distributed&quot;)</span>
<span class="sd">            &gt;&gt;&gt; # On worker 0:</span>
<span class="sd">            &gt;&gt;&gt; import torch</span>
<span class="sd">            &gt;&gt;&gt; import torch.distributed.rpc as rpc</span>
<span class="sd">            &gt;&gt;&gt; from torch import nn, Tensor</span>
<span class="sd">            &gt;&gt;&gt; from torch.distributed.nn.api.remote_module import RemoteModule</span>
<span class="sd">            &gt;&gt;&gt;</span>
<span class="sd">            &gt;&gt;&gt; rpc.init_rpc(&quot;worker0&quot;, rank=0, world_size=2)</span>
<span class="sd">            &gt;&gt;&gt; remote_linear_module = RemoteModule(</span>
<span class="sd">            &gt;&gt;&gt;     &quot;worker1/cpu&quot;, nn.Linear, args=(20, 30),</span>
<span class="sd">            &gt;&gt;&gt; )</span>
<span class="sd">            &gt;&gt;&gt; input = torch.randn(128, 20)</span>
<span class="sd">            &gt;&gt;&gt; ret_fut = remote_linear_module.forward_async(input)</span>
<span class="sd">            &gt;&gt;&gt; ret = ret_fut.wait()</span>
<span class="sd">            &gt;&gt;&gt; rpc.shutdown()</span>

<span class="sd">            &gt;&gt;&gt; # On worker 1:</span>
<span class="sd">            &gt;&gt;&gt; import torch</span>
<span class="sd">            &gt;&gt;&gt; import torch.distributed.rpc as rpc</span>
<span class="sd">            &gt;&gt;&gt;</span>
<span class="sd">            &gt;&gt;&gt; rpc.init_rpc(&quot;worker1&quot;, rank=1, world_size=2)</span>
<span class="sd">            &gt;&gt;&gt; rpc.shutdown()</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="n">enable_moving_cpu_tensors_to_cuda</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_prepare_init</span><span class="p">(</span><span class="n">remote_device</span><span class="p">)</span>

        <span class="c1"># Default arguments preparation.</span>
        <span class="n">args</span> <span class="o">=</span> <span class="n">args</span> <span class="k">if</span> <span class="n">args</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="p">()</span>
        <span class="n">kwargs</span> <span class="o">=</span> <span class="n">kwargs</span> <span class="k">if</span> <span class="n">kwargs</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="p">{}</span>

        <span class="k">if</span> <span class="n">_module_interface_cls</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># Users reply on this field to know if this generated RemoteModule is TorchScript-able.</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">is_scriptable</span> <span class="o">=</span> <span class="kc">True</span>

            <span class="c1"># Instantiate template on remote side.</span>
            <span class="n">fut</span> <span class="o">=</span> <span class="n">rpc</span><span class="o">.</span><span class="n">rpc_async</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">on</span><span class="p">,</span>
                <span class="n">_instantiate_template</span><span class="p">,</span>
                <span class="p">(</span><span class="n">_module_interface_cls</span><span class="p">,</span> <span class="n">enable_moving_cpu_tensors_to_cuda</span><span class="p">),</span>
            <span class="p">)</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">_init_template</span><span class="p">(</span>
                <span class="n">_module_interface_cls</span><span class="p">,</span> <span class="n">enable_moving_cpu_tensors_to_cuda</span>
            <span class="p">)</span>

            <span class="c1"># Instantiate template on remote side.</span>
            <span class="n">fut</span> <span class="o">=</span> <span class="n">rpc</span><span class="o">.</span><span class="n">rpc_async</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">on</span><span class="p">,</span>
                <span class="n">_instantiate_template</span><span class="p">,</span>
                <span class="p">(</span><span class="n">_module_interface_cls</span><span class="p">,</span> <span class="n">enable_moving_cpu_tensors_to_cuda</span><span class="p">),</span>
            <span class="p">)</span>

            <span class="c1"># Create the module on the remote side.</span>
            <span class="n">fut</span><span class="o">.</span><span class="n">wait</span><span class="p">()</span>  <span class="c1"># Ensure remote_module_cls is available on remote side.</span>

            <span class="c1"># TODO: We need to change this to rpc.remote, and make it async (see the else branch below).</span>
            <span class="c1"># For that we need to be able to apply _module_interface_cls to the RRef returned by rpc.remote</span>
            <span class="c1"># See https://github.com/pytorch/pytorch/issues/58098 for more context.</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">module_rref</span> <span class="o">=</span> <span class="n">rpc</span><span class="o">.</span><span class="n">rpc_sync</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">on</span><span class="p">,</span>
                <span class="n">_create_module_with_interface</span><span class="p">,</span>
                <span class="p">(</span><span class="n">module_cls</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="n">_module_interface_cls</span><span class="p">),</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">is_scriptable</span> <span class="o">=</span> <span class="kc">False</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">generated_methods</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">_NON_SCRIPTABLE_REMOTE_MODULE_MODULE</span><span class="o">.</span><span class="n">_generated_methods</span>
            <span class="p">)</span>
            <span class="c1"># Create the module on the remote side.</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">module_rref</span> <span class="o">=</span> <span class="n">rpc</span><span class="o">.</span><span class="n">remote</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">on</span><span class="p">,</span>
                <span class="n">_create_module</span><span class="p">,</span>
                <span class="p">(</span><span class="n">module_cls</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">),</span>
            <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_install_generated_methods</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_check_attribute_picklability</span><span class="p">()</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">remote_parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">recurse</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">rpc</span><span class="o">.</span><span class="n">RRef</span><span class="p">[</span><span class="n">Parameter</span><span class="p">]]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Return a list of :class:`~torch.distributed.rpc.RRef` pointing to the remote module&#39;s parameters.</span>

<span class="sd">        This can typically be used in conjunction</span>
<span class="sd">        with :class:`~torch.distributed.optim.DistributedOptimizer`.</span>

<span class="sd">        Args:</span>
<span class="sd">            recurse (bool): if True, then returns parameters of the remote</span>
<span class="sd">                module and all submodules of the remote module. Otherwise,</span>
<span class="sd">                returns only parameters that are direct members of the</span>
<span class="sd">                remote module.</span>

<span class="sd">        Returns:</span>
<span class="sd">            A list of :class:`~torch.distributed.rpc.RRef` (``List[RRef[nn.Parameter]]``)</span>
<span class="sd">            to remote module&#39;s parameters.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">rpc</span><span class="o">.</span><span class="n">rpc_sync</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">on</span><span class="p">,</span> <span class="n">_param_rrefs</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">module_rref</span><span class="p">,</span> <span class="n">recurse</span><span class="p">))</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">get_module_rref</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">rpc</span><span class="o">.</span><span class="n">RRef</span><span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Return an :class:`~torch.distributed.rpc.RRef` (``RRef[nn.Module]``) pointing to the remote module.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">module_rref</span>

    <span class="nd">@torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">export</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">__getstate__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
            <span class="s2">&quot;Cannot pickle RemoteModule in python pickler. RemoteModule can only be pickled when using RPC&quot;</span>
        <span class="p">)</span>

    <span class="nd">@torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">export</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">__setstate__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
            <span class="s2">&quot;Cannot unpickle RemoteModule in python pickler. RemoteModule can only be unpickled when using RPC&quot;</span>
        <span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">register_buffer</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">tensor</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span> <span class="n">persistent</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">_raise_not_supported</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="o">.</span><span class="vm">__name__</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">register_parameter</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">param</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Parameter</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">_raise_not_supported</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">register_parameter</span><span class="o">.</span><span class="vm">__name__</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">add_module</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">module</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Module</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">_raise_not_supported</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">add_module</span><span class="o">.</span><span class="vm">__name__</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">apply</span><span class="p">(</span><span class="bp">self</span><span class="p">:</span> <span class="n">T</span><span class="p">,</span> <span class="n">fn</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[[</span><span class="n">Module</span><span class="p">],</span> <span class="kc">None</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">T</span><span class="p">:</span>  <span class="c1"># type: ignore[return]</span>
        <span class="n">_raise_not_supported</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">apply</span><span class="o">.</span><span class="vm">__name__</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">cuda</span><span class="p">(</span><span class="bp">self</span><span class="p">:</span> <span class="n">T</span><span class="p">,</span> <span class="n">device</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">device</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">T</span><span class="p">:</span>  <span class="c1"># type: ignore[return]</span>
        <span class="n">_raise_not_supported</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="vm">__name__</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">ipu</span><span class="p">(</span><span class="bp">self</span><span class="p">:</span> <span class="n">T</span><span class="p">,</span> <span class="n">device</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">device</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">T</span><span class="p">:</span>  <span class="c1"># type: ignore[return]</span>
        <span class="n">_raise_not_supported</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ipu</span><span class="o">.</span><span class="vm">__name__</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">xpu</span><span class="p">(</span><span class="bp">self</span><span class="p">:</span> <span class="n">T</span><span class="p">,</span> <span class="n">device</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">device</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">T</span><span class="p">:</span>  <span class="c1"># type: ignore[return]</span>
        <span class="n">_raise_not_supported</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">xpu</span><span class="o">.</span><span class="vm">__name__</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">cpu</span><span class="p">(</span><span class="bp">self</span><span class="p">:</span> <span class="n">T</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">T</span><span class="p">:</span>  <span class="c1"># type: ignore[return]</span>
        <span class="n">_raise_not_supported</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">cpu</span><span class="o">.</span><span class="vm">__name__</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">type</span><span class="p">(</span><span class="bp">self</span><span class="p">:</span> <span class="n">T</span><span class="p">,</span> <span class="n">dst_type</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">dtype</span><span class="p">,</span> <span class="nb">str</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">T</span><span class="p">:</span>  <span class="c1"># type: ignore[return]</span>
        <span class="n">_raise_not_supported</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">type</span><span class="o">.</span><span class="vm">__name__</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">float</span><span class="p">(</span><span class="bp">self</span><span class="p">:</span> <span class="n">T</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">T</span><span class="p">:</span>  <span class="c1"># type: ignore[return]</span>
        <span class="n">_raise_not_supported</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">float</span><span class="o">.</span><span class="vm">__name__</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">double</span><span class="p">(</span><span class="bp">self</span><span class="p">:</span> <span class="n">T</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">T</span><span class="p">:</span>  <span class="c1"># type: ignore[return]</span>
        <span class="n">_raise_not_supported</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">double</span><span class="o">.</span><span class="vm">__name__</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">half</span><span class="p">(</span><span class="bp">self</span><span class="p">:</span> <span class="n">T</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">T</span><span class="p">:</span>  <span class="c1"># type: ignore[return]</span>
        <span class="n">_raise_not_supported</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">half</span><span class="o">.</span><span class="vm">__name__</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">bfloat16</span><span class="p">(</span><span class="bp">self</span><span class="p">:</span> <span class="n">T</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">T</span><span class="p">:</span>  <span class="c1"># type: ignore[return]</span>
        <span class="n">_raise_not_supported</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bfloat16</span><span class="o">.</span><span class="vm">__name__</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">to</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">T</span><span class="p">:</span>  <span class="c1"># type: ignore[misc, return, type-var]</span>
        <span class="n">_raise_not_supported</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">to</span><span class="o">.</span><span class="vm">__name__</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">register_backward_hook</span><span class="p">(</span>  <span class="c1"># type: ignore[return]</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">hook</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[[</span><span class="n">Module</span><span class="p">,</span> <span class="n">_grad_t</span><span class="p">,</span> <span class="n">_grad_t</span><span class="p">],</span> <span class="n">Union</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="n">_grad_t</span><span class="p">]]</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">RemovableHandle</span><span class="p">:</span>
        <span class="n">_raise_not_supported</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">register_backward_hook</span><span class="o">.</span><span class="vm">__name__</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">register_forward_pre_hook</span><span class="p">(</span>  <span class="c1"># type: ignore[return]</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">hook</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span>
            <span class="n">Callable</span><span class="p">[[</span><span class="n">T</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Any</span><span class="p">,</span> <span class="o">...</span><span class="p">]],</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Any</span><span class="p">]],</span>
            <span class="n">Callable</span><span class="p">[</span>
                <span class="p">[</span><span class="n">T</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Any</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]],</span>
                <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">Any</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]],</span>
            <span class="p">],</span>
        <span class="p">],</span>
        <span class="n">prepend</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">with_kwargs</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">RemovableHandle</span><span class="p">:</span>
        <span class="n">_raise_not_supported</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">register_forward_pre_hook</span><span class="o">.</span><span class="vm">__name__</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">register_forward_hook</span><span class="p">(</span>  <span class="c1"># type: ignore[return, override]</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">hook</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span>
            <span class="n">Callable</span><span class="p">[[</span><span class="n">T</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Any</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span> <span class="n">Any</span><span class="p">],</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Any</span><span class="p">]],</span>
            <span class="n">Callable</span><span class="p">[[</span><span class="n">T</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Any</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">],</span> <span class="n">Any</span><span class="p">],</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Any</span><span class="p">]],</span>
        <span class="p">],</span>
        <span class="n">prepend</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">with_kwargs</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">RemovableHandle</span><span class="p">:</span>
        <span class="n">_raise_not_supported</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">register_forward_hook</span><span class="o">.</span><span class="vm">__name__</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">state_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="n">_raise_not_supported</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">state_dict</span><span class="o">.</span><span class="vm">__name__</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">load_state_dict</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">state_dict</span><span class="p">:</span> <span class="n">Mapping</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">],</span>
        <span class="n">strict</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">assign</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="n">_raise_not_supported</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">load_state_dict</span><span class="o">.</span><span class="vm">__name__</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">recurse</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Iterator</span><span class="p">[</span><span class="n">Parameter</span><span class="p">]:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s2">&quot;Method ``parameters`` not supported for RemoteModule. Please use ``remote_parameters`` instead.&quot;</span>
        <span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">named_parameters</span><span class="p">(</span>  <span class="c1"># type: ignore[return]</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">prefix</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span><span class="p">,</span> <span class="n">recurse</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span> <span class="n">remove_duplicate</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Iterator</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Parameter</span><span class="p">]]:</span>
        <span class="n">_raise_not_supported</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">named_parameters</span><span class="o">.</span><span class="vm">__name__</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">buffers</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">recurse</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Iterator</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]:</span>  <span class="c1"># type: ignore[return]</span>
        <span class="n">_raise_not_supported</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">buffers</span><span class="o">.</span><span class="vm">__name__</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">named_buffers</span><span class="p">(</span>  <span class="c1"># type: ignore[return]</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">prefix</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span><span class="p">,</span> <span class="n">recurse</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span> <span class="n">remove_duplicate</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Iterator</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]]:</span>
        <span class="n">_raise_not_supported</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">named_buffers</span><span class="o">.</span><span class="vm">__name__</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">children</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Iterator</span><span class="p">[</span><span class="n">Module</span><span class="p">]:</span>  <span class="c1"># type: ignore[return]</span>
        <span class="n">_raise_not_supported</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">children</span><span class="o">.</span><span class="vm">__name__</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">named_children</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Iterator</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Module</span><span class="p">]]:</span>  <span class="c1"># type: ignore[return]</span>
        <span class="n">_raise_not_supported</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">named_children</span><span class="o">.</span><span class="vm">__name__</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">modules</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Iterator</span><span class="p">[</span><span class="n">Module</span><span class="p">]:</span>  <span class="c1"># type: ignore[return]</span>
        <span class="n">_raise_not_supported</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">modules</span><span class="o">.</span><span class="vm">__name__</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">named_modules</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">memo</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Set</span><span class="p">[</span><span class="n">Module</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">prefix</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span><span class="p">,</span>
        <span class="n">remove_duplicate</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="n">_raise_not_supported</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">named_modules</span><span class="o">.</span><span class="vm">__name__</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">train</span><span class="p">(</span><span class="bp">self</span><span class="p">:</span> <span class="n">T</span><span class="p">,</span> <span class="n">mode</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">T</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">module_rref</span><span class="o">.</span><span class="n">rpc_sync</span><span class="p">()</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>  <span class="c1"># type: ignore[operator, union-attr]</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">eval</span><span class="p">(</span><span class="bp">self</span><span class="p">:</span> <span class="n">T</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">T</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">module_rref</span><span class="o">.</span><span class="n">rpc_sync</span><span class="p">()</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>  <span class="c1"># type: ignore[operator, union-attr]</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">requires_grad_</span><span class="p">(</span><span class="bp">self</span><span class="p">:</span> <span class="n">T</span><span class="p">,</span> <span class="n">requires_grad</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">T</span><span class="p">:</span>  <span class="c1"># type: ignore[return]</span>
        <span class="n">_raise_not_supported</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">requires_grad_</span><span class="o">.</span><span class="vm">__name__</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">zero_grad</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">set_to_none</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">_raise_not_supported</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">zero_grad</span><span class="o">.</span><span class="vm">__name__</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">share_memory</span><span class="p">(</span><span class="bp">self</span><span class="p">:</span> <span class="n">T</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">T</span><span class="p">:</span>  <span class="c1"># type: ignore[return]</span>
        <span class="n">_raise_not_supported</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">share_memory</span><span class="o">.</span><span class="vm">__name__</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">extra_repr</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>  <span class="c1"># type: ignore[return]</span>
        <span class="n">_raise_not_supported</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">extra_repr</span><span class="o">.</span><span class="vm">__name__</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_prepare_init</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">remote_device_str</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Prepare the initialization and returns whether to enable automatically moving CPU tensors to CUDA devices.&quot;&quot;&quot;</span>
        <span class="c1"># Sanity check.</span>
        <span class="k">assert</span> <span class="n">rpc</span><span class="o">.</span><span class="n">_is_current_rpc_agent_set</span><span class="p">(),</span> <span class="s2">&quot;RemoteModule only works in RPC.&quot;</span>

        <span class="n">remote_device</span> <span class="o">=</span> <span class="n">_remote_device</span><span class="p">(</span><span class="n">remote_device_str</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">on</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">remote_device</span><span class="o">.</span><span class="n">worker_name</span><span class="p">()</span>
            <span class="k">if</span> <span class="n">remote_device</span><span class="o">.</span><span class="n">worker_name</span><span class="p">()</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
            <span class="k">else</span> <span class="n">remote_device</span><span class="o">.</span><span class="n">rank</span><span class="p">()</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">device</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="n">remote_device</span><span class="o">.</span><span class="n">device</span><span class="p">())</span>
        <span class="n">agent</span> <span class="o">=</span> <span class="n">rpc</span><span class="o">.</span><span class="n">_get_current_rpc_agent</span><span class="p">()</span>
        <span class="c1"># If the device map of the remote worker is set,</span>
        <span class="c1"># then enable moving any input CPU tensors to the same cuda device.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">is_device_map_set</span> <span class="o">=</span> <span class="nb">bool</span><span class="p">(</span>
            <span class="n">agent</span><span class="o">.</span><span class="n">_get_device_map</span><span class="p">(</span><span class="n">agent</span><span class="o">.</span><span class="n">get_worker_info</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">on</span><span class="p">))</span>  <span class="c1"># type: ignore[arg-type]</span>
        <span class="p">)</span>
        <span class="c1"># ``enable_moving_cpu_tensors_to_cuda`` is less strict than ``is_device_map_set``:</span>
        <span class="c1"># If ``enable_moving_cpu_tensors_to_cuda`` is true, but the device map is not set,</span>
        <span class="c1"># then any CPU tensors can still be moved to a cuda device to run forward,</span>
        <span class="c1"># but the output must be moved back to CPU before being sent over the wire.</span>
        <span class="n">enable_moving_cpu_tensors_to_cuda</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span><span class="o">.</span><span class="n">type</span> <span class="o">==</span> <span class="s2">&quot;cuda&quot;</span>
        <span class="k">return</span> <span class="n">enable_moving_cpu_tensors_to_cuda</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_init_template</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">module_interface_cls</span><span class="p">,</span> <span class="n">enable_moving_cpu_tensors_to_cuda</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Instantiate template on local side.&quot;&quot;&quot;</span>
        <span class="n">generated_module</span> <span class="o">=</span> <span class="n">instantiator</span><span class="o">.</span><span class="n">instantiate_scriptable_remote_module_template</span><span class="p">(</span>
            <span class="n">module_interface_cls</span><span class="p">,</span> <span class="n">enable_moving_cpu_tensors_to_cuda</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">generated_methods</span> <span class="o">=</span> <span class="n">generated_module</span><span class="o">.</span><span class="n">_generated_methods</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_check_attribute_picklability</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Check if all the attribute has explicitly defined whether to be pickled (i.e., picklability).&quot;&quot;&quot;</span>
        <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="vm">__dict__</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
            <span class="k">if</span> <span class="p">(</span>
                <span class="n">k</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">_REMOTE_MODULE_PICKLED_ATTRIBUTES</span>
                <span class="ow">and</span> <span class="n">k</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">_REMOTE_MODULE_ATTRIBUTES_IGNORE_FOR_PICKLING</span>
            <span class="p">):</span>
                <span class="k">raise</span> <span class="ne">AttributeError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;Attribute </span><span class="si">{</span><span class="n">k</span><span class="si">}</span><span class="s2"> must be either in ``_REMOTE_MODULE_PICKLED_ATTRIBUTES`` or &quot;</span>
                    <span class="s2">&quot;``_REMOTE_MODULE_ATTRIBUTES_IGNORE_FOR_PICKLING``.&quot;</span>
                <span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_install_generated_methods</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">method</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">generated_methods</span><span class="p">:</span>
            <span class="n">method_name</span> <span class="o">=</span> <span class="n">method</span><span class="o">.</span><span class="vm">__name__</span>
            <span class="n">method</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">export</span><span class="p">(</span><span class="n">method</span><span class="p">)</span>
            <span class="nb">setattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">method_name</span><span class="p">,</span> <span class="n">types</span><span class="o">.</span><span class="n">MethodType</span><span class="p">(</span><span class="n">method</span><span class="p">,</span> <span class="bp">self</span><span class="p">))</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">init_from_module_rref</span><span class="p">(</span>
        <span class="n">remote_device</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="n">module_rref</span><span class="p">:</span> <span class="n">rpc</span><span class="o">.</span><span class="n">RRef</span><span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">],</span>
        <span class="n">_module_interface_cls</span><span class="p">:</span> <span class="n">Any</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Besides the constructor, a RemoteModule instance can also be initialized given a module RRef.</span>

<span class="sd">        This alternate initialization method can be particularly useful if we want to create multiple</span>
<span class="sd">        RemoteModule instances that share the same underlying module and reduce memory consumption.</span>

<span class="sd">        Moreover, this also provides a workaround for passing script RemoteModule over RPC,</span>
<span class="sd">        which is not supported. The recommended way is as follows:</span>

<span class="sd">            1. the sender creates a RemoteModule;</span>
<span class="sd">            2. the sender sends its ``module_rref`` over RPC;</span>
<span class="sd">            3. the receiver calls this method to initialize another RemoteModule using the same ``module_rref``.</span>

<span class="sd">        Example::</span>
<span class="sd">            Run the following code in two different processes:</span>

<span class="sd">            &gt;&gt;&gt; # xdoctest: +SKIP(&quot;distributed&quot;)</span>
<span class="sd">            &gt;&gt;&gt; # On worker 0:</span>
<span class="sd">            &gt;&gt;&gt; import torch</span>
<span class="sd">            &gt;&gt;&gt; import torch.distributed.rpc as rpc</span>
<span class="sd">            &gt;&gt;&gt; from torch import nn, Tensor</span>
<span class="sd">            &gt;&gt;&gt; from torch.distributed.nn.api.remote_module import RemoteModule</span>
<span class="sd">            &gt;&gt;&gt;</span>
<span class="sd">            &gt;&gt;&gt; rpc.init_rpc(&quot;worker0&quot;, rank=0, world_size=2)</span>
<span class="sd">            &gt;&gt;&gt; remote_module = RemoteModule(</span>
<span class="sd">            &gt;&gt;&gt;     &quot;worker1/cpu&quot;, nn.Linear, args=(20, 30),</span>
<span class="sd">            &gt;&gt;&gt; )</span>
<span class="sd">            &gt;&gt;&gt;</span>
<span class="sd">            &gt;&gt;&gt; remote_module1 = rpc.rpc_sync(</span>
<span class="sd">            &gt;&gt;&gt;     &quot;worker1/cpu&quot;,</span>
<span class="sd">            &gt;&gt;&gt;     RemoteModule.init_from_module_rref,</span>
<span class="sd">            &gt;&gt;&gt;     (&quot;worker1/cpu&quot;, remote_module1.get_module_rref()),</span>
<span class="sd">            &gt;&gt;&gt; )</span>
<span class="sd">            &gt;&gt;&gt; rpc.shutdown()</span>

<span class="sd">            &gt;&gt;&gt; # On worker 1:</span>
<span class="sd">            &gt;&gt;&gt; import torch</span>
<span class="sd">            &gt;&gt;&gt; import torch.distributed.rpc as rpc</span>
<span class="sd">            &gt;&gt;&gt;</span>
<span class="sd">            &gt;&gt;&gt; rpc.init_rpc(&quot;worker1&quot;, rank=1, world_size=2)</span>
<span class="sd">            &gt;&gt;&gt; rpc.shutdown()</span>

<span class="sd">        Args:</span>
<span class="sd">            remote_device (str): Device on the destination worker where we&#39;d like to place this module.</span>
<span class="sd">                The device can be a local device or a remote device specified by one of the following remote</span>
<span class="sd">                formats:</span>

<span class="sd">                    1. &quot;rank:&lt;rank&gt;/&lt;device&gt;&quot; (ex: &quot;rank:0/cuda:0&quot;).</span>
<span class="sd">                    2. &quot;&lt;worker_name&gt;/&lt;device&gt;&quot; (ex: &quot;trainer0/cuda:0&quot;).</span>

<span class="sd">                In addition, the device field can be optional and the default value is &quot;cpu&quot;.</span>
<span class="sd">            module_rref (RRef[nn.Module]): The module reference shared by both the caller and</span>
<span class="sd">                the created remote module.</span>
<span class="sd">            _module_interface_cls (type, optional): The TorchScript interface type for the module</span>
<span class="sd">                to be created. The type object should be decorated by @torch.jit.interface.</span>
<span class="sd">                If not provided, the generated RemoteModule is not torchscript-able.</span>
<span class="sd">                Warning, this is an experimental API and susceptible to frequent changes.</span>

<span class="sd">        Returns:</span>
<span class="sd">            A remote module instance which wraps the :class:`~nn.Module` created by the</span>
<span class="sd">            user-provided ``module_rref``, it has a blocking ``forward`` method and an</span>
<span class="sd">            asynchronous ``forward_async`` method that returns a future of the ``forward`` call</span>
<span class="sd">            on the user-provided module on the remote side.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># NOTE: if a new attribute is added to this class, also need to add it</span>
        <span class="c1"># to ``_REMOTE_MODULE_PICKLED_ATTRIBUTES`` for pickling/unpickling.</span>

        <span class="n">remote_module</span> <span class="o">=</span> <span class="nb">object</span><span class="o">.</span><span class="fm">__new__</span><span class="p">(</span><span class="n">RemoteModule</span><span class="p">)</span>

        <span class="n">enable_moving_cpu_tensors_to_cuda</span> <span class="o">=</span> <span class="n">remote_module</span><span class="o">.</span><span class="n">_prepare_init</span><span class="p">(</span><span class="n">remote_device</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">_module_interface_cls</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># Users reply on this field to know if this generated RemoteModule is TorchScript-able.</span>
            <span class="n">remote_module</span><span class="o">.</span><span class="n">is_scriptable</span> <span class="o">=</span> <span class="kc">True</span>

            <span class="n">remote_module</span><span class="o">.</span><span class="n">_init_template</span><span class="p">(</span>
                <span class="n">_module_interface_cls</span><span class="p">,</span> <span class="n">enable_moving_cpu_tensors_to_cuda</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">remote_module</span><span class="o">.</span><span class="n">is_scriptable</span> <span class="o">=</span> <span class="kc">False</span>
            <span class="n">remote_module</span><span class="o">.</span><span class="n">generated_methods</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">_NON_SCRIPTABLE_REMOTE_MODULE_MODULE</span><span class="o">.</span><span class="n">_generated_methods</span>
            <span class="p">)</span>
        <span class="n">remote_module</span><span class="o">.</span><span class="n">module_rref</span> <span class="o">=</span> <span class="n">module_rref</span>

        <span class="n">remote_module</span><span class="o">.</span><span class="n">_install_generated_methods</span><span class="p">()</span>
        <span class="n">remote_module</span><span class="o">.</span><span class="n">_check_attribute_picklability</span><span class="p">()</span>

        <span class="k">return</span> <span class="n">remote_module</span>


<div class="viewcode-block" id="RemoteModule">
<a class="viewcode-back" href="../../../../../python-api/rpc.html#torch.distributed.nn.api.remote_module.RemoteModule">[docs]</a>
<span class="k">class</span><span class="w"> </span><span class="nc">RemoteModule</span><span class="p">(</span><span class="n">_RemoteModule</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        A RemoteModule instance can only be created after RPC initialization.</span>

<span class="sd">        It creates a user-specified module on a specified remote node.</span>
<span class="sd">        It behaves like a regular ``nn.Module`` except that the ``forward`` method is</span>
<span class="sd">        executed on the remote node.</span>
<span class="sd">        It takes care of autograd recording to ensure the backward pass propagates</span>
<span class="sd">        gradients back to the corresponding remote module.</span>

<span class="sd">        It generates two methods ``forward_async`` and ``forward`` based on the</span>
<span class="sd">        signature of the ``forward`` method of ``module_cls``. ``forward_async``</span>
<span class="sd">        runs asynchronously and returns a Future. The arguments of ``forward_async``</span>
<span class="sd">        and ``forward`` are the same as the ``forward`` method of the module</span>
<span class="sd">        returned by the ``module_cls``.</span>

<span class="sd">        For example, if ``module_cls`` returns an instance of ``nn.Linear``,</span>
<span class="sd">        that has ``forward`` method signature: ``def forward(input: Tensor) -&gt; Tensor:``,</span>
<span class="sd">        the generated ``RemoteModule`` will have 2 methods with the signatures:</span>

<span class="sd">        | ``def forward(input: Tensor) -&gt; Tensor:``</span>
<span class="sd">        | ``def forward_async(input: Tensor) -&gt; Future[Tensor]:``</span>

<span class="sd">    Args:</span>
<span class="sd">        remote_device (str): Device on the destination worker where we&#39;d like to place this module.</span>
<span class="sd">            The format should be &quot;&lt;workername&gt;/&lt;device&gt;&quot;, where the device field can be parsed as torch.device type.</span>
<span class="sd">            E.g., &quot;trainer0/cpu&quot;, &quot;trainer0&quot;, &quot;ps0/cuda:0&quot;.</span>
<span class="sd">            In addition, the device field can be optional and the default value is &quot;cpu&quot;.</span>
<span class="sd">        module_cls (nn.Module): Class for the module to be created remotely. For example,</span>

<span class="sd">            &gt;&gt;&gt; class MyModule(nn.Module):</span>
<span class="sd">            &gt;&gt;&gt;     def forward(input):</span>
<span class="sd">            &gt;&gt;&gt;         return input + 1</span>
<span class="sd">            &gt;&gt;&gt;</span>
<span class="sd">            &gt;&gt;&gt; module_cls = MyModule</span>

<span class="sd">        args (Sequence, optional): args to be passed to ``module_cls``.</span>
<span class="sd">        kwargs (Dict, optional): kwargs to be passed to ``module_cls``.</span>

<span class="sd">    Returns:</span>
<span class="sd">        A remote module instance which wraps the :class:`~nn.Module` created by the</span>
<span class="sd">        user-provided ``module_cls``, it has a blocking ``forward`` method and an</span>
<span class="sd">        asynchronous ``forward_async`` method that returns a future of the ``forward`` call</span>
<span class="sd">        on the user-provided module on the remote side.</span>

<span class="sd">    Example::</span>
<span class="sd">        Run the following code in two different processes:</span>

<span class="sd">        &gt;&gt;&gt; # xdoctest: +SKIP(&quot;distributed&quot;)</span>
<span class="sd">        &gt;&gt;&gt; # On worker 0:</span>
<span class="sd">        &gt;&gt;&gt; import torch</span>
<span class="sd">        &gt;&gt;&gt; import torch.distributed.rpc as rpc</span>
<span class="sd">        &gt;&gt;&gt; from torch import nn, Tensor</span>
<span class="sd">        &gt;&gt;&gt; from torch.distributed.nn.api.remote_module import RemoteModule</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; rpc.init_rpc(&quot;worker0&quot;, rank=0, world_size=2)</span>
<span class="sd">        &gt;&gt;&gt; remote_linear_module = RemoteModule(</span>
<span class="sd">        &gt;&gt;&gt;     &quot;worker1/cpu&quot;, nn.Linear, args=(20, 30),</span>
<span class="sd">        &gt;&gt;&gt; )</span>
<span class="sd">        &gt;&gt;&gt; input = torch.randn(128, 20)</span>
<span class="sd">        &gt;&gt;&gt; ret_fut = remote_linear_module.forward_async(input)</span>
<span class="sd">        &gt;&gt;&gt; ret = ret_fut.wait()</span>
<span class="sd">        &gt;&gt;&gt; rpc.shutdown()</span>

<span class="sd">        &gt;&gt;&gt; # On worker 1:</span>
<span class="sd">        &gt;&gt;&gt; import torch</span>
<span class="sd">        &gt;&gt;&gt; import torch.distributed.rpc as rpc</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; rpc.init_rpc(&quot;worker1&quot;, rank=1, world_size=2)</span>
<span class="sd">        &gt;&gt;&gt; rpc.shutdown()</span>

<span class="sd">        Furthermore, a more practical example that is combined with</span>
<span class="sd">        `DistributedDataParallel &lt;https://pytorch.org/docs/stable/nn.html#torch.nn.parallel.DistributedDataParallel&gt;`__ (DDP)</span>
<span class="sd">        can be found in this `tutorial &lt;https://pytorch.org/tutorials/advanced/rpc_ddp_tutorial.html&gt;`__.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">remote_device</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="n">module_cls</span><span class="p">:</span> <span class="n">Type</span><span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">],</span>
        <span class="n">args</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">kwargs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">remote_device</span><span class="p">,</span> <span class="n">module_cls</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">)</span></div>



<span class="k">def</span><span class="w"> </span><span class="nf">_remote_module_receiver</span><span class="p">(</span>
    <span class="o">*</span><span class="n">remote_module_pickled_attrs</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Deserializes a RemoteModule.&quot;&quot;&quot;</span>
    <span class="n">serialized_remote_module</span> <span class="o">=</span> <span class="n">_SerializedRemoteModule</span><span class="o">.</span><span class="n">_make</span><span class="p">(</span>
        <span class="n">remote_module_pickled_attrs</span>
    <span class="p">)</span>
    <span class="n">m</span> <span class="o">=</span> <span class="nb">object</span><span class="o">.</span><span class="fm">__new__</span><span class="p">(</span><span class="n">RemoteModule</span><span class="p">)</span>
    <span class="n">m</span><span class="o">.</span><span class="vm">__dict__</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">serialized_remote_module</span><span class="o">.</span><span class="n">_asdict</span><span class="p">())</span>

    <span class="c1"># Unpickling the attribute `module_rref` must invoke RRef&#39;s `_deserialize()` method.</span>
    <span class="n">m</span><span class="o">.</span><span class="n">module_rref</span> <span class="o">=</span> <span class="n">rpc</span><span class="o">.</span><span class="n">PyRRef</span><span class="o">.</span><span class="n">_deserialize</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">module_rref</span><span class="p">)</span>

    <span class="c1"># Install generated methods when unpickled.</span>
    <span class="k">for</span> <span class="n">method</span> <span class="ow">in</span> <span class="n">m</span><span class="o">.</span><span class="n">generated_methods</span><span class="p">:</span>
        <span class="n">method_name</span> <span class="o">=</span> <span class="n">method</span><span class="o">.</span><span class="vm">__name__</span>
        <span class="n">method</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">export</span><span class="p">(</span><span class="n">method</span><span class="p">)</span>
        <span class="nb">setattr</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">method_name</span><span class="p">,</span> <span class="n">types</span><span class="o">.</span><span class="n">MethodType</span><span class="p">(</span><span class="n">method</span><span class="p">,</span> <span class="n">m</span><span class="p">))</span>

    <span class="k">return</span> <span class="n">m</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_remote_module_reducer</span><span class="p">(</span><span class="n">remote_module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Serialize a RemoteModule.&quot;&quot;&quot;</span>
    <span class="n">pickled_attrs</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">remote_module</span><span class="o">.</span><span class="vm">__dict__</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="c1"># Pickling the attribute `module_rref` must invoke RRef&#39;s `_serialize()` method.</span>
        <span class="k">if</span> <span class="n">k</span> <span class="o">==</span> <span class="s2">&quot;module_rref&quot;</span><span class="p">:</span>
            <span class="n">pickled_attrs</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">v</span><span class="o">.</span><span class="n">_serialize</span><span class="p">()</span>
        <span class="k">elif</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">_REMOTE_MODULE_PICKLED_ATTRIBUTES</span><span class="p">:</span>
            <span class="n">pickled_attrs</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">v</span>
        <span class="c1"># Check if unpickled attributes are all in _REMOTE_MODULE_ATTRIBUTES_IGNORE_FOR_PICKLING.</span>
        <span class="k">elif</span> <span class="n">k</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">_REMOTE_MODULE_ATTRIBUTES_IGNORE_FOR_PICKLING</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;The new attribute ``</span><span class="si">{</span><span class="n">k</span><span class="si">}</span><span class="s2">`` of RemoteModule is ignored during RPC pickling. &quot;</span>
                <span class="s2">&quot;To pickle this attribute, please add it to ``_REMOTE_MODULE_PICKLED_ATTRIBUTES``. &quot;</span>
                <span class="s2">&quot;Otherwise, please explicitly add it to ``_REMOTE_MODULE_ATTRIBUTES_IGNORE_FOR_PICKLING``.&quot;</span><span class="p">,</span>
                <span class="n">file</span><span class="o">=</span><span class="n">sys</span><span class="o">.</span><span class="n">stderr</span><span class="p">,</span>
            <span class="p">)</span>

    <span class="k">return</span> <span class="p">(</span>
        <span class="n">_remote_module_receiver</span><span class="p">,</span>
        <span class="nb">tuple</span><span class="p">(</span><span class="n">pickled_attrs</span><span class="o">.</span><span class="n">values</span><span class="p">()),</span>
    <span class="p">)</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_recursive_script_module_receiver</span><span class="p">(</span>
    <span class="n">recursive_script_module_serialized</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Deserializes a RecursiveScriptModule that does not contain a script RemoteModule.&quot;&quot;&quot;</span>
    <span class="n">f</span> <span class="o">=</span> <span class="n">io</span><span class="o">.</span><span class="n">BytesIO</span><span class="p">(</span><span class="n">recursive_script_module_serialized</span><span class="p">)</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">m</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_recursive_script_module_reducer</span><span class="p">(</span><span class="n">recursive_script_module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Serialize a RecursiveScriptModule that does not contain a script RemoteModule, and raises an error otherwise.&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">recursive_script_module</span><span class="o">.</span><span class="n">_c</span><span class="p">,</span> <span class="s2">&quot;module_rref&quot;</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
            <span class="s2">&quot;Passing a script RemoteModule over RPC is not supported. Please create a RemoteModule in the sender, &quot;</span>
            <span class="s2">&quot;send the `module_rref` to the receiver, and create a new instance on the receiver end by passing this `module_rref`.&quot;</span>
        <span class="p">)</span>

    <span class="n">f</span> <span class="o">=</span> <span class="n">io</span><span class="o">.</span><span class="n">BytesIO</span><span class="p">()</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">recursive_script_module</span><span class="p">,</span> <span class="n">f</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">_recursive_script_module_receiver</span><span class="p">,</span> <span class="p">(</span><span class="n">f</span><span class="o">.</span><span class="n">getvalue</span><span class="p">(),))</span>


<span class="n">_internal_rpc_pickler</span><span class="o">.</span><span class="n">_register_reducer</span><span class="p">(</span><span class="n">RemoteModule</span><span class="p">,</span> <span class="n">_remote_module_reducer</span><span class="p">)</span>
<span class="n">_internal_rpc_pickler</span><span class="o">.</span><span class="n">_register_reducer</span><span class="p">(</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">RecursiveScriptModule</span><span class="p">,</span> <span class="n">_recursive_script_module_reducer</span>
<span class="p">)</span>
</pre></div>

                </article>
              
              
              
                <footer class="bd-footer-article">
                  <div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item">
<div class="feedback">
  
<div class="rating">
    Rate this Page
    <div class="stars">
        
        <span class="star" data-value="1">★</span>
        
        <span class="star" data-value="2">★</span>
        
        <span class="star" data-value="3">★</span>
        
        <span class="star" data-value="4">★</span>
        
        <span class="star" data-value="5">★</span>
        
    </div>
</div>

  <div class="feedback-send">
    <button class="feedback-btn" onclick="openGitHubIssue()">Send Feedback</button>
  </div>
</div>
</div>
  
</div>
                </footer>
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
</div>
                </footer>
              
            </div>
            
            
              
            
          </div>
          <footer class="bd-footer-content">
            
          </footer>
        
      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script defer src="../../../../../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf"></script>
<script defer src="../../../../../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf"></script>


  <footer class="bd-footer">
    
    <div class="bd-footer__inner bd-page-width">
      
        <div class="footer-items__start">
          
            <div class="footer-item">

  <p class="copyright">
    
      © Copyright PyTorch Contributors.
      <br/>
    
  </p>
</div>
          
            <div class="footer-item">

  <p class="sphinx-version">
    Created using <a href="https://www.sphinx-doc.org/">Sphinx</a> 8.1.0.
    <br/>
  </p>
</div>
          
        </div>
      
    
    
      <div class="footer-items__end">
        
          <div class="footer-item">
<p class="theme-version">
  <!-- # L10n: Setting the PST URL as an argument as this does not need to be localized -->
  Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.16.1.
</p></div>
        
      </div>
    
   </div>
   

   <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
      <div class="container">
        <div class="row">
          <div class="col-md-4 text-center">
            <h2>Docs</h2>
            <p>Access comprehensive developer documentation for PyTorch</p>
            <a class="with-right-arrow" href="">View Docs</a>
          </div>

          <div class="col-md-4 text-center">
            <h2>Tutorials</h2>
            <p>Get in-depth tutorials for beginners and advanced developers</p>
            <a class="with-right-arrow" href="">View Tutorials</a>
          </div>

          <div class="col-md-4 text-center">
            <h2>Resources</h2>
            <p>Find development resources and get your questions answered</p>
            <a class="with-right-arrow" href="">View Resources</a>
          </div>
        </div>
      </div>
  </div>

  <footer class="site-footer">
      <div class="container footer-container">
        <div class="footer-logo-wrapper">
          <a href="" class="footer-logo"></a>
        </div>

        <div class="footer-links-wrapper">
          <div class="footer-links-col">
            <ul>
              <li class="list-title"><a href="">PyTorch</a></li>
              <li><a href="">Get Started</a></li>
              <li><a href="">Features</a></li>
              <li><a href="">Ecosystem</a></li>
              <li><a href="">Blog</a></li>
              <li><a href="">Contributing</a></li>
            </ul>
          </div>

          <div class="footer-links-col">
            <ul>
              <li class="list-title"><a href="">Resources</a></li>
              <li><a href="">Tutorials</a></li>
              <li><a href="">Docs</a></li>
              <li><a href="" target="_blank">Discuss</a></li>
              <li><a href="" target="_blank">Github Issues</a></li>
              <li><a href="" target="_blank">Brand Guidelines</a></li>
            </ul>
          </div>

          <div class="footer-links-col">
            <ul>
              <li class="list-title">Stay up to date</li>
              <li><a href="" target="_blank">Facebook</a></li>
              <li><a href="" target="_blank">Twitter</a></li>
              <li><a href="" target="_blank">YouTube</a></li>
              <li><a href="" target="_blank">LinkedIn</a></li>
            </ul>
            </div>

          <div class="footer-links-col">
            <ul>
              <li class="list-title">PyTorch Podcasts</li>
              <li><a href="" target="_blank">Spotify</a></li>
              <li><a href="" target="_blank">Apple</a></li>
              <li><a href="" target="_blank">Google</a></li>
              <li><a href="" target="_blank">Amazon</a></li>
            </ul>
           </div>
          </div>

          <div class="privacy-policy">
            <ul>
              <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/terms/" target="_blank">Terms</a></li>
              <li class="privacy-policy-links">|</li>
              <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/privacy-policy/" target="_blank">Privacy</a></li>
            </ul>
          </div>
          <div class="copyright">
          <p>© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation.
            For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see
            <a href="https://www.linuxfoundation.org/policies/">www.linuxfoundation.org/policies/</a>. The PyTorch Foundation supports the PyTorch open source
            project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC,
            please see <a href="https://www.lfprojects.org/policies/">www.lfprojects.org/policies/</a>.</p>
        </div>
       </div>

  <div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="../../../../../_static/img/pytorch-x.svg">
  </div>
</div>
  </footer>


  </body>
</html>