
<!DOCTYPE html>


<html lang="en" data-content_root="../../../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>torch.utils.data.dataloader &#8212; PyTorch main documentation</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  <!--
    this give us a css class that will be invisible only if js is disabled
  -->
  <noscript>
    <style>
      .pst-js-only { display: none !important; }

    </style>
  </noscript>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../../../_static/styles/theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />
<link href="../../../../_static/styles/pydata-sphinx-theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />

    <link rel="stylesheet" type="text/css" href="../../../../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/copybutton.css?v=949a1ff5" />
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/katex.min.css" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/katex-math.css?v=91adb8b6" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/css/jit.css?v=8de1ea5d" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/css/custom.css?v=3ccf5357" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/css/custom2.css?v=a9f46c5e" />
  
  <!-- So that users can add custom icons -->
  <script src="../../../../_static/scripts/fontawesome.js?digest=8878045cc6db502f8baf"></script>
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../../../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf" />
<link rel="preload" as="script" href="../../../../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf" />

    <script src="../../../../_static/documentation_options.js?v=a8da1a53"></script>
    <script src="../../../../_static/doctools.js?v=9bcbadda"></script>
    <script src="../../../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../../../_static/copybutton.js?v=940804e7"></script>
    <script src="../../../../_static/design-tabs.js?v=f930bc37"></script>
    <script type="module" src="https://cdn.jsdelivr.net/npm/mermaid@11.2.0/dist/mermaid.esm.min.mjs"></script>
    <script type="module" src="https://cdn.jsdelivr.net/npm/@mermaid-js/layout-elk@0.1.4/dist/mermaid-layout-elk.esm.min.mjs"></script>
    <script type="module">import mermaid from "https://cdn.jsdelivr.net/npm/mermaid@11.2.0/dist/mermaid.esm.min.mjs";import elkLayouts from "https://cdn.jsdelivr.net/npm/@mermaid-js/layout-elk@0.1.4/dist/mermaid-layout-elk.esm.min.mjs";mermaid.registerLayoutLoaders(elkLayouts);mermaid.initialize({startOnLoad:false});</script>
    <script src="https://cdn.jsdelivr.net/npm/d3@7.9.0/dist/d3.min.js"></script>
    <script type="module">
import mermaid from "https://cdn.jsdelivr.net/npm/mermaid@11.2.0/dist/mermaid.esm.min.mjs";
window.addEventListener("load", () => mermaid.run());
</script>
    <script>DOCUMENTATION_OPTIONS.pagename = '_modules/torch/utils/data/dataloader';</script>
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  <meta name="docsearch:version" content="main (2.6.0 )" />
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>

          <li class="main-menu-item">
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Learn
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="">
                  <span class=dropdown-title>Get Started</span>
                  <p>Run PyTorch locally or get started quickly with one of the supported cloud platforms</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Tutorials</span>
                  <p>Whats new in PyTorch tutorials</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Learn the Basics</span>
                  <p>Familiarize yourself with PyTorch concepts and modules</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">PyTorch Recipes</span>
                  <p>Bite-size, ready-to-deploy PyTorch code examples</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Intro to PyTorch - YouTube Series</span>
                  <p>Master PyTorch basics with our engaging YouTube tutorial series</p>
                </a>
              </div>
            </div>
          </li>

          <li>
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Ecosystem
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Tools</span>
                  <p>Learn about the tools and frameworks in the PyTorch Ecosystem</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class=dropdown-title>Community</span>
                  <p>Join the PyTorch developer community to contribute, learn, and get your questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="" target="_blank">
                  <span class=dropdown-title>Forums</span>
                  <p>A place to discuss PyTorch code, issues, install, research</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class=dropdown-title>Developer Resources</span>
                  <p>Find resources and get questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Contributor Awards - 2024</span>
                  <p>Award winners announced at this year's PyTorch Conference</p>
                </a>
              </div>
            </div>
          </li>

          <li>
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Edge
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">About PyTorch Edge</span>
                  <p>Build innovative and privacy-aware AI experiences for edge devices</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">ExecuTorch</span>
                  <p>End-to-end solution for enabling on-device inference capabilities across mobile and edge devices</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">ExecuTorch Docs</span>
                </a>
              </div>
            </div>  
          </li>

          <li class="main-menu-item">
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Docs
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">PyTorch</span>
                  <p>Explore the documentation for comprehensive guidance on how to use PyTorch</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">PyTorch Domains</span>
                  <p>Read the PyTorch Domains documentation to learn more about domain-specific libraries</p>
                </a>
              </div>
            </div>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Blogs & News 
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">PyTorch Blog</span>
                  <p>Catch up on the latest technical news and happenings</p>
                </a>
                 <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Community Blog</span>
                  <p>Stories from the PyTorch ecosystem</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Videos</span>
                  <p>Learn about the latest PyTorch tutorials, new, and more </p>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Community Stories</span>
                  <p>Learn how our community solves real, everyday machine learning problems with PyTorch</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Events</span>
                  <p>Find events, webinars, and podcasts</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Newsletter</span>
                  <p>Stay up-to-date with the latest updates</p>
                </a>
            </div>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                About
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">PyTorch Foundation</span>
                  <p>Learn more about the PyTorch Foundation</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Governing Board</span>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Cloud Credit Program</span>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Technical Advisory Council</span>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Staff</span>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Contact Us</span>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
            <div class="no-dropdown">
              <a href="" data-cta="join">
                Become a Member
              </a>
            </div>
          </li>
          <li>
           <div class="main-menu-item">
             <a href="" class="github-icon">
             </a>
           </div>
          </li>
          <!--- TODO: This block adds the search icon to the nav bar. We will enable it later. 
          <li>
            <div class="main-menu-item">
             <a href="" class="search-icon">
             </a>
            </div>
          </li>
          --->
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>
  </div>
</div>

  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <dialog id="pst-search-dialog">
    
<form class="bd-search d-flex align-items-center"
      action="../../../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </dialog>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
<div class="bd-header__inner bd-page-width">
  <button class="pst-navbar-icon sidebar-toggle primary-toggle" aria-label="Site navigation">
    <span class="fa-solid fa-bars"></span>
  </button>
  
  
  <div class=" navbar-header-items__start">
    
      <div class="navbar-item">
<div class="version-switcher__container dropdown pst-js-only">
  <button id="pst-version-switcher-button-2"
    type="button"
    class="version-switcher__button btn btn-sm dropdown-toggle"
    data-bs-toggle="dropdown"
    aria-haspopup="listbox"
    aria-controls="pst-version-switcher-list-2"
    aria-label="Version switcher list"
  >
    Choose version  <!-- this text may get changed later by javascript -->
    <span class="caret"></span>
  </button>
  <div id="pst-version-switcher-list-2"
    class="version-switcher__menu dropdown-menu list-group-flush py-0"
    role="listbox" aria-labelledby="pst-version-switcher-button-2">
    <!-- dropdown will be populated by javascript on page load -->
  </div>
</div></div>
    
      <div class="navbar-item">

  
    
  

<a class="navbar-brand logo" href="../../../../index.html">
  
  
  
  
  
  
    <p class="title logo__title">Home</p>
  
</a></div>
    
  </div>
  
  <div class=" navbar-header-items">
    
    <div class="me-auto navbar-header-items__center">
      
        <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../../python-api/index.html">
    Python API
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../../notes/index.html">
    Developer Notes
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../../community/index.html">
    Community
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../../lang-bindings/index.html">
    Language Bindings
  </a>
</li>

  </ul>
</nav></div>
      
    </div>
    
    
    <div class="navbar-header-items__end">
      
        <div class="navbar-item navbar-persistent--container">
          

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button>
        </div>
      
      
        <div class="navbar-item">

<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button></div>
      
        <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://x.com/PyTorch" title="X" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-x-twitter fa-lg" aria-hidden="true"></i>
            <span class="sr-only">X</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/pytorch/pytorch" title="GitHub" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">GitHub</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://pypi.org/project/torch" title="PyPI" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-custom fa-pypi fa-lg" aria-hidden="true"></i>
            <span class="sr-only">PyPI</span></a>
        </li>
</ul></div>
      
    </div>
    
  </div>
  
  
    <div class="navbar-persistent--mobile">

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button>
    </div>
  

  
</div>

    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
        
      
      <dialog id="pst-primary-sidebar-modal"></dialog>
      <div id="pst-primary-sidebar" class="bd-sidebar-primary bd-sidebar hide-on-wide">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
      <div class="sidebar-header-items__center">
        
          
          
            <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../../python-api/index.html">
    Python API
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../../notes/index.html">
    Developer Notes
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../../community/index.html">
    Community
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../../lang-bindings/index.html">
    Language Bindings
  </a>
</li>

  </ul>
</nav></div>
          
        
      </div>
    
    
    
      <div class="sidebar-header-items__end">
        
          <div class="navbar-item">

<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button></div>
        
          <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://x.com/PyTorch" title="X" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-x-twitter fa-lg" aria-hidden="true"></i>
            <span class="sr-only">X</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/pytorch/pytorch" title="GitHub" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">GitHub</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://pypi.org/project/torch" title="PyPI" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-custom fa-pypi fa-lg" aria-hidden="true"></i>
            <span class="sr-only">PyPI</span></a>
        </li>
</ul></div>
        
      </div>
    
  </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
      <div class="sidebar-primary-item">
<div id="ethical-ad-placement"
      class="flat"
      data-ea-publisher="readthedocs"
      data-ea-type="readthedocs-sidebar"
      data-ea-manual="true">
</div></div>
  </div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        
          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item">

<nav aria-label="Breadcrumb" class="d-print-none">
  <ul class="bd-breadcrumbs">
    
    <li class="breadcrumb-item breadcrumb-home">
      <a href="../../../../index.html" class="nav-link" aria-label="Home">
        <i class="fa-solid fa-home"></i>
      </a>
    </li>
    
    <li class="breadcrumb-item"><a href="../../../index.html" class="nav-link">Module code</a></li>
    
    
    <li class="breadcrumb-item"><a href="../../../torch.html" class="nav-link">torch</a></li>
    
    
    <li class="breadcrumb-item"><a href="../../utils.html" class="nav-link">torch.utils</a></li>
    
    <li class="breadcrumb-item active" aria-current="page"><span class="ellipsis">torch.utils.data.dataloader</span></li>
  </ul>
</nav>
</div>
      
    </div>
  
  
</div>
</div>
              
              
              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <h1>Source code for torch.utils.data.dataloader</h1><div class="highlight"><pre>
<span></span><span class="c1"># mypy: allow-untyped-defs</span>
<span class="sa">r</span><span class="sd">&quot;&quot;&quot;Definition of the DataLoader and associated iterators that subclass _BaseDataLoaderIter.</span>

<span class="sd">To support these two classes, in `./_utils` we define many utility methods and</span>
<span class="sd">functions to be run in multiprocessing. E.g., the data loading worker loop is</span>
<span class="sd">in `./_utils/worker.py`.</span>
<span class="sd">&quot;&quot;&quot;</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">functools</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">itertools</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">logging</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">multiprocessing</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">python_multiprocessing</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">os</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">queue</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">threading</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">warnings</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">typing</span><span class="w"> </span><span class="kn">import</span> <span class="n">Any</span><span class="p">,</span> <span class="n">Callable</span><span class="p">,</span> <span class="n">Generic</span><span class="p">,</span> <span class="n">Iterable</span><span class="p">,</span> <span class="n">List</span><span class="p">,</span> <span class="n">Optional</span><span class="p">,</span> <span class="n">TypeVar</span><span class="p">,</span> <span class="n">Union</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.distributed</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">dist</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.utils.data.graph_settings</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch._utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">ExceptionWrapper</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch.utils.data</span><span class="w"> </span><span class="kn">import</span> <span class="n">_utils</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch.utils.data.datapipes.datapipe</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">_IterDataPipeSerializationWrapper</span><span class="p">,</span>
    <span class="n">_MapDataPipeSerializationWrapper</span><span class="p">,</span>
    <span class="n">IterDataPipe</span><span class="p">,</span>
    <span class="n">MapDataPipe</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch.utils.data.dataset</span><span class="w"> </span><span class="kn">import</span> <span class="n">Dataset</span><span class="p">,</span> <span class="n">IterableDataset</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch.utils.data.sampler</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">BatchSampler</span><span class="p">,</span>
    <span class="n">RandomSampler</span><span class="p">,</span>
    <span class="n">Sampler</span><span class="p">,</span>
    <span class="n">SequentialSampler</span><span class="p">,</span>
<span class="p">)</span>


<span class="n">__all__</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s2">&quot;DataLoader&quot;</span><span class="p">,</span>
    <span class="s2">&quot;get_worker_info&quot;</span><span class="p">,</span>
    <span class="s2">&quot;default_collate&quot;</span><span class="p">,</span>
    <span class="s2">&quot;default_convert&quot;</span><span class="p">,</span>
<span class="p">]</span>


<span class="n">_T</span> <span class="o">=</span> <span class="n">TypeVar</span><span class="p">(</span><span class="s2">&quot;_T&quot;</span><span class="p">)</span>
<span class="n">_T_co</span> <span class="o">=</span> <span class="n">TypeVar</span><span class="p">(</span><span class="s2">&quot;_T_co&quot;</span><span class="p">,</span> <span class="n">covariant</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">_worker_init_fn_t</span> <span class="o">=</span> <span class="n">Callable</span><span class="p">[[</span><span class="nb">int</span><span class="p">],</span> <span class="kc">None</span><span class="p">]</span>

<span class="c1"># Ideally we would parameterize `DataLoader` by the return type of `collate_fn`, but there is currently no way to have that</span>
<span class="c1"># type parameter set to a default value if the user doesn&#39;t pass in a custom &#39;collate_fn&#39;.</span>
<span class="c1"># See https://github.com/python/mypy/issues/3737.</span>
<span class="n">_collate_fn_t</span> <span class="o">=</span> <span class="n">Callable</span><span class="p">[[</span><span class="n">List</span><span class="p">[</span><span class="n">_T</span><span class="p">]],</span> <span class="n">Any</span><span class="p">]</span>


<span class="c1"># These functions used to be defined in this file. However, it was moved to</span>
<span class="c1"># _utils/collate.py. Although it is rather hard to access this from user land</span>
<span class="c1"># (one has to explicitly directly `import torch.utils.data.dataloader`), there</span>
<span class="c1"># probably is user code out there using it. This aliasing maintains BC in this</span>
<span class="c1"># aspect.</span>
<span class="n">default_collate</span><span class="p">:</span> <span class="n">_collate_fn_t</span> <span class="o">=</span> <span class="n">_utils</span><span class="o">.</span><span class="n">collate</span><span class="o">.</span><span class="n">default_collate</span>
<span class="n">default_convert</span> <span class="o">=</span> <span class="n">_utils</span><span class="o">.</span><span class="n">collate</span><span class="o">.</span><span class="n">default_convert</span>

<span class="n">get_worker_info</span> <span class="o">=</span> <span class="n">_utils</span><span class="o">.</span><span class="n">worker</span><span class="o">.</span><span class="n">get_worker_info</span>

<span class="n">logger</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">(</span><span class="vm">__name__</span><span class="p">)</span>


<span class="k">class</span><span class="w"> </span><span class="nc">_DatasetKind</span><span class="p">:</span>
    <span class="n">Map</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">Iterable</span> <span class="o">=</span> <span class="mi">1</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">create_fetcher</span><span class="p">(</span><span class="n">kind</span><span class="p">,</span> <span class="n">dataset</span><span class="p">,</span> <span class="n">auto_collation</span><span class="p">,</span> <span class="n">collate_fn</span><span class="p">,</span> <span class="n">drop_last</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">kind</span> <span class="o">==</span> <span class="n">_DatasetKind</span><span class="o">.</span><span class="n">Map</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">_utils</span><span class="o">.</span><span class="n">fetch</span><span class="o">.</span><span class="n">_MapDatasetFetcher</span><span class="p">(</span>
                <span class="n">dataset</span><span class="p">,</span> <span class="n">auto_collation</span><span class="p">,</span> <span class="n">collate_fn</span><span class="p">,</span> <span class="n">drop_last</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">_utils</span><span class="o">.</span><span class="n">fetch</span><span class="o">.</span><span class="n">_IterableDatasetFetcher</span><span class="p">(</span>
                <span class="n">dataset</span><span class="p">,</span> <span class="n">auto_collation</span><span class="p">,</span> <span class="n">collate_fn</span><span class="p">,</span> <span class="n">drop_last</span>
            <span class="p">)</span>


<span class="k">class</span><span class="w"> </span><span class="nc">_InfiniteConstantSampler</span><span class="p">(</span><span class="n">Sampler</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Analogous to ``itertools.repeat(None, None)``.</span>

<span class="sd">    Used as sampler for :class:`~torch.utils.data.IterableDataset`.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__iter__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
            <span class="k">yield</span> <span class="kc">None</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_get_distributed_settings</span><span class="p">():</span>
    <span class="k">if</span> <span class="n">dist</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="ow">and</span> <span class="n">dist</span><span class="o">.</span><span class="n">is_initialized</span><span class="p">():</span>
        <span class="k">return</span> <span class="n">dist</span><span class="o">.</span><span class="n">get_world_size</span><span class="p">(),</span> <span class="n">dist</span><span class="o">.</span><span class="n">get_rank</span><span class="p">()</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_sharding_worker_init_fn</span><span class="p">(</span><span class="n">worker_init_fn</span><span class="p">,</span> <span class="n">world_size</span><span class="p">,</span> <span class="n">rank_id</span><span class="p">,</span> <span class="n">worker_id</span><span class="p">):</span>
    <span class="n">global_worker_id</span> <span class="o">=</span> <span class="n">worker_id</span>
    <span class="n">info</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">get_worker_info</span><span class="p">()</span>
    <span class="k">assert</span> <span class="n">info</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
    <span class="n">total_workers</span> <span class="o">=</span> <span class="n">info</span><span class="o">.</span><span class="n">num_workers</span>
    <span class="n">datapipe</span> <span class="o">=</span> <span class="n">info</span><span class="o">.</span><span class="n">dataset</span>
    <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">datapipe</span><span class="p">,</span> <span class="p">(</span><span class="n">IterDataPipe</span><span class="p">,</span> <span class="n">MapDataPipe</span><span class="p">))</span>
    <span class="c1"># To distribute elements across distributed process evenly, we should shard data on distributed</span>
    <span class="c1"># processes first then shard on worker processes</span>
    <span class="n">total_workers</span> <span class="o">*=</span> <span class="n">world_size</span>
    <span class="n">global_worker_id</span> <span class="o">=</span> <span class="n">global_worker_id</span> <span class="o">*</span> <span class="n">world_size</span> <span class="o">+</span> <span class="n">rank_id</span>
    <span class="c1"># For BC, use default SHARDING_PRIORITIES</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">graph_settings</span><span class="o">.</span><span class="n">apply_sharding</span><span class="p">(</span>
        <span class="n">datapipe</span><span class="p">,</span> <span class="n">total_workers</span><span class="p">,</span> <span class="n">global_worker_id</span>
    <span class="p">)</span>
    <span class="k">if</span> <span class="n">worker_init_fn</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">worker_init_fn</span><span class="p">(</span><span class="n">worker_id</span><span class="p">)</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_share_dist_seed</span><span class="p">(</span><span class="n">generator</span><span class="p">,</span> <span class="n">pg</span><span class="p">):</span>
    <span class="n">_shared_seed</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">((),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span><span class="o">.</span><span class="n">random_</span><span class="p">(</span><span class="n">generator</span><span class="o">=</span><span class="n">generator</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">pg</span><span class="p">,</span> <span class="n">dist</span><span class="o">.</span><span class="n">ProcessGroup</span><span class="p">):</span>
        <span class="n">dist</span><span class="o">.</span><span class="n">broadcast</span><span class="p">(</span><span class="n">_shared_seed</span><span class="p">,</span> <span class="n">src</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="n">pg</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">_shared_seed</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>


<div class="viewcode-block" id="DataLoader">
<a class="viewcode-back" href="../../../../python-api/data.html#torch.utils.data.DataLoader">[docs]</a>
<span class="k">class</span><span class="w"> </span><span class="nc">DataLoader</span><span class="p">(</span><span class="n">Generic</span><span class="p">[</span><span class="n">_T_co</span><span class="p">]):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Data loader combines a dataset and a sampler, and provides an iterable over the given dataset.</span>

<span class="sd">    The :class:`~torch.utils.data.DataLoader` supports both map-style and</span>
<span class="sd">    iterable-style datasets with single- or multi-process loading, customizing</span>
<span class="sd">    loading order and optional automatic batching (collation) and memory pinning.</span>

<span class="sd">    See :py:mod:`torch.utils.data` documentation page for more details.</span>

<span class="sd">    Args:</span>
<span class="sd">        dataset (Dataset): dataset from which to load the data.</span>
<span class="sd">        batch_size (int, optional): how many samples per batch to load</span>
<span class="sd">            (default: ``1``).</span>
<span class="sd">        shuffle (bool, optional): set to ``True`` to have the data reshuffled</span>
<span class="sd">            at every epoch (default: ``False``).</span>
<span class="sd">        sampler (Sampler or Iterable, optional): defines the strategy to draw</span>
<span class="sd">            samples from the dataset. Can be any ``Iterable`` with ``__len__``</span>
<span class="sd">            implemented. If specified, :attr:`shuffle` must not be specified.</span>
<span class="sd">        batch_sampler (Sampler or Iterable, optional): like :attr:`sampler`, but</span>
<span class="sd">            returns a batch of indices at a time. Mutually exclusive with</span>
<span class="sd">            :attr:`batch_size`, :attr:`shuffle`, :attr:`sampler`,</span>
<span class="sd">            and :attr:`drop_last`.</span>
<span class="sd">        num_workers (int, optional): how many subprocesses to use for data</span>
<span class="sd">            loading. ``0`` means that the data will be loaded in the main process.</span>
<span class="sd">            (default: ``0``)</span>
<span class="sd">        collate_fn (Callable, optional): merges a list of samples to form a</span>
<span class="sd">            mini-batch of Tensor(s).  Used when using batched loading from a</span>
<span class="sd">            map-style dataset.</span>
<span class="sd">        pin_memory (bool, optional): If ``True``, the data loader will copy Tensors</span>
<span class="sd">            into device/CUDA pinned memory before returning them.  If your data elements</span>
<span class="sd">            are a custom type, or your :attr:`collate_fn` returns a batch that is a custom type,</span>
<span class="sd">            see the example below.</span>
<span class="sd">        drop_last (bool, optional): set to ``True`` to drop the last incomplete batch,</span>
<span class="sd">            if the dataset size is not divisible by the batch size. If ``False`` and</span>
<span class="sd">            the size of dataset is not divisible by the batch size, then the last batch</span>
<span class="sd">            will be smaller. (default: ``False``)</span>
<span class="sd">        timeout (numeric, optional): if positive, the timeout value for collecting a batch</span>
<span class="sd">            from workers. Should always be non-negative. (default: ``0``)</span>
<span class="sd">        worker_init_fn (Callable, optional): If not ``None``, this will be called on each</span>
<span class="sd">            worker subprocess with the worker id (an int in ``[0, num_workers - 1]``) as</span>
<span class="sd">            input, after seeding and before data loading. (default: ``None``)</span>
<span class="sd">        multiprocessing_context (str or multiprocessing.context.BaseContext, optional): If</span>
<span class="sd">            ``None``, the default `multiprocessing context`_ of your operating system will</span>
<span class="sd">            be used. (default: ``None``)</span>
<span class="sd">        generator (torch.Generator, optional): If not ``None``, this RNG will be used</span>
<span class="sd">            by RandomSampler to generate random indexes and multiprocessing to generate</span>
<span class="sd">            ``base_seed`` for workers. (default: ``None``)</span>
<span class="sd">        prefetch_factor (int, optional, keyword-only arg): Number of batches loaded</span>
<span class="sd">            in advance by each worker. ``2`` means there will be a total of</span>
<span class="sd">            2 * num_workers batches prefetched across all workers. (default value depends</span>
<span class="sd">            on the set value for num_workers. If value of num_workers=0 default is ``None``.</span>
<span class="sd">            Otherwise, if value of ``num_workers &gt; 0`` default is ``2``).</span>
<span class="sd">        persistent_workers (bool, optional): If ``True``, the data loader will not shut down</span>
<span class="sd">            the worker processes after a dataset has been consumed once. This allows to</span>
<span class="sd">            maintain the workers `Dataset` instances alive. (default: ``False``)</span>
<span class="sd">        pin_memory_device (str, optional): the device to :attr:`pin_memory` to if ``pin_memory`` is</span>
<span class="sd">            ``True``.</span>
<span class="sd">        in_order (bool, optional): If ``False``, the data loader will not enforce that batches</span>
<span class="sd">            are returned in a first-in, first-out order. Only applies when ``num_workers &gt; 0``. (default: ``True``)</span>


<span class="sd">    .. warning:: If the ``spawn`` start method is used, :attr:`worker_init_fn`</span>
<span class="sd">                 cannot be an unpicklable object, e.g., a lambda function. See</span>
<span class="sd">                 :ref:`multiprocessing-best-practices` on more details related</span>
<span class="sd">                 to multiprocessing in PyTorch.</span>

<span class="sd">    .. warning:: ``len(dataloader)`` heuristic is based on the length of the sampler used.</span>
<span class="sd">                 When :attr:`dataset` is an :class:`~torch.utils.data.IterableDataset`,</span>
<span class="sd">                 it instead returns an estimate based on ``len(dataset) / batch_size``, with proper</span>
<span class="sd">                 rounding depending on :attr:`drop_last`, regardless of multi-process loading</span>
<span class="sd">                 configurations. This represents the best guess PyTorch can make because PyTorch</span>
<span class="sd">                 trusts user :attr:`dataset` code in correctly handling multi-process</span>
<span class="sd">                 loading to avoid duplicate data.</span>

<span class="sd">                 However, if sharding results in multiple workers having incomplete last batches,</span>
<span class="sd">                 this estimate can still be inaccurate, because (1) an otherwise complete batch can</span>
<span class="sd">                 be broken into multiple ones and (2) more than one batch worth of samples can be</span>
<span class="sd">                 dropped when :attr:`drop_last` is set. Unfortunately, PyTorch can not detect such</span>
<span class="sd">                 cases in general.</span>

<span class="sd">                 See `Dataset Types`_ for more details on these two types of datasets and how</span>
<span class="sd">                 :class:`~torch.utils.data.IterableDataset` interacts with</span>
<span class="sd">                 `Multi-process data loading`_.</span>

<span class="sd">    .. warning:: See :ref:`reproducibility`, and :ref:`dataloader-workers-random-seed`, and</span>
<span class="sd">                 :ref:`data-loading-randomness` notes for random seed related questions.</span>

<span class="sd">    .. warning:: Setting `in_order` to `False` can harm reproducibility and may lead to a skewed data</span>
<span class="sd">                 distribution being fed to the trainer in cases with imbalanced data.</span>

<span class="sd">    .. _multiprocessing context:</span>
<span class="sd">        https://docs.python.org/3/library/multiprocessing.html#contexts-and-start-methods</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">dataset</span><span class="p">:</span> <span class="n">Dataset</span><span class="p">[</span><span class="n">_T_co</span><span class="p">]</span>
    <span class="n">batch_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span>
    <span class="n">num_workers</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">pin_memory</span><span class="p">:</span> <span class="nb">bool</span>
    <span class="n">drop_last</span><span class="p">:</span> <span class="nb">bool</span>
    <span class="n">timeout</span><span class="p">:</span> <span class="nb">float</span>
    <span class="n">sampler</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">Sampler</span><span class="p">,</span> <span class="n">Iterable</span><span class="p">]</span>
    <span class="n">pin_memory_device</span><span class="p">:</span> <span class="nb">str</span>
    <span class="n">prefetch_factor</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span>
    <span class="n">_iterator</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="s2">&quot;_BaseDataLoaderIter&quot;</span><span class="p">]</span>
    <span class="n">__initialized</span> <span class="o">=</span> <span class="kc">False</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">dataset</span><span class="p">:</span> <span class="n">Dataset</span><span class="p">[</span><span class="n">_T_co</span><span class="p">],</span>
        <span class="n">batch_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">shuffle</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">sampler</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">Sampler</span><span class="p">,</span> <span class="n">Iterable</span><span class="p">,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">batch_sampler</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">Sampler</span><span class="p">[</span><span class="n">List</span><span class="p">],</span> <span class="n">Iterable</span><span class="p">[</span><span class="n">List</span><span class="p">],</span> <span class="kc">None</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">num_workers</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
        <span class="n">collate_fn</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">_collate_fn_t</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">pin_memory</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">drop_last</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">timeout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
        <span class="n">worker_init_fn</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">_worker_init_fn_t</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">multiprocessing_context</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">generator</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">prefetch_factor</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">persistent_workers</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">pin_memory_device</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span><span class="p">,</span>
        <span class="n">in_order</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_log_api_usage_once</span><span class="p">(</span><span class="s2">&quot;python.data_loader&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">num_workers</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;num_workers option should be non-negative; &quot;</span>
                <span class="s2">&quot;use num_workers=0 to disable multiprocessing.&quot;</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="n">timeout</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;timeout option should be non-negative&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">num_workers</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">prefetch_factor</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;prefetch_factor option could only be specified in multiprocessing.&quot;</span>
                <span class="s2">&quot;let num_workers &gt; 0 to enable multiprocessing, otherwise set prefetch_factor to None.&quot;</span>
            <span class="p">)</span>
        <span class="k">elif</span> <span class="n">num_workers</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">prefetch_factor</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">prefetch_factor</span> <span class="o">=</span> <span class="mi">2</span>
        <span class="k">elif</span> <span class="n">prefetch_factor</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">prefetch_factor</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;prefetch_factor option should be non-negative&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">persistent_workers</span> <span class="ow">and</span> <span class="n">num_workers</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;persistent_workers option needs num_workers &gt; 0&quot;</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_workers</span> <span class="o">=</span> <span class="n">num_workers</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">prefetch_factor</span> <span class="o">=</span> <span class="n">prefetch_factor</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pin_memory</span> <span class="o">=</span> <span class="n">pin_memory</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pin_memory_device</span> <span class="o">=</span> <span class="n">pin_memory_device</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">timeout</span> <span class="o">=</span> <span class="n">timeout</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">worker_init_fn</span> <span class="o">=</span> <span class="n">worker_init_fn</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">multiprocessing_context</span> <span class="o">=</span> <span class="n">multiprocessing_context</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">in_order</span> <span class="o">=</span> <span class="n">in_order</span>

        <span class="c1"># Adds forward compatibilities so classic DataLoader can work with DataPipes:</span>
        <span class="c1">#   _DataPipeSerializationWrapper container makes it easier to serialize without redefining pickler</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dataset</span><span class="p">,</span> <span class="n">IterDataPipe</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">dataset</span> <span class="o">=</span> <span class="n">_IterDataPipeSerializationWrapper</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dataset</span><span class="p">)</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dataset</span><span class="p">,</span> <span class="n">MapDataPipe</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">dataset</span> <span class="o">=</span> <span class="n">_MapDataPipeSerializationWrapper</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dataset</span><span class="p">)</span>

        <span class="c1"># Arg-check dataset related before checking samplers because we want to</span>
        <span class="c1"># tell users that iterable-style datasets are incompatible with custom</span>
        <span class="c1"># samplers first, so that they don&#39;t learn that this combo doesn&#39;t work</span>
        <span class="c1"># after spending time fixing the custom sampler errors.</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">IterableDataset</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_dataset_kind</span> <span class="o">=</span> <span class="n">_DatasetKind</span><span class="o">.</span><span class="n">Iterable</span>
            <span class="c1"># NOTE [ Custom Samplers and IterableDataset ]</span>
            <span class="c1">#</span>
            <span class="c1"># `IterableDataset` does not support custom `batch_sampler` or</span>
            <span class="c1"># `sampler` since the key is irrelevant (unless we support</span>
            <span class="c1"># generator-style dataset one day...).</span>
            <span class="c1">#</span>
            <span class="c1"># For `sampler`, we always create a dummy sampler. This is an</span>
            <span class="c1"># infinite sampler even when the dataset may have an implemented</span>
            <span class="c1"># finite `__len__` because in multi-process data loading, naive</span>
            <span class="c1"># settings will return duplicated data (which may be desired), and</span>
            <span class="c1"># thus using a sampler with length matching that of dataset will</span>
            <span class="c1"># cause data lost (you may have duplicates of the first couple</span>
            <span class="c1"># batches, but never see anything afterwards). Therefore,</span>
            <span class="c1"># `Iterabledataset` always uses an infinite sampler, an instance of</span>
            <span class="c1"># `_InfiniteConstantSampler` defined above.</span>
            <span class="c1">#</span>
            <span class="c1"># A custom `batch_sampler` essentially only controls the batch size.</span>
            <span class="c1"># However, it is unclear how useful it would be since an iterable-style</span>
            <span class="c1"># dataset can handle that within itself. Moreover, it is pointless</span>
            <span class="c1"># in multi-process data loading as the assignment order of batches</span>
            <span class="c1"># to workers is an implementation detail so users can not control</span>
            <span class="c1"># how to batchify each worker&#39;s iterable. Thus, we disable this</span>
            <span class="c1"># option. If this turns out to be useful in future, we can re-enable</span>
            <span class="c1"># this, and support custom samplers that specify the assignments to</span>
            <span class="c1"># specific workers.</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">IterDataPipe</span><span class="p">):</span>
                <span class="k">if</span> <span class="n">shuffle</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="n">dataset</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">graph_settings</span><span class="o">.</span><span class="n">apply_shuffle_settings</span><span class="p">(</span>
                        <span class="n">dataset</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="n">shuffle</span>
                    <span class="p">)</span>
            <span class="c1"># We cannot check `shuffle is not None` here, since previously `shuffle=False` was the default.</span>
            <span class="k">elif</span> <span class="n">shuffle</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">{</span><span class="kc">False</span><span class="p">,</span> <span class="kc">None</span><span class="p">}:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;DataLoader with IterableDataset: expected unspecified shuffle option, but got shuffle=</span><span class="si">{</span><span class="n">shuffle</span><span class="si">}</span><span class="s2">&quot;</span>
                <span class="p">)</span>

            <span class="k">if</span> <span class="n">sampler</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="c1"># See NOTE [ Custom Samplers and IterableDataset ]</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;DataLoader with IterableDataset: expected unspecified sampler option, but got sampler=</span><span class="si">{</span><span class="n">sampler</span><span class="si">}</span><span class="s2">&quot;</span>
                <span class="p">)</span>
            <span class="k">elif</span> <span class="n">batch_sampler</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="c1"># See NOTE [ Custom Samplers and IterableDataset ]</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s2">&quot;DataLoader with IterableDataset: expected unspecified &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;batch_sampler option, but got batch_sampler=</span><span class="si">{</span><span class="n">batch_sampler</span><span class="si">}</span><span class="s2">&quot;</span>
                <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">shuffle</span> <span class="o">=</span> <span class="nb">bool</span><span class="p">(</span><span class="n">shuffle</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_dataset_kind</span> <span class="o">=</span> <span class="n">_DatasetKind</span><span class="o">.</span><span class="n">Map</span>

        <span class="k">if</span> <span class="n">sampler</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">shuffle</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;sampler option is mutually exclusive with &quot;</span> <span class="s2">&quot;shuffle&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">batch_sampler</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># auto_collation with custom batch_sampler</span>
            <span class="k">if</span> <span class="n">batch_size</span> <span class="o">!=</span> <span class="mi">1</span> <span class="ow">or</span> <span class="n">shuffle</span> <span class="ow">or</span> <span class="n">sampler</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">drop_last</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s2">&quot;batch_sampler option is mutually exclusive &quot;</span>
                    <span class="s2">&quot;with batch_size, shuffle, sampler, and &quot;</span>
                    <span class="s2">&quot;drop_last&quot;</span>
                <span class="p">)</span>
            <span class="n">batch_size</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="n">drop_last</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="k">elif</span> <span class="n">batch_size</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># no auto_collation</span>
            <span class="k">if</span> <span class="n">drop_last</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s2">&quot;batch_size=None option disables auto-batching &quot;</span>
                    <span class="s2">&quot;and is mutually exclusive with drop_last&quot;</span>
                <span class="p">)</span>

        <span class="k">if</span> <span class="n">sampler</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>  <span class="c1"># give default samplers</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dataset_kind</span> <span class="o">==</span> <span class="n">_DatasetKind</span><span class="o">.</span><span class="n">Iterable</span><span class="p">:</span>
                <span class="c1"># See NOTE [ Custom Samplers and IterableDataset ]</span>
                <span class="n">sampler</span> <span class="o">=</span> <span class="n">_InfiniteConstantSampler</span><span class="p">()</span>
            <span class="k">else</span><span class="p">:</span>  <span class="c1"># map-style</span>
                <span class="k">if</span> <span class="n">shuffle</span><span class="p">:</span>
                    <span class="n">sampler</span> <span class="o">=</span> <span class="n">RandomSampler</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">generator</span><span class="o">=</span><span class="n">generator</span><span class="p">)</span>  <span class="c1"># type: ignore[arg-type]</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">sampler</span> <span class="o">=</span> <span class="n">SequentialSampler</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>  <span class="c1"># type: ignore[arg-type]</span>

        <span class="k">if</span> <span class="n">batch_size</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">batch_sampler</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># auto_collation without custom batch_sampler</span>
            <span class="n">batch_sampler</span> <span class="o">=</span> <span class="n">BatchSampler</span><span class="p">(</span><span class="n">sampler</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">drop_last</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span> <span class="o">=</span> <span class="n">batch_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">drop_last</span> <span class="o">=</span> <span class="n">drop_last</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sampler</span> <span class="o">=</span> <span class="n">sampler</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">batch_sampler</span> <span class="o">=</span> <span class="n">batch_sampler</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">generator</span> <span class="o">=</span> <span class="n">generator</span>

        <span class="k">if</span> <span class="n">collate_fn</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_auto_collation</span><span class="p">:</span>
                <span class="n">collate_fn</span> <span class="o">=</span> <span class="n">_utils</span><span class="o">.</span><span class="n">collate</span><span class="o">.</span><span class="n">default_collate</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">collate_fn</span> <span class="o">=</span> <span class="n">_utils</span><span class="o">.</span><span class="n">collate</span><span class="o">.</span><span class="n">default_convert</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">collate_fn</span> <span class="o">=</span> <span class="n">collate_fn</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">persistent_workers</span> <span class="o">=</span> <span class="n">persistent_workers</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">__initialized</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_IterableDataset_len_called</span> <span class="o">=</span> <span class="p">(</span>
            <span class="kc">None</span>  <span class="c1"># See NOTE [ IterableDataset and __len__ ]</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_iterator</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">check_worker_number_rationality</span><span class="p">()</span>

        <span class="n">torch</span><span class="o">.</span><span class="n">set_vital</span><span class="p">(</span><span class="s2">&quot;Dataloader&quot;</span><span class="p">,</span> <span class="s2">&quot;enabled&quot;</span><span class="p">,</span> <span class="s2">&quot;True&quot;</span><span class="p">)</span>  <span class="c1"># type: ignore[attr-defined]</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_get_iterator</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;_BaseDataLoaderIter&quot;</span><span class="p">:</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_workers</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">_SingleProcessDataLoaderIter</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">check_worker_number_rationality</span><span class="p">()</span>
            <span class="k">return</span> <span class="n">_MultiProcessingDataLoaderIter</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">multiprocessing_context</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">__multiprocessing_context</span>

    <span class="nd">@multiprocessing_context</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">multiprocessing_context</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">multiprocessing_context</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">multiprocessing_context</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_workers</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">multiprocessing_context</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
                    <span class="n">valid_start_methods</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">multiprocessing</span><span class="o">.</span><span class="n">get_all_start_methods</span><span class="p">()</span>
                    <span class="k">if</span> <span class="n">multiprocessing_context</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">valid_start_methods</span><span class="p">:</span>
                        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                            <span class="s2">&quot;multiprocessing_context option &quot;</span>
                            <span class="sa">f</span><span class="s2">&quot;should specify a valid start method in </span><span class="si">{</span><span class="n">valid_start_methods</span><span class="si">!r}</span><span class="s2">, but got &quot;</span>
                            <span class="sa">f</span><span class="s2">&quot;multiprocessing_context=</span><span class="si">{</span><span class="n">multiprocessing_context</span><span class="si">!r}</span><span class="s2">&quot;</span>
                        <span class="p">)</span>
                    <span class="n">multiprocessing_context</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">multiprocessing</span><span class="o">.</span><span class="n">get_context</span><span class="p">(</span>
                        <span class="n">multiprocessing_context</span>
                    <span class="p">)</span>

                <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span>
                    <span class="n">multiprocessing_context</span><span class="p">,</span> <span class="n">python_multiprocessing</span><span class="o">.</span><span class="n">context</span><span class="o">.</span><span class="n">BaseContext</span>
                <span class="p">):</span>
                    <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span>
                        <span class="s2">&quot;multiprocessing_context option should be a valid context &quot;</span>
                        <span class="s2">&quot;object or a string specifying the start method, but got &quot;</span>
                        <span class="sa">f</span><span class="s2">&quot;multiprocessing_context=</span><span class="si">{</span><span class="n">multiprocessing_context</span><span class="si">}</span><span class="s2">&quot;</span>
                    <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s2">&quot;multiprocessing_context can only be used with &quot;</span>
                    <span class="s2">&quot;multi-process loading (num_workers &gt; 0), but got &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;num_workers=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">num_workers</span><span class="si">}</span><span class="s2">&quot;</span>
                <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">__multiprocessing_context</span> <span class="o">=</span> <span class="n">multiprocessing_context</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__setattr__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">attr</span><span class="p">,</span> <span class="n">val</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">__initialized</span> <span class="ow">and</span> <span class="n">attr</span> <span class="ow">in</span> <span class="p">(</span>
            <span class="s2">&quot;batch_size&quot;</span><span class="p">,</span>
            <span class="s2">&quot;batch_sampler&quot;</span><span class="p">,</span>
            <span class="s2">&quot;sampler&quot;</span><span class="p">,</span>
            <span class="s2">&quot;drop_last&quot;</span><span class="p">,</span>
            <span class="s2">&quot;dataset&quot;</span><span class="p">,</span>
            <span class="s2">&quot;persistent_workers&quot;</span><span class="p">,</span>
        <span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">attr</span><span class="si">}</span><span class="s2"> attribute should not be set after </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2"> is initialized&quot;</span>
            <span class="p">)</span>

        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__setattr__</span><span class="p">(</span><span class="n">attr</span><span class="p">,</span> <span class="n">val</span><span class="p">)</span>

    <span class="c1"># We quote &#39;_BaseDataLoaderIter&#39; since it isn&#39;t defined yet and the definition can&#39;t be moved up</span>
    <span class="c1"># since &#39;_BaseDataLoaderIter&#39; references &#39;DataLoader&#39;.</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__iter__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;_BaseDataLoaderIter&quot;</span><span class="p">:</span>
        <span class="c1"># When using a single worker the returned iterator should be</span>
        <span class="c1"># created everytime to avoid resetting its state</span>
        <span class="c1"># However, in the case of a multiple workers iterator</span>
        <span class="c1"># the iterator is only created once in the lifetime of the</span>
        <span class="c1"># DataLoader object so that workers can be reused</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">persistent_workers</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_workers</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_iterator</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_iterator</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_iterator</span><span class="p">()</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_iterator</span><span class="o">.</span><span class="n">_reset</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_iterator</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_iterator</span><span class="p">()</span>

    <span class="nd">@property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">_auto_collation</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_sampler</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>

    <span class="nd">@property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">_index_sampler</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># The actual sampler used for generating indices for `_DatasetFetcher`</span>
        <span class="c1"># (see _utils/fetch.py) to read data at each time. This would be</span>
        <span class="c1"># `.batch_sampler` if in auto-collation mode, and `.sampler` otherwise.</span>
        <span class="c1"># We can&#39;t change `.sampler` and `.batch_sampler` attributes for BC</span>
        <span class="c1"># reasons.</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_auto_collation</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_sampler</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">sampler</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dataset_kind</span> <span class="o">==</span> <span class="n">_DatasetKind</span><span class="o">.</span><span class="n">Iterable</span><span class="p">:</span>
            <span class="c1"># NOTE [ IterableDataset and __len__ ]</span>
            <span class="c1">#</span>
            <span class="c1"># For `IterableDataset`, `__len__` could be inaccurate when one naively</span>
            <span class="c1"># does multi-processing data loading, since the samples will be duplicated.</span>
            <span class="c1"># However, no real use case should be actually using that behavior, so</span>
            <span class="c1"># it should count as a user error. We should generally trust user</span>
            <span class="c1"># code to do the proper thing (e.g., configure each replica differently</span>
            <span class="c1"># in `__iter__`), and give us the correct `__len__` if they choose to</span>
            <span class="c1"># implement it (this will still throw if the dataset does not implement</span>
            <span class="c1"># a `__len__`).</span>
            <span class="c1">#</span>
            <span class="c1"># To provide a further warning, we track if `__len__` was called on the</span>
            <span class="c1"># `DataLoader`, save the returned value in `self._len_called`, and warn</span>
            <span class="c1"># if the iterator ends up yielding more than this number of samples.</span>

            <span class="c1"># Cannot statically verify that dataset is Sized</span>
            <span class="n">length</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_IterableDataset_len_called</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dataset</span><span class="p">)</span>  <span class="c1"># type: ignore[assignment, arg-type]</span>
            <span class="k">if</span> <span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
            <span class="p">):</span>  <span class="c1"># IterableDataset doesn&#39;t allow custom sampler or batch_sampler</span>
                <span class="kn">from</span><span class="w"> </span><span class="nn">math</span><span class="w"> </span><span class="kn">import</span> <span class="n">ceil</span>

                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">drop_last</span><span class="p">:</span>
                    <span class="n">length</span> <span class="o">=</span> <span class="n">length</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">length</span> <span class="o">=</span> <span class="n">ceil</span><span class="p">(</span><span class="n">length</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">length</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_index_sampler</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">check_worker_number_rationality</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># This function check whether the dataloader&#39;s worker number is rational based on</span>
        <span class="c1"># current system&#39;s resource. Current rule is that if the number of workers this</span>
        <span class="c1"># Dataloader will create is bigger than the number of logical cpus that is allowed to</span>
        <span class="c1"># use, than we will pop up a warning to let user pay attention.</span>
        <span class="c1">#</span>
        <span class="c1"># eg. If current system has 2 physical CPUs with 16 cores each. And each core support 2</span>
        <span class="c1">#     threads, then the total logical cpus here is 2 * 16 * 2 = 64. Let&#39;s say current</span>
        <span class="c1">#     DataLoader process can use half of them which is 32, then the rational max number of</span>
        <span class="c1">#     worker that initiated from this process is 32.</span>
        <span class="c1">#     Now, let&#39;s say the created DataLoader has num_works = 40, which is bigger than 32.</span>
        <span class="c1">#     So the warning message is triggered to notify the user to lower the worker number if</span>
        <span class="c1">#     necessary.</span>
        <span class="c1">#</span>
        <span class="c1">#</span>
        <span class="c1"># [Note] Please note that this function repects `cpuset` only when os.sched_getaffinity is</span>
        <span class="c1">#        available (available in most of Linux system, but not OSX and Windows).</span>
        <span class="c1">#        When os.sched_getaffinity is not available, os.cpu_count() is called instead, but</span>
        <span class="c1">#        it doesn&#39;t repect cpuset.</span>
        <span class="c1">#        We don&#39;t take threading into account since each worker process is single threaded</span>
        <span class="c1">#        at this time.</span>
        <span class="c1">#</span>
        <span class="c1">#        We don&#39;t set any threading flags (eg. OMP_NUM_THREADS, MKL_NUM_THREADS, etc)</span>
        <span class="c1">#        other than `torch.set_num_threads` to 1 in the worker process, if the passing</span>
        <span class="c1">#        in functions use 3rd party modules that rely on those threading flags to determine</span>
        <span class="c1">#        how many thread to create (eg. numpy, etc), then it is caller&#39;s responsibility to</span>
        <span class="c1">#        set those flags correctly.</span>
        <span class="k">def</span><span class="w"> </span><span class="nf">_create_warning_msg</span><span class="p">(</span><span class="n">num_worker_suggest</span><span class="p">,</span> <span class="n">num_worker_created</span><span class="p">,</span> <span class="n">cpuset_checked</span><span class="p">):</span>
            <span class="n">suggested_max_worker_msg</span> <span class="o">=</span> <span class="p">(</span>
                <span class="p">(</span>
                    <span class="p">(</span>
                        <span class="s2">&quot;Our suggested max number of worker in current system is </span><span class="si">{}{}</span><span class="s2">, which is smaller &quot;</span>
                        <span class="s2">&quot;than what this DataLoader is going to create.&quot;</span>
                    <span class="p">)</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                        <span class="n">num_worker_suggest</span><span class="p">,</span>
                        <span class="p">(</span>
                            <span class="s2">&quot;&quot;</span>
                            <span class="k">if</span> <span class="n">cpuset_checked</span>
                            <span class="k">else</span> <span class="s2">&quot; (`cpuset` is not taken into account)&quot;</span>
                        <span class="p">),</span>
                    <span class="p">)</span>
                <span class="p">)</span>
                <span class="k">if</span> <span class="n">num_worker_suggest</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
                <span class="k">else</span> <span class="p">(</span>
                    <span class="s2">&quot;DataLoader is not able to compute a suggested max number of worker in current system.&quot;</span>
                <span class="p">)</span>
            <span class="p">)</span>

            <span class="n">warn_msg</span> <span class="o">=</span> <span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;This DataLoader will create </span><span class="si">{</span><span class="n">num_worker_created</span><span class="si">}</span><span class="s2"> worker processes in total. </span><span class="si">{</span><span class="n">suggested_max_worker_msg</span><span class="si">}</span><span class="s2"> &quot;</span>
                <span class="s2">&quot;Please be aware that excessive worker creation might get DataLoader running slow or even freeze, &quot;</span>
                <span class="s2">&quot;lower the worker number to avoid potential slowness/freeze if necessary.&quot;</span>
            <span class="p">)</span>
            <span class="k">return</span> <span class="n">warn_msg</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_workers</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_workers</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">return</span>

        <span class="c1"># try to compute a suggested max number of worker based on system&#39;s resource</span>
        <span class="n">max_num_worker_suggest</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="n">cpuset_checked</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">os</span><span class="p">,</span> <span class="s2">&quot;sched_getaffinity&quot;</span><span class="p">):</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="n">max_num_worker_suggest</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">sched_getaffinity</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>
                <span class="n">cpuset_checked</span> <span class="o">=</span> <span class="kc">True</span>
            <span class="k">except</span> <span class="ne">Exception</span><span class="p">:</span>
                <span class="k">pass</span>
        <span class="k">if</span> <span class="n">max_num_worker_suggest</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># os.cpu_count() could return Optional[int]</span>
            <span class="c1"># get cpu count first and check None in order to satisfy mypy check</span>
            <span class="n">cpu_count</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">cpu_count</span><span class="p">()</span>
            <span class="k">if</span> <span class="n">cpu_count</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">max_num_worker_suggest</span> <span class="o">=</span> <span class="n">cpu_count</span>

        <span class="k">if</span> <span class="n">max_num_worker_suggest</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
                <span class="n">_create_warning_msg</span><span class="p">(</span>
                    <span class="n">max_num_worker_suggest</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_workers</span><span class="p">,</span> <span class="n">cpuset_checked</span>
                <span class="p">)</span>
            <span class="p">)</span>
            <span class="k">return</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_workers</span> <span class="o">&gt;</span> <span class="n">max_num_worker_suggest</span><span class="p">:</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
                <span class="n">_create_warning_msg</span><span class="p">(</span>
                    <span class="n">max_num_worker_suggest</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_workers</span><span class="p">,</span> <span class="n">cpuset_checked</span>
                <span class="p">)</span>
            <span class="p">)</span></div>



<span class="k">class</span><span class="w"> </span><span class="nc">_BaseDataLoaderIter</span><span class="p">:</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">loader</span><span class="p">:</span> <span class="n">DataLoader</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_dataset</span> <span class="o">=</span> <span class="n">loader</span><span class="o">.</span><span class="n">dataset</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_shared_seed</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_pg</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_dataset</span><span class="p">,</span> <span class="n">IterDataPipe</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">dist</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="ow">and</span> <span class="n">dist</span><span class="o">.</span><span class="n">is_initialized</span><span class="p">():</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_pg</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">new_group</span><span class="p">(</span><span class="n">backend</span><span class="o">=</span><span class="s2">&quot;gloo&quot;</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_shared_seed</span> <span class="o">=</span> <span class="n">_share_dist_seed</span><span class="p">(</span><span class="n">loader</span><span class="o">.</span><span class="n">generator</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_pg</span><span class="p">)</span>
            <span class="n">shared_rng</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Generator</span><span class="p">()</span>
            <span class="n">shared_rng</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_shared_seed</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_dataset</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">graph_settings</span><span class="o">.</span><span class="n">apply_random_seed</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_dataset</span><span class="p">,</span> <span class="n">shared_rng</span>
            <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_dataset_kind</span> <span class="o">=</span> <span class="n">loader</span><span class="o">.</span><span class="n">_dataset_kind</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_IterableDataset_len_called</span> <span class="o">=</span> <span class="n">loader</span><span class="o">.</span><span class="n">_IterableDataset_len_called</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_auto_collation</span> <span class="o">=</span> <span class="n">loader</span><span class="o">.</span><span class="n">_auto_collation</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_drop_last</span> <span class="o">=</span> <span class="n">loader</span><span class="o">.</span><span class="n">drop_last</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_index_sampler</span> <span class="o">=</span> <span class="n">loader</span><span class="o">.</span><span class="n">_index_sampler</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_num_workers</span> <span class="o">=</span> <span class="n">loader</span><span class="o">.</span><span class="n">num_workers</span>
        <span class="n">ws</span><span class="p">,</span> <span class="n">rank</span> <span class="o">=</span> <span class="n">_get_distributed_settings</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_world_size</span> <span class="o">=</span> <span class="n">ws</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_rank</span> <span class="o">=</span> <span class="n">rank</span>
        <span class="c1"># for other backends, pin_memory_device need to set. if not set</span>
        <span class="c1"># default behaviour is CUDA device. if pin_memory_device is selected</span>
        <span class="c1"># and pin_memory is not set, the default behaviour false.</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">loader</span><span class="o">.</span><span class="n">pin_memory_device</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_pin_memory</span> <span class="o">=</span> <span class="n">loader</span><span class="o">.</span><span class="n">pin_memory</span> <span class="ow">and</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_pin_memory_device</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">loader</span><span class="o">.</span><span class="n">pin_memory</span><span class="p">:</span>
                <span class="n">warn_msg</span> <span class="o">=</span> <span class="p">(</span>
                    <span class="s2">&quot;pin memory device is set and pin_memory flag is not used then device pinned memory won&#39;t be used&quot;</span>
                    <span class="s2">&quot;please set pin_memory to true, if you need to use the device pin memory&quot;</span>
                <span class="p">)</span>
                <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="n">warn_msg</span><span class="p">)</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">_pin_memory</span> <span class="o">=</span> <span class="n">loader</span><span class="o">.</span><span class="n">pin_memory</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_pin_memory_device</span> <span class="o">=</span> <span class="n">loader</span><span class="o">.</span><span class="n">pin_memory_device</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_timeout</span> <span class="o">=</span> <span class="n">loader</span><span class="o">.</span><span class="n">timeout</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_collate_fn</span> <span class="o">=</span> <span class="n">loader</span><span class="o">.</span><span class="n">collate_fn</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_sampler_iter</span> <span class="o">=</span> <span class="nb">iter</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_index_sampler</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_base_seed</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">((),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span>
            <span class="o">.</span><span class="n">random_</span><span class="p">(</span><span class="n">generator</span><span class="o">=</span><span class="n">loader</span><span class="o">.</span><span class="n">generator</span><span class="p">)</span>
            <span class="o">.</span><span class="n">item</span><span class="p">()</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_persistent_workers</span> <span class="o">=</span> <span class="n">loader</span><span class="o">.</span><span class="n">persistent_workers</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_num_yielded</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_profile_name</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;enumerate(DataLoader)#</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">.__next__&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__iter__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;_BaseDataLoaderIter&quot;</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_reset</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">loader</span><span class="p">,</span> <span class="n">first_iter</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_sampler_iter</span> <span class="o">=</span> <span class="nb">iter</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_index_sampler</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_num_yielded</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_IterableDataset_len_called</span> <span class="o">=</span> <span class="n">loader</span><span class="o">.</span><span class="n">_IterableDataset_len_called</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_dataset</span><span class="p">,</span> <span class="n">IterDataPipe</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_shared_seed</span> <span class="o">=</span> <span class="n">_share_dist_seed</span><span class="p">(</span><span class="n">loader</span><span class="o">.</span><span class="n">generator</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_pg</span><span class="p">)</span>
            <span class="n">shared_rng</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Generator</span><span class="p">()</span>
            <span class="n">shared_rng</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_shared_seed</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_dataset</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">graph_settings</span><span class="o">.</span><span class="n">apply_random_seed</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_dataset</span><span class="p">,</span> <span class="n">shared_rng</span>
            <span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_next_index</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">next</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_sampler_iter</span><span class="p">)</span>  <span class="c1"># may raise StopIteration</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_next_data</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__next__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Any</span><span class="p">:</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">profiler</span><span class="o">.</span><span class="n">record_function</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_profile_name</span><span class="p">):</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_sampler_iter</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="c1"># TODO(https://github.com/pytorch/pytorch/issues/76750)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_reset</span><span class="p">()</span>  <span class="c1"># type: ignore[call-arg]</span>
            <span class="n">data</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_next_data</span><span class="p">()</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_num_yielded</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="k">if</span> <span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_dataset_kind</span> <span class="o">==</span> <span class="n">_DatasetKind</span><span class="o">.</span><span class="n">Iterable</span>
                <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">_IterableDataset_len_called</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
                <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_yielded</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">_IterableDataset_len_called</span>
            <span class="p">):</span>
                <span class="n">warn_msg</span> <span class="o">=</span> <span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;Length of IterableDataset </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">_dataset</span><span class="si">}</span><span class="s2"> was reported to be </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">_IterableDataset_len_called</span><span class="si">}</span><span class="s2">&quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;(when accessing len(dataloader)), but </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">_num_yielded</span><span class="si">}</span><span class="s2"> samples have been fetched. &quot;</span>
                <span class="p">)</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_workers</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="n">warn_msg</span> <span class="o">+=</span> <span class="p">(</span>
                        <span class="s2">&quot;For multiprocessing data-loading, this could be caused by not properly configuring the &quot;</span>
                        <span class="s2">&quot;IterableDataset replica at each worker. Please see &quot;</span>
                        <span class="s2">&quot;https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.&quot;</span>
                    <span class="p">)</span>
                <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="n">warn_msg</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">data</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
        <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_index_sampler</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">__getstate__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># TODO: add limited pickling support for sharing an iterator</span>
        <span class="c1"># across multiple threads for HOGWILD.</span>
        <span class="c1"># Probably the best way to do this is by moving the sample pushing</span>
        <span class="c1"># to a separate thread and then just sharing the data queue</span>
        <span class="c1"># but signalling the end is tricky without a non-blocking API</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">{}</span><span class="s2"> cannot be pickled&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="p">)</span>


<span class="k">class</span><span class="w"> </span><span class="nc">_SingleProcessDataLoaderIter</span><span class="p">(</span><span class="n">_BaseDataLoaderIter</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">loader</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">loader</span><span class="p">)</span>
        <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">_timeout</span> <span class="o">==</span> <span class="mi">0</span>
        <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_workers</span> <span class="o">==</span> <span class="mi">0</span>

        <span class="c1"># Adds forward compatibilities so classic DataLoader can work with DataPipes:</span>
        <span class="c1">#   Taking care of distributed sharding</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_dataset</span><span class="p">,</span> <span class="p">(</span><span class="n">IterDataPipe</span><span class="p">,</span> <span class="n">MapDataPipe</span><span class="p">)):</span>
            <span class="c1"># For BC, use default SHARDING_PRIORITIES</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">graph_settings</span><span class="o">.</span><span class="n">apply_sharding</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_dataset</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_world_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_rank</span>
            <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_dataset_fetcher</span> <span class="o">=</span> <span class="n">_DatasetKind</span><span class="o">.</span><span class="n">create_fetcher</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_dataset_kind</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_dataset</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_auto_collation</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_collate_fn</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_drop_last</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_next_data</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">index</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_next_index</span><span class="p">()</span>  <span class="c1"># may raise StopIteration</span>
        <span class="n">data</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dataset_fetcher</span><span class="o">.</span><span class="n">fetch</span><span class="p">(</span><span class="n">index</span><span class="p">)</span>  <span class="c1"># may raise StopIteration</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_pin_memory</span><span class="p">:</span>
            <span class="n">data</span> <span class="o">=</span> <span class="n">_utils</span><span class="o">.</span><span class="n">pin_memory</span><span class="o">.</span><span class="n">pin_memory</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_pin_memory_device</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">data</span>


<span class="k">class</span><span class="w"> </span><span class="nc">_MultiProcessingDataLoaderIter</span><span class="p">(</span><span class="n">_BaseDataLoaderIter</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Iterates once over the DataLoader&#39;s dataset, as specified by the sampler.&quot;&quot;&quot;</span>

    <span class="c1"># NOTE [ Data Loader Multiprocessing Shutdown Logic ]</span>
    <span class="c1">#</span>
    <span class="c1"># Preliminary:</span>
    <span class="c1">#</span>
    <span class="c1"># Our data model looks like this (queues are indicated with curly brackets):</span>
    <span class="c1">#</span>
    <span class="c1">#                main process                              ||</span>
    <span class="c1">#                     |                                    ||</span>
    <span class="c1">#               {index_queue}                              ||</span>
    <span class="c1">#                     |                                    ||</span>
    <span class="c1">#              worker processes                            ||     DATA</span>
    <span class="c1">#                     |                                    ||</span>
    <span class="c1">#            {worker_result_queue}                         ||     FLOW</span>
    <span class="c1">#                     |                                    ||</span>
    <span class="c1">#      pin_memory_thread of main process                   ||   DIRECTION</span>
    <span class="c1">#                     |                                    ||</span>
    <span class="c1">#               {data_queue}                               ||</span>
    <span class="c1">#                     |                                    ||</span>
    <span class="c1">#                data output                               \/</span>
    <span class="c1">#</span>
    <span class="c1"># P.S. `worker_result_queue` and `pin_memory_thread` part may be omitted if</span>
    <span class="c1">#      `pin_memory=False`.</span>
    <span class="c1">#</span>
    <span class="c1">#</span>
    <span class="c1"># Terminating multiprocessing logic requires very careful design. In</span>
    <span class="c1"># particular, we need to make sure that</span>
    <span class="c1">#</span>
    <span class="c1">#   1. The iterator gracefully exits the workers when its last reference is</span>
    <span class="c1">#      gone or it is depleted.</span>
    <span class="c1">#</span>
    <span class="c1">#      In this case, the workers should be gracefully exited because the</span>
    <span class="c1">#      main process may still need to continue to run, and we want cleaning</span>
    <span class="c1">#      up code in the workers to be executed (e.g., releasing GPU memory).</span>
    <span class="c1">#      Naturally, we implement the shutdown logic in `__del__` of</span>
    <span class="c1">#      DataLoaderIterator.</span>
    <span class="c1">#</span>
    <span class="c1">#      We delay the discussion on the logic in this case until later.</span>
    <span class="c1">#</span>
    <span class="c1">#   2. The iterator exits the workers when the loader process and/or worker</span>
    <span class="c1">#      processes exits normally or with error.</span>
    <span class="c1">#</span>
    <span class="c1">#      We set all workers and `pin_memory_thread` to have `daemon=True`.</span>
    <span class="c1">#</span>
    <span class="c1">#      You may ask, why can&#39;t we make the workers non-daemonic, and</span>
    <span class="c1">#      gracefully exit using the same logic as we have in `__del__` when the</span>
    <span class="c1">#      iterator gets deleted (see 1 above)?</span>
    <span class="c1">#</span>
    <span class="c1">#      First of all, `__del__` is **not** guaranteed to be called when</span>
    <span class="c1">#      interpreter exits. Even if it is called, by the time it executes,</span>
    <span class="c1">#      many Python core library resources may already be freed, and even</span>
    <span class="c1">#      simple things like acquiring an internal lock of a queue may hang.</span>
    <span class="c1">#      Therefore, in this case, we actually need to prevent `__del__` from</span>
    <span class="c1">#      being executed, and rely on the automatic termination of daemonic</span>
    <span class="c1">#      children.</span>
    <span class="c1">#</span>
    <span class="c1">#      Thus, we register an `atexit` hook that sets a global flag</span>
    <span class="c1">#      `_utils.python_exit_status`. Since `atexit` hooks are executed in the</span>
    <span class="c1">#      reverse order of registration, we are guaranteed that this flag is</span>
    <span class="c1">#      set before library resources we use are freed (which, at least in</span>
    <span class="c1">#      CPython, is done via an `atexit` handler defined in</span>
    <span class="c1">#      `multiprocessing/util.py`</span>
    <span class="c1">#      https://github.com/python/cpython/blob/c606624af8d4cb3b4a052fb263bb983b3f87585b/Lib/multiprocessing/util.py#L320-L362</span>
    <span class="c1">#      registered when an object requiring this mechanism is first</span>
    <span class="c1">#      created, e.g., `mp.Queue`</span>
    <span class="c1">#      https://github.com/python/cpython/blob/c606624af8d4cb3b4a052fb263bb983b3f87585b/Lib/multiprocessing/context.py#L100-L103</span>
    <span class="c1">#      https://github.com/python/cpython/blob/c606624af8d4cb3b4a052fb263bb983b3f87585b/Lib/multiprocessing/queues.py#L29</span>
    <span class="c1">#      )</span>
    <span class="c1">#</span>
    <span class="c1">#      So in `__del__`, we check if `_utils.python_exit_status` is set or</span>
    <span class="c1">#      `None` (freed), and perform no-op if so.</span>
    <span class="c1">#</span>
    <span class="c1">#      However, simply letting library clean-up codes run can also be bad,</span>
    <span class="c1">#      because such codes (i.e., `multiprocessing.util._exit_function()`)</span>
    <span class="c1">#      include join putting threads for `mp.Queue`, which can be blocking.</span>
    <span class="c1">#      Hence, the main process putting threads are called with</span>
    <span class="c1">#      `cancel_join_thread` at creation.  See later section</span>
    <span class="c1">#      [ 3b. A process won&#39;t hang when putting into a queue; ]</span>
    <span class="c1">#      for more details.</span>
    <span class="c1">#</span>
    <span class="c1">#      Here are two example cases where library clean-up codes can run</span>
    <span class="c1">#      before `__del__` is called:</span>
    <span class="c1">#</span>
    <span class="c1">#        1. If we hold onto a reference to the iterator, it more often</span>
    <span class="c1">#           than not tries to do `multiprocessing` library cleaning before</span>
    <span class="c1">#           clearing the alive referenced objects (https://github.com/pytorch/pytorch/issues/48666)</span>
    <span class="c1">#           and thus prevents our cleaning-up code to run first.</span>
    <span class="c1">#</span>
    <span class="c1">#        2. A similar issue araises when a `DataLoader` is used in a subprocess.</span>
    <span class="c1">#           When a process ends, it shuts the all its daemonic children</span>
    <span class="c1">#           down with a SIGTERM (instead of joining them without a timeout).</span>
    <span class="c1">#           Simiarly for threads, but by a different mechanism. This fact,</span>
    <span class="c1">#           together with a few implementation details of multiprocessing, forces</span>
    <span class="c1">#           us to make workers daemonic. All of our problems arise when a</span>
    <span class="c1">#           DataLoader is used in a subprocess, and are caused by multiprocessing</span>
    <span class="c1">#           code which looks more or less like this:</span>
    <span class="c1">#</span>
    <span class="c1">#               try:</span>
    <span class="c1">#                   your_function_using_a_dataloader()</span>
    <span class="c1">#               finally:</span>
    <span class="c1">#                   multiprocessing.util._exit_function()</span>
    <span class="c1">#</span>
    <span class="c1">#           The joining/termination mentioned above happens inside</span>
    <span class="c1">#           `_exit_function()`. Now, if `your_function_using_a_dataloader()`</span>
    <span class="c1">#           throws, the stack trace stored in the exception will prevent the</span>
    <span class="c1">#           frame which uses `DataLoaderIter` to be freed. If the frame has any</span>
    <span class="c1">#           reference to the `DataLoaderIter` (e.g., in a method of the iter),</span>
    <span class="c1">#           its  `__del__`, which starts the shutdown procedure, will not be</span>
    <span class="c1">#           called. That, in turn, means that workers aren&#39;t notified. Attempting</span>
    <span class="c1">#           to join in `_exit_function` will then result in a hang.</span>
    <span class="c1">#</span>
    <span class="c1">#           For context, `_exit_function` is also registered as an `atexit` call.</span>
    <span class="c1">#           So it is unclear to me (@ssnl) why this is needed in a finally block.</span>
    <span class="c1">#           The code dates back to 2008 and there is no comment on the original</span>
    <span class="c1">#           PEP 371 or patch https://bugs.python.org/issue3050 (containing both</span>
    <span class="c1">#           the finally block and the `atexit` registration) that explains this.</span>
    <span class="c1">#</span>
    <span class="c1">#</span>
    <span class="c1">#      Finally, another choice is to just shutdown workers with logic in 1</span>
    <span class="c1">#      above whenever we see an error in `next`. This isn&#39;t ideal because</span>
    <span class="c1">#        a. It prevents users from using try-catch to resume data loading.</span>
    <span class="c1">#        b. It doesn&#39;t prevent hanging if users have references to the</span>
    <span class="c1">#           iterator.</span>
    <span class="c1">#</span>
    <span class="c1">#   3. All processes exit if any of them die unexpectedly by fatal signals.</span>
    <span class="c1">#</span>
    <span class="c1">#      As shown above, the workers are set as daemonic children of the main</span>
    <span class="c1">#      process. However, automatic cleaning-up of such child processes only</span>
    <span class="c1">#      happens if the parent process exits gracefully (e.g., not via fatal</span>
    <span class="c1">#      signals like SIGKILL). So we must ensure that each process will exit</span>
    <span class="c1">#      even the process that should send/receive data to/from it were</span>
    <span class="c1">#      killed, i.e.,</span>
    <span class="c1">#</span>
    <span class="c1">#        a. A process won&#39;t hang when getting from a queue.</span>
    <span class="c1">#</span>
    <span class="c1">#           Even with carefully designed data dependencies (i.e., a `put()`</span>
    <span class="c1">#           always corresponding to a `get()`), hanging on `get()` can still</span>
    <span class="c1">#           happen when data in queue is corrupted (e.g., due to</span>
    <span class="c1">#           `cancel_join_thread` or unexpected exit).</span>
    <span class="c1">#</span>
    <span class="c1">#           For child exit, we set a timeout whenever we try to get data</span>
    <span class="c1">#           from `data_queue`, and check the workers&#39; status on each timeout</span>
    <span class="c1">#           and error.</span>
    <span class="c1">#           See `_DataLoaderiter._get_batch()` and</span>
    <span class="c1">#           `_DataLoaderiter._try_get_data()` for details.</span>
    <span class="c1">#</span>
    <span class="c1">#           Additionally, for child exit on non-Windows platforms, we also</span>
    <span class="c1">#           register a SIGCHLD handler (which is supported on Windows) on</span>
    <span class="c1">#           the main process, which checks if any of the workers fail in the</span>
    <span class="c1">#           (Python) handler. This is more efficient and faster in detecting</span>
    <span class="c1">#           worker failures, compared to only using the above mechanism.</span>
    <span class="c1">#           See `DataLoader.cpp` and `_utils/signal_handling.py` for details.</span>
    <span class="c1">#</span>
    <span class="c1">#           For `.get()` calls where the sender(s) is not the workers, we</span>
    <span class="c1">#           guard them with timeouts, and check the status of the sender</span>
    <span class="c1">#           when timeout happens:</span>
    <span class="c1">#             + in the workers, the `_utils.worker.ManagerWatchdog` class</span>
    <span class="c1">#               checks the status of the main process.</span>
    <span class="c1">#             + if `pin_memory=True`, when getting from `pin_memory_thread`,</span>
    <span class="c1">#               check `pin_memory_thread` status periodically until `.get()`</span>
    <span class="c1">#               returns or see that `pin_memory_thread` died.</span>
    <span class="c1">#</span>
    <span class="c1">#        b. A process won&#39;t hang when putting into a queue;</span>
    <span class="c1">#</span>
    <span class="c1">#           We use `mp.Queue` which has a separate background thread to put</span>
    <span class="c1">#           objects from an unbounded buffer array. The background thread is</span>
    <span class="c1">#           daemonic and usually automatically joined when the process</span>
    <span class="c1">#           *exits*.</span>
    <span class="c1">#</span>
    <span class="c1">#           In case that the receiver has ended abruptly while</span>
    <span class="c1">#           reading from the pipe, the join will hang forever.  The usual</span>
    <span class="c1">#           solution for this in Python is calling  `q.cancel_join_thread`,</span>
    <span class="c1">#           which prevents automatically joining it when finalizing</span>
    <span class="c1">#           (exiting).</span>
    <span class="c1">#</span>
    <span class="c1">#           Nonetheless, `cancel_join_thread` must only be called when the</span>
    <span class="c1">#           queue is **not** going to be read from or write into by another</span>
    <span class="c1">#           process, because it may hold onto a lock or leave corrupted data</span>
    <span class="c1">#           in the queue, leading other readers/writers to hang.</span>
    <span class="c1">#</span>
    <span class="c1">#           Hence,</span>
    <span class="c1">#             + For worker processes, we only do so (for their output</span>
    <span class="c1">#               queues, i.e., `worker_result_queue`) before exiting.</span>
    <span class="c1">#             + For `pin_memory_thread`, its output queue `data_queue` is a</span>
    <span class="c1">#               `queue.Queue` that does blocking `put` if the queue is full.</span>
    <span class="c1">#               So there is no above problem, but as a result, in</span>
    <span class="c1">#               `_pin_memory_loop`, we do need to  wrap the `put` in a loop</span>
    <span class="c1">#               that breaks not only upon success, but also when the main</span>
    <span class="c1">#               process stops reading, i.e., is shutting down.</span>
    <span class="c1">#             + For loader process, we `cancel_join_thread()` for all</span>
    <span class="c1">#               `_index_queues` because the whole purpose of workers and</span>
    <span class="c1">#               `pin_memory_thread` is to serve the loader process.  If</span>
    <span class="c1">#               loader process is already exiting, we don&#39;t really care if</span>
    <span class="c1">#               the queues are corrupted.</span>
    <span class="c1">#</span>
    <span class="c1">#</span>
    <span class="c1"># Now let&#39;s get back to 1:</span>
    <span class="c1">#   how we gracefully exit the workers when the last reference to the</span>
    <span class="c1">#   iterator is gone.</span>
    <span class="c1">#</span>
    <span class="c1"># To achieve this, we implement the following logic along with the design</span>
    <span class="c1"># choices mentioned above:</span>
    <span class="c1">#</span>
    <span class="c1"># `workers_done_event`:</span>
    <span class="c1">#   A `multiprocessing.Event` shared among the main process and all worker</span>
    <span class="c1">#   processes. This is used to signal the workers that the iterator is</span>
    <span class="c1">#   shutting down. After it is set, they will not send processed data to</span>
    <span class="c1">#   queues anymore, and only wait for the final `None` before exiting.</span>
    <span class="c1">#   `done_event` isn&#39;t strictly needed. I.e., we can just check for `None`</span>
    <span class="c1">#   from the input queue, but it allows us to skip wasting resources</span>
    <span class="c1">#   processing data if we are already shutting down.</span>
    <span class="c1">#</span>
    <span class="c1"># `pin_memory_thread_done_event`:</span>
    <span class="c1">#   A `threading.Event` for a similar purpose to that of</span>
    <span class="c1">#   `workers_done_event`, but is for the `pin_memory_thread`. The reason</span>
    <span class="c1">#   that separate events are needed is that `pin_memory_thread` reads from</span>
    <span class="c1">#   the output queue of the workers. But the workers, upon seeing that</span>
    <span class="c1">#   `workers_done_event` is set, only wants to see the final `None`, and is</span>
    <span class="c1">#   not required to flush all data in the output queue (e.g., it may call</span>
    <span class="c1">#   `cancel_join_thread` on that queue if its `IterableDataset` iterator</span>
    <span class="c1">#   happens to exhaust coincidentally, which is out of the control of the</span>
    <span class="c1">#   main process). Thus, since we will exit `pin_memory_thread` before the</span>
    <span class="c1">#   workers (see below), two separete events are used.</span>
    <span class="c1">#</span>
    <span class="c1"># NOTE: In short, the protocol is that the main process will set these</span>
    <span class="c1">#       `done_event`s and then the corresponding processes/threads a `None`,</span>
    <span class="c1">#       and that they may exit at any time after receiving the `None`.</span>
    <span class="c1">#</span>
    <span class="c1"># NOTE: Using `None` as the final signal is valid, since normal data will</span>
    <span class="c1">#       always be a 2-tuple with the 1st element being the index of the data</span>
    <span class="c1">#       transferred (different from dataset index/key), and the 2nd being</span>
    <span class="c1">#       either the dataset key or the data sample (depending on which part</span>
    <span class="c1">#       of the data model the queue is at).</span>
    <span class="c1">#</span>
    <span class="c1"># [ worker processes ]</span>
    <span class="c1">#   While loader process is alive:</span>
    <span class="c1">#     Get from `index_queue`.</span>
    <span class="c1">#       If get anything else,</span>
    <span class="c1">#          Check `workers_done_event`.</span>
    <span class="c1">#            If set, continue to next iteration</span>
    <span class="c1">#                    i.e., keep getting until see the `None`, then exit.</span>
    <span class="c1">#            Otherwise, process data:</span>
    <span class="c1">#                If is fetching from an `IterableDataset` and the iterator</span>
    <span class="c1">#                    is exhausted, send an `_IterableDatasetStopIteration`</span>
    <span class="c1">#                    object to signal iteration end. The main process, upon</span>
    <span class="c1">#                    receiving such an object, will send `None` to this</span>
    <span class="c1">#                    worker and not use the corresponding `index_queue`</span>
    <span class="c1">#                    anymore.</span>
    <span class="c1">#       If timed out,</span>
    <span class="c1">#          No matter `workers_done_event` is set (still need to see `None`)</span>
    <span class="c1">#          or not, must continue to next iteration.</span>
    <span class="c1">#   (outside loop)</span>
    <span class="c1">#   If `workers_done_event` is set,  (this can be False with `IterableDataset`)</span>
    <span class="c1">#     `data_queue.cancel_join_thread()`.  (Everything is ending here:</span>
    <span class="c1">#                                          main process won&#39;t read from it;</span>
    <span class="c1">#                                          other workers will also call</span>
    <span class="c1">#                                          `cancel_join_thread`.)</span>
    <span class="c1">#</span>
    <span class="c1"># [ pin_memory_thread ]</span>
    <span class="c1">#   # No need to check main thread. If this thread is alive, the main loader</span>
    <span class="c1">#   # thread must be alive, because this thread is set as daemonic.</span>
    <span class="c1">#   While `pin_memory_thread_done_event` is not set:</span>
    <span class="c1">#     Get from `worker_result_queue`.</span>
    <span class="c1">#       If timed out, continue to get in the next iteration.</span>
    <span class="c1">#       Otherwise, process data.</span>
    <span class="c1">#       While `pin_memory_thread_done_event` is not set:</span>
    <span class="c1">#         Put processed data to `data_queue` (a `queue.Queue` with blocking put)</span>
    <span class="c1">#         If timed out, continue to put in the next iteration.</span>
    <span class="c1">#         Otherwise, break, i.e., continuing to the out loop.</span>
    <span class="c1">#</span>
    <span class="c1">#   NOTE: we don&#39;t check the status of the main thread because</span>
    <span class="c1">#           1. if the process is killed by fatal signal, `pin_memory_thread`</span>
    <span class="c1">#              ends.</span>
    <span class="c1">#           2. in other cases, either the cleaning-up in __del__ or the</span>
    <span class="c1">#              automatic exit of daemonic thread will take care of it.</span>
    <span class="c1">#              This won&#39;t busy-wait either because `.get(timeout)` does not</span>
    <span class="c1">#              busy-wait.</span>
    <span class="c1">#</span>
    <span class="c1"># [ main process ]</span>
    <span class="c1">#   In the DataLoader Iter&#39;s `__del__`</span>
    <span class="c1">#     b. Exit `pin_memory_thread`</span>
    <span class="c1">#          i.   Set `pin_memory_thread_done_event`.</span>
    <span class="c1">#          ii   Put `None` in `worker_result_queue`.</span>
    <span class="c1">#          iii. Join the `pin_memory_thread`.</span>
    <span class="c1">#          iv.  `worker_result_queue.cancel_join_thread()`.</span>
    <span class="c1">#</span>
    <span class="c1">#     c. Exit the workers.</span>
    <span class="c1">#          i.   Set `workers_done_event`.</span>
    <span class="c1">#          ii.  Put `None` in each worker&#39;s `index_queue`.</span>
    <span class="c1">#          iii. Join the workers.</span>
    <span class="c1">#          iv.  Call `.cancel_join_thread()` on each worker&#39;s `index_queue`.</span>
    <span class="c1">#</span>
    <span class="c1">#        NOTE: (c) is better placed after (b) because it may leave corrupted</span>
    <span class="c1">#              data in `worker_result_queue`, which `pin_memory_thread`</span>
    <span class="c1">#              reads from, in which case the `pin_memory_thread` can only</span>
    <span class="c1">#              happen at timing out, which is slow. Nonetheless, same thing</span>
    <span class="c1">#              happens if a worker is killed by signal at unfortunate times,</span>
    <span class="c1">#              but in other cases, we are better off having a non-corrupted</span>
    <span class="c1">#              `worker_result_queue` for `pin_memory_thread`.</span>
    <span class="c1">#</span>
    <span class="c1">#   NOTE: If `pin_memory=False`, there is no `pin_memory_thread` and (b)</span>
    <span class="c1">#         can be omitted</span>
    <span class="c1">#</span>
    <span class="c1"># NB: `done_event`s isn&#39;t strictly needed. E.g., we can just check for</span>
    <span class="c1">#     `None` from `index_queue`, but it allows us to skip wasting resources</span>
    <span class="c1">#     processing indices already in `index_queue` if we are already shutting</span>
    <span class="c1">#     down.</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">loader</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">loader</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_prefetch_factor</span> <span class="o">=</span> <span class="n">loader</span><span class="o">.</span><span class="n">prefetch_factor</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_in_order</span> <span class="o">=</span> <span class="n">loader</span><span class="o">.</span><span class="n">in_order</span>

        <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_workers</span> <span class="o">&gt;</span> <span class="mi">0</span>
        <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">_prefetch_factor</span> <span class="o">&gt;</span> <span class="mi">0</span>

        <span class="k">if</span> <span class="n">loader</span><span class="o">.</span><span class="n">multiprocessing_context</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">multiprocessing_context</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">multiprocessing</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">multiprocessing_context</span> <span class="o">=</span> <span class="n">loader</span><span class="o">.</span><span class="n">multiprocessing_context</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_worker_init_fn</span> <span class="o">=</span> <span class="n">loader</span><span class="o">.</span><span class="n">worker_init_fn</span>

        <span class="c1"># Adds forward compatibilities so classic DataLoader can work with DataPipes:</span>
        <span class="c1">#   Additional worker init function will take care of sharding in MP and Distributed</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_dataset</span><span class="p">,</span> <span class="p">(</span><span class="n">IterDataPipe</span><span class="p">,</span> <span class="n">MapDataPipe</span><span class="p">)):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_worker_init_fn</span> <span class="o">=</span> <span class="n">functools</span><span class="o">.</span><span class="n">partial</span><span class="p">(</span>
                <span class="n">_sharding_worker_init_fn</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_worker_init_fn</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_world_size</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_rank</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="c1"># No certainty which module multiprocessing_context is</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_worker_result_queue</span> <span class="o">=</span> <span class="n">multiprocessing_context</span><span class="o">.</span><span class="n">Queue</span><span class="p">()</span>  <span class="c1"># type: ignore[var-annotated]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_worker_pids_set</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_shutdown</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_workers_done_event</span> <span class="o">=</span> <span class="n">multiprocessing_context</span><span class="o">.</span><span class="n">Event</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_index_queues</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_workers</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_num_workers</span><span class="p">):</span>
            <span class="c1"># No certainty which module multiprocessing_context is</span>
            <span class="n">index_queue</span> <span class="o">=</span> <span class="n">multiprocessing_context</span><span class="o">.</span><span class="n">Queue</span><span class="p">()</span>  <span class="c1"># type: ignore[var-annotated]</span>
            <span class="c1"># Need to `cancel_join_thread` here!</span>
            <span class="c1"># See sections (2) and (3b) above.</span>
            <span class="n">index_queue</span><span class="o">.</span><span class="n">cancel_join_thread</span><span class="p">()</span>
            <span class="n">w</span> <span class="o">=</span> <span class="n">multiprocessing_context</span><span class="o">.</span><span class="n">Process</span><span class="p">(</span>
                <span class="n">target</span><span class="o">=</span><span class="n">_utils</span><span class="o">.</span><span class="n">worker</span><span class="o">.</span><span class="n">_worker_loop</span><span class="p">,</span>
                <span class="n">args</span><span class="o">=</span><span class="p">(</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_dataset_kind</span><span class="p">,</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_dataset</span><span class="p">,</span>
                    <span class="n">index_queue</span><span class="p">,</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_worker_result_queue</span><span class="p">,</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_workers_done_event</span><span class="p">,</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_auto_collation</span><span class="p">,</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_collate_fn</span><span class="p">,</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_drop_last</span><span class="p">,</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_base_seed</span><span class="p">,</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_worker_init_fn</span><span class="p">,</span>
                    <span class="n">i</span><span class="p">,</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_num_workers</span><span class="p">,</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_persistent_workers</span><span class="p">,</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_shared_seed</span><span class="p">,</span>
                <span class="p">),</span>
            <span class="p">)</span>
            <span class="n">w</span><span class="o">.</span><span class="n">daemon</span> <span class="o">=</span> <span class="kc">True</span>
            <span class="c1"># NB: Process.start() actually take some time as it needs to</span>
            <span class="c1">#     start a process and pass the arguments over via a pipe.</span>
            <span class="c1">#     Therefore, we only add a worker to self._workers list after</span>
            <span class="c1">#     it started, so that we do not call .join() if program dies</span>
            <span class="c1">#     before it starts, and __del__ tries to join but will get:</span>
            <span class="c1">#     AssertionError: can only join a started process.</span>
            <span class="n">w</span><span class="o">.</span><span class="n">start</span><span class="p">()</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_index_queues</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">index_queue</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_workers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_pin_memory</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_pin_memory_thread_done_event</span> <span class="o">=</span> <span class="n">threading</span><span class="o">.</span><span class="n">Event</span><span class="p">()</span>

            <span class="c1"># Queue is not type-annotated</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_data_queue</span> <span class="o">=</span> <span class="n">queue</span><span class="o">.</span><span class="n">Queue</span><span class="p">()</span>  <span class="c1"># type: ignore[var-annotated]</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_pin_memory_device</span> <span class="o">==</span> <span class="s2">&quot;xpu&quot;</span><span class="p">:</span>
                <span class="n">current_device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">xpu</span><span class="o">.</span><span class="n">current_device</span><span class="p">()</span>  <span class="c1"># type: ignore[attr-defined]</span>
            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">_pin_memory_device</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_get_privateuse1_backend_name</span><span class="p">():</span>
                <span class="n">custom_device_mod</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span>
                    <span class="n">torch</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_get_privateuse1_backend_name</span><span class="p">()</span>
                <span class="p">)</span>
                <span class="n">current_device</span> <span class="o">=</span> <span class="n">custom_device_mod</span><span class="o">.</span><span class="n">current_device</span><span class="p">()</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">current_device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">current_device</span><span class="p">()</span>  <span class="c1"># choose cuda for default</span>
            <span class="n">pin_memory_thread</span> <span class="o">=</span> <span class="n">threading</span><span class="o">.</span><span class="n">Thread</span><span class="p">(</span>
                <span class="n">target</span><span class="o">=</span><span class="n">_utils</span><span class="o">.</span><span class="n">pin_memory</span><span class="o">.</span><span class="n">_pin_memory_loop</span><span class="p">,</span>
                <span class="n">args</span><span class="o">=</span><span class="p">(</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_worker_result_queue</span><span class="p">,</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_data_queue</span><span class="p">,</span>
                    <span class="n">current_device</span><span class="p">,</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_pin_memory_thread_done_event</span><span class="p">,</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_pin_memory_device</span><span class="p">,</span>
                <span class="p">),</span>
            <span class="p">)</span>
            <span class="n">pin_memory_thread</span><span class="o">.</span><span class="n">daemon</span> <span class="o">=</span> <span class="kc">True</span>
            <span class="n">pin_memory_thread</span><span class="o">.</span><span class="n">start</span><span class="p">()</span>
            <span class="c1"># Similar to workers (see comment above), we only register</span>
            <span class="c1"># pin_memory_thread once it is started.</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_pin_memory_thread</span> <span class="o">=</span> <span class="n">pin_memory_thread</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_data_queue</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_worker_result_queue</span>  <span class="c1"># type: ignore[assignment]</span>

        <span class="c1"># In some rare cases, persistent workers (daemonic processes)</span>
        <span class="c1"># would be terminated before `__del__` of iterator is invoked</span>
        <span class="c1"># when main process exits</span>
        <span class="c1"># It would cause failure when pin_memory_thread tries to read</span>
        <span class="c1"># corrupted data from worker_result_queue</span>
        <span class="c1"># atexit is used to shutdown thread and child processes in the</span>
        <span class="c1"># right sequence before main process exits</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_persistent_workers</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">_pin_memory</span><span class="p">:</span>
            <span class="kn">import</span><span class="w"> </span><span class="nn">atexit</span>

            <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_workers</span><span class="p">:</span>
                <span class="n">atexit</span><span class="o">.</span><span class="n">register</span><span class="p">(</span><span class="n">_MultiProcessingDataLoaderIter</span><span class="o">.</span><span class="n">_clean_up_worker</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span>

        <span class="c1"># .pid can be None only before process is spawned (not the case, so ignore)</span>
        <span class="n">_utils</span><span class="o">.</span><span class="n">signal_handling</span><span class="o">.</span><span class="n">_set_worker_pids</span><span class="p">(</span><span class="nb">id</span><span class="p">(</span><span class="bp">self</span><span class="p">),</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">w</span><span class="o">.</span><span class="n">pid</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_workers</span><span class="p">))</span>  <span class="c1"># type: ignore[misc]</span>
        <span class="n">_utils</span><span class="o">.</span><span class="n">signal_handling</span><span class="o">.</span><span class="n">_set_SIGCHLD_handler</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_worker_pids_set</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_reset</span><span class="p">(</span><span class="n">loader</span><span class="p">,</span> <span class="n">first_iter</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_reset</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">loader</span><span class="p">,</span> <span class="n">first_iter</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">_reset</span><span class="p">(</span><span class="n">loader</span><span class="p">,</span> <span class="n">first_iter</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_send_idx</span> <span class="o">=</span> <span class="mi">0</span>  <span class="c1"># idx of the next task to be sent to workers</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_rcvd_idx</span> <span class="o">=</span> <span class="mi">0</span>  <span class="c1"># idx of the next task to be returned in __next__</span>
        <span class="c1"># information about data not yet yielded, i.e., tasks w/ indices in range [rcvd_idx, send_idx).</span>
        <span class="c1"># map: task idx =&gt; - (worker_id,)        if data isn&#39;t fetched (outstanding)</span>
        <span class="c1">#                  \ (worker_id, data)   if data is already fetched (out-of-order)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_task_info</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_tasks_outstanding</span> <span class="o">=</span> <span class="p">(</span>
            <span class="mi">0</span>  <span class="c1"># always equal to count(v for v in task_info.values() if len(v) == 1)</span>
        <span class="p">)</span>
        <span class="c1"># A list of booleans representing whether each worker still has work to</span>
        <span class="c1"># do, i.e., not having exhausted its iterable dataset object. It always</span>
        <span class="c1"># contains all `True`s if not using an iterable-style dataset</span>
        <span class="c1"># (i.e., if kind != Iterable).</span>
        <span class="c1"># Not that this indicates that a worker still has work to do *for this epoch*.</span>
        <span class="c1"># It does not mean that a worker is dead. In case of `_persistent_workers`,</span>
        <span class="c1"># the worker will be reset to available in the next epoch.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_workers_status</span> <span class="o">=</span> <span class="p">[</span><span class="kc">True</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_num_workers</span><span class="p">)]</span>
        <span class="c1"># Reset the worker queue cycle so it resumes next epoch at worker 0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_worker_queue_idx_cycle</span> <span class="o">=</span> <span class="n">itertools</span><span class="o">.</span><span class="n">cycle</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_num_workers</span><span class="p">))</span>
        <span class="c1"># We resume the prefetching in case it was enabled</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">first_iter</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_num_workers</span><span class="p">):</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_index_queues</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">put</span><span class="p">(</span>
                    <span class="n">_utils</span><span class="o">.</span><span class="n">worker</span><span class="o">.</span><span class="n">_ResumeIteration</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_shared_seed</span><span class="p">)</span>
                <span class="p">)</span>
            <span class="n">resume_iteration_cnt</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_workers</span>
            <span class="k">while</span> <span class="n">resume_iteration_cnt</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">return_idx</span><span class="p">,</span> <span class="n">return_data</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_data</span><span class="p">()</span>
                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">return_idx</span><span class="p">,</span> <span class="n">_utils</span><span class="o">.</span><span class="n">worker</span><span class="o">.</span><span class="n">_ResumeIteration</span><span class="p">):</span>
                    <span class="k">assert</span> <span class="n">return_data</span> <span class="ow">is</span> <span class="kc">None</span>
                    <span class="n">resume_iteration_cnt</span> <span class="o">-=</span> <span class="mi">1</span>
        <span class="c1"># prime the prefetch loop</span>
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_prefetch_factor</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_workers</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_try_put_index</span><span class="p">()</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_try_get_data</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">timeout</span><span class="o">=</span><span class="n">_utils</span><span class="o">.</span><span class="n">MP_STATUS_CHECK_INTERVAL</span><span class="p">):</span>
        <span class="c1"># Tries to fetch data from `self._data_queue` once for a given timeout.</span>
        <span class="c1"># This can also be used as inner loop of fetching without timeout, with</span>
        <span class="c1"># the sender status as the loop condition.</span>
        <span class="c1">#</span>
        <span class="c1"># This raises a `RuntimeError` if any worker died expectedly. This error</span>
        <span class="c1"># can come from either the SIGCHLD handler in `_utils/signal_handling.py`</span>
        <span class="c1"># (only for non-Windows platforms), or the manual check below on errors</span>
        <span class="c1"># and timeouts.</span>
        <span class="c1">#</span>
        <span class="c1"># Returns a 2-tuple:</span>
        <span class="c1">#   (bool: whether successfully get data, any: data if successful else None)</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">data</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_data_queue</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">timeout</span><span class="o">=</span><span class="n">timeout</span><span class="p">)</span>
            <span class="k">return</span> <span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">data</span><span class="p">)</span>
        <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
            <span class="c1"># At timeout and error, we manually check whether any worker has</span>
            <span class="c1"># failed. Note that this is the only mechanism for Windows to detect</span>
            <span class="c1"># worker failures.</span>
            <span class="n">failed_workers</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">for</span> <span class="n">worker_id</span><span class="p">,</span> <span class="n">w</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_workers</span><span class="p">):</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_workers_status</span><span class="p">[</span><span class="n">worker_id</span><span class="p">]</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">w</span><span class="o">.</span><span class="n">is_alive</span><span class="p">():</span>
                    <span class="n">failed_workers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_mark_worker_as_unavailable</span><span class="p">(</span><span class="n">worker_id</span><span class="p">)</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">failed_workers</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">pids_str</span> <span class="o">=</span> <span class="s2">&quot;, &quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">w</span><span class="o">.</span><span class="n">pid</span><span class="p">)</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">failed_workers</span><span class="p">)</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;DataLoader worker (pid(s) </span><span class="si">{</span><span class="n">pids_str</span><span class="si">}</span><span class="s2">) exited unexpectedly&quot;</span>
                <span class="p">)</span> <span class="kn">from</span><span class="w"> </span><span class="nn">e</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">e</span><span class="p">,</span> <span class="n">queue</span><span class="o">.</span><span class="n">Empty</span><span class="p">):</span>
                <span class="k">return</span> <span class="p">(</span><span class="kc">False</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>

            <span class="kn">import</span><span class="w"> </span><span class="nn">errno</span>
            <span class="kn">import</span><span class="w"> </span><span class="nn">tempfile</span>

            <span class="k">try</span><span class="p">:</span>
                <span class="c1"># Raise an exception if we are this close to the FDs limit.</span>
                <span class="c1"># Apparently, trying to open only one file is not a sufficient</span>
                <span class="c1"># test.</span>
                <span class="c1"># See NOTE [ DataLoader on Linux and open files limit ]</span>
                <span class="n">fds_limit_margin</span> <span class="o">=</span> <span class="mi">10</span>
                <span class="p">[</span><span class="n">tempfile</span><span class="o">.</span><span class="n">NamedTemporaryFile</span><span class="p">()</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">fds_limit_margin</span><span class="p">)]</span>
            <span class="k">except</span> <span class="ne">OSError</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">e</span><span class="o">.</span><span class="n">errno</span> <span class="o">==</span> <span class="n">errno</span><span class="o">.</span><span class="n">EMFILE</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                        <span class="s2">&quot;Too many open files. Communication with the&quot;</span>
                        <span class="s2">&quot; workers is no longer possible. Please increase the&quot;</span>
                        <span class="s2">&quot; limit using `ulimit -n` in the shell or change the&quot;</span>
                        <span class="s2">&quot; sharing strategy by calling&quot;</span>
                        <span class="s2">&quot; `torch.multiprocessing.set_sharing_strategy(&#39;file_system&#39;)`&quot;</span>
                        <span class="s2">&quot; at the beginning of your code&quot;</span>
                    <span class="p">)</span> <span class="kn">from</span><span class="w"> </span><span class="kc">None</span>
            <span class="k">raise</span>

    <span class="c1"># NOTE [ DataLoader on Linux and open files limit ]</span>
    <span class="c1">#</span>
    <span class="c1"># On Linux when DataLoader is used with multiprocessing we pass the data between</span>
    <span class="c1"># the root process and the workers through SHM files. We remove those files from</span>
    <span class="c1"># the filesystem as soon as they are created and keep them alive by</span>
    <span class="c1"># passing around their file descriptors through AF_UNIX sockets. (See</span>
    <span class="c1"># docs/source/multiprocessing.rst and &#39;Multiprocessing Technical Notes` in</span>
    <span class="c1"># the wiki (https://github.com/pytorch/pytorch/wiki).)</span>
    <span class="c1">#</span>
    <span class="c1"># This sometimes leads us to exceeding the open files limit. When that happens,</span>
    <span class="c1"># and the offending file descriptor is coming over a socket, the `socket` Python</span>
    <span class="c1"># package silently strips the file descriptor from the message, setting only the</span>
    <span class="c1"># `MSG_CTRUNC` flag (which might be a bit misleading since the manpage says that</span>
    <span class="c1"># it _indicates that some control data were discarded due to lack of space in</span>
    <span class="c1"># the buffer for ancillary data_). This might reflect the C implementation of</span>
    <span class="c1"># AF_UNIX sockets.</span>
    <span class="c1">#</span>
    <span class="c1"># This behaviour can be reproduced with the script and instructions at the</span>
    <span class="c1"># bottom of this note.</span>
    <span class="c1">#</span>
    <span class="c1"># When that happens, the standard Python `multiprocessing` (and not</span>
    <span class="c1"># `torch.multiprocessing`) raises a `RuntimeError: received 0 items of ancdata`</span>
    <span class="c1">#</span>
    <span class="c1"># Sometimes, instead of the FD being stripped, you may get an `OSError:</span>
    <span class="c1"># Too many open files`, both in the script below and in DataLoader. However,</span>
    <span class="c1"># this is rare and seems to be nondeterministic.</span>
    <span class="c1">#</span>
    <span class="c1">#</span>
    <span class="c1">#   #!/usr/bin/env python3</span>
    <span class="c1">#   import sys</span>
    <span class="c1">#   import socket</span>
    <span class="c1">#   import os</span>
    <span class="c1">#   import array</span>
    <span class="c1">#   import shutil</span>
    <span class="c1">#   import socket</span>
    <span class="c1">#</span>
    <span class="c1">#</span>
    <span class="c1">#   if len(sys.argv) != 4:</span>
    <span class="c1">#       print(&quot;Usage: &quot;, sys.argv[0], &quot; tmp_dirname iteration (send|recv)&quot;)</span>
    <span class="c1">#       sys.exit(1)</span>
    <span class="c1">#</span>
    <span class="c1">#   if __name__ == &#39;__main__&#39;:</span>
    <span class="c1">#       dirname = sys.argv[1]</span>
    <span class="c1">#       sock_path = dirname + &quot;/sock&quot;</span>
    <span class="c1">#       iterations = int(sys.argv[2])</span>
    <span class="c1">#       def dummy_path(i):</span>
    <span class="c1">#           return dirname + &quot;/&quot; + str(i) + &quot;.dummy&quot;</span>
    <span class="c1">#</span>
    <span class="c1">#</span>
    <span class="c1">#       if sys.argv[3] == &#39;send&#39;:</span>
    <span class="c1">#           while not os.path.exists(sock_path):</span>
    <span class="c1">#               pass</span>
    <span class="c1">#           client = socket.socket(socket.AF_UNIX, socket.SOCK_DGRAM)</span>
    <span class="c1">#           client.connect(sock_path)</span>
    <span class="c1">#           for i in range(iterations):</span>
    <span class="c1">#               fd = os.open(dummy_path(i), os.O_WRONLY | os.O_CREAT)</span>
    <span class="c1">#               ancdata = array.array(&#39;i&#39;, [fd])</span>
    <span class="c1">#               msg = bytes([i % 256])</span>
    <span class="c1">#               print(&quot;Sending fd &quot;, fd, &quot; (iteration #&quot;, i, &quot;)&quot;)</span>
    <span class="c1">#               client.sendmsg([msg], [(socket.SOL_SOCKET, socket.SCM_RIGHTS, ancdata)])</span>
    <span class="c1">#</span>
    <span class="c1">#</span>
    <span class="c1">#       else:</span>
    <span class="c1">#           assert sys.argv[3] == &#39;recv&#39;</span>
    <span class="c1">#</span>
    <span class="c1">#           if os.path.exists(dirname):</span>
    <span class="c1">#               raise Exception(&quot;Directory exists&quot;)</span>
    <span class="c1">#</span>
    <span class="c1">#           os.mkdir(dirname)</span>
    <span class="c1">#</span>
    <span class="c1">#           print(&quot;Opening socket...&quot;)</span>
    <span class="c1">#           server = socket.socket(socket.AF_UNIX, socket.SOCK_DGRAM)</span>
    <span class="c1">#           server.bind(sock_path)</span>
    <span class="c1">#</span>
    <span class="c1">#           print(&quot;Listening...&quot;)</span>
    <span class="c1">#           for i in range(iterations):</span>
    <span class="c1">#               a = array.array(&#39;i&#39;)</span>
    <span class="c1">#               msg, ancdata, flags, addr = server.recvmsg(1, socket.CMSG_SPACE(a.itemsize))</span>
    <span class="c1">#               assert(len(ancdata) == 1)</span>
    <span class="c1">#               cmsg_level, cmsg_type, cmsg_data = ancdata[0]</span>
    <span class="c1">#               a.frombytes(cmsg_data)</span>
    <span class="c1">#               print(&quot;Received fd &quot;, a[0], &quot; (iteration #&quot;, i, &quot;)&quot;)</span>
    <span class="c1">#</span>
    <span class="c1">#           shutil.rmtree(dirname)</span>
    <span class="c1">#</span>
    <span class="c1"># Steps to reproduce:</span>
    <span class="c1">#</span>
    <span class="c1"># 1. Run two shells and set lower file descriptor limit in the receiving one:</span>
    <span class="c1"># (shell1) ulimit -n 1020</span>
    <span class="c1"># (shell2) ulimit -n 1022</span>
    <span class="c1">#</span>
    <span class="c1"># 2. Run the script above with the `recv` option in the first shell</span>
    <span class="c1"># (shell1) ./test_socket.py sock_tmp 1017 recv</span>
    <span class="c1">#</span>
    <span class="c1"># 3. Run the script with the `send` option in the second shell:</span>
    <span class="c1"># (shell2) ./test_socket.py sock_tmp 1017 send</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_get_data</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># Fetches data from `self._data_queue`.</span>
        <span class="c1">#</span>
        <span class="c1"># We check workers&#39; status every `MP_STATUS_CHECK_INTERVAL` seconds,</span>
        <span class="c1"># which we achieve by running `self._try_get_data(timeout=MP_STATUS_CHECK_INTERVAL)`</span>
        <span class="c1"># in a loop. This is the only mechanism to detect worker failures for</span>
        <span class="c1"># Windows. For other platforms, a SIGCHLD handler is also used for</span>
        <span class="c1"># worker failure detection.</span>
        <span class="c1">#</span>
        <span class="c1"># If `pin_memory=True`, we also need check if `pin_memory_thread` had</span>
        <span class="c1"># died at timeouts.</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_timeout</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">success</span><span class="p">,</span> <span class="n">data</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_try_get_data</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_timeout</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">success</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">data</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;DataLoader timed out after </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">_timeout</span><span class="si">}</span><span class="s2"> seconds&quot;</span>
                <span class="p">)</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">_pin_memory</span><span class="p">:</span>
            <span class="k">while</span> <span class="bp">self</span><span class="o">.</span><span class="n">_pin_memory_thread</span><span class="o">.</span><span class="n">is_alive</span><span class="p">():</span>
                <span class="n">success</span><span class="p">,</span> <span class="n">data</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_try_get_data</span><span class="p">()</span>
                <span class="k">if</span> <span class="n">success</span><span class="p">:</span>
                    <span class="k">return</span> <span class="n">data</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="c1"># while condition is false, i.e., pin_memory_thread died.</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;Pin memory thread exited unexpectedly&quot;</span><span class="p">)</span>
            <span class="c1"># In this case, `self._data_queue` is a `queue.Queue`,. But we don&#39;t</span>
            <span class="c1"># need to call `.task_done()` because we don&#39;t use `.join()`.</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
                <span class="n">success</span><span class="p">,</span> <span class="n">data</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_try_get_data</span><span class="p">()</span>
                <span class="k">if</span> <span class="n">success</span><span class="p">:</span>
                    <span class="k">return</span> <span class="n">data</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_next_data</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
            <span class="c1"># If the worker responsible for `self._rcvd_idx` has already ended</span>
            <span class="c1"># and was unable to fulfill this task (due to exhausting an `IterableDataset`),</span>
            <span class="c1"># we try to advance `self._rcvd_idx` to find the next valid index.</span>
            <span class="c1">#</span>
            <span class="c1"># This part needs to run in the loop because both the `self._get_data()`</span>
            <span class="c1"># call and `_IterableDatasetStopIteration` check below can mark</span>
            <span class="c1"># extra worker(s) as dead.</span>
            <span class="k">while</span> <span class="bp">self</span><span class="o">.</span><span class="n">_rcvd_idx</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">_send_idx</span><span class="p">:</span>
                <span class="n">info</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_task_info</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_rcvd_idx</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">info</span><span class="p">:</span>
                    <span class="n">worker_id</span> <span class="o">=</span> <span class="n">info</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
                    <span class="k">if</span> <span class="p">(</span>
                        <span class="nb">len</span><span class="p">(</span><span class="n">info</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">_workers_status</span><span class="p">[</span><span class="n">worker_id</span><span class="p">]</span>
                    <span class="p">):</span>  <span class="c1"># has data or is still active</span>
                        <span class="k">break</span>
                    <span class="k">del</span> <span class="bp">self</span><span class="o">.</span><span class="n">_task_info</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_rcvd_idx</span><span class="p">]</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_rcvd_idx</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="c1"># no valid `self._rcvd_idx` is found (i.e., didn&#39;t break)</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_persistent_workers</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_shutdown_workers</span><span class="p">()</span>
                <span class="k">raise</span> <span class="ne">StopIteration</span>

            <span class="c1"># Now `self._rcvd_idx` is the batch index we want to fetch</span>

            <span class="c1"># Check if the next sample has already been generated</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_task_info</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_rcvd_idx</span><span class="p">])</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
                <span class="n">data</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_task_info</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_rcvd_idx</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_rcvd_idx</span> <span class="o">+=</span> <span class="mi">1</span>
                <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_process_data</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

            <span class="k">assert</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_shutdown</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">_tasks_outstanding</span> <span class="o">&gt;</span> <span class="mi">0</span>
            <span class="n">idx</span><span class="p">,</span> <span class="n">data</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_data</span><span class="p">()</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_tasks_outstanding</span> <span class="o">-=</span> <span class="mi">1</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dataset_kind</span> <span class="o">==</span> <span class="n">_DatasetKind</span><span class="o">.</span><span class="n">Iterable</span><span class="p">:</span>
                <span class="c1"># Check for _IterableDatasetStopIteration</span>
                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">_utils</span><span class="o">.</span><span class="n">worker</span><span class="o">.</span><span class="n">_IterableDatasetStopIteration</span><span class="p">):</span>
                    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_persistent_workers</span><span class="p">:</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">_workers_status</span><span class="p">[</span><span class="n">data</span><span class="o">.</span><span class="n">worker_id</span><span class="p">]</span> <span class="o">=</span> <span class="kc">False</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">_mark_worker_as_unavailable</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">worker_id</span><span class="p">)</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_try_put_index</span><span class="p">()</span>
                    <span class="k">continue</span>

            <span class="k">if</span> <span class="n">idx</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_rcvd_idx</span><span class="p">:</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_in_order</span><span class="p">:</span>
                    <span class="c1"># don&#39;t store it for later, process now</span>
                    <span class="k">del</span> <span class="bp">self</span><span class="o">.</span><span class="n">_task_info</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
                    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_process_data</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
                <span class="c1"># store out-of-order samples</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_task_info</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">+=</span> <span class="p">(</span><span class="n">data</span><span class="p">,)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">del</span> <span class="bp">self</span><span class="o">.</span><span class="n">_task_info</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_rcvd_idx</span> <span class="o">+=</span> <span class="mi">1</span>
                <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_process_data</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_try_put_index</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">_tasks_outstanding</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">_prefetch_factor</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_workers</span>

        <span class="k">try</span><span class="p">:</span>
            <span class="n">index</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_next_index</span><span class="p">()</span>
        <span class="k">except</span> <span class="ne">StopIteration</span><span class="p">:</span>
            <span class="k">return</span>
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_num_workers</span><span class="p">):</span>  <span class="c1"># find the next active worker, if any</span>
            <span class="n">worker_queue_idx</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_worker_queue_idx_cycle</span><span class="p">)</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_workers_status</span><span class="p">[</span><span class="n">worker_queue_idx</span><span class="p">]:</span>
                <span class="k">break</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># not found (i.e., didn&#39;t break)</span>
            <span class="k">return</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_index_queues</span><span class="p">[</span><span class="n">worker_queue_idx</span><span class="p">]</span><span class="o">.</span><span class="n">put</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">_send_idx</span><span class="p">,</span> <span class="n">index</span><span class="p">))</span>  <span class="c1"># type: ignore[possibly-undefined]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_task_info</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_send_idx</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">worker_queue_idx</span><span class="p">,)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_tasks_outstanding</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_send_idx</span> <span class="o">+=</span> <span class="mi">1</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_process_data</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_try_put_index</span><span class="p">()</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">ExceptionWrapper</span><span class="p">):</span>
            <span class="n">data</span><span class="o">.</span><span class="n">reraise</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">data</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_mark_worker_as_unavailable</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">worker_id</span><span class="p">,</span> <span class="n">shutdown</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="c1"># Mark a worker as having finished its work e.g., due to</span>
        <span class="c1"># exhausting an `IterableDataset`. This should be used only when this</span>
        <span class="c1"># `_MultiProcessingDataLoaderIter` is going to continue running.</span>

        <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">_workers_status</span><span class="p">[</span><span class="n">worker_id</span><span class="p">]</span> <span class="ow">or</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_persistent_workers</span> <span class="ow">and</span> <span class="n">shutdown</span>
        <span class="p">)</span>

        <span class="c1"># Signal termination to that specific worker.</span>
        <span class="n">q</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_index_queues</span><span class="p">[</span><span class="n">worker_id</span><span class="p">]</span>
        <span class="c1"># Indicate that no more data will be put on this queue by the current</span>
        <span class="c1"># process.</span>
        <span class="n">q</span><span class="o">.</span><span class="n">put</span><span class="p">(</span><span class="kc">None</span><span class="p">)</span>

        <span class="c1"># Note that we don&#39;t actually join the worker here, nor do we remove the</span>
        <span class="c1"># worker&#39;s pid from C side struct because (1) joining may be slow, and</span>
        <span class="c1"># (2) since we don&#39;t join, the worker may still raise error, and we</span>
        <span class="c1"># prefer capturing those, rather than ignoring them, even though they</span>
        <span class="c1"># are raised after the worker has finished its job.</span>
        <span class="c1"># Joinning is deferred to `_shutdown_workers`, which it is called when</span>
        <span class="c1"># all workers finish their jobs (e.g., `IterableDataset` replicas) or</span>
        <span class="c1"># when this iterator is garbage collected.</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_workers_status</span><span class="p">[</span><span class="n">worker_id</span><span class="p">]</span> <span class="o">=</span> <span class="kc">False</span>

        <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">_workers_done_event</span><span class="o">.</span><span class="n">is_set</span><span class="p">()</span> <span class="o">==</span> <span class="n">shutdown</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_shutdown_workers</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># Called when shutting down this `_MultiProcessingDataLoaderIter`.</span>
        <span class="c1"># See NOTE [ Data Loader Multiprocessing Shutdown Logic ] for details on</span>
        <span class="c1"># the logic of this function.</span>
        <span class="k">if</span> <span class="p">(</span>
            <span class="n">_utils</span> <span class="ow">is</span> <span class="kc">None</span>
            <span class="ow">or</span> <span class="n">_utils</span><span class="o">.</span><span class="n">python_exit_status</span> <span class="ow">is</span> <span class="kc">True</span>
            <span class="ow">or</span> <span class="n">_utils</span><span class="o">.</span><span class="n">python_exit_status</span> <span class="ow">is</span> <span class="kc">None</span>
        <span class="p">):</span>
            <span class="c1"># See (2) of the note. If Python is shutting down, do no-op.</span>
            <span class="k">return</span>
        <span class="c1"># Normal exit when last reference is gone / iterator is depleted.</span>
        <span class="c1"># See (1) and the second half of the note.</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_shutdown</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_shutdown</span> <span class="o">=</span> <span class="kc">True</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="c1"># Normal exit when last reference is gone / iterator is depleted.</span>
                <span class="c1"># See (1) and the second half of the note.</span>

                <span class="c1"># Exit `pin_memory_thread` first because exiting workers may leave</span>
                <span class="c1"># corrupted data in `worker_result_queue` which `pin_memory_thread`</span>
                <span class="c1"># reads from.</span>
                <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;_pin_memory_thread&quot;</span><span class="p">):</span>
                    <span class="c1"># Use hasattr in case error happens before we set the attribute.</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_pin_memory_thread_done_event</span><span class="o">.</span><span class="n">set</span><span class="p">()</span>
                    <span class="c1"># Send something to pin_memory_thread in case it is waiting</span>
                    <span class="c1"># so that it can wake up and check `pin_memory_thread_done_event`</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_worker_result_queue</span><span class="o">.</span><span class="n">put</span><span class="p">((</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">))</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_pin_memory_thread</span><span class="o">.</span><span class="n">join</span><span class="p">()</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_worker_result_queue</span><span class="o">.</span><span class="n">cancel_join_thread</span><span class="p">()</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_worker_result_queue</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>

                <span class="c1"># Exit workers now.</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_workers_done_event</span><span class="o">.</span><span class="n">set</span><span class="p">()</span>
                <span class="k">for</span> <span class="n">worker_id</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_workers</span><span class="p">)):</span>
                    <span class="c1"># Get number of workers from `len(self._workers)` instead of</span>
                    <span class="c1"># `self._num_workers` in case we error before starting all</span>
                    <span class="c1"># workers.</span>
                    <span class="c1"># If we are using workers_status with persistent_workers</span>
                    <span class="c1"># we have to shut it down because the worker is paused</span>
                    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_persistent_workers</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">_workers_status</span><span class="p">[</span><span class="n">worker_id</span><span class="p">]:</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">_mark_worker_as_unavailable</span><span class="p">(</span><span class="n">worker_id</span><span class="p">,</span> <span class="n">shutdown</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
                <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_workers</span><span class="p">:</span>
                    <span class="c1"># We should be able to join here, but in case anything went</span>
                    <span class="c1"># wrong, we set a timeout and if the workers fail to join,</span>
                    <span class="c1"># they are killed in the `finally` block.</span>
                    <span class="n">w</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">timeout</span><span class="o">=</span><span class="n">_utils</span><span class="o">.</span><span class="n">MP_STATUS_CHECK_INTERVAL</span><span class="p">)</span>
                <span class="k">for</span> <span class="n">q</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_index_queues</span><span class="p">:</span>
                    <span class="n">q</span><span class="o">.</span><span class="n">cancel_join_thread</span><span class="p">()</span>
                    <span class="n">q</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
            <span class="k">finally</span><span class="p">:</span>
                <span class="c1"># Even though all this function does is putting into queues that</span>
                <span class="c1"># we have called `cancel_join_thread` on, weird things can</span>
                <span class="c1"># happen when a worker is killed by a signal, e.g., hanging in</span>
                <span class="c1"># `Event.set()`. So we need to guard this with SIGCHLD handler,</span>
                <span class="c1"># and remove pids from the C side data structure only at the</span>
                <span class="c1"># end.</span>
                <span class="c1">#</span>
                <span class="c1"># FIXME: Unfortunately, for Windows, we are missing a worker</span>
                <span class="c1">#        error detection mechanism here in this function, as it</span>
                <span class="c1">#        doesn&#39;t provide a SIGCHLD handler.</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_worker_pids_set</span><span class="p">:</span>
                    <span class="n">_utils</span><span class="o">.</span><span class="n">signal_handling</span><span class="o">.</span><span class="n">_remove_worker_pids</span><span class="p">(</span><span class="nb">id</span><span class="p">(</span><span class="bp">self</span><span class="p">))</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_worker_pids_set</span> <span class="o">=</span> <span class="kc">False</span>
                <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_workers</span><span class="p">:</span>
                    <span class="k">if</span> <span class="n">w</span><span class="o">.</span><span class="n">is_alive</span><span class="p">():</span>
                        <span class="c1"># Existing mechanisms try to make the workers exit</span>
                        <span class="c1"># peacefully, but in case that we unfortunately reach</span>
                        <span class="c1"># here, which we shouldn&#39;t, (e.g., pytorch/pytorch#39570),</span>
                        <span class="c1"># we kill the worker.</span>
                        <span class="n">w</span><span class="o">.</span><span class="n">terminate</span><span class="p">()</span>

    <span class="c1"># staticmethod is used to remove reference to `_MultiProcessingDataLoaderIter`</span>
    <span class="nd">@staticmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">_clean_up_worker</span><span class="p">(</span><span class="n">w</span><span class="p">):</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">w</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">timeout</span><span class="o">=</span><span class="n">_utils</span><span class="o">.</span><span class="n">MP_STATUS_CHECK_INTERVAL</span><span class="p">)</span>
        <span class="k">finally</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">w</span><span class="o">.</span><span class="n">is_alive</span><span class="p">():</span>
                <span class="n">w</span><span class="o">.</span><span class="n">terminate</span><span class="p">()</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__del__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_shutdown_workers</span><span class="p">()</span>
</pre></div>

                </article>
              
              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
</div>
                </footer>
              
            </div>
            
            
              
            
          </div>
          <footer class="bd-footer-content">
            
          </footer>
        
      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script defer src="../../../../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf"></script>
<script defer src="../../../../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf"></script>


  <footer class="bd-footer">
<div class="bd-footer__inner bd-page-width">
  
    <div class="footer-items__start">
      
        <div class="footer-item">

  <p class="copyright">
    
      © Copyright PyTorch Contributors.
      <br/>
    
  </p>
</div>
      
        <div class="footer-item">

  <p class="sphinx-version">
    Created using <a href="https://www.sphinx-doc.org/">Sphinx</a> 8.1.0.
    <br/>
  </p>
</div>
      
    </div>
  
  
  
    <div class="footer-items__end">
      
        <div class="footer-item">
<p class="theme-version">
  <!-- # L10n: Setting the PST URL as an argument as this does not need to be localized -->
  Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.16.1.
</p></div>
      
    </div>
  
</div>

  </footer>
  </body>
</html>