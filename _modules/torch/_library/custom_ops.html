
<!DOCTYPE html>


<html lang="en" data-content_root="../../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>torch._library.custom_ops &#8212; PyTorch main documentation</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  <!--
    this give us a css class that will be invisible only if js is disabled
  -->
  <noscript>
    <style>
      .pst-js-only { display: none !important; }

    </style>
  </noscript>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../../_static/styles/theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />
<link href="../../../_static/styles/pydata-sphinx-theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />

    <link rel="stylesheet" type="text/css" href="../../../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../../../_static/copybutton.css?v=949a1ff5" />
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/katex.min.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/katex-math.css?v=91adb8b6" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="../../../_static/css/jit.css?v=8de1ea5d" />
    <link rel="stylesheet" type="text/css" href="../../../_static/css/custom.css?v=3ccf5357" />
    <link rel="stylesheet" type="text/css" href="../../../_static/css/custom2.css?v=baa440dc" />
  
  <!-- So that users can add custom icons -->
  <script src="../../../_static/scripts/fontawesome.js?digest=8878045cc6db502f8baf"></script>
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf" />
<link rel="preload" as="script" href="../../../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf" />

    <script src="../../../_static/documentation_options.js?v=a8da1a53"></script>
    <script src="../../../_static/doctools.js?v=9bcbadda"></script>
    <script src="../../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../../_static/copybutton.js?v=940804e7"></script>
    <script src="../../../_static/design-tabs.js?v=f930bc37"></script>
    <script type="module" src="https://cdn.jsdelivr.net/npm/mermaid@11.2.0/dist/mermaid.esm.min.mjs"></script>
    <script type="module" src="https://cdn.jsdelivr.net/npm/@mermaid-js/layout-elk@0.1.4/dist/mermaid-layout-elk.esm.min.mjs"></script>
    <script type="module">import mermaid from "https://cdn.jsdelivr.net/npm/mermaid@11.2.0/dist/mermaid.esm.min.mjs";import elkLayouts from "https://cdn.jsdelivr.net/npm/@mermaid-js/layout-elk@0.1.4/dist/mermaid-layout-elk.esm.min.mjs";mermaid.registerLayoutLoaders(elkLayouts);mermaid.initialize({startOnLoad:false});</script>
    <script src="https://cdn.jsdelivr.net/npm/d3@7.9.0/dist/d3.min.js"></script>
    <script type="module">
import mermaid from "https://cdn.jsdelivr.net/npm/mermaid@11.2.0/dist/mermaid.esm.min.mjs";
window.addEventListener("load", () => mermaid.run());
</script>
    <script>DOCUMENTATION_OPTIONS.pagename = '_modules/torch/_library/custom_ops';</script>
    <script src="../../../_static/js/star-rating.js?v=8861fcb6"></script>
    <script src="../../../_static/js/send-feedback.js?v=5646bf45"></script>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />

  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  <meta name="docsearch:version" content="main (2.6.0 )" />
<script src="https://code.jquery.com/jquery-3.7.1.min.js"></script>
<script type="text/javascript" src="../../../_static/js/send-feedback.js"></script>
<script type="text/javascript" src="../../../_static/js/star-rating.js"></script>
<script type="text/javascript" src="../../../_static/js/cookie-banner.js"></script>

<script async src="https://www.googletagmanager.com/gtag/js?id=G-TEST12345"></script>
    <script type="text/javascript">
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-TEST12345');
    </script>

  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
<body data-feedback-url="https://github.com/pytorch/pytorch">
  <div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>

          <li class="main-menu-item">
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Learn
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="">
                  <span class=dropdown-title>Get Started</span>
                  <p>Run PyTorch locally or get started quickly with one of the supported cloud platforms</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Tutorials</span>
                  <p>Whats new in PyTorch tutorials</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Learn the Basics</span>
                  <p>Familiarize yourself with PyTorch concepts and modules</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">PyTorch Recipes</span>
                  <p>Bite-size, ready-to-deploy PyTorch code examples</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Intro to PyTorch - YouTube Series</span>
                  <p>Master PyTorch basics with our engaging YouTube tutorial series</p>
                </a>
              </div>
            </div>
          </li>

          <li>
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Ecosystem
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Tools</span>
                  <p>Learn about the tools and frameworks in the PyTorch Ecosystem</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class=dropdown-title>Community</span>
                  <p>Join the PyTorch developer community to contribute, learn, and get your questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="" target="_blank">
                  <span class=dropdown-title>Forums</span>
                  <p>A place to discuss PyTorch code, issues, install, research</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class=dropdown-title>Developer Resources</span>
                  <p>Find resources and get questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Contributor Awards - 2024</span>
                  <p>Award winners announced at this year's PyTorch Conference</p>
                </a>
              </div>
            </div>
          </li>

          <li>
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Edge
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">About PyTorch Edge</span>
                  <p>Build innovative and privacy-aware AI experiences for edge devices</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">ExecuTorch</span>
                  <p>End-to-end solution for enabling on-device inference capabilities across mobile and edge devices</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">ExecuTorch Docs</span>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Docs
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">PyTorch</span>
                  <p>Explore the documentation for comprehensive guidance on how to use PyTorch</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">PyTorch Domains</span>
                  <p>Read the PyTorch Domains documentation to learn more about domain-specific libraries</p>
                </a>
              </div>
            </div>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Blogs & News
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">PyTorch Blog</span>
                  <p>Catch up on the latest technical news and happenings</p>
                </a>
                 <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Community Blog</span>
                  <p>Stories from the PyTorch ecosystem</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Videos</span>
                  <p>Learn about the latest PyTorch tutorials, new, and more </p>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Community Stories</span>
                  <p>Learn how our community solves real, everyday machine learning problems with PyTorch</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Events</span>
                  <p>Find events, webinars, and podcasts</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Newsletter</span>
                  <p>Stay up-to-date with the latest updates</p>
                </a>
            </div>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                About
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">PyTorch Foundation</span>
                  <p>Learn more about the PyTorch Foundation</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Governing Board</span>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Cloud Credit Program</span>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Technical Advisory Council</span>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Staff</span>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Contact Us</span>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
            <div class="no-dropdown">
              <a href="" data-cta="join">
                Become a Member
              </a>
            </div>
          </li>
          <li>
           <div class="main-menu-item">
             <a href="" class="github-icon">
             </a>
           </div>
          </li>
          <!--- TODO: This block adds the search icon to the nav bar. We will enable it later.
          <li>
            <div class="main-menu-item">
             <a href="" class="search-icon">
             </a>
            </div>
          </li>
          --->
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>
  </div>
</div>

  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <dialog id="pst-search-dialog">
    
<form class="bd-search d-flex align-items-center"
      action="../../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </dialog>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
<div class="bd-header__inner bd-page-width">
  <button class="pst-navbar-icon sidebar-toggle primary-toggle" aria-label="Site navigation">
    <span class="fa-solid fa-bars"></span>
  </button>
  
  
  <div class=" navbar-header-items__start">
    
      <div class="navbar-item">

  
    
  

<a class="navbar-brand logo" href="../../../index.html">
  
  
  
  
  
  
    <p class="title logo__title">Home</p>
  
</a></div>
    
  </div>
  
  <div class=" navbar-header-items">
    
    <div class="me-auto navbar-header-items__center">
      
        <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../python-api/index.html">
    Python API
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../notes/index.html">
    Developer Notes
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-external" href="https://pytorch.org/tutorials/">
    Tutorials
  </a>
</li>

  </ul>
</nav></div>
      
    </div>
    
    
    <div class="navbar-header-items__end">
      
        <div class="navbar-item navbar-persistent--container">
          

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button>
        </div>
      
      
        <div class="navbar-item">
<div class="version-switcher__container dropdown pst-js-only">
  <button id="pst-version-switcher-button-2"
    type="button"
    class="version-switcher__button btn btn-sm dropdown-toggle"
    data-bs-toggle="dropdown"
    aria-haspopup="listbox"
    aria-controls="pst-version-switcher-list-2"
    aria-label="Version switcher list"
  >
    Choose version  <!-- this text may get changed later by javascript -->
    <span class="caret"></span>
  </button>
  <div id="pst-version-switcher-list-2"
    class="version-switcher__menu dropdown-menu list-group-flush py-0"
    role="listbox" aria-labelledby="pst-version-switcher-button-2">
    <!-- dropdown will be populated by javascript on page load -->
  </div>
</div></div>
      
        <div class="navbar-item">

<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button></div>
      
        <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://x.com/PyTorch" title="X" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-x-twitter fa-lg" aria-hidden="true"></i>
            <span class="sr-only">X</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/pytorch/pytorch" title="GitHub" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">GitHub</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://pypi.org/project/torch" title="PyPI" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-custom fa-pypi fa-lg" aria-hidden="true"></i>
            <span class="sr-only">PyPI</span></a>
        </li>
</ul></div>
      
    </div>
    
  </div>
  
  
    <div class="navbar-persistent--mobile">

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button>
    </div>
  

  
</div>

    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
        
      
      <dialog id="pst-primary-sidebar-modal"></dialog>
      <div id="pst-primary-sidebar" class="bd-sidebar-primary bd-sidebar hide-on-wide">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
      <div class="sidebar-header-items__center">
        
          
          
            <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../python-api/index.html">
    Python API
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../notes/index.html">
    Developer Notes
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-external" href="https://pytorch.org/tutorials/">
    Tutorials
  </a>
</li>

  </ul>
</nav></div>
          
        
      </div>
    
    
    
      <div class="sidebar-header-items__end">
        
          <div class="navbar-item">
<div class="version-switcher__container dropdown pst-js-only">
  <button id="pst-version-switcher-button-3"
    type="button"
    class="version-switcher__button btn btn-sm dropdown-toggle"
    data-bs-toggle="dropdown"
    aria-haspopup="listbox"
    aria-controls="pst-version-switcher-list-3"
    aria-label="Version switcher list"
  >
    Choose version  <!-- this text may get changed later by javascript -->
    <span class="caret"></span>
  </button>
  <div id="pst-version-switcher-list-3"
    class="version-switcher__menu dropdown-menu list-group-flush py-0"
    role="listbox" aria-labelledby="pst-version-switcher-button-3">
    <!-- dropdown will be populated by javascript on page load -->
  </div>
</div></div>
        
          <div class="navbar-item">

<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button></div>
        
          <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://x.com/PyTorch" title="X" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-x-twitter fa-lg" aria-hidden="true"></i>
            <span class="sr-only">X</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/pytorch/pytorch" title="GitHub" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">GitHub</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://pypi.org/project/torch" title="PyPI" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-custom fa-pypi fa-lg" aria-hidden="true"></i>
            <span class="sr-only">PyPI</span></a>
        </li>
</ul></div>
        
      </div>
    
  </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
      <div class="sidebar-primary-item">
<div id="ethical-ad-placement"
      class="flat"
      data-ea-publisher="readthedocs"
      data-ea-type="readthedocs-sidebar"
      data-ea-manual="true">
</div></div>
  </div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        
          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item">

<nav aria-label="Breadcrumb" class="d-print-none">
  <ul class="bd-breadcrumbs">
    
    <li class="breadcrumb-item breadcrumb-home">
      <a href="../../../index.html" class="nav-link" aria-label="Home">
        <i class="fa-solid fa-home"></i>
      </a>
    </li>
    
    <li class="breadcrumb-item"><a href="../../index.html" class="nav-link">Module code</a></li>
    
    
    <li class="breadcrumb-item"><a href="../../torch.html" class="nav-link">torch</a></li>
    
    <li class="breadcrumb-item active" aria-current="page"><span class="ellipsis">torch._library.custom_ops</span></li>
  </ul>
</nav>
</div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">
<div class="rating">
    Rate this Page
    <div class="stars">
        
        <span class="star" data-value="1">★</span>
        
        <span class="star" data-value="2">★</span>
        
        <span class="star" data-value="3">★</span>
        
        <span class="star" data-value="4">★</span>
        
        <span class="star" data-value="5">★</span>
        
    </div>
</div>
</div>
      
    </div>
  
</div>
</div>
              
              
              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <h1>Source code for torch._library.custom_ops</h1><div class="highlight"><pre>
<span></span><span class="c1"># mypy: allow-untyped-decorators</span>
<span class="c1"># mypy: allow-untyped-defs</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">inspect</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">logging</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">weakref</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">contextlib</span><span class="w"> </span><span class="kn">import</span> <span class="n">contextmanager</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">typing</span><span class="w"> </span><span class="kn">import</span> <span class="n">Any</span><span class="p">,</span> <span class="n">Callable</span><span class="p">,</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">Iterable</span><span class="p">,</span> <span class="n">List</span><span class="p">,</span> <span class="n">Optional</span><span class="p">,</span> <span class="n">Sequence</span><span class="p">,</span> <span class="n">Set</span><span class="p">,</span> <span class="n">Union</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch</span><span class="w"> </span><span class="kn">import</span> <span class="n">_C</span><span class="p">,</span> <span class="n">_ops</span><span class="p">,</span> <span class="n">Tensor</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch.utils._exposed_in</span><span class="w"> </span><span class="kn">import</span> <span class="n">exposed_in</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">.</span><span class="w"> </span><span class="kn">import</span> <span class="n">autograd</span><span class="p">,</span> <span class="n">utils</span>


<span class="n">device_types_t</span> <span class="o">=</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Sequence</span><span class="p">[</span><span class="nb">str</span><span class="p">]]]</span>
<span class="n">log</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">(</span><span class="vm">__name__</span><span class="p">)</span>


<span class="nd">@exposed_in</span><span class="p">(</span><span class="s2">&quot;torch.library&quot;</span><span class="p">)</span>
<span class="k">def</span><span class="w"> </span><span class="nf">custom_op</span><span class="p">(</span>
    <span class="n">name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="n">fn</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Callable</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="o">/</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">mutates_args</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Iterable</span><span class="p">[</span><span class="nb">str</span><span class="p">]],</span>
    <span class="n">device_types</span><span class="p">:</span> <span class="n">device_types_t</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">schema</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Any</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Wraps a function into custom operator.</span>

<span class="sd">    Reasons why you may want to create a custom op include:</span>
<span class="sd">    - Wrapping a third-party library or custom kernel to work with PyTorch</span>
<span class="sd">    subsystems like Autograd.</span>
<span class="sd">    - Preventing torch.compile/export/FX tracing from peeking inside your function.</span>

<span class="sd">    This API is used as a decorator around a function (please see examples).</span>
<span class="sd">    The provided function must have type hints; these are needed to interface</span>
<span class="sd">    with PyTorch&#39;s various subsystems.</span>

<span class="sd">    Args:</span>
<span class="sd">        name (str): A name for the custom op that looks like &quot;{namespace}::{name}&quot;,</span>
<span class="sd">            e.g. &quot;mylib::my_linear&quot;. The name is used as the op&#39;s stable identifier</span>
<span class="sd">            in PyTorch subsystems (e.g. torch.export, FX graphs).</span>
<span class="sd">            To avoid name collisions, please use your project name as the namespace;</span>
<span class="sd">            e.g. all custom ops in pytorch/fbgemm use &quot;fbgemm&quot; as the namespace.</span>
<span class="sd">        mutates_args (Iterable[str] or &quot;unknown&quot;): The names of args that the function mutates.</span>
<span class="sd">            This MUST be accurate, otherwise, the behavior is undefined. If &quot;unknown&quot;,</span>
<span class="sd">            it pessimistically assumes that all inputs to the operator are being mutated.</span>
<span class="sd">        device_types (None | str | Sequence[str]): The device type(s) the function</span>
<span class="sd">            is valid for. If no device type is provided, then the function</span>
<span class="sd">            is used as the default implementation for all device types.</span>
<span class="sd">            Examples: &quot;cpu&quot;, &quot;cuda&quot;.</span>
<span class="sd">            When registering a device-specific implementation for an operator that accepts no Tensors,</span>
<span class="sd">            we require the operator to have a &quot;device: torch.device argument&quot;.</span>
<span class="sd">        schema (None | str): A schema string for the operator. If None</span>
<span class="sd">            (recommended) we&#39;ll infer a schema for the operator from its type</span>
<span class="sd">            annotations. We recommend letting us infer a schema unless you</span>
<span class="sd">            have a specific reason not to.</span>
<span class="sd">            Example: &quot;(Tensor x, int y) -&gt; (Tensor, Tensor)&quot;.</span>

<span class="sd">    .. note::</span>
<span class="sd">        We recommend not passing in a ``schema`` arg and instead letting us infer</span>
<span class="sd">        it from the type annotations. It is error-prone to write your own schema.</span>
<span class="sd">        You may wish to provide your own schema if our interpretation of</span>
<span class="sd">        the type annotation is not what you want.</span>
<span class="sd">        For more info on how to write a schema string, see</span>
<span class="sd">        `here &lt;https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/native/README.md#func&gt;`_</span>

<span class="sd">    Examples::</span>
<span class="sd">        &gt;&gt;&gt; import torch</span>
<span class="sd">        &gt;&gt;&gt; from torch import Tensor</span>
<span class="sd">        &gt;&gt;&gt; from torch.library import custom_op</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; @custom_op(&quot;mylib::numpy_sin&quot;, mutates_args=())</span>
<span class="sd">        &gt;&gt;&gt; def numpy_sin(x: Tensor) -&gt; Tensor:</span>
<span class="sd">        &gt;&gt;&gt;     x_np = x.cpu().numpy()</span>
<span class="sd">        &gt;&gt;&gt;     y_np = np.sin(x_np)</span>
<span class="sd">        &gt;&gt;&gt;     return torch.from_numpy(y_np).to(device=x.device)</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; x = torch.randn(3)</span>
<span class="sd">        &gt;&gt;&gt; y = numpy_sin(x)</span>
<span class="sd">        &gt;&gt;&gt; assert torch.allclose(y, x.sin())</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; # Example of a custom op that only works for one device type.</span>
<span class="sd">        &gt;&gt;&gt; @custom_op(&quot;mylib::numpy_sin_cpu&quot;, mutates_args=(), device_types=&quot;cpu&quot;)</span>
<span class="sd">        &gt;&gt;&gt; def numpy_sin_cpu(x: Tensor) -&gt; Tensor:</span>
<span class="sd">        &gt;&gt;&gt;     x_np = x.numpy()</span>
<span class="sd">        &gt;&gt;&gt;     y_np = np.sin(x_np)</span>
<span class="sd">        &gt;&gt;&gt;     return torch.from_numpy(y_np)</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; x = torch.randn(3)</span>
<span class="sd">        &gt;&gt;&gt; y = numpy_sin_cpu(x)</span>
<span class="sd">        &gt;&gt;&gt; assert torch.allclose(y, x.sin())</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; # Example of a custom op that mutates an input</span>
<span class="sd">        &gt;&gt;&gt; @custom_op(&quot;mylib::numpy_sin_inplace&quot;, mutates_args={&quot;x&quot;}, device_types=&quot;cpu&quot;)</span>
<span class="sd">        &gt;&gt;&gt; def numpy_sin_inplace(x: Tensor) -&gt; None:</span>
<span class="sd">        &gt;&gt;&gt;     x_np = x.numpy()</span>
<span class="sd">        &gt;&gt;&gt;     np.sin(x_np, out=x_np)</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; x = torch.randn(3)</span>
<span class="sd">        &gt;&gt;&gt; expected = x.sin()</span>
<span class="sd">        &gt;&gt;&gt; numpy_sin_inplace(x)</span>
<span class="sd">        &gt;&gt;&gt; assert torch.allclose(x, expected)</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; # Example of a factory function</span>
<span class="sd">        &gt;&gt;&gt; @torch.library.custom_op(&quot;mylib::bar&quot;, mutates_args={}, device_types=&quot;cpu&quot;)</span>
<span class="sd">        &gt;&gt;&gt; def bar(device: torch.device) -&gt; Tensor:</span>
<span class="sd">        &gt;&gt;&gt;     return torch.ones(3)</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; bar(&quot;cpu&quot;)</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">inner</span><span class="p">(</span><span class="n">fn</span><span class="p">):</span>
        <span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>

        <span class="k">if</span> <span class="n">schema</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">schema_str</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">library</span><span class="o">.</span><span class="n">infer_schema</span><span class="p">(</span><span class="n">fn</span><span class="p">,</span> <span class="n">mutates_args</span><span class="o">=</span><span class="n">mutates_args</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">schema_str</span> <span class="o">=</span> <span class="n">schema</span>

        <span class="n">namespace</span><span class="p">,</span> <span class="n">opname</span> <span class="o">=</span> <span class="n">name</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;::&quot;</span><span class="p">)</span>
        <span class="n">result</span> <span class="o">=</span> <span class="n">CustomOpDef</span><span class="p">(</span><span class="n">namespace</span><span class="p">,</span> <span class="n">opname</span><span class="p">,</span> <span class="n">schema_str</span><span class="p">,</span> <span class="n">fn</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">schema</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># Check that schema&#39;s alias annotations match those of `mutates_args`.</span>
            <span class="n">expected</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
            <span class="k">for</span> <span class="n">arg</span> <span class="ow">in</span> <span class="n">result</span><span class="o">.</span><span class="n">_opoverload</span><span class="o">.</span><span class="n">_schema</span><span class="o">.</span><span class="n">arguments</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">arg</span><span class="o">.</span><span class="n">alias_info</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">arg</span><span class="o">.</span><span class="n">alias_info</span><span class="o">.</span><span class="n">is_write</span><span class="p">:</span>
                    <span class="n">expected</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">arg</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">expected</span> <span class="o">!=</span> <span class="nb">set</span><span class="p">(</span><span class="n">mutates_args</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;Attempted to create a custom op with `mutates_args=</span><span class="si">{</span><span class="n">mutates_args</span><span class="si">}</span><span class="s2">` &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;and `schema=</span><span class="si">{</span><span class="n">schema</span><span class="si">}</span><span class="s2">. The schema suggests that the op mutates </span><span class="si">{</span><span class="n">expected</span><span class="si">}</span><span class="s2">&quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;which is different from what was provided to us in `mutates_args`. &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;Please make these consistent.&quot;</span>
                <span class="p">)</span>
        <span class="n">result</span><span class="o">.</span><span class="n">register_kernel</span><span class="p">(</span><span class="n">device_types</span><span class="p">)(</span><span class="n">fn</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">result</span>

    <span class="k">if</span> <span class="n">fn</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">inner</span>
    <span class="k">return</span> <span class="n">inner</span><span class="p">(</span><span class="n">fn</span><span class="p">)</span>


<div class="viewcode-block" id="CustomOpDef">
<a class="viewcode-back" href="../../../python-api/library.html#torch._library.custom_ops.CustomOpDef">[docs]</a>
<span class="k">class</span><span class="w"> </span><span class="nc">CustomOpDef</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;CustomOpDef is a wrapper around a function that turns it into a custom op.</span>

<span class="sd">    It has various methods for registering additional behavior for this</span>
<span class="sd">    custom op.</span>

<span class="sd">    You should not instantiate CustomOpDef directly; instead, use the</span>
<span class="sd">    :func:`torch.library.custom_op` API.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">namespace</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">schema</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">fn</span><span class="p">:</span> <span class="n">Callable</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="c1"># Fields used to interface with the PyTorch dispatcher</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_namespace</span> <span class="o">=</span> <span class="n">namespace</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_name</span> <span class="o">=</span> <span class="n">name</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_schema</span> <span class="o">=</span> <span class="n">schema</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_init_fn</span> <span class="o">=</span> <span class="n">fn</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_backend_fns</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="kc">None</span><span class="p">],</span> <span class="n">Callable</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_abstract_fn</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Callable</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_setup_context_fn</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Callable</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_backward_fn</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Callable</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_torch_dispatch_fns</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">type</span><span class="p">,</span> <span class="n">Callable</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_vmap_fn</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Callable</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_lib</span> <span class="o">=</span> <span class="n">get_library_allowing_overwrite</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_namespace</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_register_to_dispatcher</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_disabled_kernel</span><span class="p">:</span> <span class="n">Set</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
        <span class="n">OPDEFS</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_qualname</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span>

    <span class="nd">@property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">_qualname</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
        <span class="k">return</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">_namespace</span><span class="si">}</span><span class="s2">::</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">_name</span><span class="si">}</span><span class="s2">&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
        <span class="k">return</span> <span class="sa">f</span><span class="s2">&quot;&lt;CustomOpDef(</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">_qualname</span><span class="si">}</span><span class="s2">)&gt;&quot;</span>

<div class="viewcode-block" id="CustomOpDef.set_kernel_enabled">
<a class="viewcode-back" href="../../../python-api/library.html#torch._library.custom_ops.CustomOpDef.set_kernel_enabled">[docs]</a>
    <span class="nd">@contextmanager</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">set_kernel_enabled</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">device_type</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">enabled</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Disable or re-enable an already registered kernel for this custom operator.</span>

<span class="sd">        If the kernel is already disabled/enabled, this is a no-op.</span>

<span class="sd">        Note:</span>
<span class="sd">            If a kernel is first disabled and then registered, it is disabled until enabled again.</span>

<span class="sd">        Args:</span>
<span class="sd">            device_type (str): The device type to disable/enable the kernel for.</span>
<span class="sd">            disable (bool): Whether to disable or enable the kernel.</span>

<span class="sd">        Example:</span>
<span class="sd">            &gt;&gt;&gt; inp = torch.randn(1)</span>
<span class="sd">            &gt;&gt;&gt;</span>
<span class="sd">            &gt;&gt;&gt; # define custom op `f`.</span>
<span class="sd">            &gt;&gt;&gt; @custom_op(&quot;mylib::f&quot;, mutates_args=())</span>
<span class="sd">            &gt;&gt;&gt; def f(x: Tensor) -&gt; Tensor:</span>
<span class="sd">            &gt;&gt;&gt;     return torch.zeros(1)</span>
<span class="sd">            &gt;&gt;&gt;</span>
<span class="sd">            &gt;&gt;&gt; print(f(inp))  # tensor([0.]), default kernel</span>
<span class="sd">            &gt;&gt;&gt;</span>
<span class="sd">            &gt;&gt;&gt; @f.register_kernel(&quot;cpu&quot;)</span>
<span class="sd">            &gt;&gt;&gt; def _(x):</span>
<span class="sd">            &gt;&gt;&gt;     return torch.ones(1)</span>
<span class="sd">            &gt;&gt;&gt;</span>
<span class="sd">            &gt;&gt;&gt; print(f(inp))  # tensor([1.]), CPU kernel</span>
<span class="sd">            &gt;&gt;&gt;</span>
<span class="sd">            &gt;&gt;&gt; # temporarily disable the CPU kernel</span>
<span class="sd">            &gt;&gt;&gt; with f.set_kernel_enabled(&quot;cpu&quot;, enabled = False):</span>
<span class="sd">            &gt;&gt;&gt;     print(f(inp))  # tensor([0.]) with CPU kernel disabled</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">action</span> <span class="o">=</span> <span class="s2">&quot;enable&quot;</span> <span class="k">if</span> <span class="n">enabled</span> <span class="k">else</span> <span class="s2">&quot;disable&quot;</span>
        <span class="n">originally_disabled</span> <span class="o">=</span> <span class="n">device_type</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_disabled_kernel</span>
        <span class="k">if</span> <span class="n">device_type</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_backend_fns</span><span class="p">:</span>
            <span class="n">log</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                <span class="s2">&quot;Attempted to </span><span class="si">%s</span><span class="s2"> kernel for </span><span class="si">%s</span><span class="s2"> but no kernel was registered for this device type.&quot;</span><span class="p">,</span>
                <span class="n">action</span><span class="p">,</span>
                <span class="n">device_type</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">enabled</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">originally_disabled</span><span class="p">:</span>
                <span class="n">log</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                    <span class="s2">&quot;Attempted to disable kernel for </span><span class="si">%s</span><span class="s2"> but it was already disabled.&quot;</span><span class="p">,</span>
                    <span class="n">device_type</span><span class="p">,</span>
                <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_disabled_kernel</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">device_type</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>  <span class="c1"># enable the kernel</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">originally_disabled</span><span class="p">:</span>
                <span class="n">log</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                    <span class="s2">&quot;Attempted to enable kernel for  </span><span class="si">%s</span><span class="s2"> but it was already enabled.&quot;</span><span class="p">,</span>
                    <span class="n">device_type</span><span class="p">,</span>
                <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_disabled_kernel</span><span class="o">.</span><span class="n">remove</span><span class="p">(</span><span class="n">device_type</span><span class="p">)</span>

        <span class="k">try</span><span class="p">:</span>
            <span class="k">yield</span>
        <span class="k">finally</span><span class="p">:</span>
            <span class="c1"># restore original state</span>
            <span class="k">if</span> <span class="n">originally_disabled</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_disabled_kernel</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">device_type</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_disabled_kernel</span><span class="o">.</span><span class="n">discard</span><span class="p">(</span><span class="n">device_type</span><span class="p">)</span></div>


    <span class="k">def</span><span class="w"> </span><span class="nf">register_kernel</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">device_types</span><span class="p">:</span> <span class="n">device_types_t</span><span class="p">,</span> <span class="n">fn</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Callable</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="o">/</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Callable</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Register an implementation for a device type for this operator.</span>

<span class="sd">        Some valid device_types are: &quot;cpu&quot;, &quot;cuda&quot;, &quot;xla&quot;, &quot;mps&quot;, &quot;ipu&quot;, &quot;xpu&quot;.</span>
<span class="sd">        This API may be used as a decorator.</span>

<span class="sd">        Args:</span>
<span class="sd">            fn (Callable): The function to register as the implementation for</span>
<span class="sd">                the given device types.</span>
<span class="sd">            device_types (str | Sequence[str]): The device device_types to register an impl to.</span>

<span class="sd">        Examples::</span>
<span class="sd">            &gt;&gt;&gt; # xdoctest: +REQUIRES(env:TORCH_DOCTEST_CUDA)</span>
<span class="sd">            &gt;&gt;&gt; import torch</span>
<span class="sd">            &gt;&gt;&gt; from torch import Tensor</span>
<span class="sd">            &gt;&gt;&gt; from torch.library import custom_op</span>
<span class="sd">            &gt;&gt;&gt; import numpy as np</span>
<span class="sd">            &gt;&gt;&gt;</span>
<span class="sd">            &gt;&gt;&gt; # Create a custom op that works on cpu</span>
<span class="sd">            &gt;&gt;&gt; @custom_op(&quot;mylib::numpy_sin&quot;, mutates_args=(), device_types=&quot;cpu&quot;)</span>
<span class="sd">            &gt;&gt;&gt; def numpy_sin(x: Tensor) -&gt; Tensor:</span>
<span class="sd">            &gt;&gt;&gt;     x_np = x.numpy()</span>
<span class="sd">            &gt;&gt;&gt;     y_np = np.sin(x_np)</span>
<span class="sd">            &gt;&gt;&gt;     return torch.from_numpy(y_np)</span>
<span class="sd">            &gt;&gt;&gt;</span>
<span class="sd">            &gt;&gt;&gt; # Add implementations for the cuda device</span>
<span class="sd">            &gt;&gt;&gt; @numpy_sin.register_kernel(&quot;cuda&quot;)</span>
<span class="sd">            &gt;&gt;&gt; def _(x):</span>
<span class="sd">            &gt;&gt;&gt;     x_np = x.cpu().numpy()</span>
<span class="sd">            &gt;&gt;&gt;     y_np = np.sin(x_np)</span>
<span class="sd">            &gt;&gt;&gt;     return torch.from_numpy(y_np).to(device=x.device)</span>
<span class="sd">            &gt;&gt;&gt;</span>
<span class="sd">            &gt;&gt;&gt; x_cpu = torch.randn(3)</span>
<span class="sd">            &gt;&gt;&gt; x_cuda = x_cpu.cuda()</span>
<span class="sd">            &gt;&gt;&gt; assert torch.allclose(numpy_sin(x_cpu), x_cpu.sin())</span>
<span class="sd">            &gt;&gt;&gt; assert torch.allclose(numpy_sin(x_cuda), x_cuda.sin())</span>

<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">def</span><span class="w"> </span><span class="nf">inner</span><span class="p">(</span><span class="n">fn</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">device_types</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">device_types</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
                <span class="n">dtypes</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="kc">None</span><span class="p">]]</span> <span class="o">=</span> <span class="p">[</span><span class="n">device_types</span><span class="p">]</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">dtypes</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">device_types</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">device_type</span> <span class="ow">in</span> <span class="n">dtypes</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">device_type</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_backend_fns</span><span class="p">:</span>

                    <span class="k">def</span><span class="w"> </span><span class="nf">backend_impl</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
                        <span class="n">result</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_backend_fns</span><span class="p">[</span><span class="n">device_type</span><span class="p">](</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

                        <span class="k">def</span><span class="w"> </span><span class="nf">get_module</span><span class="p">():</span>
                            <span class="n">fn</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_backend_fns</span><span class="p">[</span><span class="n">device_type</span><span class="p">]</span>
                            <span class="k">return</span> <span class="n">inspect</span><span class="o">.</span><span class="n">getmodule</span><span class="p">(</span><span class="n">fn</span><span class="p">)</span>

                        <span class="n">utils</span><span class="o">.</span><span class="n">check_aliasing_constraint</span><span class="p">(</span>
                            <span class="bp">self</span><span class="o">.</span><span class="n">_name</span><span class="p">,</span>
                            <span class="n">utils</span><span class="o">.</span><span class="n">iter_tensors</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">),</span>
                            <span class="n">result</span><span class="p">,</span>
                            <span class="n">get_module</span><span class="p">,</span>
                        <span class="p">)</span>
                        <span class="k">return</span> <span class="n">result</span>

                    <span class="k">if</span> <span class="n">device_type</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">_lib</span><span class="o">.</span><span class="n">impl</span><span class="p">(</span>
                            <span class="bp">self</span><span class="o">.</span><span class="n">_name</span><span class="p">,</span> <span class="n">backend_impl</span><span class="p">,</span> <span class="s2">&quot;CompositeExplicitAutograd&quot;</span>
                        <span class="p">)</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">_lib</span><span class="o">.</span><span class="n">impl</span><span class="p">(</span>
                            <span class="bp">self</span><span class="o">.</span><span class="n">_name</span><span class="p">,</span>
                            <span class="n">backend_impl</span><span class="p">,</span>
                            <span class="n">_C</span><span class="o">.</span><span class="n">_dispatch_key_for_device</span><span class="p">(</span><span class="n">device_type</span><span class="p">),</span>
                        <span class="p">)</span>

                <span class="c1"># Wrap function to choose between the default implementation or the device-specific</span>
                <span class="c1"># implementation depending on if the kernel is disabled.</span>
                <span class="nd">@torch</span><span class="o">.</span><span class="n">_disable_dynamo</span>
                <span class="k">def</span><span class="w"> </span><span class="nf">wrapped_fn</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
                    <span class="k">if</span> <span class="n">device_type</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_disabled_kernel</span><span class="p">:</span>
                        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_init_fn</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="k">return</span> <span class="n">fn</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

                <span class="bp">self</span><span class="o">.</span><span class="n">_backend_fns</span><span class="p">[</span><span class="n">device_type</span><span class="p">]</span> <span class="o">=</span> <span class="n">wrapped_fn</span>
            <span class="k">return</span> <span class="n">fn</span>

        <span class="k">if</span> <span class="n">device_types</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">utils</span><span class="o">.</span><span class="n">has_tensor_arg</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_opoverload</span><span class="o">.</span><span class="n">_schema</span>
        <span class="p">):</span>
            <span class="n">device_arg_index</span> <span class="o">=</span> <span class="n">utils</span><span class="o">.</span><span class="n">get_device_arg_index</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_opoverload</span><span class="o">.</span><span class="n">_schema</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">device_arg_index</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s2">&quot;Functions without tensor inputs are required to have a `device: torch.device` argument&quot;</span>
                <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_register_backend_select_dispatcher</span><span class="p">(</span><span class="n">device_arg_index</span><span class="p">)</span>

        <span class="c1"># See NOTE: [Supporting decorator and non-decorator usage]</span>
        <span class="k">if</span> <span class="n">fn</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">inner</span>
        <span class="k">return</span> <span class="n">inner</span><span class="p">(</span><span class="n">fn</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">register_fake</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">fn</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span> <span class="o">/</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Callable</span><span class="p">:</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Register a FakeTensor implementation for this custom op.</span>

<span class="sd">        This is necessary to get the operator to work efficiently with torch.compile.</span>

<span class="sd">        The Fake impl (sometimes also known as a meta kernel or abstract impl)</span>
<span class="sd">        specifies the behavior of this operator on Tensors that carry no data.</span>
<span class="sd">        Given some input Tensors with certain properties</span>
<span class="sd">        (sizes/strides/storage_offset/device), it specifies what the properties of</span>
<span class="sd">        the output Tensors are.</span>

<span class="sd">        Please see :func:`torch.library.impl_abstract` for more details.</span>

<span class="sd">        Args:</span>
<span class="sd">            fn (Callable): The function to register as the FakeTensor</span>
<span class="sd">                implementation.</span>

<span class="sd">        Examples:</span>
<span class="sd">            &gt;&gt;&gt; import torch</span>
<span class="sd">            &gt;&gt;&gt; import numpy as np</span>
<span class="sd">            &gt;&gt;&gt; from torch import Tensor</span>
<span class="sd">            &gt;&gt;&gt;</span>
<span class="sd">            &gt;&gt;&gt; # Example 1: an operator without data-dependent output shape</span>
<span class="sd">            &gt;&gt;&gt; @torch.library.custom_op(&quot;mylib::linear&quot;, mutates_args=())</span>
<span class="sd">            &gt;&gt;&gt; def linear(x: Tensor, weight: Tensor, bias: Tensor) -&gt; Tensor:</span>
<span class="sd">            &gt;&gt;&gt;     return (x @ weight.t()) + bias</span>
<span class="sd">            &gt;&gt;&gt;</span>
<span class="sd">            &gt;&gt;&gt; @linear.register_fake</span>
<span class="sd">            &gt;&gt;&gt; def _(x, weight, bias):</span>
<span class="sd">            &gt;&gt;&gt;     assert x.dim() == 2</span>
<span class="sd">            &gt;&gt;&gt;     assert weight.dim() == 2</span>
<span class="sd">            &gt;&gt;&gt;     assert bias.dim() == 1</span>
<span class="sd">            &gt;&gt;&gt;     assert x.shape[1] == weight.shape[1]</span>
<span class="sd">            &gt;&gt;&gt;     assert weight.shape[0] == bias.shape[0]</span>
<span class="sd">            &gt;&gt;&gt;     assert x.device == weight.device</span>
<span class="sd">            &gt;&gt;&gt;     return x.new_empty(x.size(0), weight.size(0))</span>
<span class="sd">            &gt;&gt;&gt;</span>
<span class="sd">            &gt;&gt;&gt; x = torch.randn(2, 2)</span>
<span class="sd">            &gt;&gt;&gt; weight = torch.randn(2, 2)</span>
<span class="sd">            &gt;&gt;&gt; bias = torch.randn(2)</span>
<span class="sd">            &gt;&gt;&gt; # xdoctest: +SKIP(&quot;Requires Python &lt;= 3.11&quot;)</span>
<span class="sd">            &gt;&gt;&gt; out = torch.compile(linear, fullgraph=True)(x, weight, bias)</span>
<span class="sd">            &gt;&gt;&gt; # xdoctest: +SKIP(&quot;Requires Python &lt;= 3.11&quot;)</span>
<span class="sd">            &gt;&gt;&gt; assert torch.allclose(out, torch.nn.functional.linear(x, weight, bias))</span>
<span class="sd">            &gt;&gt;&gt;</span>
<span class="sd">            &gt;&gt;&gt; # Example 2: an operator with data-dependent output shape</span>
<span class="sd">            &gt;&gt;&gt; @torch.library.custom_op(&quot;mylib::nonzero&quot;, mutates_args=())</span>
<span class="sd">            &gt;&gt;&gt; def nonzero(x: Tensor) -&gt; Tensor:</span>
<span class="sd">            &gt;&gt;&gt;     x_np = x.cpu().numpy()</span>
<span class="sd">            &gt;&gt;&gt;     res = np.stack(np.nonzero(x_np), axis=1)</span>
<span class="sd">            &gt;&gt;&gt;     return torch.tensor(res, device=x.device)</span>
<span class="sd">            &gt;&gt;&gt;</span>
<span class="sd">            &gt;&gt;&gt; @nonzero.register_fake</span>
<span class="sd">            &gt;&gt;&gt; def _(x):</span>
<span class="sd">            &gt;&gt;&gt;     # Number of nonzero-elements is data-dependent.</span>
<span class="sd">            &gt;&gt;&gt;     # Since we cannot peek at the data in an abstract impl,</span>
<span class="sd">            &gt;&gt;&gt;     # we use the ctx object to construct a new symint that</span>
<span class="sd">            &gt;&gt;&gt;     # represents the data-dependent size.</span>
<span class="sd">            &gt;&gt;&gt;     ctx = torch.library.get_ctx()</span>
<span class="sd">            &gt;&gt;&gt;     nnz = ctx.new_dynamic_size()</span>
<span class="sd">            &gt;&gt;&gt;     shape = [nnz, x.dim()]</span>
<span class="sd">            &gt;&gt;&gt;     result = x.new_empty(shape, dtype=torch.int64)</span>
<span class="sd">            &gt;&gt;&gt;     return result</span>
<span class="sd">            &gt;&gt;&gt;</span>
<span class="sd">            &gt;&gt;&gt; x = torch.tensor([0, 1, 2, 0, 0, 1])</span>
<span class="sd">            &gt;&gt;&gt; # xdoctest: +SKIP(&quot;Requires Python &lt;= 3.11&quot;)</span>
<span class="sd">            &gt;&gt;&gt; out = torch.compile(nonzero, fullgraph=True)(x)</span>
<span class="sd">            &gt;&gt;&gt; # xdoctest: +SKIP(&quot;Requires Python &lt;= 3.11&quot;)</span>
<span class="sd">            &gt;&gt;&gt; assert torch.allclose(out, x.nonzero())</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_abstract_fn</span> <span class="o">=</span> <span class="n">fn</span>
        <span class="k">return</span> <span class="n">fn</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">register_torch_dispatch</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">torch_dispatch_class</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="n">fn</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Callable</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="o">/</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Callable</span><span class="p">:</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Registers a torch_dispatch rule for the given operator and ``torch_dispatch_class``.</span>

<span class="sd">        This allows for open registration to specify the behavior between the operator</span>
<span class="sd">        and the ``torch_dispatch_class`` without needing to modify the ``torch_dispatch_class``</span>
<span class="sd">        or the operator directly.</span>

<span class="sd">        Please see :func:`torch.library.register_torch_dispatch` for examples and more details.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">def</span><span class="w"> </span><span class="nf">register</span><span class="p">(</span><span class="n">fn</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">torch_dispatch_class</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_torch_dispatch_fns</span><span class="p">:</span>

                <span class="k">def</span><span class="w"> </span><span class="nf">inner</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
                    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_torch_dispatch_fns</span><span class="p">[</span><span class="n">torch_dispatch_class</span><span class="p">](</span>
                        <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span>
                    <span class="p">)</span>

                <span class="bp">self</span><span class="o">.</span><span class="n">_lib</span><span class="o">.</span><span class="n">_register_torch_dispatch_rule</span><span class="p">(</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_name</span><span class="p">,</span> <span class="n">torch_dispatch_class</span><span class="p">,</span> <span class="n">inner</span>
                <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_torch_dispatch_fns</span><span class="p">[</span><span class="n">torch_dispatch_class</span><span class="p">]</span> <span class="o">=</span> <span class="n">fn</span>
            <span class="k">return</span> <span class="n">fn</span>

        <span class="k">if</span> <span class="n">fn</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">register</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">register</span><span class="p">(</span><span class="n">fn</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">register_autograd</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">backward</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span>
        <span class="o">/</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">setup_context</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Callable</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Register a backward formula for this custom op.</span>

<span class="sd">        In order for an operator to work with autograd, you need to register</span>
<span class="sd">        a backward formula:</span>
<span class="sd">        1. You must tell us how to compute gradients during the backward pass</span>
<span class="sd">        by providing us a &quot;backward&quot; function.</span>
<span class="sd">        2. If you need any values from the forward to compute gradients, you can</span>
<span class="sd">        use `setup_context` to save values for backward.</span>

<span class="sd">        ``backward_fn`` runs during the backward pass. It accepts ``(ctx, *grads)``:</span>
<span class="sd">        - ``grads`` is one or more gradients. The number of gradients matches</span>
<span class="sd">        the number of outputs of the operator.</span>
<span class="sd">        The ``ctx`` object is `the same ctx object &lt;context_method_mixins&gt;`_ used by</span>
<span class="sd">        :class:`torch.autograd.Function`. The semantics of ``backward_fn`` are the</span>
<span class="sd">        same as :meth:`torch.autograd.Function.backward`.</span>

<span class="sd">        ``setup_context(ctx, inputs, output)`` runs during the forward pass.</span>
<span class="sd">        Please save quantities needed for backward onto the ``ctx`` object via</span>
<span class="sd">        either :meth:`torch.autograd.function.FunctionCtx.save_for_backward`</span>
<span class="sd">        or assigning them as attributes of ``ctx``. If your custom op has</span>
<span class="sd">        kwarg-only arguments, we expect the signature of ``setup_context``</span>
<span class="sd">        to be ``setup_context(ctx, inputs, keyword_only_inputs, output)``.</span>

<span class="sd">        Both ``setup_context_fn`` and ``backward_fn`` must be traceable. That is,</span>
<span class="sd">        they may not directly access :meth:`torch.Tensor.data_ptr` and they must</span>
<span class="sd">        not depend on or mutate global state. If you need a non-traceable backward,</span>
<span class="sd">        you can make it a separate custom_op that you call inside ``backward_fn``.</span>

<span class="sd">        If you need different autograd behavior on different devices, then we</span>
<span class="sd">        recommend creating two different custom operators, one for each device</span>
<span class="sd">        that needs different behavior, and switching between them at runtime.</span>

<span class="sd">        Examples:</span>
<span class="sd">            &gt;&gt;&gt; import torch</span>
<span class="sd">            &gt;&gt;&gt; import numpy as np</span>
<span class="sd">            &gt;&gt;&gt; from torch import Tensor</span>
<span class="sd">            &gt;&gt;&gt;</span>
<span class="sd">            &gt;&gt;&gt; @torch.library.custom_op(&quot;mylib::numpy_sin&quot;, mutates_args=())</span>
<span class="sd">            &gt;&gt;&gt; def numpy_sin(x: Tensor) -&gt; Tensor:</span>
<span class="sd">            &gt;&gt;&gt;     x_np = x.cpu().numpy()</span>
<span class="sd">            &gt;&gt;&gt;     y_np = np.sin(x_np)</span>
<span class="sd">            &gt;&gt;&gt;     return torch.from_numpy(y_np).to(device=x.device)</span>
<span class="sd">            &gt;&gt;&gt;</span>
<span class="sd">            &gt;&gt;&gt; def setup_context(ctx, inputs, output) -&gt; Tensor:</span>
<span class="sd">            &gt;&gt;&gt;     x, = inputs</span>
<span class="sd">            &gt;&gt;&gt;     ctx.save_for_backward(x)</span>
<span class="sd">            &gt;&gt;&gt;</span>
<span class="sd">            &gt;&gt;&gt; def backward(ctx, grad):</span>
<span class="sd">            &gt;&gt;&gt;     x, = ctx.saved_tensors</span>
<span class="sd">            &gt;&gt;&gt;     return grad * x.cos()</span>
<span class="sd">            &gt;&gt;&gt;</span>
<span class="sd">            &gt;&gt;&gt; numpy_sin.register_autograd(backward, setup_context=setup_context)</span>
<span class="sd">            &gt;&gt;&gt;</span>
<span class="sd">            &gt;&gt;&gt; x = torch.randn(3, requires_grad=True)</span>
<span class="sd">            &gt;&gt;&gt; y = numpy_sin(x)</span>
<span class="sd">            &gt;&gt;&gt; grad_x, = torch.autograd.grad(y, x, torch.ones_like(y))</span>
<span class="sd">            &gt;&gt;&gt; assert torch.allclose(grad_x, x.cos())</span>
<span class="sd">            &gt;&gt;&gt;</span>
<span class="sd">            &gt;&gt;&gt; # Example with a keyword-only arg</span>
<span class="sd">            &gt;&gt;&gt; @torch.library.custom_op(&quot;mylib::numpy_mul&quot;, mutates_args=())</span>
<span class="sd">            &gt;&gt;&gt; def numpy_mul(x: Tensor, *, val: float) -&gt; Tensor:</span>
<span class="sd">            &gt;&gt;&gt;     x_np = x.cpu().numpy()</span>
<span class="sd">            &gt;&gt;&gt;     y_np = x_np * val</span>
<span class="sd">            &gt;&gt;&gt;     return torch.from_numpy(y_np).to(device=x.device)</span>
<span class="sd">            &gt;&gt;&gt;</span>
<span class="sd">            &gt;&gt;&gt; def setup_context(ctx, inputs, keyword_only_inputs, output) -&gt; Tensor:</span>
<span class="sd">            &gt;&gt;&gt;     ctx.val = keyword_only_inputs[&quot;val&quot;]</span>
<span class="sd">            &gt;&gt;&gt;</span>
<span class="sd">            &gt;&gt;&gt; def backward(ctx, grad):</span>
<span class="sd">            &gt;&gt;&gt;     return grad * ctx.val</span>
<span class="sd">            &gt;&gt;&gt;</span>
<span class="sd">            &gt;&gt;&gt; numpy_mul.register_autograd(backward, setup_context=setup_context)</span>
<span class="sd">            &gt;&gt;&gt;</span>
<span class="sd">            &gt;&gt;&gt; x = torch.randn(3, requires_grad=True)</span>
<span class="sd">            &gt;&gt;&gt; y = numpy_mul(x, val=3.14)</span>
<span class="sd">            &gt;&gt;&gt; grad_x, = torch.autograd.grad(y, x, torch.ones_like(y))</span>
<span class="sd">            &gt;&gt;&gt; assert torch.allclose(grad_x, torch.full_like(x, 3.14))</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">schema</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_opoverload</span><span class="o">.</span><span class="n">_schema</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">utils</span><span class="o">.</span><span class="n">is_functional_schema</span><span class="p">(</span><span class="n">schema</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Cannot register autograd formula for non-functional operator &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="si">}</span><span class="s2"> with schema </span><span class="si">{</span><span class="n">schema</span><span class="si">}</span><span class="s2">. Please create &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;a functional operator and register an autograd formula for that.&quot;</span>
            <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_backward_fn</span> <span class="o">=</span> <span class="n">backward</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_setup_context_fn</span> <span class="o">=</span> <span class="n">setup_context</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_register_to_dispatcher</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">_running_with_deploy</span><span class="p">():</span>
            <span class="n">utils</span><span class="o">.</span><span class="n">warn_deploy</span><span class="p">(</span><span class="n">stacklevel</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
            <span class="k">return</span>

        <span class="n">lib</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_lib</span>
        <span class="n">schema_str</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_name</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">_schema</span>
        <span class="n">cpp_schema</span> <span class="o">=</span> <span class="n">_C</span><span class="o">.</span><span class="n">parse_schema</span><span class="p">(</span><span class="n">schema_str</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">utils</span><span class="o">.</span><span class="n">has_kwarg_only_tensors</span><span class="p">(</span><span class="n">cpp_schema</span><span class="p">):</span>
            <span class="c1"># If you want to support this, the progression is:</span>
            <span class="c1"># - supporting kwarg-only Tensors that are non-differentiable</span>
            <span class="c1"># - supporting kwarg-only Tensors (regardless of differentiability)</span>
            <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;custom_op with kwarg-only Tensor args. Please make your &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;tensors not kwarg-only. Got: </span><span class="si">{</span><span class="n">schema_str</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span>

        <span class="n">lib</span><span class="o">.</span><span class="n">define</span><span class="p">(</span>
            <span class="n">schema_str</span><span class="p">,</span>
            <span class="n">tags</span><span class="o">=</span><span class="p">[</span><span class="n">_C</span><span class="o">.</span><span class="n">Tag</span><span class="o">.</span><span class="n">pt2_compliant_tag</span><span class="p">,</span> <span class="n">_C</span><span class="o">.</span><span class="n">Tag</span><span class="o">.</span><span class="n">needs_fixed_stride_order</span><span class="p">],</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_opoverload</span> <span class="o">=</span> <span class="n">utils</span><span class="o">.</span><span class="n">lookup_op</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_qualname</span><span class="p">)</span>

        <span class="k">def</span><span class="w"> </span><span class="nf">fake_impl</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_abstract_fn</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">utils</span><span class="o">.</span><span class="n">can_generate_trivial_fake_impl</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_opoverload</span><span class="p">):</span>
                    <span class="k">return</span> <span class="kc">None</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;There was no fake impl registered for </span><span class="si">{</span><span class="bp">self</span><span class="si">}</span><span class="s2">. &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;This is necessary for torch.compile/export/fx tracing to work. &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;Please use `</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">_init_fn</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">.register_fake` to add an &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;fake impl.&quot;</span>
                <span class="p">)</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_abstract_fn</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

        <span class="n">lib</span><span class="o">.</span><span class="n">_register_fake</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_name</span><span class="p">,</span> <span class="n">fake_impl</span><span class="p">,</span> <span class="n">_stacklevel</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>

        <span class="n">autograd_impl</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">make_autograd_impl</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_opoverload</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span>
        <span class="n">lib</span><span class="o">.</span><span class="n">impl</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_name</span><span class="p">,</span> <span class="n">autograd_impl</span><span class="p">,</span> <span class="s2">&quot;Autograd&quot;</span><span class="p">,</span> <span class="n">with_keyset</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="n">schema</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_opoverload</span><span class="o">.</span><span class="n">_schema</span>
        <span class="k">if</span> <span class="n">schema</span><span class="o">.</span><span class="n">is_mutable</span><span class="p">:</span>
            <span class="n">mutated_idxs</span><span class="p">,</span> <span class="n">mutated_keys</span> <span class="o">=</span> <span class="n">utils</span><span class="o">.</span><span class="n">mutated_args_kwargs</span><span class="p">(</span><span class="n">schema</span><span class="p">)</span>

            <span class="k">def</span><span class="w"> </span><span class="nf">adinplaceorview_impl</span><span class="p">(</span><span class="n">keyset</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
                <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="n">mutated_idxs</span><span class="p">:</span>
                    <span class="n">increment_version</span><span class="p">(</span><span class="n">args</span><span class="p">[</span><span class="n">idx</span><span class="p">])</span>
                <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">mutated_keys</span><span class="p">:</span>
                    <span class="n">increment_version</span><span class="p">(</span><span class="n">kwargs</span><span class="p">[</span><span class="n">key</span><span class="p">])</span>
                <span class="k">with</span> <span class="n">_C</span><span class="o">.</span><span class="n">_AutoDispatchBelowADInplaceOrView</span><span class="p">():</span>
                    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_opoverload</span><span class="o">.</span><span class="n">redispatch</span><span class="p">(</span>
                        <span class="n">keyset</span> <span class="o">&amp;</span> <span class="n">_C</span><span class="o">.</span><span class="n">_after_ADInplaceOrView_keyset</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span>
                    <span class="p">)</span>

            <span class="n">lib</span><span class="o">.</span><span class="n">impl</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_name</span><span class="p">,</span>
                <span class="n">adinplaceorview_impl</span><span class="p">,</span>
                <span class="s2">&quot;ADInplaceOrView&quot;</span><span class="p">,</span>
                <span class="n">with_keyset</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_register_backend_select_dispatcher</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">device_arg_index</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Switch on the device argument to select the correct backend to dispatch to.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">def</span><span class="w"> </span><span class="nf">backend_select</span><span class="p">(</span><span class="n">keyset</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
            <span class="n">device</span> <span class="o">=</span> <span class="n">args</span><span class="p">[</span><span class="n">device_arg_index</span><span class="p">]</span><span class="o">.</span><span class="n">type</span>
            <span class="k">if</span> <span class="n">device</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_backend_fns</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">_name</span><span class="si">}</span><span class="s2"> does not have a kernel registered for </span><span class="si">{</span><span class="n">device</span><span class="si">}</span><span class="s2">. &quot;</span>
                    <span class="s2">&quot;Please use register_kernel to do so.&quot;</span>
                <span class="p">)</span>
            <span class="n">dispatch_key</span> <span class="o">=</span> <span class="n">_C</span><span class="o">.</span><span class="n">_dispatch_key_for_device</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
            <span class="n">dispatch_key</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">_C</span><span class="o">.</span><span class="n">DispatchKey</span><span class="p">,</span> <span class="n">dispatch_key</span><span class="p">)</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_opoverload</span><span class="o">.</span><span class="n">redispatch</span><span class="p">(</span>
                <span class="n">_C</span><span class="o">.</span><span class="n">DispatchKeySet</span><span class="p">(</span><span class="n">dispatch_key</span><span class="p">),</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span>
            <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_lib</span><span class="o">.</span><span class="n">impl</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_name</span><span class="p">,</span> <span class="n">backend_select</span><span class="p">,</span> <span class="s2">&quot;BackendSelect&quot;</span><span class="p">,</span> <span class="n">with_keyset</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_opoverload</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">register_vmap</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">func</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Callable</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Register a vmap implementation to support :func:`torch.vmap` for this custom op.</span>

<span class="sd">        This API may be used as a decorator.</span>

<span class="sd">        In order for an operator to work with :func:`torch.vmap`, you may need to register a</span>
<span class="sd">        vmap implementation in the following signature:</span>

<span class="sd">            ``vmap_func(info, in_dims: Tuple[Optional[int]], *args, **kwargs)``,</span>

<span class="sd">        where ``*args`` and ``**kwargs`` are the arguments and kwargs for ``op``.</span>

<span class="sd">        It specifies how do we compute the batched version of ``op`` given inputs with an additional</span>
<span class="sd">        dimension (specified by ``in_dims``).</span>

<span class="sd">        For each arg in ``args``, ``in_dims`` has a corresponding ``Optional[int]``. It is ``None``</span>
<span class="sd">        if the arg is not a Tensor or if the arg is not being vmapped over, otherwise, it is an integer</span>
<span class="sd">        specifying what dimension of the Tensor is being vmapped over.</span>

<span class="sd">        ``info`` is a collection of additional metadata that may be helpful:</span>
<span class="sd">        ``info.batch_size`` specifies the size of the dimension being vmapped over, while</span>
<span class="sd">        ``info.randomness`` is the ``randomness`` option that was passed to :func:`torch.vmap`.</span>

<span class="sd">        The return of the function ``func`` is a tuple of ``(output, out_dims)``. Similar to ``in_dims``,</span>
<span class="sd">        ``out_dims`` should be of the same structure as ``output`` and contain one ``out_dim``</span>
<span class="sd">        per output that specifies if the output has the vmapped dimension and what index it is in.</span>

<span class="sd">        Examples:</span>
<span class="sd">            &gt;&gt;&gt; import torch</span>
<span class="sd">            &gt;&gt;&gt; import numpy as np</span>
<span class="sd">            &gt;&gt;&gt; from torch import Tensor</span>
<span class="sd">            &gt;&gt;&gt; from typing import Tuple</span>
<span class="sd">            &gt;&gt;&gt;</span>
<span class="sd">            &gt;&gt;&gt; def to_numpy(tensor):</span>
<span class="sd">            &gt;&gt;&gt;     return tensor.cpu().numpy()</span>
<span class="sd">            &gt;&gt;&gt;</span>
<span class="sd">            &gt;&gt;&gt; lib = torch.library.Library(&quot;mylib&quot;, &quot;FRAGMENT&quot;)</span>
<span class="sd">            &gt;&gt;&gt; @torch.library.custom_op(&quot;mylib::numpy_cube&quot;, mutates_args=())</span>
<span class="sd">            &gt;&gt;&gt; def numpy_cube(x: Tensor) -&gt; Tuple[Tensor, Tensor]:</span>
<span class="sd">            &gt;&gt;&gt;     x_np = to_numpy(x)</span>
<span class="sd">            &gt;&gt;&gt;     dx = torch.tensor(3 * x_np ** 2, device=x.device)</span>
<span class="sd">            &gt;&gt;&gt;     return torch.tensor(x_np ** 3, device=x.device), dx</span>
<span class="sd">            &gt;&gt;&gt;</span>
<span class="sd">            &gt;&gt;&gt; def numpy_cube_vmap(info, in_dims, x):</span>
<span class="sd">            &gt;&gt;&gt;     result = numpy_cube(x)</span>
<span class="sd">            &gt;&gt;&gt;     return result, (in_dims[0], in_dims[0])</span>
<span class="sd">            &gt;&gt;&gt;</span>
<span class="sd">            &gt;&gt;&gt; numpy_cube.register_vmap(numpy_cube_vmap)</span>
<span class="sd">            &gt;&gt;&gt;</span>
<span class="sd">            &gt;&gt;&gt; x = torch.randn(3)</span>
<span class="sd">            &gt;&gt;&gt; torch.vmap(numpy_cube)(x)</span>
<span class="sd">            &gt;&gt;&gt;</span>
<span class="sd">            &gt;&gt;&gt; @torch.library.custom_op(&quot;mylib::numpy_mul&quot;, mutates_args=())</span>
<span class="sd">            &gt;&gt;&gt; def numpy_mul(x: Tensor, y: Tensor) -&gt; Tensor:</span>
<span class="sd">            &gt;&gt;&gt;     return torch.tensor(to_numpy(x) * to_numpy(y), device=x.device)</span>
<span class="sd">            &gt;&gt;&gt;</span>
<span class="sd">            &gt;&gt;&gt; @numpy_mul.register_vmap</span>
<span class="sd">            &gt;&gt;&gt; def numpy_mul_vmap(info, in_dims, x, y):</span>
<span class="sd">            &gt;&gt;&gt;     x_bdim, y_bdim = in_dims</span>
<span class="sd">            &gt;&gt;&gt;     x = x.movedim(x_bdim, -1) if x_bdim is not None else x.unsqueeze(-1)</span>
<span class="sd">            &gt;&gt;&gt;     y = y.movedim(y_bdim, -1) if y_bdim is not None else y.unsqueeze(-1)</span>
<span class="sd">            &gt;&gt;&gt;     result = x * y</span>
<span class="sd">            &gt;&gt;&gt;     result = result.movedim(-1, 0)</span>
<span class="sd">            &gt;&gt;&gt;     return result, 0</span>
<span class="sd">            &gt;&gt;&gt;</span>
<span class="sd">            &gt;&gt;&gt;</span>
<span class="sd">            &gt;&gt;&gt; x = torch.randn(3)</span>
<span class="sd">            &gt;&gt;&gt; y = torch.randn(3)</span>
<span class="sd">            &gt;&gt;&gt; torch.vmap(numpy_mul)(x, y)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="kn">from</span><span class="w"> </span><span class="nn">torch._functorch.autograd_function</span><span class="w"> </span><span class="kn">import</span> <span class="n">custom_function_call_vmap_helper</span>
        <span class="kn">from</span><span class="w"> </span><span class="nn">torch._functorch.pyfunctorch</span><span class="w"> </span><span class="kn">import</span> <span class="n">retrieve_current_functorch_interpreter</span>

        <span class="k">def</span><span class="w"> </span><span class="nf">register</span><span class="p">(</span><span class="n">func</span><span class="p">):</span>
            <span class="n">need_register</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_vmap_fn</span> <span class="ow">is</span> <span class="kc">None</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_vmap_fn</span> <span class="o">=</span> <span class="n">func</span>

            <span class="k">if</span> <span class="n">need_register</span><span class="p">:</span>

                <span class="k">def</span><span class="w"> </span><span class="nf">wrapped_func</span><span class="p">(</span><span class="n">keyset</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
                    <span class="n">interpreter</span> <span class="o">=</span> <span class="n">retrieve_current_functorch_interpreter</span><span class="p">()</span>
                    <span class="k">return</span> <span class="n">custom_function_call_vmap_helper</span><span class="p">(</span>
                        <span class="n">interpreter</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_vmap_fn</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_opoverload</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span>
                    <span class="p">)</span>

                <span class="bp">self</span><span class="o">.</span><span class="n">_lib</span><span class="o">.</span><span class="n">impl</span><span class="p">(</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_name</span><span class="p">,</span> <span class="n">wrapped_func</span><span class="p">,</span> <span class="s2">&quot;FuncTorchBatched&quot;</span><span class="p">,</span> <span class="n">with_keyset</span><span class="o">=</span><span class="kc">True</span>
                <span class="p">)</span>

        <span class="k">if</span> <span class="n">func</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">register</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">register</span><span class="p">(</span><span class="n">func</span><span class="p">)</span></div>



<span class="k">def</span><span class="w"> </span><span class="nf">increment_version</span><span class="p">(</span><span class="n">val</span><span class="p">:</span> <span class="n">Any</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">val</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">graph</span><span class="o">.</span><span class="n">increment_version</span><span class="p">(</span><span class="n">val</span><span class="p">)</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">val</span><span class="p">,</span> <span class="p">(</span><span class="nb">tuple</span><span class="p">,</span> <span class="nb">list</span><span class="p">)):</span>
        <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">val</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">graph</span><span class="o">.</span><span class="n">increment_version</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>


<span class="c1"># NOTE: [Supporting decorator and non-decorator usage]</span>
<span class="c1">#</span>
<span class="c1"># Some APIs may be both used as a decorator and not as a decorator.</span>
<span class="c1"># For example:</span>
<span class="c1">#</span>
<span class="c1"># &gt;&gt;&gt; def fn(x):</span>
<span class="c1"># &gt;&gt;&gt;     return x.sin()</span>
<span class="c1"># &gt;&gt;&gt;</span>
<span class="c1"># &gt;&gt;&gt; # Usage 1: not as a decorator</span>
<span class="c1"># &gt;&gt;&gt; numpy_sin.register_kernel(&quot;cuda&quot;, fn)</span>
<span class="c1"># &gt;&gt;&gt;</span>
<span class="c1"># &gt;&gt;&gt; # Usage 2: as a decorator</span>
<span class="c1"># &gt;&gt;&gt; @numpy_sin.register_kernel(&quot;cuda&quot;)</span>
<span class="c1"># &gt;&gt;&gt; def fn2(x):</span>
<span class="c1"># &gt;&gt;&gt;     return x.sin</span>
<span class="c1">#</span>
<span class="c1"># The way we support this is that `register_kernel` accepts an optional `fn`.</span>
<span class="c1"># If `fn` is provided (Usage 1), then we know that the user is using it not</span>
<span class="c1"># as a decorator.</span>
<span class="c1"># If `fn` is not provided (Usage 2), then `register_kernel` needs to return a</span>
<span class="c1"># decorator.</span>


<span class="n">OPDEF_TO_LIB</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="s2">&quot;torch.library.Library&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>
<span class="n">OPDEFS</span><span class="p">:</span> <span class="n">weakref</span><span class="o">.</span><span class="n">WeakValueDictionary</span> <span class="o">=</span> <span class="n">weakref</span><span class="o">.</span><span class="n">WeakValueDictionary</span><span class="p">()</span>


<span class="k">def</span><span class="w"> </span><span class="nf">get_library_allowing_overwrite</span><span class="p">(</span>
    <span class="n">namespace</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">name</span><span class="p">:</span> <span class="nb">str</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;torch.library.Library&quot;</span><span class="p">:</span>
    <span class="n">qualname</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">namespace</span><span class="si">}</span><span class="s2">::</span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2">&quot;</span>

    <span class="k">if</span> <span class="n">qualname</span> <span class="ow">in</span> <span class="n">OPDEF_TO_LIB</span><span class="p">:</span>
        <span class="n">OPDEF_TO_LIB</span><span class="p">[</span><span class="n">qualname</span><span class="p">]</span><span class="o">.</span><span class="n">_destroy</span><span class="p">()</span>
        <span class="k">del</span> <span class="n">OPDEF_TO_LIB</span><span class="p">[</span><span class="n">qualname</span><span class="p">]</span>

    <span class="n">lib</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">library</span><span class="o">.</span><span class="n">Library</span><span class="p">(</span><span class="n">namespace</span><span class="p">,</span> <span class="s2">&quot;FRAGMENT&quot;</span><span class="p">)</span>  <span class="c1"># noqa: TOR901</span>
    <span class="n">OPDEF_TO_LIB</span><span class="p">[</span><span class="n">qualname</span><span class="p">]</span> <span class="o">=</span> <span class="n">lib</span>
    <span class="k">return</span> <span class="n">lib</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_maybe_get_opdef</span><span class="p">(</span>
    <span class="n">op</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">CustomOpDef</span><span class="p">,</span> <span class="n">_ops</span><span class="o">.</span><span class="n">OpOverload</span><span class="p">,</span> <span class="nb">str</span><span class="p">]</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Optional</span><span class="p">[</span><span class="n">CustomOpDef</span><span class="p">]:</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">op</span><span class="p">,</span> <span class="n">CustomOpDef</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">op</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">op</span><span class="p">,</span> <span class="n">_ops</span><span class="o">.</span><span class="n">OpOverload</span><span class="p">):</span>
        <span class="n">op</span> <span class="o">=</span> <span class="n">op</span><span class="o">.</span><span class="n">_name</span>
    <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">op</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">op</span> <span class="ow">in</span> <span class="n">OPDEFS</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">OPDEFS</span><span class="p">[</span><span class="n">op</span><span class="p">]</span>
    <span class="k">return</span> <span class="kc">None</span>
</pre></div>

                </article>
              
              
              
                <footer class="bd-footer-article">
                  <div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item">
<div class="feedback">
  
<div class="rating">
    Rate this Page
    <div class="stars">
        
        <span class="star" data-value="1">★</span>
        
        <span class="star" data-value="2">★</span>
        
        <span class="star" data-value="3">★</span>
        
        <span class="star" data-value="4">★</span>
        
        <span class="star" data-value="5">★</span>
        
    </div>
</div>

  <div class="feedback-send">
    <button class="feedback-btn" onclick="openGitHubIssue()">Send Feedback</button>
  </div>
</div>
</div>
  
</div>
                </footer>
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
</div>
                </footer>
              
            </div>
            
            
              
            
          </div>
          <footer class="bd-footer-content">
            
          </footer>
        
      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script defer src="../../../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf"></script>
<script defer src="../../../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf"></script>


  <footer class="bd-footer">
    
    <div class="bd-footer__inner bd-page-width">
      
        <div class="footer-items__start">
          
            <div class="footer-item">

  <p class="copyright">
    
      © Copyright PyTorch Contributors.
      <br/>
    
  </p>
</div>
          
            <div class="footer-item">

  <p class="sphinx-version">
    Created using <a href="https://www.sphinx-doc.org/">Sphinx</a> 8.1.0.
    <br/>
  </p>
</div>
          
        </div>
      
    
    
      <div class="footer-items__end">
        
          <div class="footer-item">
<p class="theme-version">
  <!-- # L10n: Setting the PST URL as an argument as this does not need to be localized -->
  Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.16.1.
</p></div>
        
      </div>
    
   </div>
   

   <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
      <div class="container">
        <div class="row">
          <div class="col-md-4 text-center">
            <h2>Docs</h2>
            <p>Access comprehensive developer documentation for PyTorch</p>
            <a class="with-right-arrow" href="">View Docs</a>
          </div>

          <div class="col-md-4 text-center">
            <h2>Tutorials</h2>
            <p>Get in-depth tutorials for beginners and advanced developers</p>
            <a class="with-right-arrow" href="">View Tutorials</a>
          </div>

          <div class="col-md-4 text-center">
            <h2>Resources</h2>
            <p>Find development resources and get your questions answered</p>
            <a class="with-right-arrow" href="">View Resources</a>
          </div>
        </div>
      </div>
  </div>

  <footer class="site-footer">
      <div class="container footer-container">
        <div class="footer-logo-wrapper">
          <a href="" class="footer-logo"></a>
        </div>

        <div class="footer-links-wrapper">
          <div class="footer-links-col">
            <ul>
              <li class="list-title"><a href="">PyTorch</a></li>
              <li><a href="">Get Started</a></li>
              <li><a href="">Features</a></li>
              <li><a href="">Ecosystem</a></li>
              <li><a href="">Blog</a></li>
              <li><a href="">Contributing</a></li>
            </ul>
          </div>

          <div class="footer-links-col">
            <ul>
              <li class="list-title"><a href="">Resources</a></li>
              <li><a href="">Tutorials</a></li>
              <li><a href="">Docs</a></li>
              <li><a href="" target="_blank">Discuss</a></li>
              <li><a href="" target="_blank">Github Issues</a></li>
              <li><a href="" target="_blank">Brand Guidelines</a></li>
            </ul>
          </div>

          <div class="footer-links-col">
            <ul>
              <li class="list-title">Stay up to date</li>
              <li><a href="" target="_blank">Facebook</a></li>
              <li><a href="" target="_blank">Twitter</a></li>
              <li><a href="" target="_blank">YouTube</a></li>
              <li><a href="" target="_blank">LinkedIn</a></li>
            </ul>
            </div>

          <div class="footer-links-col">
            <ul>
              <li class="list-title">PyTorch Podcasts</li>
              <li><a href="" target="_blank">Spotify</a></li>
              <li><a href="" target="_blank">Apple</a></li>
              <li><a href="" target="_blank">Google</a></li>
              <li><a href="" target="_blank">Amazon</a></li>
            </ul>
           </div>
          </div>

          <div class="privacy-policy">
            <ul>
              <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/terms/" target="_blank">Terms</a></li>
              <li class="privacy-policy-links">|</li>
              <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/privacy-policy/" target="_blank">Privacy</a></li>
            </ul>
          </div>
          <div class="copyright">
          <p>© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation.
            For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see
            <a href="https://www.linuxfoundation.org/policies/">www.linuxfoundation.org/policies/</a>. The PyTorch Foundation supports the PyTorch open source
            project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC,
            please see <a href="https://www.lfprojects.org/policies/">www.lfprojects.org/policies/</a>.</p>
        </div>
       </div>

  <div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="../../../_static/img/pytorch-x.svg">
  </div>
</div>
  </footer>


  </body>
</html>