

<!DOCTYPE html>


<html lang="en" data-content_root="" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.19: https://docutils.sourceforge.io/" />
<meta property="og:title" content="torch" />
<meta property="og:type" content="website" />
<meta property="og:url" content="http://pytorch.org/torch.html" />
<meta property="og:site_name" content="PyTorch" />
<meta property="og:description" content="The torch package contains data structures for multi-dimensional tensors and defines mathematical operations over these tensors. Additionally, it provides many utilities for efficient serialization..." />
<meta property="og:image" content="https://pytorch.org/docs/stable/_static/img/pytorch-logo-dark.svg" />
<meta property="og:image:alt" content="PyTorch" />
<meta name="description" content="The torch package contains data structures for multi-dimensional tensors and defines mathematical operations over these tensors. Additionally, it provides many utilities for efficient serialization..." />

    <title>torch &#8212; PyTorch main documentation</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="_static/css/theme.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.css" />
    <link rel="stylesheet" type="text/css" href="_static/katex-math.css" />
    <link rel="stylesheet" type="text/css" href="_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    <link rel="stylesheet" type="text/css" href="_static/css/jit.css" />
    <link rel="stylesheet" type="text/css" href="_static/css/custom.css" />
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/sphinx_highlight.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script src="_static/design-tabs.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'torch';</script>
    <script>
        DOCUMENTATION_OPTIONS.theme_version = '0.15.4';
        DOCUMENTATION_OPTIONS.theme_switcher_json_url = 'https://pytorch.org/docs/pytorch-versions.json';
        DOCUMENTATION_OPTIONS.theme_switcher_version_match = 'main';
        DOCUMENTATION_OPTIONS.show_version_warning_banner = true;
        </script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="torch.is_tensor" href="generated/torch.is_tensor.html" />
    <link rel="prev" title="Python API" href="pytorch-api.html" />

  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
<script src="https://code.jquery.com/jquery-3.7.1.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/2.3.1/list.min.js"></script>
<link rel="stylesheet" type="text/css" href="_static/css/theme.css">
<script type="text/javascript" src="_static/js/theme.js"></script>
<link rel="stylesheet" href="_static/webfonts/all.min.css">

<!-- Google Tag Manager (noscript) -->
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-T8XT4PS"
   height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
   <!-- End Google Tag Manager (noscript) -->
   <!-- Google Tag Manager -->
<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
   new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
   j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
   'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
   j.onload = function() {
     window.dispatchEvent(new Event('gtm_loaded'));
     console.log('GTM loaded successfully');
   };
   })(window,document,'script','dataLayer','GTM-T8XT4PS');
</script>
 <!-- End Google Tag Manager -->
 <!-- Facebook Pixel Code -->
<script>
   !function(f,b,e,v,n,t,s)
   {if(f.fbq)return;n=f.fbq=function(){n.callMethod?
   n.callMethod.apply(n,arguments):n.queue.push(arguments)};
   if(!f._fbq)f._fbq=n;n.push=n;n.loaded=!0;n.version='2.0';
   n.queue=[];t=b.createElement(e);t.async=!0;
   t.src=v;s=b.getElementsByTagName(e)[0];
   s.parentNode.insertBefore(t,s)}(window,document,'script',
   'https://connect.facebook.net/en_US/fbevents.js');
   fbq('init', '243028289693773');
   fbq('track', 'PageView');
</script>
<noscript>
   <img height="1" width="1" src="https://www.facebook.com/tr?id=243028289693773&ev=PageView&noscript=1"/>
</noscript>
<!-- End Facebook Pixel Code -->

<script async src="https://cse.google.com/cse.js?cx=e65585f8c3ea1440e"></script>

   <!--
   Search engines should not index the main version of documentation.
   Stable documentation are built without release == 'main'.
   -->
   <meta name="robots" content="noindex">


  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>

  </head>
<body data-feedback-url="https://github.com/pytorch/pytorch" class="pytorch-body">
     <div class="container-fluid header-holder tutorials-header" id="header-holder">
   <div class="header-container-wrapper">
    <div class="header-container">
      <a class="header-logo" href="" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>

          <li class="main-menu-item">
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Learn
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="">
                  <span class=dropdown-title>Get Started</span>
                  <p>Run PyTorch locally or get started quickly with one of the supported cloud platforms</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Tutorials</span>
                  <p>Whats new in PyTorch tutorials</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Learn the Basics</span>
                  <p>Familiarize yourself with PyTorch concepts and modules</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">PyTorch Recipes</span>
                  <p>Bite-size, ready-to-deploy PyTorch code examples</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Intro to PyTorch - YouTube Series</span>
                  <p>Master PyTorch basics with our engaging YouTube tutorial series</p>
                </a>
              </div>
            </div>
          </li>

          <li>
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Ecosystem
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Tools</span>
                  <p>Learn about the tools and frameworks in the PyTorch Ecosystem</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class=dropdown-title>Community</span>
                  <p>Join the PyTorch developer community to contribute, learn, and get your questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="" target="_blank">
                  <span class=dropdown-title>Forums</span>
                  <p>A place to discuss PyTorch code, issues, install, research</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class=dropdown-title>Developer Resources</span>
                  <p>Find resources and get questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Contributor Awards - 2024</span>
                  <p>Award winners announced at this year's PyTorch Conference</p>
                </a>
              </div>
            </div>
          </li>

          <li>
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Edge
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">About PyTorch Edge</span>
                  <p>Build innovative and privacy-aware AI experiences for edge devices</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">ExecuTorch</span>
                  <p>End-to-end solution for enabling on-device inference capabilities across mobile and edge devices</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">ExecuTorch Docs</span>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Docs
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">PyTorch</span>
                  <p>Explore the documentation for comprehensive guidance on how to use PyTorch</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">PyTorch Domains</span>
                  <p>Read the PyTorch Domains documentation to learn more about domain-specific libraries</p>
                </a>
              </div>
            </div>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Blogs & News
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">PyTorch Blog</span>
                  <p>Catch up on the latest technical news and happenings</p>
                </a>
                 <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Community Blog</span>
                  <p>Stories from the PyTorch ecosystem</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Videos</span>
                  <p>Learn about the latest PyTorch tutorials, new, and more </p>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Community Stories</span>
                  <p>Learn how our community solves real, everyday machine learning problems with PyTorch</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Events</span>
                  <p>Find events, webinars, and podcasts</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Newsletter</span>
                  <p>Stay up-to-date with the latest updates</p>
                </a>
            </div>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                About
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">PyTorch Foundation</span>
                  <p>Learn more about the PyTorch Foundation</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Governing Board</span>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Cloud Credit Program</span>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Technical Advisory Council</span>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Staff</span>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Contact Us</span>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
            <div class="no-dropdown">
              <a href="" data-cta="join">
                Become a Member
              </a>
            </div>
          </li>
          <li>
           <div class="main-menu-item">
             <a href="" class="github-icon">
             </a>
           </div>
          </li>
          <!--- TODO: This block adds the search icon to the nav bar. We will enable it later.
          <li>
            <div class="main-menu-item">
             <a href="" class="search-icon">
             </a>
            </div>
          </li>
          --->
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu">
        <i class="fa-solid fa-ellipsis"></i>
      </a>
    </div>
  </div>
 </div>

 <!-- Begin Mobile Menu -->

<div class="mobile-main-menu">
  <div class="container-fluid">
    <div class="header-container-wrapper">
      <div class="mobile-main-menu-header-container">
        <a class="header-logo" href="" aria-label="PyTorch"></a>
        <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu">
          <i class="fa-solid fa-xmark"></i>
        </a>
      </div>
    </div>
  </div>

  <div class="mobile-main-menu-links-container">
    <div class="main-menu">
      <ul>
         <li class="resources-mobile-menu-title">
           <a>Learn</a>
         </li>
         <ul class="resources-mobile-menu-items">
           <li>
             <a href="">Get Started</a>
           </li>
           <li>
             <a href="">Tutorials</a>
           </li>
           <li>
             <a href="">Learn the Basics</a>
           </li>
           <li>
             <a href="">PyTorch Recipes</a>
           </li>
           <li>
             <a href="">Introduction to PyTorch - YouTube Series</a>
           </li>
         </ul>
         <li class="resources-mobile-menu-title">
           <a>Ecosystem</a>
         </li>
         <ul class="resources-mobile-menu-items">
           <li>
             <a href="">Tools</a>
           </li>
           <li>
             <a href="">Community</a>
           </li>
           <li>
             <a href="">Forums</a>
           </li>
           <li>
             <a href="">Developer Resources</a>
           </li>
           <li>
             <a href="">Contributor Awards - 2024</a>
           </li>
         </ul>

         <li class="resources-mobile-menu-title">
           <a>Edge</a>
         </li>

         <ul class="resources-mobile-menu-items">
           <li>
             <a href="">About PyTorch Edge</a>
           </li>

           <li>
             <a href="">ExecuTorch</a>
           </li>
           <li>
             <a href="">ExecuTorch Documentation</a>
           </li>
         </ul>

         <li class="resources-mobile-menu-title">
           <a>Docs</a>
         </li>

         <ul class="resources-mobile-menu-items">
          <li>
            <a href="">PyTorch</a>
          </li>

          <li>
            <a href="">PyTorch Domains</a>
          </li>
        </ul>

        <li class="resources-mobile-menu-title">
          <a>Blog & News</a>
        </li>

         <ul class="resources-mobile-menu-items">
          <li>
            <a href="">PyTorch Blog</a>
          </li>
          <li>
            <a href="">Community Blog</a>
          </li>

          <li>
            <a href="">Videos</a>
          </li>

          <li>
            <a href="">Community Stories</a>
          </li>
          <li>
            <a href="">Events</a>
          </li>
          <li>
             <a href="">Newsletter</a>
           </li>
        </ul>

        <li class="resources-mobile-menu-title">
          <a>About</a>
        </li>

        <ul class="resources-mobile-menu-items">
          <li>
            <a href="">PyTorch Foundation</a>
          </li>
          <li>
            <a href="">Governing Board</a>
          </li>
          <li>
             <a href="">Cloud Credit Program</a>
          </li>
          <li>
             <a href="">Technical Advisory Council</a>
          </li>
          <li>
             <a href="">Staff</a>
          </li>
          <li>
             <a href="">Contact Us</a>
          </li>
        </ul>
      </ul>
    </div>
  </div>
</div>

<!-- End Mobile Menu -->
   
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>

  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
<div class="bd-header__inner bd-page-width">
  <button class="pst-navbar-icon sidebar-toggle primary-toggle" aria-label="Site navigation">
    <span class="fa-solid fa-bars"></span>
  </button>
  
  
  <div class=" navbar-header-items__start">
    
      <div class="navbar-item">
<script>
document.write(`
  <div class="version-switcher__container dropdown">
    <button id="pst-version-switcher-button-2"
      type="button"
      class="version-switcher__button btn btn-sm dropdown-toggle"
      data-bs-toggle="dropdown"
      aria-haspopup="listbox"
      aria-controls="pst-version-switcher-list-2"
      aria-label="Version switcher list"
    >
      Choose version  <!-- this text may get changed later by javascript -->
      <span class="caret"></span>
    </button>
    <div id="pst-version-switcher-list-2"
      class="version-switcher__menu dropdown-menu list-group-flush py-0"
      role="listbox" aria-labelledby="pst-version-switcher-button-2">
      <!-- dropdown will be populated by javascript on page load -->
    </div>
  </div>
`);
</script></div>
    
      <div class="navbar-item">

  
    
  

<a class="navbar-brand logo" href="index.html">
  
  
  
  
  
  
    <p class="title logo__title">Home</p>
  
</a></div>
    
  </div>
  
  <div class=" navbar-header-items">
    
    <div class="me-auto navbar-header-items__center">
      
        <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item current active">
  <a class="nav-link nav-internal" href="pytorch-api.html">
    Python API
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="notes.html">
    Developer Notes
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-external" href="https://pytorch.org/tutorials/">
    Tutorials
  </a>
</li>

  </ul>
</nav></div>
      
    </div>
    
    
    <div class="navbar-header-items__end">
      
      
        <div class="navbar-item">
<div class="search-container-wrapper">
  <div id="sphinx-search" class="search-container">
    
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </div>

  <div id="google-search" class="search-container" style="display:none;">
    <div class="gcse-search-wrapper">
      <i class="fa-solid fa-magnifying-glass" aria-hidden="true"></i>
      <div class="gcse-search"></div>
    </div>
  </div>

  <div class="search-toggle-container" data-bs-title="Google Search Off" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <div class="search-toggle-inner">
      <label class="switch">
        <input type="checkbox" id="search-toggle">
        <span class="slider round"></span>
      </label>
    </div>
  </div>
</div>

<script>
  document.addEventListener('DOMContentLoaded', function() {
    // Define the search callback
    const myWebSearchStartingCallback = (gname, query) => {
      if (typeof dataLayer !== 'undefined' && query) {
        dataLayer.push({
          'event': 'google_search',
          'search_term': query,
          'event_category': 'Search',
          'event_label': 'Google Search'
        });
        console.log('GA event sent via callback: google_search - ' + query);
      }
      return '';
    };

    // Set up the GCSE search callbacks
    window.__gcse || (window.__gcse = {});
    window.__gcse.searchCallbacks = {
      web: {
        starting: myWebSearchStartingCallback,
      },
    };
    if (window.location.pathname.includes('/search.html')) {
      document.body.classList.add('search-page');
    }

    // Function to reinitialize Google CSE
    function reinitializeGoogleSearch() {
      if (window.__gcse) {
        // Force Google CSE to reinitialize
        if (window.__gcse.initializationCallback) {
          window.__gcse.initializationCallback();
        }
      }
    }

    // Function to handle search toggle
    function handleSearchToggle(toggle, sphinxSearch, googleSearch) {
      if (!toggle || !sphinxSearch || !googleSearch) return;

      // Check if the URL contains /stable/ or /tutorials/
      const currentUrl = window.location.href;
      const shouldDefaultToGoogle = currentUrl.includes('/stable/') || currentUrl.includes('/tutorials/');

      // Check if there's a saved preference, otherwise use the URL-based default
      const savedPreference = localStorage.getItem('searchPreference');
      if (savedPreference === 'google' || (savedPreference === null && shouldDefaultToGoogle)) {
        toggle.checked = true;
        sphinxSearch.style.display = 'none';
        googleSearch.style.display = 'block';
        // Save the preference if it wasn't already saved
        if (savedPreference === null) {
          localStorage.setItem('searchPreference', 'google');
        }
        // Ensure Google search is properly initialized
        reinitializeGoogleSearch();
      } else {
        toggle.checked = false;
        sphinxSearch.style.display = 'block';
        googleSearch.style.display = 'none';
      }

      // Update tooltip based on initial state
      const tooltipElement = document.querySelector('.search-toggle-container');
      if (tooltipElement) {
        tooltipElement.setAttribute('data-bs-title', toggle.checked ? 'Google Search On' : 'Google Search Off');
        // Reinitialize tooltip if Bootstrap's tooltip is already initialized
        if (bootstrap && bootstrap.Tooltip) {
          const tooltipInstance = bootstrap.Tooltip.getInstance(tooltipElement);
          if (tooltipInstance) tooltipInstance.dispose();
          new bootstrap.Tooltip(tooltipElement);
        }
      }

      // Add a data attribute to track if this toggle has been initialized
      if (toggle.hasAttribute('data-initialized')) {
        return; // Skip adding another event listener if already initialized
      }
      toggle.setAttribute('data-initialized', 'true');

      toggle.addEventListener('change', function() {
        if (this.checked) {
          sphinxSearch.style.display = 'none';
          googleSearch.style.display = 'block';
          localStorage.setItem('searchPreference', 'google');
          // Reinitialize Google search when switching to it
          reinitializeGoogleSearch();
          const tooltipElement = document.querySelector('.search-toggle-container');
          if (tooltipElement) {
            tooltipElement.setAttribute('data-bs-title', 'Google Search On');
            // Refresh tooltip
            if (bootstrap && bootstrap.Tooltip) {
              const tooltipInstance = bootstrap.Tooltip.getInstance(tooltipElement);
              if (tooltipInstance) tooltipInstance.dispose();
              new bootstrap.Tooltip(tooltipElement);
            }
          }
          if (typeof dataLayer !== 'undefined') {
            dataLayer.push({
              'event': 'search_engine_switch',
              'event_category': 'Search',
              'event_label': 'Google'
            });
            console.log('GA event sent via dataLayer: search_engine_switch - Google');
          } else {
            console.log('GA not available: Cannot track Google search switch');
          }
        } else {
          sphinxSearch.style.display = 'block';
          googleSearch.style.display = 'none';
          localStorage.setItem('searchPreference', 'sphinx');
          const tooltipElement = document.querySelector('.search-toggle-container');
          if (tooltipElement) {
            tooltipElement.setAttribute('data-bs-title', 'Google Search Off');
            // Refresh tooltip
            if (bootstrap && bootstrap.Tooltip) {
              const tooltipInstance = bootstrap.Tooltip.getInstance(tooltipElement);
              if (tooltipInstance) tooltipInstance.dispose();
              new bootstrap.Tooltip(tooltipElement);
            }
          }
          if (typeof dataLayer !== 'undefined') {
            dataLayer.push({
              'event': 'search_engine_switch',
              'event_category': 'Search',
              'event_label': 'Sphinx'
            });
            console.log('GA event sent via dataLayer: search_engine_switch - Sphinx');
          } else {
            console.log('GA not available: Cannot track Sphinx search switch');
          }
        }

        // Also update mobile search if it exists
        updateMobileSearch(false); // Pass false to prevent triggering another event
      });
    }


    // Function to update mobile search based on current toggle state
    function updateMobileSearch() {
      const toggle = document.getElementById('search-toggle');
      if (!toggle) return;

      // Find mobile search container
      const mobileSearchContainer = document.querySelector('.sidebar-header-items__end .navbar-item .search-container-wrapper');
      if (mobileSearchContainer) {
        const mobileSphinxSearch = mobileSearchContainer.querySelector('#sphinx-search');
        const mobileGoogleSearch = mobileSearchContainer.querySelector('#google-search');

        if (mobileSphinxSearch && mobileGoogleSearch) {
          if (toggle.checked) {
            mobileSphinxSearch.style.display = 'none';
            mobileGoogleSearch.style.display = 'block';
            // Reinitialize Google search in mobile view
            reinitializeGoogleSearch();
          } else {
            mobileSphinxSearch.style.display = 'block';
            mobileGoogleSearch.style.display = 'none';
          }
        }
      }
    }

    // Initialize desktop search toggle
    const toggle = document.getElementById('search-toggle');
    const sphinxSearch = document.getElementById('sphinx-search');
    const googleSearch = document.getElementById('google-search');
    handleSearchToggle(toggle, sphinxSearch, googleSearch);

    // Set placeholder text for Google search input
    const observer = new MutationObserver(function(mutations, obs) {
      const searchInputs = document.querySelectorAll('.gsc-input input');
      searchInputs.forEach(input => {
      if (input) {
        input.setAttribute('placeholder', 'Search the docs ...');

        if (!input.hasAttribute('data-tracking-added')) {
          input.setAttribute('data-tracking-added', 'true');
        }
      }
      });
    });

    observer.observe(document.body, { childList: true, subtree: true });

    // Watch for mobile menu creation
    const mobileMenuObserver = new MutationObserver(function(mutations) {
      for (const mutation of mutations) {
        const mobileSearchInputs = document.querySelectorAll('.sidebar-header-items__end .navbar-item .search-container-wrapper .gsc-input input');
      mobileSearchInputs.forEach(input => {
        if (input) {
          input.setAttribute('placeholder', 'Search the docs ...');
          input.style.paddingLeft = '36px';
          }
        });
        if (mutation.addedNodes.length) {
          // Check if the mobile search container was added
          const mobileSearchContainer = document.querySelector('.sidebar-header-items__end .navbar-item .search-container-wrapper');
          if (mobileSearchContainer) {
            // Clone the toggle for mobile if needed
            const mobileToggle = mobileSearchContainer.querySelector('#search-toggle');
            if (mobileToggle) {
              // Sync mobile toggle with desktop toggle
              mobileToggle.checked = toggle.checked;

              // Update mobile search display
              updateMobileSearch();

              // Add event listener to mobile toggle
              mobileToggle.addEventListener('change', function() {
                // Sync desktop toggle with mobile toggle
                toggle.checked = this.checked;
                // Trigger change event on desktop toggle to update both
                toggle.dispatchEvent(new Event('change'));
              });
            }
          }
        }
      }
    });

    mobileMenuObserver.observe(document.body, { childList: true, subtree: true });

    // Ensure Google CSE is properly loaded
    if (window.__gcse) {
      window.__gcse.callback = function() {
        // This will run after Google CSE is fully loaded
        if (toggle && toggle.checked) {
          // If Google search is active, make sure it's properly initialized
          reinitializeGoogleSearch();
        }
      };
    } else {
      // If Google CSE hasn't loaded yet, set up a callback
      window.__gcse = {
        callback: function() {
          if (toggle && toggle.checked) {
            reinitializeGoogleSearch();
          }
        }
      };
    }
  });
</script>
</div>
      
        <div class="navbar-item">

<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script></div>
      
        <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://x.com/PyTorch" title="X" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-x-twitter fa-lg" aria-hidden="true"></i>
            <span class="sr-only">X</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/pytorch/pytorch" title="GitHub" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">GitHub</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://discuss.pytorch.org/" title="PyTorch Forum" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-discourse fa-lg" aria-hidden="true"></i>
            <span class="sr-only">PyTorch Forum</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://pypi.org/project/torch/" title="PyPi" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-python fa-lg" aria-hidden="true"></i>
            <span class="sr-only">PyPi</span></a>
        </li>
</ul></div>
      
    </div>
    
  </div>
  
  

  
    <button class="pst-navbar-icon sidebar-toggle secondary-toggle" aria-label="On this page">
      <span class="fa-solid fa-outdent"></span>
    </button>
  
</div>

    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
      <div class="sidebar-header-items__center">
        
          
          
            <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item current active">
  <a class="nav-link nav-internal" href="pytorch-api.html">
    Python API
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="notes.html">
    Developer Notes
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-external" href="https://pytorch.org/tutorials/">
    Tutorials
  </a>
</li>

  </ul>
</nav></div>
          
        
      </div>
    
    
    
      <div class="sidebar-header-items__end">
        
          <div class="navbar-item">
<div class="search-container-wrapper">
  <div id="sphinx-search" class="search-container">
    
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </div>

  <div id="google-search" class="search-container" style="display:none;">
    <div class="gcse-search-wrapper">
      <i class="fa-solid fa-magnifying-glass" aria-hidden="true"></i>
      <div class="gcse-search"></div>
    </div>
  </div>

  <div class="search-toggle-container" data-bs-title="Google Search Off" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <div class="search-toggle-inner">
      <label class="switch">
        <input type="checkbox" id="search-toggle">
        <span class="slider round"></span>
      </label>
    </div>
  </div>
</div>

<script>
  document.addEventListener('DOMContentLoaded', function() {
    // Define the search callback
    const myWebSearchStartingCallback = (gname, query) => {
      if (typeof dataLayer !== 'undefined' && query) {
        dataLayer.push({
          'event': 'google_search',
          'search_term': query,
          'event_category': 'Search',
          'event_label': 'Google Search'
        });
        console.log('GA event sent via callback: google_search - ' + query);
      }
      return '';
    };

    // Set up the GCSE search callbacks
    window.__gcse || (window.__gcse = {});
    window.__gcse.searchCallbacks = {
      web: {
        starting: myWebSearchStartingCallback,
      },
    };
    if (window.location.pathname.includes('/search.html')) {
      document.body.classList.add('search-page');
    }

    // Function to reinitialize Google CSE
    function reinitializeGoogleSearch() {
      if (window.__gcse) {
        // Force Google CSE to reinitialize
        if (window.__gcse.initializationCallback) {
          window.__gcse.initializationCallback();
        }
      }
    }

    // Function to handle search toggle
    function handleSearchToggle(toggle, sphinxSearch, googleSearch) {
      if (!toggle || !sphinxSearch || !googleSearch) return;

      // Check if the URL contains /stable/ or /tutorials/
      const currentUrl = window.location.href;
      const shouldDefaultToGoogle = currentUrl.includes('/stable/') || currentUrl.includes('/tutorials/');

      // Check if there's a saved preference, otherwise use the URL-based default
      const savedPreference = localStorage.getItem('searchPreference');
      if (savedPreference === 'google' || (savedPreference === null && shouldDefaultToGoogle)) {
        toggle.checked = true;
        sphinxSearch.style.display = 'none';
        googleSearch.style.display = 'block';
        // Save the preference if it wasn't already saved
        if (savedPreference === null) {
          localStorage.setItem('searchPreference', 'google');
        }
        // Ensure Google search is properly initialized
        reinitializeGoogleSearch();
      } else {
        toggle.checked = false;
        sphinxSearch.style.display = 'block';
        googleSearch.style.display = 'none';
      }

      // Update tooltip based on initial state
      const tooltipElement = document.querySelector('.search-toggle-container');
      if (tooltipElement) {
        tooltipElement.setAttribute('data-bs-title', toggle.checked ? 'Google Search On' : 'Google Search Off');
        // Reinitialize tooltip if Bootstrap's tooltip is already initialized
        if (bootstrap && bootstrap.Tooltip) {
          const tooltipInstance = bootstrap.Tooltip.getInstance(tooltipElement);
          if (tooltipInstance) tooltipInstance.dispose();
          new bootstrap.Tooltip(tooltipElement);
        }
      }

      // Add a data attribute to track if this toggle has been initialized
      if (toggle.hasAttribute('data-initialized')) {
        return; // Skip adding another event listener if already initialized
      }
      toggle.setAttribute('data-initialized', 'true');

      toggle.addEventListener('change', function() {
        if (this.checked) {
          sphinxSearch.style.display = 'none';
          googleSearch.style.display = 'block';
          localStorage.setItem('searchPreference', 'google');
          // Reinitialize Google search when switching to it
          reinitializeGoogleSearch();
          const tooltipElement = document.querySelector('.search-toggle-container');
          if (tooltipElement) {
            tooltipElement.setAttribute('data-bs-title', 'Google Search On');
            // Refresh tooltip
            if (bootstrap && bootstrap.Tooltip) {
              const tooltipInstance = bootstrap.Tooltip.getInstance(tooltipElement);
              if (tooltipInstance) tooltipInstance.dispose();
              new bootstrap.Tooltip(tooltipElement);
            }
          }
          if (typeof dataLayer !== 'undefined') {
            dataLayer.push({
              'event': 'search_engine_switch',
              'event_category': 'Search',
              'event_label': 'Google'
            });
            console.log('GA event sent via dataLayer: search_engine_switch - Google');
          } else {
            console.log('GA not available: Cannot track Google search switch');
          }
        } else {
          sphinxSearch.style.display = 'block';
          googleSearch.style.display = 'none';
          localStorage.setItem('searchPreference', 'sphinx');
          const tooltipElement = document.querySelector('.search-toggle-container');
          if (tooltipElement) {
            tooltipElement.setAttribute('data-bs-title', 'Google Search Off');
            // Refresh tooltip
            if (bootstrap && bootstrap.Tooltip) {
              const tooltipInstance = bootstrap.Tooltip.getInstance(tooltipElement);
              if (tooltipInstance) tooltipInstance.dispose();
              new bootstrap.Tooltip(tooltipElement);
            }
          }
          if (typeof dataLayer !== 'undefined') {
            dataLayer.push({
              'event': 'search_engine_switch',
              'event_category': 'Search',
              'event_label': 'Sphinx'
            });
            console.log('GA event sent via dataLayer: search_engine_switch - Sphinx');
          } else {
            console.log('GA not available: Cannot track Sphinx search switch');
          }
        }

        // Also update mobile search if it exists
        updateMobileSearch(false); // Pass false to prevent triggering another event
      });
    }


    // Function to update mobile search based on current toggle state
    function updateMobileSearch() {
      const toggle = document.getElementById('search-toggle');
      if (!toggle) return;

      // Find mobile search container
      const mobileSearchContainer = document.querySelector('.sidebar-header-items__end .navbar-item .search-container-wrapper');
      if (mobileSearchContainer) {
        const mobileSphinxSearch = mobileSearchContainer.querySelector('#sphinx-search');
        const mobileGoogleSearch = mobileSearchContainer.querySelector('#google-search');

        if (mobileSphinxSearch && mobileGoogleSearch) {
          if (toggle.checked) {
            mobileSphinxSearch.style.display = 'none';
            mobileGoogleSearch.style.display = 'block';
            // Reinitialize Google search in mobile view
            reinitializeGoogleSearch();
          } else {
            mobileSphinxSearch.style.display = 'block';
            mobileGoogleSearch.style.display = 'none';
          }
        }
      }
    }

    // Initialize desktop search toggle
    const toggle = document.getElementById('search-toggle');
    const sphinxSearch = document.getElementById('sphinx-search');
    const googleSearch = document.getElementById('google-search');
    handleSearchToggle(toggle, sphinxSearch, googleSearch);

    // Set placeholder text for Google search input
    const observer = new MutationObserver(function(mutations, obs) {
      const searchInputs = document.querySelectorAll('.gsc-input input');
      searchInputs.forEach(input => {
      if (input) {
        input.setAttribute('placeholder', 'Search the docs ...');

        if (!input.hasAttribute('data-tracking-added')) {
          input.setAttribute('data-tracking-added', 'true');
        }
      }
      });
    });

    observer.observe(document.body, { childList: true, subtree: true });

    // Watch for mobile menu creation
    const mobileMenuObserver = new MutationObserver(function(mutations) {
      for (const mutation of mutations) {
        const mobileSearchInputs = document.querySelectorAll('.sidebar-header-items__end .navbar-item .search-container-wrapper .gsc-input input');
      mobileSearchInputs.forEach(input => {
        if (input) {
          input.setAttribute('placeholder', 'Search the docs ...');
          input.style.paddingLeft = '36px';
          }
        });
        if (mutation.addedNodes.length) {
          // Check if the mobile search container was added
          const mobileSearchContainer = document.querySelector('.sidebar-header-items__end .navbar-item .search-container-wrapper');
          if (mobileSearchContainer) {
            // Clone the toggle for mobile if needed
            const mobileToggle = mobileSearchContainer.querySelector('#search-toggle');
            if (mobileToggle) {
              // Sync mobile toggle with desktop toggle
              mobileToggle.checked = toggle.checked;

              // Update mobile search display
              updateMobileSearch();

              // Add event listener to mobile toggle
              mobileToggle.addEventListener('change', function() {
                // Sync desktop toggle with mobile toggle
                toggle.checked = this.checked;
                // Trigger change event on desktop toggle to update both
                toggle.dispatchEvent(new Event('change'));
              });
            }
          }
        }
      }
    });

    mobileMenuObserver.observe(document.body, { childList: true, subtree: true });

    // Ensure Google CSE is properly loaded
    if (window.__gcse) {
      window.__gcse.callback = function() {
        // This will run after Google CSE is fully loaded
        if (toggle && toggle.checked) {
          // If Google search is active, make sure it's properly initialized
          reinitializeGoogleSearch();
        }
      };
    } else {
      // If Google CSE hasn't loaded yet, set up a callback
      window.__gcse = {
        callback: function() {
          if (toggle && toggle.checked) {
            reinitializeGoogleSearch();
          }
        }
      };
    }
  });
</script>
</div>
        
          <div class="navbar-item">

<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script></div>
        
          <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://x.com/PyTorch" title="X" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-x-twitter fa-lg" aria-hidden="true"></i>
            <span class="sr-only">X</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/pytorch/pytorch" title="GitHub" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">GitHub</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://discuss.pytorch.org/" title="PyTorch Forum" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-discourse fa-lg" aria-hidden="true"></i>
            <span class="sr-only">PyTorch Forum</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://pypi.org/project/torch/" title="PyPi" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-python fa-lg" aria-hidden="true"></i>
            <span class="sr-only">PyPi</span></a>
        </li>
</ul></div>
        
      </div>
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
<nav class="bd-docs-nav bd-links"
     aria-label="Section Navigation">
  <p class="bd-links__title" role="heading" aria-level="1">Section Navigation</p>
  <div class="bd-toc-item navbar-nav"><ul class="current nav bd-sidenav">
<li class="toctree-l1 current active has-children"><a class="current reference internal" href="#">torch</a><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.is_tensor.html">torch.is_tensor</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.is_storage.html">torch.is_storage</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.is_complex.html">torch.is_complex</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.is_conj.html">torch.is_conj</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.is_floating_point.html">torch.is_floating_point</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.is_nonzero.html">torch.is_nonzero</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.set_default_dtype.html">torch.set_default_dtype</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.get_default_dtype.html">torch.get_default_dtype</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.set_default_device.html">torch.set_default_device</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.get_default_device.html">torch.get_default_device</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.set_default_tensor_type.html">torch.set_default_tensor_type</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.numel.html">torch.numel</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.set_printoptions.html">torch.set_printoptions</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.set_flush_denormal.html">torch.set_flush_denormal</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.tensor.html">torch.tensor</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.sparse_coo_tensor.html">torch.sparse_coo_tensor</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.sparse_csr_tensor.html">torch.sparse_csr_tensor</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.sparse_csc_tensor.html">torch.sparse_csc_tensor</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.sparse_bsr_tensor.html">torch.sparse_bsr_tensor</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.sparse_bsc_tensor.html">torch.sparse_bsc_tensor</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.asarray.html">torch.asarray</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.as_tensor.html">torch.as_tensor</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.as_strided.html">torch.as_strided</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.from_file.html">torch.from_file</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.from_numpy.html">torch.from_numpy</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.from_dlpack.html">torch.from_dlpack</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.frombuffer.html">torch.frombuffer</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.zeros.html">torch.zeros</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.zeros_like.html">torch.zeros_like</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.ones.html">torch.ones</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.ones_like.html">torch.ones_like</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.arange.html">torch.arange</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.range.html">torch.range</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.linspace.html">torch.linspace</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.logspace.html">torch.logspace</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.eye.html">torch.eye</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.empty.html">torch.empty</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.empty_like.html">torch.empty_like</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.empty_strided.html">torch.empty_strided</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.full.html">torch.full</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.full_like.html">torch.full_like</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.quantize_per_tensor.html">torch.quantize_per_tensor</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.quantize_per_channel.html">torch.quantize_per_channel</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.dequantize.html">torch.dequantize</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.complex.html">torch.complex</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.polar.html">torch.polar</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.heaviside.html">torch.heaviside</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.adjoint.html">torch.adjoint</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.argwhere.html">torch.argwhere</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cat.html">torch.cat</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.concat.html">torch.concat</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.concatenate.html">torch.concatenate</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.conj.html">torch.conj</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.chunk.html">torch.chunk</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.dsplit.html">torch.dsplit</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.column_stack.html">torch.column_stack</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.dstack.html">torch.dstack</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.gather.html">torch.gather</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.hsplit.html">torch.hsplit</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.hstack.html">torch.hstack</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.index_add.html">torch.index_add</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.index_copy.html">torch.index_copy</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.index_reduce.html">torch.index_reduce</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.index_select.html">torch.index_select</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.masked_select.html">torch.masked_select</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.movedim.html">torch.movedim</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.moveaxis.html">torch.moveaxis</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.narrow.html">torch.narrow</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.narrow_copy.html">torch.narrow_copy</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nonzero.html">torch.nonzero</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.permute.html">torch.permute</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.reshape.html">torch.reshape</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.row_stack.html">torch.row_stack</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.select.html">torch.select</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.scatter.html">torch.scatter</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.diagonal_scatter.html">torch.diagonal_scatter</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.select_scatter.html">torch.select_scatter</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.slice_scatter.html">torch.slice_scatter</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.scatter_add.html">torch.scatter_add</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.scatter_reduce.html">torch.scatter_reduce</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.split.html">torch.split</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.squeeze.html">torch.squeeze</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.stack.html">torch.stack</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.swapaxes.html">torch.swapaxes</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.swapdims.html">torch.swapdims</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.t.html">torch.t</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.take.html">torch.take</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.take_along_dim.html">torch.take_along_dim</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.tensor_split.html">torch.tensor_split</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.tile.html">torch.tile</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.transpose.html">torch.transpose</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.unbind.html">torch.unbind</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.unravel_index.html">torch.unravel_index</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.unsqueeze.html">torch.unsqueeze</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.vsplit.html">torch.vsplit</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.vstack.html">torch.vstack</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.where.html">torch.where</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Stream.html">Stream</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Event.html">Event</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Generator.html">Generator</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.seed.html">torch.seed</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.manual_seed.html">torch.manual_seed</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.initial_seed.html">torch.initial_seed</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.get_rng_state.html">torch.get_rng_state</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.set_rng_state.html">torch.set_rng_state</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.bernoulli.html">torch.bernoulli</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.multinomial.html">torch.multinomial</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.normal.html">torch.normal</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.poisson.html">torch.poisson</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.rand.html">torch.rand</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.rand_like.html">torch.rand_like</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.randint.html">torch.randint</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.randint_like.html">torch.randint_like</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.randn.html">torch.randn</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.randn_like.html">torch.randn_like</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.randperm.html">torch.randperm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.quasirandom.SobolEngine.html">SobolEngine</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.save.html">torch.save</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.load.html">torch.load</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.get_num_threads.html">torch.get_num_threads</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.set_num_threads.html">torch.set_num_threads</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.get_num_interop_threads.html">torch.get_num_interop_threads</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.set_num_interop_threads.html">torch.set_num_interop_threads</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.no_grad.html">no_grad</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.enable_grad.html">enable_grad</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.grad_mode.set_grad_enabled.html">set_grad_enabled</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.is_grad_enabled.html">torch.is_grad_enabled</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.grad_mode.inference_mode.html">inference_mode</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.is_inference_mode_enabled.html">torch.is_inference_mode_enabled</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.abs.html">torch.abs</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.absolute.html">torch.absolute</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.acos.html">torch.acos</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.arccos.html">torch.arccos</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.acosh.html">torch.acosh</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.arccosh.html">torch.arccosh</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.add.html">torch.add</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.addcdiv.html">torch.addcdiv</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.addcmul.html">torch.addcmul</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.angle.html">torch.angle</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.asin.html">torch.asin</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.arcsin.html">torch.arcsin</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.asinh.html">torch.asinh</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.arcsinh.html">torch.arcsinh</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.atan.html">torch.atan</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.arctan.html">torch.arctan</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.atanh.html">torch.atanh</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.arctanh.html">torch.arctanh</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.atan2.html">torch.atan2</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.arctan2.html">torch.arctan2</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.bitwise_not.html">torch.bitwise_not</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.bitwise_and.html">torch.bitwise_and</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.bitwise_or.html">torch.bitwise_or</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.bitwise_xor.html">torch.bitwise_xor</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.bitwise_left_shift.html">torch.bitwise_left_shift</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.bitwise_right_shift.html">torch.bitwise_right_shift</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.ceil.html">torch.ceil</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.clamp.html">torch.clamp</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.clip.html">torch.clip</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.conj_physical.html">torch.conj_physical</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.copysign.html">torch.copysign</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cos.html">torch.cos</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cosh.html">torch.cosh</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.deg2rad.html">torch.deg2rad</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.div.html">torch.div</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.divide.html">torch.divide</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.digamma.html">torch.digamma</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.erf.html">torch.erf</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.erfc.html">torch.erfc</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.erfinv.html">torch.erfinv</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.exp.html">torch.exp</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.exp2.html">torch.exp2</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.expm1.html">torch.expm1</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fake_quantize_per_channel_affine.html">torch.fake_quantize_per_channel_affine</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fake_quantize_per_tensor_affine.html">torch.fake_quantize_per_tensor_affine</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fix.html">torch.fix</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.float_power.html">torch.float_power</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.floor.html">torch.floor</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.floor_divide.html">torch.floor_divide</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fmod.html">torch.fmod</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.frac.html">torch.frac</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.frexp.html">torch.frexp</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.gradient.html">torch.gradient</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.imag.html">torch.imag</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.ldexp.html">torch.ldexp</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.lerp.html">torch.lerp</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.lgamma.html">torch.lgamma</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.log.html">torch.log</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.log10.html">torch.log10</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.log1p.html">torch.log1p</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.log2.html">torch.log2</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.logaddexp.html">torch.logaddexp</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.logaddexp2.html">torch.logaddexp2</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.logical_and.html">torch.logical_and</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.logical_not.html">torch.logical_not</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.logical_or.html">torch.logical_or</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.logical_xor.html">torch.logical_xor</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.logit.html">torch.logit</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.hypot.html">torch.hypot</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.i0.html">torch.i0</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.igamma.html">torch.igamma</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.igammac.html">torch.igammac</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mul.html">torch.mul</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.multiply.html">torch.multiply</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mvlgamma.html">torch.mvlgamma</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nan_to_num.html">torch.nan_to_num</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.neg.html">torch.neg</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.negative.html">torch.negative</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nextafter.html">torch.nextafter</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.polygamma.html">torch.polygamma</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.positive.html">torch.positive</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.pow.html">torch.pow</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.quantized_batch_norm.html">torch.quantized_batch_norm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.quantized_max_pool1d.html">torch.quantized_max_pool1d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.quantized_max_pool2d.html">torch.quantized_max_pool2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.rad2deg.html">torch.rad2deg</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.real.html">torch.real</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.reciprocal.html">torch.reciprocal</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.remainder.html">torch.remainder</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.round.html">torch.round</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.rsqrt.html">torch.rsqrt</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.sigmoid.html">torch.sigmoid</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.sign.html">torch.sign</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.sgn.html">torch.sgn</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.signbit.html">torch.signbit</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.sin.html">torch.sin</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.sinc.html">torch.sinc</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.sinh.html">torch.sinh</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.softmax.html">torch.softmax</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.sqrt.html">torch.sqrt</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.square.html">torch.square</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.sub.html">torch.sub</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.subtract.html">torch.subtract</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.tan.html">torch.tan</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.tanh.html">torch.tanh</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.true_divide.html">torch.true_divide</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.trunc.html">torch.trunc</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xlogy.html">torch.xlogy</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.argmax.html">torch.argmax</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.argmin.html">torch.argmin</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.amax.html">torch.amax</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.amin.html">torch.amin</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.aminmax.html">torch.aminmax</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.all.html">torch.all</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.any.html">torch.any</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.max.html">torch.max</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.min.html">torch.min</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.dist.html">torch.dist</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.logsumexp.html">torch.logsumexp</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mean.html">torch.mean</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nanmean.html">torch.nanmean</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.median.html">torch.median</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nanmedian.html">torch.nanmedian</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mode.html">torch.mode</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.norm.html">torch.norm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nansum.html">torch.nansum</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.prod.html">torch.prod</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.quantile.html">torch.quantile</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nanquantile.html">torch.nanquantile</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.std.html">torch.std</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.std_mean.html">torch.std_mean</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.sum.html">torch.sum</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.unique.html">torch.unique</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.unique_consecutive.html">torch.unique_consecutive</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.var.html">torch.var</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.var_mean.html">torch.var_mean</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.count_nonzero.html">torch.count_nonzero</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.allclose.html">torch.allclose</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.argsort.html">torch.argsort</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.eq.html">torch.eq</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.equal.html">torch.equal</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.ge.html">torch.ge</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.greater_equal.html">torch.greater_equal</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.gt.html">torch.gt</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.greater.html">torch.greater</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.isclose.html">torch.isclose</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.isfinite.html">torch.isfinite</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.isin.html">torch.isin</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.isinf.html">torch.isinf</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.isposinf.html">torch.isposinf</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.isneginf.html">torch.isneginf</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.isnan.html">torch.isnan</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.isreal.html">torch.isreal</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.kthvalue.html">torch.kthvalue</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.le.html">torch.le</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.less_equal.html">torch.less_equal</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.lt.html">torch.lt</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.less.html">torch.less</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.maximum.html">torch.maximum</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.minimum.html">torch.minimum</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fmax.html">torch.fmax</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fmin.html">torch.fmin</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.ne.html">torch.ne</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.not_equal.html">torch.not_equal</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.sort.html">torch.sort</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.topk.html">torch.topk</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.msort.html">torch.msort</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.stft.html">torch.stft</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.istft.html">torch.istft</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.bartlett_window.html">torch.bartlett_window</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.blackman_window.html">torch.blackman_window</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.hamming_window.html">torch.hamming_window</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.hann_window.html">torch.hann_window</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.kaiser_window.html">torch.kaiser_window</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.atleast_1d.html">torch.atleast_1d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.atleast_2d.html">torch.atleast_2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.atleast_3d.html">torch.atleast_3d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.bincount.html">torch.bincount</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.block_diag.html">torch.block_diag</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.broadcast_tensors.html">torch.broadcast_tensors</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.broadcast_to.html">torch.broadcast_to</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.broadcast_shapes.html">torch.broadcast_shapes</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.bucketize.html">torch.bucketize</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cartesian_prod.html">torch.cartesian_prod</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cdist.html">torch.cdist</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.clone.html">torch.clone</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.combinations.html">torch.combinations</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.corrcoef.html">torch.corrcoef</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cov.html">torch.cov</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cross.html">torch.cross</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cummax.html">torch.cummax</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cummin.html">torch.cummin</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cumprod.html">torch.cumprod</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cumsum.html">torch.cumsum</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.diag.html">torch.diag</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.diag_embed.html">torch.diag_embed</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.diagflat.html">torch.diagflat</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.diagonal.html">torch.diagonal</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.diff.html">torch.diff</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.einsum.html">torch.einsum</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.flatten.html">torch.flatten</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.flip.html">torch.flip</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fliplr.html">torch.fliplr</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.flipud.html">torch.flipud</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.kron.html">torch.kron</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.rot90.html">torch.rot90</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.gcd.html">torch.gcd</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.histc.html">torch.histc</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.histogram.html">torch.histogram</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.histogramdd.html">torch.histogramdd</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.meshgrid.html">torch.meshgrid</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.lcm.html">torch.lcm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.logcumsumexp.html">torch.logcumsumexp</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.ravel.html">torch.ravel</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.renorm.html">torch.renorm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.repeat_interleave.html">torch.repeat_interleave</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.roll.html">torch.roll</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.searchsorted.html">torch.searchsorted</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.tensordot.html">torch.tensordot</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.trace.html">torch.trace</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.tril.html">torch.tril</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.tril_indices.html">torch.tril_indices</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.triu.html">torch.triu</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.triu_indices.html">torch.triu_indices</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.unflatten.html">torch.unflatten</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.vander.html">torch.vander</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.view_as_real.html">torch.view_as_real</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.view_as_complex.html">torch.view_as_complex</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.resolve_conj.html">torch.resolve_conj</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.resolve_neg.html">torch.resolve_neg</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.addbmm.html">torch.addbmm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.addmm.html">torch.addmm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.addmv.html">torch.addmv</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.addr.html">torch.addr</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.baddbmm.html">torch.baddbmm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.bmm.html">torch.bmm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.chain_matmul.html">torch.chain_matmul</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cholesky.html">torch.cholesky</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cholesky_inverse.html">torch.cholesky_inverse</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cholesky_solve.html">torch.cholesky_solve</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.dot.html">torch.dot</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.geqrf.html">torch.geqrf</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.ger.html">torch.ger</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.inner.html">torch.inner</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.inverse.html">torch.inverse</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.det.html">torch.det</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.logdet.html">torch.logdet</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.slogdet.html">torch.slogdet</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.lu.html">torch.lu</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.lu_solve.html">torch.lu_solve</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.lu_unpack.html">torch.lu_unpack</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.matmul.html">torch.matmul</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.matrix_power.html">torch.matrix_power</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.matrix_exp.html">torch.matrix_exp</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mm.html">torch.mm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mv.html">torch.mv</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.orgqr.html">torch.orgqr</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.ormqr.html">torch.ormqr</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.outer.html">torch.outer</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.pinverse.html">torch.pinverse</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.qr.html">torch.qr</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.svd.html">torch.svd</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.svd_lowrank.html">torch.svd_lowrank</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.pca_lowrank.html">torch.pca_lowrank</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.lobpcg.html">torch.lobpcg</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.trapz.html">torch.trapz</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.trapezoid.html">torch.trapezoid</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cumulative_trapezoid.html">torch.cumulative_trapezoid</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.triangular_solve.html">torch.triangular_solve</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.vdot.html">torch.vdot</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_abs.html">torch._foreach_abs</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_abs_.html">torch._foreach_abs_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_acos.html">torch._foreach_acos</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_acos_.html">torch._foreach_acos_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_asin.html">torch._foreach_asin</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_asin_.html">torch._foreach_asin_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_atan.html">torch._foreach_atan</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_atan_.html">torch._foreach_atan_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_ceil.html">torch._foreach_ceil</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_ceil_.html">torch._foreach_ceil_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_cos.html">torch._foreach_cos</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_cos_.html">torch._foreach_cos_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_cosh.html">torch._foreach_cosh</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_cosh_.html">torch._foreach_cosh_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_erf.html">torch._foreach_erf</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_erf_.html">torch._foreach_erf_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_erfc.html">torch._foreach_erfc</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_erfc_.html">torch._foreach_erfc_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_exp.html">torch._foreach_exp</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_exp_.html">torch._foreach_exp_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_expm1.html">torch._foreach_expm1</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_expm1_.html">torch._foreach_expm1_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_floor.html">torch._foreach_floor</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_floor_.html">torch._foreach_floor_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_log.html">torch._foreach_log</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_log_.html">torch._foreach_log_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_log10.html">torch._foreach_log10</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_log10_.html">torch._foreach_log10_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_log1p.html">torch._foreach_log1p</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_log1p_.html">torch._foreach_log1p_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_log2.html">torch._foreach_log2</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_log2_.html">torch._foreach_log2_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_neg.html">torch._foreach_neg</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_neg_.html">torch._foreach_neg_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_tan.html">torch._foreach_tan</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_tan_.html">torch._foreach_tan_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_sin.html">torch._foreach_sin</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_sin_.html">torch._foreach_sin_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_sinh.html">torch._foreach_sinh</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_sinh_.html">torch._foreach_sinh_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_round.html">torch._foreach_round</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_round_.html">torch._foreach_round_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_sqrt.html">torch._foreach_sqrt</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_sqrt_.html">torch._foreach_sqrt_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_lgamma.html">torch._foreach_lgamma</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_lgamma_.html">torch._foreach_lgamma_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_frac.html">torch._foreach_frac</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_frac_.html">torch._foreach_frac_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_reciprocal.html">torch._foreach_reciprocal</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_reciprocal_.html">torch._foreach_reciprocal_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_sigmoid.html">torch._foreach_sigmoid</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_sigmoid_.html">torch._foreach_sigmoid_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_trunc.html">torch._foreach_trunc</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_trunc_.html">torch._foreach_trunc_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_zero_.html">torch._foreach_zero_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.compiled_with_cxx11_abi.html">torch.compiled_with_cxx11_abi</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.result_type.html">torch.result_type</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.can_cast.html">torch.can_cast</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.promote_types.html">torch.promote_types</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.use_deterministic_algorithms.html">torch.use_deterministic_algorithms</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.are_deterministic_algorithms_enabled.html">torch.are_deterministic_algorithms_enabled</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.is_deterministic_algorithms_warn_only_enabled.html">torch.is_deterministic_algorithms_warn_only_enabled</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.set_deterministic_debug_mode.html">torch.set_deterministic_debug_mode</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.get_deterministic_debug_mode.html">torch.get_deterministic_debug_mode</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.set_float32_matmul_precision.html">torch.set_float32_matmul_precision</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.get_float32_matmul_precision.html">torch.get_float32_matmul_precision</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.set_warn_always.html">torch.set_warn_always</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.get_device_module.html">torch.get_device_module</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.is_warn_always_enabled.html">torch.is_warn_always_enabled</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.vmap.html">torch.vmap</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._assert.html">torch._assert</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.sym_float.html">torch.sym_float</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.sym_fresh_size.html">torch.sym_fresh_size</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.sym_int.html">torch.sym_int</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.sym_max.html">torch.sym_max</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.sym_min.html">torch.sym_min</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.sym_not.html">torch.sym_not</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.sym_ite.html">torch.sym_ite</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.sym_sum.html">torch.sym_sum</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cond.html">torch.cond</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.compile.html">torch.compile</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="nn.html">torch.nn</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.parameter.Buffer.html">Buffer</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.parameter.Parameter.html">Parameter</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.parameter.UninitializedParameter.html">UninitializedParameter</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.parameter.UninitializedBuffer.html">UninitializedBuffer</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.Module.html">Module</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.Sequential.html">Sequential</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.ModuleList.html">ModuleList</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.ModuleDict.html">ModuleDict</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.ParameterList.html">ParameterList</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.ParameterDict.html">ParameterDict</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.module.register_module_forward_pre_hook.html">torch.nn.modules.module.register_module_forward_pre_hook</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.module.register_module_forward_hook.html">torch.nn.modules.module.register_module_forward_hook</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.module.register_module_backward_hook.html">torch.nn.modules.module.register_module_backward_hook</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.module.register_module_full_backward_pre_hook.html">torch.nn.modules.module.register_module_full_backward_pre_hook</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.module.register_module_full_backward_hook.html">torch.nn.modules.module.register_module_full_backward_hook</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.module.register_module_buffer_registration_hook.html">torch.nn.modules.module.register_module_buffer_registration_hook</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.module.register_module_module_registration_hook.html">torch.nn.modules.module.register_module_module_registration_hook</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.module.register_module_parameter_registration_hook.html">torch.nn.modules.module.register_module_parameter_registration_hook</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.Conv1d.html">Conv1d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.Conv2d.html">Conv2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.Conv3d.html">Conv3d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.ConvTranspose1d.html">ConvTranspose1d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.ConvTranspose2d.html">ConvTranspose2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.ConvTranspose3d.html">ConvTranspose3d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.LazyConv1d.html">LazyConv1d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.LazyConv2d.html">LazyConv2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.LazyConv3d.html">LazyConv3d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.LazyConvTranspose1d.html">LazyConvTranspose1d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.LazyConvTranspose2d.html">LazyConvTranspose2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.LazyConvTranspose3d.html">LazyConvTranspose3d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.Unfold.html">Unfold</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.Fold.html">Fold</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.MaxPool1d.html">MaxPool1d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.MaxPool2d.html">MaxPool2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.MaxPool3d.html">MaxPool3d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.MaxUnpool1d.html">MaxUnpool1d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.MaxUnpool2d.html">MaxUnpool2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.MaxUnpool3d.html">MaxUnpool3d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.AvgPool1d.html">AvgPool1d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.AvgPool2d.html">AvgPool2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.AvgPool3d.html">AvgPool3d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.FractionalMaxPool2d.html">FractionalMaxPool2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.FractionalMaxPool3d.html">FractionalMaxPool3d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.LPPool1d.html">LPPool1d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.LPPool2d.html">LPPool2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.LPPool3d.html">LPPool3d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.AdaptiveMaxPool1d.html">AdaptiveMaxPool1d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.AdaptiveMaxPool2d.html">AdaptiveMaxPool2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.AdaptiveMaxPool3d.html">AdaptiveMaxPool3d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.AdaptiveAvgPool1d.html">AdaptiveAvgPool1d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.AdaptiveAvgPool2d.html">AdaptiveAvgPool2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.AdaptiveAvgPool3d.html">AdaptiveAvgPool3d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.ReflectionPad1d.html">ReflectionPad1d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.ReflectionPad2d.html">ReflectionPad2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.ReflectionPad3d.html">ReflectionPad3d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.ReplicationPad1d.html">ReplicationPad1d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.ReplicationPad2d.html">ReplicationPad2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.ReplicationPad3d.html">ReplicationPad3d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.ZeroPad1d.html">ZeroPad1d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.ZeroPad2d.html">ZeroPad2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.ZeroPad3d.html">ZeroPad3d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.ConstantPad1d.html">ConstantPad1d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.ConstantPad2d.html">ConstantPad2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.ConstantPad3d.html">ConstantPad3d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.CircularPad1d.html">CircularPad1d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.CircularPad2d.html">CircularPad2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.CircularPad3d.html">CircularPad3d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.ELU.html">ELU</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.Hardshrink.html">Hardshrink</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.Hardsigmoid.html">Hardsigmoid</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.Hardtanh.html">Hardtanh</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.Hardswish.html">Hardswish</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.LeakyReLU.html">LeakyReLU</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.LogSigmoid.html">LogSigmoid</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.MultiheadAttention.html">MultiheadAttention</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.PReLU.html">PReLU</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.ReLU.html">ReLU</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.ReLU6.html">ReLU6</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.RReLU.html">RReLU</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.SELU.html">SELU</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.CELU.html">CELU</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.GELU.html">GELU</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.Sigmoid.html">Sigmoid</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.SiLU.html">SiLU</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.Mish.html">Mish</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.Softplus.html">Softplus</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.Softshrink.html">Softshrink</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.Softsign.html">Softsign</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.Tanh.html">Tanh</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.Tanhshrink.html">Tanhshrink</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.Threshold.html">Threshold</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.GLU.html">GLU</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.Softmin.html">Softmin</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.Softmax.html">Softmax</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.Softmax2d.html">Softmax2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.LogSoftmax.html">LogSoftmax</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.AdaptiveLogSoftmaxWithLoss.html">AdaptiveLogSoftmaxWithLoss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.BatchNorm1d.html">BatchNorm1d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.BatchNorm2d.html">BatchNorm2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.BatchNorm3d.html">BatchNorm3d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.LazyBatchNorm1d.html">LazyBatchNorm1d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.LazyBatchNorm2d.html">LazyBatchNorm2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.LazyBatchNorm3d.html">LazyBatchNorm3d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.GroupNorm.html">GroupNorm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.SyncBatchNorm.html">SyncBatchNorm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.InstanceNorm1d.html">InstanceNorm1d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.InstanceNorm2d.html">InstanceNorm2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.InstanceNorm3d.html">InstanceNorm3d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.LazyInstanceNorm1d.html">LazyInstanceNorm1d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.LazyInstanceNorm2d.html">LazyInstanceNorm2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.LazyInstanceNorm3d.html">LazyInstanceNorm3d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.LayerNorm.html">LayerNorm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.LocalResponseNorm.html">LocalResponseNorm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.RMSNorm.html">RMSNorm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.RNNBase.html">RNNBase</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.RNN.html">RNN</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.LSTM.html">LSTM</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.GRU.html">GRU</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.RNNCell.html">RNNCell</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.LSTMCell.html">LSTMCell</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.GRUCell.html">GRUCell</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.Transformer.html">Transformer</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.TransformerEncoder.html">TransformerEncoder</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.TransformerDecoder.html">TransformerDecoder</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.TransformerEncoderLayer.html">TransformerEncoderLayer</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.TransformerDecoderLayer.html">TransformerDecoderLayer</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.Identity.html">Identity</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.Linear.html">Linear</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.Bilinear.html">Bilinear</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.LazyLinear.html">LazyLinear</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.Dropout.html">Dropout</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.Dropout1d.html">Dropout1d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.Dropout2d.html">Dropout2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.Dropout3d.html">Dropout3d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.AlphaDropout.html">AlphaDropout</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.FeatureAlphaDropout.html">FeatureAlphaDropout</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.Embedding.html">Embedding</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.EmbeddingBag.html">EmbeddingBag</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.CosineSimilarity.html">CosineSimilarity</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.PairwiseDistance.html">PairwiseDistance</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.L1Loss.html">L1Loss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.MSELoss.html">MSELoss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.CrossEntropyLoss.html">CrossEntropyLoss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.CTCLoss.html">CTCLoss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.NLLLoss.html">NLLLoss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.PoissonNLLLoss.html">PoissonNLLLoss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.GaussianNLLLoss.html">GaussianNLLLoss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.KLDivLoss.html">KLDivLoss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.BCELoss.html">BCELoss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.BCEWithLogitsLoss.html">BCEWithLogitsLoss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.MarginRankingLoss.html">MarginRankingLoss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.HingeEmbeddingLoss.html">HingeEmbeddingLoss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.MultiLabelMarginLoss.html">MultiLabelMarginLoss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.HuberLoss.html">HuberLoss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.SmoothL1Loss.html">SmoothL1Loss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.SoftMarginLoss.html">SoftMarginLoss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.MultiLabelSoftMarginLoss.html">MultiLabelSoftMarginLoss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.CosineEmbeddingLoss.html">CosineEmbeddingLoss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.MultiMarginLoss.html">MultiMarginLoss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.TripletMarginLoss.html">TripletMarginLoss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.TripletMarginWithDistanceLoss.html">TripletMarginWithDistanceLoss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.PixelShuffle.html">PixelShuffle</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.PixelUnshuffle.html">PixelUnshuffle</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.Upsample.html">Upsample</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.UpsamplingNearest2d.html">UpsamplingNearest2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.UpsamplingBilinear2d.html">UpsamplingBilinear2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.ChannelShuffle.html">ChannelShuffle</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.DataParallel.html">DataParallel</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.parallel.DistributedDataParallel.html">DistributedDataParallel</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.clip_grad_norm_.html">torch.nn.utils.clip_grad_norm_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.clip_grad_norm.html">torch.nn.utils.clip_grad_norm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.clip_grad_value_.html">torch.nn.utils.clip_grad_value_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.get_total_norm.html">torch.nn.utils.get_total_norm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.clip_grads_with_norm_.html">torch.nn.utils.clip_grads_with_norm_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.parameters_to_vector.html">torch.nn.utils.parameters_to_vector</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.vector_to_parameters.html">torch.nn.utils.vector_to_parameters</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.fuse_conv_bn_eval.html">torch.nn.utils.fuse_conv_bn_eval</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.fuse_conv_bn_weights.html">torch.nn.utils.fuse_conv_bn_weights</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.fuse_linear_bn_eval.html">torch.nn.utils.fuse_linear_bn_eval</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.fuse_linear_bn_weights.html">torch.nn.utils.fuse_linear_bn_weights</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.convert_conv2d_weight_memory_format.html">torch.nn.utils.convert_conv2d_weight_memory_format</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.convert_conv3d_weight_memory_format.html">torch.nn.utils.convert_conv3d_weight_memory_format</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.weight_norm.html">torch.nn.utils.weight_norm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.remove_weight_norm.html">torch.nn.utils.remove_weight_norm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.spectral_norm.html">torch.nn.utils.spectral_norm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.remove_spectral_norm.html">torch.nn.utils.remove_spectral_norm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.skip_init.html">torch.nn.utils.skip_init</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.prune.BasePruningMethod.html">BasePruningMethod</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.prune.PruningContainer.html">PruningContainer</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.prune.Identity.html">torch.nn.utils.prune.identity</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.prune.RandomUnstructured.html">RandomUnstructured</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.prune.L1Unstructured.html">L1Unstructured</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.prune.RandomStructured.html">RandomStructured</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.prune.LnStructured.html">LnStructured</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.prune.CustomFromMask.html">CustomFromMask</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.prune.random_unstructured.html">torch.nn.utils.prune.random_unstructured</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.prune.l1_unstructured.html">torch.nn.utils.prune.l1_unstructured</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.prune.random_structured.html">torch.nn.utils.prune.random_structured</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.prune.ln_structured.html">torch.nn.utils.prune.ln_structured</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.prune.global_unstructured.html">torch.nn.utils.prune.global_unstructured</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.prune.custom_from_mask.html">torch.nn.utils.prune.custom_from_mask</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.prune.remove.html">torch.nn.utils.prune.remove</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.prune.is_pruned.html">torch.nn.utils.prune.is_pruned</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.parametrizations.orthogonal.html">torch.nn.utils.parametrizations.orthogonal</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.parametrizations.weight_norm.html">torch.nn.utils.parametrizations.weight_norm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.parametrizations.spectral_norm.html">torch.nn.utils.parametrizations.spectral_norm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.parametrize.register_parametrization.html">torch.nn.utils.parametrize.register_parametrization</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.parametrize.remove_parametrizations.html">torch.nn.utils.parametrize.remove_parametrizations</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.parametrize.cached.html">torch.nn.utils.parametrize.cached</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.parametrize.is_parametrized.html">torch.nn.utils.parametrize.is_parametrized</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.parametrize.ParametrizationList.html">ParametrizationList</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.stateless.functional_call.html">torch.nn.utils.stateless.functional_call</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.rnn.PackedSequence.html">PackedSequence</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.rnn.pack_padded_sequence.html">torch.nn.utils.rnn.pack_padded_sequence</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.rnn.pad_packed_sequence.html">torch.nn.utils.rnn.pad_packed_sequence</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.rnn.pad_sequence.html">torch.nn.utils.rnn.pad_sequence</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.rnn.pack_sequence.html">torch.nn.utils.rnn.pack_sequence</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.rnn.unpack_sequence.html">torch.nn.utils.rnn.unpack_sequence</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.rnn.unpad_sequence.html">torch.nn.utils.rnn.unpad_sequence</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.Flatten.html">Flatten</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.Unflatten.html">Unflatten</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.lazy.LazyModuleMixin.html">LazyModuleMixin</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.normalization.RMSNorm.html">RMSNorm</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="nn.functional.html">torch.nn.functional</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.conv1d.html">torch.nn.functional.conv1d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.conv2d.html">torch.nn.functional.conv2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.conv3d.html">torch.nn.functional.conv3d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.conv_transpose1d.html">torch.nn.functional.conv_transpose1d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.conv_transpose2d.html">torch.nn.functional.conv_transpose2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.conv_transpose3d.html">torch.nn.functional.conv_transpose3d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.unfold.html">torch.nn.functional.unfold</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.fold.html">torch.nn.functional.fold</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.avg_pool1d.html">torch.nn.functional.avg_pool1d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.avg_pool2d.html">torch.nn.functional.avg_pool2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.avg_pool3d.html">torch.nn.functional.avg_pool3d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.max_pool1d.html">torch.nn.functional.max_pool1d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.max_pool2d.html">torch.nn.functional.max_pool2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.max_pool3d.html">torch.nn.functional.max_pool3d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.max_unpool1d.html">torch.nn.functional.max_unpool1d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.max_unpool2d.html">torch.nn.functional.max_unpool2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.max_unpool3d.html">torch.nn.functional.max_unpool3d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.lp_pool1d.html">torch.nn.functional.lp_pool1d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.lp_pool2d.html">torch.nn.functional.lp_pool2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.lp_pool3d.html">torch.nn.functional.lp_pool3d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.adaptive_max_pool1d.html">torch.nn.functional.adaptive_max_pool1d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.adaptive_max_pool2d.html">torch.nn.functional.adaptive_max_pool2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.adaptive_max_pool3d.html">torch.nn.functional.adaptive_max_pool3d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.adaptive_avg_pool1d.html">torch.nn.functional.adaptive_avg_pool1d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.adaptive_avg_pool2d.html">torch.nn.functional.adaptive_avg_pool2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.adaptive_avg_pool3d.html">torch.nn.functional.adaptive_avg_pool3d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.fractional_max_pool2d.html">torch.nn.functional.fractional_max_pool2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.fractional_max_pool3d.html">torch.nn.functional.fractional_max_pool3d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.scaled_dot_product_attention.html">torch.nn.functional.scaled_dot_product_attention</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.threshold.html">torch.nn.functional.threshold</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.threshold_.html">torch.nn.functional.threshold_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.relu.html">torch.nn.functional.relu</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.relu_.html">torch.nn.functional.relu_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.hardtanh.html">torch.nn.functional.hardtanh</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.hardtanh_.html">torch.nn.functional.hardtanh_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.hardswish.html">torch.nn.functional.hardswish</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.relu6.html">torch.nn.functional.relu6</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.elu.html">torch.nn.functional.elu</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.elu_.html">torch.nn.functional.elu_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.selu.html">torch.nn.functional.selu</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.celu.html">torch.nn.functional.celu</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.leaky_relu.html">torch.nn.functional.leaky_relu</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.leaky_relu_.html">torch.nn.functional.leaky_relu_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.prelu.html">torch.nn.functional.prelu</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.rrelu.html">torch.nn.functional.rrelu</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.rrelu_.html">torch.nn.functional.rrelu_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.glu.html">torch.nn.functional.glu</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.gelu.html">torch.nn.functional.gelu</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.logsigmoid.html">torch.nn.functional.logsigmoid</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.hardshrink.html">torch.nn.functional.hardshrink</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.tanhshrink.html">torch.nn.functional.tanhshrink</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.softsign.html">torch.nn.functional.softsign</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.softplus.html">torch.nn.functional.softplus</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.softmin.html">torch.nn.functional.softmin</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.softmax.html">torch.nn.functional.softmax</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.softshrink.html">torch.nn.functional.softshrink</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.gumbel_softmax.html">torch.nn.functional.gumbel_softmax</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.log_softmax.html">torch.nn.functional.log_softmax</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.tanh.html">torch.nn.functional.tanh</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.sigmoid.html">torch.nn.functional.sigmoid</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.hardsigmoid.html">torch.nn.functional.hardsigmoid</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.silu.html">torch.nn.functional.silu</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.mish.html">torch.nn.functional.mish</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.batch_norm.html">torch.nn.functional.batch_norm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.group_norm.html">torch.nn.functional.group_norm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.instance_norm.html">torch.nn.functional.instance_norm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.layer_norm.html">torch.nn.functional.layer_norm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.local_response_norm.html">torch.nn.functional.local_response_norm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.rms_norm.html">torch.nn.functional.rms_norm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.normalize.html">torch.nn.functional.normalize</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.linear.html">torch.nn.functional.linear</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.bilinear.html">torch.nn.functional.bilinear</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.dropout.html">torch.nn.functional.dropout</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.alpha_dropout.html">torch.nn.functional.alpha_dropout</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.feature_alpha_dropout.html">torch.nn.functional.feature_alpha_dropout</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.dropout1d.html">torch.nn.functional.dropout1d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.dropout2d.html">torch.nn.functional.dropout2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.dropout3d.html">torch.nn.functional.dropout3d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.embedding.html">torch.nn.functional.embedding</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.embedding_bag.html">torch.nn.functional.embedding_bag</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.one_hot.html">torch.nn.functional.one_hot</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.pairwise_distance.html">torch.nn.functional.pairwise_distance</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.cosine_similarity.html">torch.nn.functional.cosine_similarity</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.pdist.html">torch.nn.functional.pdist</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.binary_cross_entropy.html">torch.nn.functional.binary_cross_entropy</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.binary_cross_entropy_with_logits.html">torch.nn.functional.binary_cross_entropy_with_logits</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.poisson_nll_loss.html">torch.nn.functional.poisson_nll_loss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.cosine_embedding_loss.html">torch.nn.functional.cosine_embedding_loss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.cross_entropy.html">torch.nn.functional.cross_entropy</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.ctc_loss.html">torch.nn.functional.ctc_loss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.gaussian_nll_loss.html">torch.nn.functional.gaussian_nll_loss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.hinge_embedding_loss.html">torch.nn.functional.hinge_embedding_loss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.kl_div.html">torch.nn.functional.kl_div</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.l1_loss.html">torch.nn.functional.l1_loss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.mse_loss.html">torch.nn.functional.mse_loss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.margin_ranking_loss.html">torch.nn.functional.margin_ranking_loss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.multilabel_margin_loss.html">torch.nn.functional.multilabel_margin_loss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.multilabel_soft_margin_loss.html">torch.nn.functional.multilabel_soft_margin_loss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.multi_margin_loss.html">torch.nn.functional.multi_margin_loss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.nll_loss.html">torch.nn.functional.nll_loss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.huber_loss.html">torch.nn.functional.huber_loss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.smooth_l1_loss.html">torch.nn.functional.smooth_l1_loss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.soft_margin_loss.html">torch.nn.functional.soft_margin_loss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.triplet_margin_loss.html">torch.nn.functional.triplet_margin_loss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.triplet_margin_with_distance_loss.html">torch.nn.functional.triplet_margin_with_distance_loss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.pixel_shuffle.html">torch.nn.functional.pixel_shuffle</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.pixel_unshuffle.html">torch.nn.functional.pixel_unshuffle</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.pad.html">torch.nn.functional.pad</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.interpolate.html">torch.nn.functional.interpolate</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.upsample.html">torch.nn.functional.upsample</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.upsample_nearest.html">torch.nn.functional.upsample_nearest</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.upsample_bilinear.html">torch.nn.functional.upsample_bilinear</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.grid_sample.html">torch.nn.functional.grid_sample</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.affine_grid.html">torch.nn.functional.affine_grid</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.torch.nn.parallel.data_parallel.html">torch.nn.functional.torch.nn.parallel.data_parallel</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="tensors.html">torch.Tensor</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.new_tensor.html">torch.Tensor.new_tensor</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.new_full.html">torch.Tensor.new_full</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.new_empty.html">torch.Tensor.new_empty</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.new_ones.html">torch.Tensor.new_ones</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.new_zeros.html">torch.Tensor.new_zeros</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.is_cuda.html">torch.Tensor.is_cuda</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.is_quantized.html">torch.Tensor.is_quantized</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.is_meta.html">torch.Tensor.is_meta</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.device.html">torch.Tensor.device</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.grad.html">torch.Tensor.grad</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.ndim.html">torch.Tensor.ndim</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.real.html">torch.Tensor.real</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.imag.html">torch.Tensor.imag</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.nbytes.html">torch.Tensor.nbytes</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.itemsize.html">torch.Tensor.itemsize</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.abs.html">torch.Tensor.abs</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.abs_.html">torch.Tensor.abs_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.absolute.html">torch.Tensor.absolute</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.absolute_.html">torch.Tensor.absolute_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.acos.html">torch.Tensor.acos</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.acos_.html">torch.Tensor.acos_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.arccos.html">torch.Tensor.arccos</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.arccos_.html">torch.Tensor.arccos_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.add.html">torch.Tensor.add</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.add_.html">torch.Tensor.add_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.addbmm.html">torch.Tensor.addbmm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.addbmm_.html">torch.Tensor.addbmm_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.addcdiv.html">torch.Tensor.addcdiv</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.addcdiv_.html">torch.Tensor.addcdiv_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.addcmul.html">torch.Tensor.addcmul</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.addcmul_.html">torch.Tensor.addcmul_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.addmm.html">torch.Tensor.addmm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.addmm_.html">torch.Tensor.addmm_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.sspaddmm.html">torch.Tensor.sspaddmm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.addmv.html">torch.Tensor.addmv</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.addmv_.html">torch.Tensor.addmv_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.addr.html">torch.Tensor.addr</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.addr_.html">torch.Tensor.addr_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.adjoint.html">torch.Tensor.adjoint</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.allclose.html">torch.Tensor.allclose</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.amax.html">torch.Tensor.amax</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.amin.html">torch.Tensor.amin</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.aminmax.html">torch.Tensor.aminmax</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.angle.html">torch.Tensor.angle</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.apply_.html">torch.Tensor.apply_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.argmax.html">torch.Tensor.argmax</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.argmin.html">torch.Tensor.argmin</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.argsort.html">torch.Tensor.argsort</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.argwhere.html">torch.Tensor.argwhere</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.asin.html">torch.Tensor.asin</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.asin_.html">torch.Tensor.asin_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.arcsin.html">torch.Tensor.arcsin</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.arcsin_.html">torch.Tensor.arcsin_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.as_strided.html">torch.Tensor.as_strided</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.atan.html">torch.Tensor.atan</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.atan_.html">torch.Tensor.atan_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.arctan.html">torch.Tensor.arctan</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.arctan_.html">torch.Tensor.arctan_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.atan2.html">torch.Tensor.atan2</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.atan2_.html">torch.Tensor.atan2_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.arctan2.html">torch.Tensor.arctan2</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.arctan2_.html">torch.Tensor.arctan2_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.all.html">torch.Tensor.all</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.any.html">torch.Tensor.any</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.backward.html">torch.Tensor.backward</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.baddbmm.html">torch.Tensor.baddbmm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.baddbmm_.html">torch.Tensor.baddbmm_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.bernoulli.html">torch.Tensor.bernoulli</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.bernoulli_.html">torch.Tensor.bernoulli_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.bfloat16.html">torch.Tensor.bfloat16</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.bincount.html">torch.Tensor.bincount</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.bitwise_not.html">torch.Tensor.bitwise_not</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.bitwise_not_.html">torch.Tensor.bitwise_not_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.bitwise_and.html">torch.Tensor.bitwise_and</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.bitwise_and_.html">torch.Tensor.bitwise_and_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.bitwise_or.html">torch.Tensor.bitwise_or</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.bitwise_or_.html">torch.Tensor.bitwise_or_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.bitwise_xor.html">torch.Tensor.bitwise_xor</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.bitwise_xor_.html">torch.Tensor.bitwise_xor_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.bitwise_left_shift.html">torch.Tensor.bitwise_left_shift</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.bitwise_left_shift_.html">torch.Tensor.bitwise_left_shift_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.bitwise_right_shift.html">torch.Tensor.bitwise_right_shift</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.bitwise_right_shift_.html">torch.Tensor.bitwise_right_shift_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.bmm.html">torch.Tensor.bmm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.bool.html">torch.Tensor.bool</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.byte.html">torch.Tensor.byte</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.broadcast_to.html">torch.Tensor.broadcast_to</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.cauchy_.html">torch.Tensor.cauchy_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.ceil.html">torch.Tensor.ceil</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.ceil_.html">torch.Tensor.ceil_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.char.html">torch.Tensor.char</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.cholesky.html">torch.Tensor.cholesky</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.cholesky_inverse.html">torch.Tensor.cholesky_inverse</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.cholesky_solve.html">torch.Tensor.cholesky_solve</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.chunk.html">torch.Tensor.chunk</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.clamp.html">torch.Tensor.clamp</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.clamp_.html">torch.Tensor.clamp_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.clip.html">torch.Tensor.clip</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.clip_.html">torch.Tensor.clip_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.clone.html">torch.Tensor.clone</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.contiguous.html">torch.Tensor.contiguous</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.copy_.html">torch.Tensor.copy_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.conj.html">torch.Tensor.conj</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.conj_physical.html">torch.Tensor.conj_physical</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.conj_physical_.html">torch.Tensor.conj_physical_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.resolve_conj.html">torch.Tensor.resolve_conj</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.resolve_neg.html">torch.Tensor.resolve_neg</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.copysign.html">torch.Tensor.copysign</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.copysign_.html">torch.Tensor.copysign_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.cos.html">torch.Tensor.cos</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.cos_.html">torch.Tensor.cos_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.cosh.html">torch.Tensor.cosh</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.cosh_.html">torch.Tensor.cosh_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.corrcoef.html">torch.Tensor.corrcoef</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.count_nonzero.html">torch.Tensor.count_nonzero</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.cov.html">torch.Tensor.cov</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.acosh.html">torch.Tensor.acosh</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.acosh_.html">torch.Tensor.acosh_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.arccosh.html">torch.Tensor.arccosh</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.arccosh_.html">torch.Tensor.arccosh_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.cpu.html">torch.Tensor.cpu</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.cross.html">torch.Tensor.cross</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.cuda.html">torch.Tensor.cuda</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.logcumsumexp.html">torch.Tensor.logcumsumexp</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.cummax.html">torch.Tensor.cummax</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.cummin.html">torch.Tensor.cummin</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.cumprod.html">torch.Tensor.cumprod</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.cumprod_.html">torch.Tensor.cumprod_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.cumsum.html">torch.Tensor.cumsum</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.cumsum_.html">torch.Tensor.cumsum_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.chalf.html">torch.Tensor.chalf</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.cfloat.html">torch.Tensor.cfloat</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.cdouble.html">torch.Tensor.cdouble</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.data_ptr.html">torch.Tensor.data_ptr</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.deg2rad.html">torch.Tensor.deg2rad</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.dequantize.html">torch.Tensor.dequantize</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.det.html">torch.Tensor.det</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.dense_dim.html">torch.Tensor.dense_dim</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.detach.html">torch.Tensor.detach</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.detach_.html">torch.Tensor.detach_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.diag.html">torch.Tensor.diag</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.diag_embed.html">torch.Tensor.diag_embed</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.diagflat.html">torch.Tensor.diagflat</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.diagonal.html">torch.Tensor.diagonal</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.diagonal_scatter.html">torch.Tensor.diagonal_scatter</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.fill_diagonal_.html">torch.Tensor.fill_diagonal_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.fmax.html">torch.Tensor.fmax</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.fmin.html">torch.Tensor.fmin</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.diff.html">torch.Tensor.diff</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.digamma.html">torch.Tensor.digamma</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.digamma_.html">torch.Tensor.digamma_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.dim.html">torch.Tensor.dim</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.dim_order.html">torch.Tensor.dim_order</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.dist.html">torch.Tensor.dist</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.div.html">torch.Tensor.div</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.div_.html">torch.Tensor.div_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.divide.html">torch.Tensor.divide</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.divide_.html">torch.Tensor.divide_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.dot.html">torch.Tensor.dot</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.double.html">torch.Tensor.double</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.dsplit.html">torch.Tensor.dsplit</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.element_size.html">torch.Tensor.element_size</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.eq.html">torch.Tensor.eq</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.eq_.html">torch.Tensor.eq_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.equal.html">torch.Tensor.equal</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.erf.html">torch.Tensor.erf</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.erf_.html">torch.Tensor.erf_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.erfc.html">torch.Tensor.erfc</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.erfc_.html">torch.Tensor.erfc_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.erfinv.html">torch.Tensor.erfinv</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.erfinv_.html">torch.Tensor.erfinv_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.exp.html">torch.Tensor.exp</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.exp_.html">torch.Tensor.exp_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.expm1.html">torch.Tensor.expm1</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.expm1_.html">torch.Tensor.expm1_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.expand.html">torch.Tensor.expand</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.expand_as.html">torch.Tensor.expand_as</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.exponential_.html">torch.Tensor.exponential_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.fix.html">torch.Tensor.fix</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.fix_.html">torch.Tensor.fix_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.fill_.html">torch.Tensor.fill_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.flatten.html">torch.Tensor.flatten</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.flip.html">torch.Tensor.flip</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.fliplr.html">torch.Tensor.fliplr</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.flipud.html">torch.Tensor.flipud</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.float.html">torch.Tensor.float</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.float_power.html">torch.Tensor.float_power</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.float_power_.html">torch.Tensor.float_power_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.floor.html">torch.Tensor.floor</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.floor_.html">torch.Tensor.floor_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.floor_divide.html">torch.Tensor.floor_divide</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.floor_divide_.html">torch.Tensor.floor_divide_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.fmod.html">torch.Tensor.fmod</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.fmod_.html">torch.Tensor.fmod_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.frac.html">torch.Tensor.frac</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.frac_.html">torch.Tensor.frac_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.frexp.html">torch.Tensor.frexp</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.gather.html">torch.Tensor.gather</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.gcd.html">torch.Tensor.gcd</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.gcd_.html">torch.Tensor.gcd_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.ge.html">torch.Tensor.ge</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.ge_.html">torch.Tensor.ge_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.greater_equal.html">torch.Tensor.greater_equal</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.greater_equal_.html">torch.Tensor.greater_equal_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.geometric_.html">torch.Tensor.geometric_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.geqrf.html">torch.Tensor.geqrf</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.ger.html">torch.Tensor.ger</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.get_device.html">torch.Tensor.get_device</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.gt.html">torch.Tensor.gt</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.gt_.html">torch.Tensor.gt_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.greater.html">torch.Tensor.greater</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.greater_.html">torch.Tensor.greater_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.half.html">torch.Tensor.half</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.hardshrink.html">torch.Tensor.hardshrink</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.heaviside.html">torch.Tensor.heaviside</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.histc.html">torch.Tensor.histc</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.histogram.html">torch.Tensor.histogram</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.hsplit.html">torch.Tensor.hsplit</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.hypot.html">torch.Tensor.hypot</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.hypot_.html">torch.Tensor.hypot_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.i0.html">torch.Tensor.i0</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.i0_.html">torch.Tensor.i0_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.igamma.html">torch.Tensor.igamma</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.igamma_.html">torch.Tensor.igamma_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.igammac.html">torch.Tensor.igammac</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.igammac_.html">torch.Tensor.igammac_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.index_add_.html">torch.Tensor.index_add_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.index_add.html">torch.Tensor.index_add</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.index_copy_.html">torch.Tensor.index_copy_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.index_copy.html">torch.Tensor.index_copy</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.index_fill_.html">torch.Tensor.index_fill_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.index_fill.html">torch.Tensor.index_fill</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.index_put_.html">torch.Tensor.index_put_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.index_put.html">torch.Tensor.index_put</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.index_reduce_.html">torch.Tensor.index_reduce_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.index_reduce.html">torch.Tensor.index_reduce</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.index_select.html">torch.Tensor.index_select</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.indices.html">torch.Tensor.indices</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.inner.html">torch.Tensor.inner</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.int.html">torch.Tensor.int</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.int_repr.html">torch.Tensor.int_repr</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.inverse.html">torch.Tensor.inverse</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.isclose.html">torch.Tensor.isclose</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.isfinite.html">torch.Tensor.isfinite</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.isinf.html">torch.Tensor.isinf</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.isposinf.html">torch.Tensor.isposinf</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.isneginf.html">torch.Tensor.isneginf</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.isnan.html">torch.Tensor.isnan</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.is_contiguous.html">torch.Tensor.is_contiguous</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.is_complex.html">torch.Tensor.is_complex</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.is_conj.html">torch.Tensor.is_conj</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.is_floating_point.html">torch.Tensor.is_floating_point</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.is_inference.html">torch.Tensor.is_inference</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.is_leaf.html">torch.Tensor.is_leaf</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.is_pinned.html">torch.Tensor.is_pinned</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.is_set_to.html">torch.Tensor.is_set_to</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.is_shared.html">torch.Tensor.is_shared</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.is_signed.html">torch.Tensor.is_signed</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.is_sparse.html">torch.Tensor.is_sparse</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.istft.html">torch.Tensor.istft</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.isreal.html">torch.Tensor.isreal</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.item.html">torch.Tensor.item</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.kthvalue.html">torch.Tensor.kthvalue</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.lcm.html">torch.Tensor.lcm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.lcm_.html">torch.Tensor.lcm_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.ldexp.html">torch.Tensor.ldexp</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.ldexp_.html">torch.Tensor.ldexp_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.le.html">torch.Tensor.le</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.le_.html">torch.Tensor.le_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.less_equal.html">torch.Tensor.less_equal</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.less_equal_.html">torch.Tensor.less_equal_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.lerp.html">torch.Tensor.lerp</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.lerp_.html">torch.Tensor.lerp_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.lgamma.html">torch.Tensor.lgamma</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.lgamma_.html">torch.Tensor.lgamma_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.log.html">torch.Tensor.log</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.log_.html">torch.Tensor.log_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.logdet.html">torch.Tensor.logdet</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.log10.html">torch.Tensor.log10</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.log10_.html">torch.Tensor.log10_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.log1p.html">torch.Tensor.log1p</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.log1p_.html">torch.Tensor.log1p_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.log2.html">torch.Tensor.log2</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.log2_.html">torch.Tensor.log2_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.log_normal_.html">torch.Tensor.log_normal_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.logaddexp.html">torch.Tensor.logaddexp</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.logaddexp2.html">torch.Tensor.logaddexp2</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.logsumexp.html">torch.Tensor.logsumexp</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.logical_and.html">torch.Tensor.logical_and</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.logical_and_.html">torch.Tensor.logical_and_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.logical_not.html">torch.Tensor.logical_not</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.logical_not_.html">torch.Tensor.logical_not_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.logical_or.html">torch.Tensor.logical_or</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.logical_or_.html">torch.Tensor.logical_or_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.logical_xor.html">torch.Tensor.logical_xor</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.logical_xor_.html">torch.Tensor.logical_xor_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.logit.html">torch.Tensor.logit</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.logit_.html">torch.Tensor.logit_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.long.html">torch.Tensor.long</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.lt.html">torch.Tensor.lt</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.lt_.html">torch.Tensor.lt_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.less.html">torch.Tensor.less</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.less_.html">torch.Tensor.less_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.lu.html">torch.Tensor.lu</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.lu_solve.html">torch.Tensor.lu_solve</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.as_subclass.html">torch.Tensor.as_subclass</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.map_.html">torch.Tensor.map_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.masked_scatter_.html">torch.Tensor.masked_scatter_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.masked_scatter.html">torch.Tensor.masked_scatter</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.masked_fill_.html">torch.Tensor.masked_fill_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.masked_fill.html">torch.Tensor.masked_fill</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.masked_select.html">torch.Tensor.masked_select</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.matmul.html">torch.Tensor.matmul</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.matrix_power.html">torch.Tensor.matrix_power</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.matrix_exp.html">torch.Tensor.matrix_exp</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.max.html">torch.Tensor.max</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.maximum.html">torch.Tensor.maximum</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.mean.html">torch.Tensor.mean</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.module_load.html">torch.Tensor.module_load</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.nanmean.html">torch.Tensor.nanmean</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.median.html">torch.Tensor.median</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.nanmedian.html">torch.Tensor.nanmedian</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.min.html">torch.Tensor.min</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.minimum.html">torch.Tensor.minimum</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.mm.html">torch.Tensor.mm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.smm.html">torch.Tensor.smm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.mode.html">torch.Tensor.mode</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.movedim.html">torch.Tensor.movedim</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.moveaxis.html">torch.Tensor.moveaxis</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.msort.html">torch.Tensor.msort</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.mul.html">torch.Tensor.mul</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.mul_.html">torch.Tensor.mul_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.multiply.html">torch.Tensor.multiply</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.multiply_.html">torch.Tensor.multiply_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.multinomial.html">torch.Tensor.multinomial</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.mv.html">torch.Tensor.mv</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.mvlgamma.html">torch.Tensor.mvlgamma</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.mvlgamma_.html">torch.Tensor.mvlgamma_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.nansum.html">torch.Tensor.nansum</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.narrow.html">torch.Tensor.narrow</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.narrow_copy.html">torch.Tensor.narrow_copy</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.ndimension.html">torch.Tensor.ndimension</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.nan_to_num.html">torch.Tensor.nan_to_num</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.nan_to_num_.html">torch.Tensor.nan_to_num_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.ne.html">torch.Tensor.ne</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.ne_.html">torch.Tensor.ne_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.not_equal.html">torch.Tensor.not_equal</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.not_equal_.html">torch.Tensor.not_equal_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.neg.html">torch.Tensor.neg</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.neg_.html">torch.Tensor.neg_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.negative.html">torch.Tensor.negative</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.negative_.html">torch.Tensor.negative_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.nelement.html">torch.Tensor.nelement</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.nextafter.html">torch.Tensor.nextafter</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.nextafter_.html">torch.Tensor.nextafter_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.nonzero.html">torch.Tensor.nonzero</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.norm.html">torch.Tensor.norm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.normal_.html">torch.Tensor.normal_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.numel.html">torch.Tensor.numel</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.numpy.html">torch.Tensor.numpy</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.orgqr.html">torch.Tensor.orgqr</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.ormqr.html">torch.Tensor.ormqr</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.outer.html">torch.Tensor.outer</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.permute.html">torch.Tensor.permute</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.pin_memory.html">torch.Tensor.pin_memory</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.pinverse.html">torch.Tensor.pinverse</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.polygamma.html">torch.Tensor.polygamma</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.polygamma_.html">torch.Tensor.polygamma_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.positive.html">torch.Tensor.positive</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.pow.html">torch.Tensor.pow</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.pow_.html">torch.Tensor.pow_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.prod.html">torch.Tensor.prod</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.put_.html">torch.Tensor.put_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.qr.html">torch.Tensor.qr</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.qscheme.html">torch.Tensor.qscheme</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.quantile.html">torch.Tensor.quantile</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.nanquantile.html">torch.Tensor.nanquantile</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.q_scale.html">torch.Tensor.q_scale</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.q_zero_point.html">torch.Tensor.q_zero_point</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.q_per_channel_scales.html">torch.Tensor.q_per_channel_scales</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.q_per_channel_zero_points.html">torch.Tensor.q_per_channel_zero_points</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.q_per_channel_axis.html">torch.Tensor.q_per_channel_axis</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.rad2deg.html">torch.Tensor.rad2deg</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.random_.html">torch.Tensor.random_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.ravel.html">torch.Tensor.ravel</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.reciprocal.html">torch.Tensor.reciprocal</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.reciprocal_.html">torch.Tensor.reciprocal_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.record_stream.html">torch.Tensor.record_stream</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.register_hook.html">torch.Tensor.register_hook</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.register_post_accumulate_grad_hook.html">torch.Tensor.register_post_accumulate_grad_hook</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.remainder.html">torch.Tensor.remainder</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.remainder_.html">torch.Tensor.remainder_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.renorm.html">torch.Tensor.renorm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.renorm_.html">torch.Tensor.renorm_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.repeat.html">torch.Tensor.repeat</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.repeat_interleave.html">torch.Tensor.repeat_interleave</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.requires_grad.html">torch.Tensor.requires_grad</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.requires_grad_.html">torch.Tensor.requires_grad_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.reshape.html">torch.Tensor.reshape</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.reshape_as.html">torch.Tensor.reshape_as</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.resize_.html">torch.Tensor.resize_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.resize_as_.html">torch.Tensor.resize_as_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.retain_grad.html">torch.Tensor.retain_grad</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.retains_grad.html">torch.Tensor.retains_grad</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.roll.html">torch.Tensor.roll</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.rot90.html">torch.Tensor.rot90</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.round.html">torch.Tensor.round</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.round_.html">torch.Tensor.round_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.rsqrt.html">torch.Tensor.rsqrt</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.rsqrt_.html">torch.Tensor.rsqrt_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.scatter.html">torch.Tensor.scatter</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.scatter_.html">torch.Tensor.scatter_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.scatter_add_.html">torch.Tensor.scatter_add_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.scatter_add.html">torch.Tensor.scatter_add</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.scatter_reduce_.html">torch.Tensor.scatter_reduce_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.scatter_reduce.html">torch.Tensor.scatter_reduce</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.select.html">torch.Tensor.select</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.select_scatter.html">torch.Tensor.select_scatter</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.set_.html">torch.Tensor.set_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.share_memory_.html">torch.Tensor.share_memory_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.short.html">torch.Tensor.short</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.sigmoid.html">torch.Tensor.sigmoid</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.sigmoid_.html">torch.Tensor.sigmoid_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.sign.html">torch.Tensor.sign</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.sign_.html">torch.Tensor.sign_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.signbit.html">torch.Tensor.signbit</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.sgn.html">torch.Tensor.sgn</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.sgn_.html">torch.Tensor.sgn_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.sin.html">torch.Tensor.sin</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.sin_.html">torch.Tensor.sin_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.sinc.html">torch.Tensor.sinc</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.sinc_.html">torch.Tensor.sinc_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.sinh.html">torch.Tensor.sinh</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.sinh_.html">torch.Tensor.sinh_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.asinh.html">torch.Tensor.asinh</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.asinh_.html">torch.Tensor.asinh_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.arcsinh.html">torch.Tensor.arcsinh</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.arcsinh_.html">torch.Tensor.arcsinh_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.shape.html">torch.Tensor.shape</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.size.html">torch.Tensor.size</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.slogdet.html">torch.Tensor.slogdet</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.slice_scatter.html">torch.Tensor.slice_scatter</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.softmax.html">torch.Tensor.softmax</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.sort.html">torch.Tensor.sort</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.split.html">torch.Tensor.split</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.sparse_mask.html">torch.Tensor.sparse_mask</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.sparse_dim.html">torch.Tensor.sparse_dim</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.sqrt.html">torch.Tensor.sqrt</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.sqrt_.html">torch.Tensor.sqrt_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.square.html">torch.Tensor.square</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.square_.html">torch.Tensor.square_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.squeeze.html">torch.Tensor.squeeze</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.squeeze_.html">torch.Tensor.squeeze_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.std.html">torch.Tensor.std</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.stft.html">torch.Tensor.stft</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.storage.html">torch.Tensor.storage</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.untyped_storage.html">torch.Tensor.untyped_storage</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.storage_offset.html">torch.Tensor.storage_offset</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.storage_type.html">torch.Tensor.storage_type</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.stride.html">torch.Tensor.stride</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.sub.html">torch.Tensor.sub</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.sub_.html">torch.Tensor.sub_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.subtract.html">torch.Tensor.subtract</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.subtract_.html">torch.Tensor.subtract_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.sum.html">torch.Tensor.sum</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.sum_to_size.html">torch.Tensor.sum_to_size</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.svd.html">torch.Tensor.svd</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.swapaxes.html">torch.Tensor.swapaxes</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.swapdims.html">torch.Tensor.swapdims</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.t.html">torch.Tensor.t</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.t_.html">torch.Tensor.t_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.tensor_split.html">torch.Tensor.tensor_split</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.tile.html">torch.Tensor.tile</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.to.html">torch.Tensor.to</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.to_mkldnn.html">torch.Tensor.to_mkldnn</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.take.html">torch.Tensor.take</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.take_along_dim.html">torch.Tensor.take_along_dim</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.tan.html">torch.Tensor.tan</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.tan_.html">torch.Tensor.tan_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.tanh.html">torch.Tensor.tanh</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.tanh_.html">torch.Tensor.tanh_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.atanh.html">torch.Tensor.atanh</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.atanh_.html">torch.Tensor.atanh_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.arctanh.html">torch.Tensor.arctanh</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.arctanh_.html">torch.Tensor.arctanh_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.tolist.html">torch.Tensor.tolist</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.topk.html">torch.Tensor.topk</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.to_dense.html">torch.Tensor.to_dense</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.to_sparse.html">torch.Tensor.to_sparse</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.to_sparse_csr.html">torch.Tensor.to_sparse_csr</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.to_sparse_csc.html">torch.Tensor.to_sparse_csc</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.to_sparse_bsr.html">torch.Tensor.to_sparse_bsr</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.to_sparse_bsc.html">torch.Tensor.to_sparse_bsc</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.trace.html">torch.Tensor.trace</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.transpose.html">torch.Tensor.transpose</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.transpose_.html">torch.Tensor.transpose_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.triangular_solve.html">torch.Tensor.triangular_solve</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.tril.html">torch.Tensor.tril</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.tril_.html">torch.Tensor.tril_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.triu.html">torch.Tensor.triu</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.triu_.html">torch.Tensor.triu_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.true_divide.html">torch.Tensor.true_divide</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.true_divide_.html">torch.Tensor.true_divide_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.trunc.html">torch.Tensor.trunc</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.trunc_.html">torch.Tensor.trunc_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.type.html">torch.Tensor.type</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.type_as.html">torch.Tensor.type_as</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.unbind.html">torch.Tensor.unbind</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.unflatten.html">torch.Tensor.unflatten</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.unfold.html">torch.Tensor.unfold</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.uniform_.html">torch.Tensor.uniform_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.unique.html">torch.Tensor.unique</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.unique_consecutive.html">torch.Tensor.unique_consecutive</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.unsqueeze.html">torch.Tensor.unsqueeze</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.unsqueeze_.html">torch.Tensor.unsqueeze_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.values.html">torch.Tensor.values</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.var.html">torch.Tensor.var</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.vdot.html">torch.Tensor.vdot</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.view.html">torch.Tensor.view</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.view_as.html">torch.Tensor.view_as</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.vsplit.html">torch.Tensor.vsplit</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.where.html">torch.Tensor.where</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.xlogy.html">torch.Tensor.xlogy</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.xlogy_.html">torch.Tensor.xlogy_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.xpu.html">torch.Tensor.xpu</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.zero_.html">torch.Tensor.zero_</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="tensor_attributes.html">Tensor Attributes</a></li>
<li class="toctree-l1"><a class="reference internal" href="tensor_view.html">Tensor Views</a></li>
<li class="toctree-l1"><a class="reference internal" href="amp.html">torch.amp</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="autograd.html">torch.autograd</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.backward.html">torch.autograd.backward</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.grad.html">torch.autograd.grad</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.forward_ad.dual_level.html">dual_level</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.forward_ad.make_dual.html">torch.autograd.forward_ad.make_dual</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.forward_ad.unpack_dual.html">torch.autograd.forward_ad.unpack_dual</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.forward_ad.enter_dual_level.html">torch.autograd.forward_ad.enter_dual_level</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.forward_ad.exit_dual_level.html">torch.autograd.forward_ad.exit_dual_level</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.forward_ad.UnpackedDualTensor.html">UnpackedDualTensor</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.functional.jacobian.html">torch.autograd.functional.jacobian</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.functional.hessian.html">torch.autograd.functional.hessian</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.functional.vjp.html">torch.autograd.functional.vjp</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.functional.jvp.html">torch.autograd.functional.jvp</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.functional.vhp.html">torch.autograd.functional.vhp</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.functional.hvp.html">torch.autograd.functional.hvp</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.Function.forward.html">torch.autograd.Function.forward</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.Function.backward.html">torch.autograd.Function.backward</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.Function.jvp.html">torch.autograd.Function.jvp</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.Function.vmap.html">torch.autograd.Function.vmap</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.function.FunctionCtx.mark_dirty.html">torch.autograd.function.FunctionCtx.mark_dirty</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.function.FunctionCtx.mark_non_differentiable.html">torch.autograd.function.FunctionCtx.mark_non_differentiable</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.function.FunctionCtx.save_for_backward.html">torch.autograd.function.FunctionCtx.save_for_backward</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.function.FunctionCtx.set_materialize_grads.html">torch.autograd.function.FunctionCtx.set_materialize_grads</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.function.once_differentiable.html">torch.autograd.function.once_differentiable</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.function.BackwardCFunction.html">BackwardCFunction</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.function.InplaceFunction.html">InplaceFunction</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.function.NestedIOFunction.html">NestedIOFunction</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.gradcheck.gradcheck.html">torch.autograd.gradcheck.gradcheck</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.gradcheck.gradgradcheck.html">torch.autograd.gradcheck.gradgradcheck</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.gradcheck.GradcheckError.html">torch.autograd.gradcheck.GradcheckError</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.profiler.profile.export_chrome_trace.html">torch.autograd.profiler.profile.export_chrome_trace</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.profiler.profile.key_averages.html">torch.autograd.profiler.profile.key_averages</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.profiler.profile.self_cpu_time_total.html">torch.autograd.profiler.profile.self_cpu_time_total</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.profiler.profile.total_average.html">torch.autograd.profiler.profile.total_average</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.profiler.parse_nvprof_trace.html">torch.autograd.profiler.parse_nvprof_trace</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.profiler.EnforceUnique.html">EnforceUnique</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.profiler.KinetoStepTracker.html">KinetoStepTracker</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.profiler.record_function.html">record_function</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.profiler_util.Interval.html">Interval</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.profiler_util.Kernel.html">Kernel</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.profiler_util.MemRecordsAcc.html">MemRecordsAcc</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.profiler_util.StringTable.html">StringTable</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.profiler.load_nvprof.html">torch.autograd.profiler.load_nvprof</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.grad_mode.set_multithreading_enabled.html">set_multithreading_enabled</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.graph.Node.name.html">torch.autograd.graph.Node.name</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.graph.Node.metadata.html">torch.autograd.graph.Node.metadata</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.graph.Node.next_functions.html">torch.autograd.graph.Node.next_functions</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.graph.Node.register_hook.html">torch.autograd.graph.Node.register_hook</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.graph.Node.register_prehook.html">torch.autograd.graph.Node.register_prehook</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.graph.increment_version.html">torch.autograd.graph.increment_version</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="library.html">torch.library</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="accelerator.html">torch.accelerator</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.accelerator.device_count.html">torch.accelerator.device_count</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.accelerator.is_available.html">torch.accelerator.is_available</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.accelerator.current_accelerator.html">torch.accelerator.current_accelerator</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.accelerator.set_device_index.html">torch.accelerator.set_device_index</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.accelerator.set_device_idx.html">torch.accelerator.set_device_idx</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.accelerator.current_device_index.html">torch.accelerator.current_device_index</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.accelerator.current_device_idx.html">torch.accelerator.current_device_idx</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.accelerator.set_stream.html">torch.accelerator.set_stream</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.accelerator.current_stream.html">torch.accelerator.current_stream</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.accelerator.synchronize.html">torch.accelerator.synchronize</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="cpu.html">torch.cpu</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cpu.current_device.html">torch.cpu.current_device</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cpu.current_stream.html">torch.cpu.current_stream</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cpu.is_available.html">torch.cpu.is_available</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cpu.synchronize.html">torch.cpu.synchronize</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cpu.set_device.html">torch.cpu.set_device</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cpu.device_count.html">torch.cpu.device_count</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cpu.StreamContext.html">StreamContext</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cpu.Stream.html">torch.cpu.stream</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="cuda.html">torch.cuda</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.StreamContext.html">StreamContext</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.can_device_access_peer.html">torch.cuda.can_device_access_peer</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.current_blas_handle.html">torch.cuda.current_blas_handle</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.current_device.html">torch.cuda.current_device</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.current_stream.html">torch.cuda.current_stream</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.cudart.html">torch.cuda.cudart</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.default_stream.html">torch.cuda.default_stream</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.device.html">device</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.device_count.html">torch.cuda.device_count</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.device_memory_used.html">torch.cuda.device_memory_used</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.device_of.html">device_of</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.get_arch_list.html">torch.cuda.get_arch_list</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.get_device_capability.html">torch.cuda.get_device_capability</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.get_device_name.html">torch.cuda.get_device_name</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.get_device_properties.html">torch.cuda.get_device_properties</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.get_gencode_flags.html">torch.cuda.get_gencode_flags</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.get_sync_debug_mode.html">torch.cuda.get_sync_debug_mode</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.init.html">torch.cuda.init</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.ipc_collect.html">torch.cuda.ipc_collect</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.is_available.html">torch.cuda.is_available</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.is_initialized.html">torch.cuda.is_initialized</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.memory_usage.html">torch.cuda.memory_usage</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.set_device.html">torch.cuda.set_device</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.set_stream.html">torch.cuda.set_stream</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.set_sync_debug_mode.html">torch.cuda.set_sync_debug_mode</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.synchronize.html">torch.cuda.synchronize</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.utilization.html">torch.cuda.utilization</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.temperature.html">torch.cuda.temperature</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.power_draw.html">torch.cuda.power_draw</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.clock_rate.html">torch.cuda.clock_rate</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.OutOfMemoryError.html">torch.cuda.OutOfMemoryError</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.get_rng_state.html">torch.cuda.get_rng_state</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.get_rng_state_all.html">torch.cuda.get_rng_state_all</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.set_rng_state.html">torch.cuda.set_rng_state</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.set_rng_state_all.html">torch.cuda.set_rng_state_all</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.manual_seed.html">torch.cuda.manual_seed</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.manual_seed_all.html">torch.cuda.manual_seed_all</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.seed.html">torch.cuda.seed</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.seed_all.html">torch.cuda.seed_all</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.initial_seed.html">torch.cuda.initial_seed</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.comm.broadcast.html">torch.cuda.comm.broadcast</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.comm.broadcast_coalesced.html">torch.cuda.comm.broadcast_coalesced</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.comm.reduce_add.html">torch.cuda.comm.reduce_add</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.comm.reduce_add_coalesced.html">torch.cuda.comm.reduce_add_coalesced</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.comm.scatter.html">torch.cuda.comm.scatter</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.comm.gather.html">torch.cuda.comm.gather</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.Stream.html">torch.cuda.stream</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.ExternalStream.html">ExternalStream</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.Event.html">Event</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.is_current_stream_capturing.html">torch.cuda.is_current_stream_capturing</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.graph_pool_handle.html">torch.cuda.graph_pool_handle</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.CUDAGraph.html">CUDAGraph</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.graph.html">graph</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.make_graphed_callables.html">torch.cuda.make_graphed_callables</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.empty_cache.html">torch.cuda.empty_cache</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.get_per_process_memory_fraction.html">torch.cuda.get_per_process_memory_fraction</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.list_gpu_processes.html">torch.cuda.list_gpu_processes</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.mem_get_info.html">torch.cuda.mem_get_info</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.memory_stats.html">torch.cuda.memory_stats</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.memory_summary.html">torch.cuda.memory_summary</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.memory_snapshot.html">torch.cuda.memory_snapshot</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.memory_allocated.html">torch.cuda.memory_allocated</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.max_memory_allocated.html">torch.cuda.max_memory_allocated</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.reset_max_memory_allocated.html">torch.cuda.reset_max_memory_allocated</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.memory_reserved.html">torch.cuda.memory_reserved</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.max_memory_reserved.html">torch.cuda.max_memory_reserved</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.set_per_process_memory_fraction.html">torch.cuda.set_per_process_memory_fraction</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.memory_cached.html">torch.cuda.memory_cached</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.max_memory_cached.html">torch.cuda.max_memory_cached</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.reset_max_memory_cached.html">torch.cuda.reset_max_memory_cached</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.reset_peak_memory_stats.html">torch.cuda.reset_peak_memory_stats</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.caching_allocator_alloc.html">torch.cuda.caching_allocator_alloc</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.caching_allocator_delete.html">torch.cuda.caching_allocator_delete</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.get_allocator_backend.html">torch.cuda.get_allocator_backend</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.CUDAPluggableAllocator.html">CUDAPluggableAllocator</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.change_current_allocator.html">torch.cuda.change_current_allocator</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.MemPool.html">MemPool</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.MemPoolContext.html">MemPoolContext</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.memory.caching_allocator_enable.html">torch.cuda.memory.caching_allocator_enable</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.nvtx.mark.html">torch.cuda.nvtx.mark</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.nvtx.range_push.html">torch.cuda.nvtx.range_push</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.nvtx.range_pop.html">torch.cuda.nvtx.range_pop</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.nvtx.range.html">torch.cuda.nvtx.range</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.jiterator._create_jit_fn.html">torch.cuda.jiterator._create_jit_fn</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.jiterator._create_multi_output_jit_fn.html">torch.cuda.jiterator._create_multi_output_jit_fn</a></li>
<li class="toctree-l2"><a class="reference internal" href="cuda.tunable.html">TunableOp</a></li>
<li class="toctree-l2"><a class="reference internal" href="cuda._sanitizer.html">CUDA Stream Sanitizer</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="torch_cuda_memory.html">Understanding CUDA Memory Usage</a></li>



<li class="toctree-l1 has-children"><a class="reference internal" href="mps.html">torch.mps</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mps.device_count.html">torch.mps.device_count</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mps.synchronize.html">torch.mps.synchronize</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mps.get_rng_state.html">torch.mps.get_rng_state</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mps.set_rng_state.html">torch.mps.set_rng_state</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mps.manual_seed.html">torch.mps.manual_seed</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mps.seed.html">torch.mps.seed</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mps.empty_cache.html">torch.mps.empty_cache</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mps.set_per_process_memory_fraction.html">torch.mps.set_per_process_memory_fraction</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mps.current_allocated_memory.html">torch.mps.current_allocated_memory</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mps.driver_allocated_memory.html">torch.mps.driver_allocated_memory</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mps.recommended_max_memory.html">torch.mps.recommended_max_memory</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mps.profiler.start.html">torch.mps.profiler.start</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mps.profiler.stop.html">torch.mps.profiler.stop</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mps.profiler.profile.html">torch.mps.profiler.profile</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mps.event.Event.html">Event</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="xpu.html">torch.xpu</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xpu.StreamContext.html">StreamContext</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xpu.current_device.html">torch.xpu.current_device</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xpu.current_stream.html">torch.xpu.current_stream</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xpu.device.html">device</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xpu.device_count.html">torch.xpu.device_count</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xpu.device_of.html">device_of</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xpu.get_arch_list.html">torch.xpu.get_arch_list</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xpu.get_device_capability.html">torch.xpu.get_device_capability</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xpu.get_device_name.html">torch.xpu.get_device_name</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xpu.get_device_properties.html">torch.xpu.get_device_properties</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xpu.get_gencode_flags.html">torch.xpu.get_gencode_flags</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xpu.init.html">torch.xpu.init</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xpu.is_available.html">torch.xpu.is_available</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xpu.is_initialized.html">torch.xpu.is_initialized</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xpu.set_device.html">torch.xpu.set_device</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xpu.set_stream.html">torch.xpu.set_stream</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xpu.synchronize.html">torch.xpu.synchronize</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xpu.get_rng_state.html">torch.xpu.get_rng_state</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xpu.get_rng_state_all.html">torch.xpu.get_rng_state_all</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xpu.initial_seed.html">torch.xpu.initial_seed</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xpu.manual_seed.html">torch.xpu.manual_seed</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xpu.manual_seed_all.html">torch.xpu.manual_seed_all</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xpu.seed.html">torch.xpu.seed</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xpu.seed_all.html">torch.xpu.seed_all</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xpu.set_rng_state.html">torch.xpu.set_rng_state</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xpu.set_rng_state_all.html">torch.xpu.set_rng_state_all</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xpu.Event.html">Event</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xpu.Stream.html">torch.xpu.stream</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xpu.empty_cache.html">torch.xpu.empty_cache</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xpu.max_memory_allocated.html">torch.xpu.max_memory_allocated</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xpu.max_memory_reserved.html">torch.xpu.max_memory_reserved</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xpu.mem_get_info.html">torch.xpu.mem_get_info</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xpu.memory_allocated.html">torch.xpu.memory_allocated</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xpu.memory_reserved.html">torch.xpu.memory_reserved</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xpu.memory_stats.html">torch.xpu.memory_stats</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xpu.memory_stats_as_nested_dict.html">torch.xpu.memory_stats_as_nested_dict</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xpu.reset_accumulated_memory_stats.html">torch.xpu.reset_accumulated_memory_stats</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xpu.reset_peak_memory_stats.html">torch.xpu.reset_peak_memory_stats</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="mtia.html">torch.mtia</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mtia.StreamContext.html">StreamContext</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mtia.current_device.html">torch.mtia.current_device</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mtia.current_stream.html">torch.mtia.current_stream</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mtia.default_stream.html">torch.mtia.default_stream</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mtia.device_count.html">torch.mtia.device_count</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mtia.init.html">torch.mtia.init</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mtia.is_available.html">torch.mtia.is_available</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mtia.is_initialized.html">torch.mtia.is_initialized</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mtia.memory_stats.html">torch.mtia.memory_stats</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mtia.get_device_capability.html">torch.mtia.get_device_capability</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mtia.empty_cache.html">torch.mtia.empty_cache</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mtia.set_device.html">torch.mtia.set_device</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mtia.set_stream.html">torch.mtia.set_stream</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mtia.synchronize.html">torch.mtia.synchronize</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mtia.device.html">device</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mtia.set_rng_state.html">torch.mtia.set_rng_state</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mtia.get_rng_state.html">torch.mtia.get_rng_state</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mtia.DeferredMtiaCallError.html">torch.mtia.DeferredMtiaCallError</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mtia.Event.html">Event</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mtia.Stream.html">torch.mtia.stream</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="mtia.memory.html">torch.mtia.memory</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mtia.memory.memory_stats.html">torch.mtia.memory.memory_stats</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="meta.html">Meta device</a></li>
<li class="toctree-l1"><a class="reference internal" href="backends.html">torch.backends</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="export.html">torch.export</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="export.programming_model.html">torch.export Programming Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="export.ir_spec.html">torch.export IR Specification</a></li>
<li class="toctree-l2"><a class="reference internal" href="torch.compiler_transformations.html">Writing Graph Transformations on ATen IR</a></li>
<li class="toctree-l2"><a class="reference internal" href="torch.compiler_ir.html">IRs</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="generated/exportdb/index.html">ExportDB</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="generated/exportdb/python.data-structure.html">python.data-structure</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/exportdb/python.control-flow.html">python.control-flow</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/exportdb/torch.cond.html">torch.cond</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/exportdb/python.closure.html">python.closure</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/exportdb/torch.escape-hatch.html">torch.escape-hatch</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/exportdb/torch.dynamic-value.html">torch.dynamic-value</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/exportdb/torch.dynamic-shape.html">torch.dynamic-shape</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/exportdb/python.builtin.html">python.builtin</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/exportdb/torch.map.html">torch.map</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/exportdb/python.assert.html">python.assert</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/exportdb/torch.operator.html">torch.operator</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/exportdb/python.object-model.html">python.object-model</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/exportdb/torch.mutation.html">torch.mutation</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/exportdb/python.context-manager.html">python.context-manager</a></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="cond.html">Control Flow - Cond</a></li>
<li class="toctree-l2"><a class="reference internal" href="torch.compiler_dynamo_overview.html">Dynamo Overview</a></li>
<li class="toctree-l2"><a class="reference internal" href="torch.compiler_dynamo_deepdive.html">Dynamo Deep-Dive</a></li>
<li class="toctree-l2"><a class="reference internal" href="torch.compiler_dynamic_shapes.html">Dynamic shapes</a></li>
<li class="toctree-l2"><a class="reference internal" href="torch.compiler_fake_tensor.html">Fake tensor</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="distributed.html">torch.distributed</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed.tensor.html">torch.distributed.tensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed.algorithms.join.html">torch.distributed.algorithms.join</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="distributed.elastic.html">torch.distributed.elastic</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="elastic/quickstart.html">Quickstart</a></li>
<li class="toctree-l2"><a class="reference internal" href="elastic/train_script.html">Train script</a></li>
<li class="toctree-l2"><a class="reference internal" href="elastic/examples.html">Examples</a></li>
<li class="toctree-l2"><a class="reference internal" href="elastic/run.html">torchrun (Elastic Launch)</a></li>
<li class="toctree-l2"><a class="reference internal" href="elastic/agent.html">Elastic Agent</a></li>
<li class="toctree-l2"><a class="reference internal" href="elastic/multiprocessing.html">Multiprocessing</a></li>
<li class="toctree-l2"><a class="reference internal" href="elastic/errors.html">Error Propagation</a></li>
<li class="toctree-l2"><a class="reference internal" href="elastic/rendezvous.html">Rendezvous</a></li>
<li class="toctree-l2"><a class="reference internal" href="elastic/timer.html">Expiration Timers</a></li>
<li class="toctree-l2"><a class="reference internal" href="elastic/metrics.html">Metrics</a></li>
<li class="toctree-l2"><a class="reference internal" href="elastic/events.html">Events</a></li>
<li class="toctree-l2"><a class="reference internal" href="elastic/subprocess_handler.html">Subprocess Handling</a></li>
<li class="toctree-l2"><a class="reference internal" href="elastic/control_plane.html">Control Plane</a></li>
<li class="toctree-l2"><a class="reference internal" href="elastic/customization.html">Customization</a></li>
<li class="toctree-l2"><a class="reference internal" href="elastic/kubernetes.html">TorchElastic Kubernetes</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="fsdp.html">torch.distributed.fsdp</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed.fsdp.fully_shard.html">torch.distributed.fsdp.fully_shard</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed.tensor.parallel.html">torch.distributed.tensor.parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed.optim.html">torch.distributed.optim</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed.pipelining.html">torch.distributed.pipelining</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed.checkpoint.html">torch.distributed.checkpoint</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributions.html">torch.distributions</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="torch.compiler.html">torch.compiler</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="torch.compiler_get_started.html">Getting Started</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="torch.compiler_api.html">torch.compiler API reference</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.compiler.compile.html">torch.compiler.compile</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.compiler.reset.html">torch.compiler.reset</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.compiler.allow_in_graph.html">torch.compiler.allow_in_graph</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.compiler.substitute_in_graph.html">torch.compiler.substitute_in_graph</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.compiler.assume_constant_result.html">torch.compiler.assume_constant_result</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.compiler.list_backends.html">torch.compiler.list_backends</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.compiler.disable.html">torch.compiler.disable</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.compiler.set_stance.html">torch.compiler.set_stance</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.compiler.cudagraph_mark_step_begin.html">torch.compiler.cudagraph_mark_step_begin</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.compiler.is_compiling.html">torch.compiler.is_compiling</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.compiler.is_dynamo_compiling.html">torch.compiler.is_dynamo_compiling</a></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="torch.compiler.config.html">torch.compiler.config</a></li>
<li class="toctree-l2"><a class="reference internal" href="torch.compiler_fine_grain_apis.html">TorchDynamo APIs for fine-grained tracing</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="torch.compiler_aot_inductor.html">AOTInductor: Ahead-Of-Time Compilation for Torch.Export-ed Models</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3 has-children"><a class="reference internal" href="logging.html">torch._logging</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l4"><a class="reference internal" href="generated/torch._logging.set_logs.html">torch._logging.set_logs</a></li>
</ul>
</details></li>
<li class="toctree-l3"><a class="reference internal" href="torch.compiler_aot_inductor_minifier.html">AOTInductor Minifier</a></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="torch.compiler_inductor_profiling.html">TorchInductor GPU Profiling</a></li>
<li class="toctree-l2"><a class="reference internal" href="torch.compiler_profiling_torch_compile.html">Profiling to understand torch.compile performance</a></li>
<li class="toctree-l2"><a class="reference internal" href="torch.compiler_faq.html">Frequently Asked Questions</a></li>
<li class="toctree-l2"><a class="reference internal" href="torch.compiler_troubleshooting.html">torch.compile Troubleshooting</a></li>
<li class="toctree-l2"><a class="reference internal" href="torch.compiler_performance_dashboard.html">PyTorch 2.0 Performance Dashboard</a></li>
<li class="toctree-l2"><a class="reference internal" href="torch.compiler_dynamo_overview.html">Dynamo Overview</a></li>
<li class="toctree-l2"><a class="reference internal" href="torch.compiler_dynamo_deepdive.html">Dynamo Deep-Dive</a></li>
<li class="toctree-l2"><a class="reference internal" href="torch.compiler_dynamic_shapes.html">Dynamic shapes</a></li>
<li class="toctree-l2"><a class="reference internal" href="torch.compiler_nn_module.html">PyTorch 2.0 NNModule Support</a></li>
<li class="toctree-l2"><a class="reference internal" href="torch.compiler_best_practices_for_backends.html">Best Practices for Backends</a></li>
<li class="toctree-l2"><a class="reference internal" href="torch.compiler_cudagraph_trees.html">CUDAGraph Trees</a></li>
<li class="toctree-l2"><a class="reference internal" href="torch.compiler_fake_tensor.html">Fake tensor</a></li>
<li class="toctree-l2"><a class="reference internal" href="torch.compiler_custom_backends.html">Custom Backends</a></li>
<li class="toctree-l2"><a class="reference internal" href="torch.compiler_transformations.html">Writing Graph Transformations on ATen IR</a></li>
<li class="toctree-l2"><a class="reference internal" href="torch.compiler_ir.html">IRs</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="fft.html">torch.fft</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fft.fft.html">torch.fft.fft</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fft.ifft.html">torch.fft.ifft</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fft.fft2.html">torch.fft.fft2</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fft.ifft2.html">torch.fft.ifft2</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fft.fftn.html">torch.fft.fftn</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fft.ifftn.html">torch.fft.ifftn</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fft.rfft.html">torch.fft.rfft</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fft.irfft.html">torch.fft.irfft</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fft.rfft2.html">torch.fft.rfft2</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fft.irfft2.html">torch.fft.irfft2</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fft.rfftn.html">torch.fft.rfftn</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fft.irfftn.html">torch.fft.irfftn</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fft.hfft.html">torch.fft.hfft</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fft.ihfft.html">torch.fft.ihfft</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fft.hfft2.html">torch.fft.hfft2</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fft.ihfft2.html">torch.fft.ihfft2</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fft.hfftn.html">torch.fft.hfftn</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fft.ihfftn.html">torch.fft.ihfftn</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fft.fftfreq.html">torch.fft.fftfreq</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fft.rfftfreq.html">torch.fft.rfftfreq</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fft.fftshift.html">torch.fft.fftshift</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fft.ifftshift.html">torch.fft.ifftshift</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="func.html">torch.func</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="func.whirlwind_tour.html">torch.func Whirlwind Tour</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="func.api.html">torch.func API Reference</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.func.vmap.html">torch.func.vmap</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.func.grad.html">torch.func.grad</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.func.grad_and_value.html">torch.func.grad_and_value</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.func.vjp.html">torch.func.vjp</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.func.jvp.html">torch.func.jvp</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.func.linearize.html">torch.func.linearize</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.func.jacrev.html">torch.func.jacrev</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.func.jacfwd.html">torch.func.jacfwd</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.func.hessian.html">torch.func.hessian</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.func.functionalize.html">torch.func.functionalize</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.func.functional_call.html">torch.func.functional_call</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.func.stack_module_state.html">torch.func.stack_module_state</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.func.replace_all_batch_norm_modules_.html">torch.func.replace_all_batch_norm_modules_</a></li>
<li class="toctree-l3"><a class="reference internal" href="func.batch_norm.html">Patching Batch Norm</a></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="func.ux_limitations.html">UX Limitations</a></li>
<li class="toctree-l2"><a class="reference internal" href="func.migrating.html">Migrating from functorch to torch.func</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="futures.html">torch.futures</a></li>
<li class="toctree-l1"><a class="reference internal" href="fx.html">torch.fx</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="fx.experimental.html">torch.fx.experimental</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fx.experimental.symbolic_shapes.ShapeEnv.html">ShapeEnv</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fx.experimental.symbolic_shapes.DimDynamic.html">DimDynamic</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fx.experimental.symbolic_shapes.StrictMinMaxConstraint.html">StrictMinMaxConstraint</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fx.experimental.symbolic_shapes.RelaxedUnspecConstraint.html">RelaxedUnspecConstraint</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fx.experimental.symbolic_shapes.EqualityConstraint.html">EqualityConstraint</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fx.experimental.symbolic_shapes.SymbolicContext.html">SymbolicContext</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fx.experimental.symbolic_shapes.StatelessSymbolicContext.html">StatelessSymbolicContext</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fx.experimental.symbolic_shapes.StatefulSymbolicContext.html">StatefulSymbolicContext</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fx.experimental.symbolic_shapes.SubclassSymbolicContext.html">SubclassSymbolicContext</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fx.experimental.symbolic_shapes.DimConstraints.html">DimConstraints</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fx.experimental.symbolic_shapes.ShapeEnvSettings.html">ShapeEnvSettings</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fx.experimental.symbolic_shapes.ConvertIntKey.html">ConvertIntKey</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fx.experimental.symbolic_shapes.CallMethodKey.html">CallMethodKey</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fx.experimental.symbolic_shapes.PropagateUnbackedSymInts.html">PropagateUnbackedSymInts</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fx.experimental.symbolic_shapes.DivideByKey.html">DivideByKey</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fx.experimental.symbolic_shapes.InnerTensorKey.html">InnerTensorKey</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fx.experimental.symbolic_shapes.hint_int.html">torch.fx.experimental.symbolic_shapes.hint_int</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fx.experimental.symbolic_shapes.is_concrete_int.html">torch.fx.experimental.symbolic_shapes.is_concrete_int</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fx.experimental.symbolic_shapes.is_concrete_bool.html">torch.fx.experimental.symbolic_shapes.is_concrete_bool</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fx.experimental.symbolic_shapes.is_concrete_float.html">torch.fx.experimental.symbolic_shapes.is_concrete_float</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fx.experimental.symbolic_shapes.has_free_symbols.html">torch.fx.experimental.symbolic_shapes.has_free_symbols</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fx.experimental.symbolic_shapes.has_free_unbacked_symbols.html">torch.fx.experimental.symbolic_shapes.has_free_unbacked_symbols</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fx.experimental.symbolic_shapes.definitely_true.html">torch.fx.experimental.symbolic_shapes.definitely_true</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fx.experimental.symbolic_shapes.definitely_false.html">torch.fx.experimental.symbolic_shapes.definitely_false</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fx.experimental.symbolic_shapes.guard_size_oblivious.html">torch.fx.experimental.symbolic_shapes.guard_size_oblivious</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fx.experimental.symbolic_shapes.sym_eq.html">torch.fx.experimental.symbolic_shapes.sym_eq</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fx.experimental.symbolic_shapes.constrain_range.html">torch.fx.experimental.symbolic_shapes.constrain_range</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fx.experimental.symbolic_shapes.constrain_unify.html">torch.fx.experimental.symbolic_shapes.constrain_unify</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fx.experimental.symbolic_shapes.canonicalize_bool_expr.html">torch.fx.experimental.symbolic_shapes.canonicalize_bool_expr</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fx.experimental.symbolic_shapes.statically_known_true.html">torch.fx.experimental.symbolic_shapes.statically_known_true</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fx.experimental.symbolic_shapes.lru_cache.html">torch.fx.experimental.symbolic_shapes.lru_cache</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fx.experimental.symbolic_shapes.check_consistent.html">torch.fx.experimental.symbolic_shapes.check_consistent</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fx.experimental.symbolic_shapes.compute_unbacked_bindings.html">torch.fx.experimental.symbolic_shapes.compute_unbacked_bindings</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fx.experimental.symbolic_shapes.rebind_unbacked.html">torch.fx.experimental.symbolic_shapes.rebind_unbacked</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fx.experimental.symbolic_shapes.resolve_unbacked_bindings.html">torch.fx.experimental.symbolic_shapes.resolve_unbacked_bindings</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fx.experimental.symbolic_shapes.is_accessor_node.html">torch.fx.experimental.symbolic_shapes.is_accessor_node</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fx.experimental.proxy_tensor.make_fx.html">torch.fx.experimental.proxy_tensor.make_fx</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fx.experimental.proxy_tensor.handle_sym_dispatch.html">torch.fx.experimental.proxy_tensor.handle_sym_dispatch</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fx.experimental.proxy_tensor.get_proxy_mode.html">torch.fx.experimental.proxy_tensor.get_proxy_mode</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fx.experimental.proxy_tensor.maybe_enable_thunkify.html">torch.fx.experimental.proxy_tensor.maybe_enable_thunkify</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fx.experimental.proxy_tensor.maybe_disable_thunkify.html">torch.fx.experimental.proxy_tensor.maybe_disable_thunkify</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="hub.html">torch.hub</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="jit.html">torch.jit</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="jit_builtin_functions.html">torch.jit.supported_ops</a></li>
<li class="toctree-l2"><a class="reference internal" href="jit_language_reference.html">TorchScript Language Reference</a></li>
<li class="toctree-l2"><a class="reference internal" href="jit_language_reference_v2.html">TorchScript Language Reference</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.jit.script.html">torch.jit.script</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.jit.trace.html">torch.jit.trace</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.jit.script_if_tracing.html">torch.jit.script_if_tracing</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.jit.trace_module.html">torch.jit.trace_module</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.jit.fork.html">torch.jit.fork</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.jit.wait.html">torch.jit.wait</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.jit.ScriptModule.html">ScriptModule</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.jit.ScriptFunction.html">ScriptFunction</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.jit.freeze.html">torch.jit.freeze</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.jit.optimize_for_inference.html">torch.jit.optimize_for_inference</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.jit.enable_onednn_fusion.html">torch.jit.enable_onednn_fusion</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.jit.onednn_fusion_enabled.html">torch.jit.onednn_fusion_enabled</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.jit.set_fusion_strategy.html">torch.jit.set_fusion_strategy</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.jit.strict_fusion.html">strict_fusion</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.jit.save.html">torch.jit.save</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.jit.load.html">torch.jit.load</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.jit.ignore.html">torch.jit.ignore</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.jit.unused.html">torch.jit.unused</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.jit.interface.html">torch.jit.interface</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.jit.isinstance.html">torch.jit.isinstance</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.jit.Attribute.html">Attribute</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.jit.annotate.html">torch.jit.annotate</a></li>
<li class="toctree-l2"><a class="reference internal" href="jit_python_reference.html">Python Language Reference Coverage</a></li>
<li class="toctree-l2"><a class="reference internal" href="jit_unsupported.html">TorchScript Unsupported PyTorch Constructs</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="linalg.html">torch.linalg</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.linalg.norm.html">torch.linalg.norm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.linalg.vector_norm.html">torch.linalg.vector_norm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.linalg.matrix_norm.html">torch.linalg.matrix_norm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.linalg.diagonal.html">torch.linalg.diagonal</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.linalg.det.html">torch.linalg.det</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.linalg.slogdet.html">torch.linalg.slogdet</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.linalg.cond.html">torch.linalg.cond</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.linalg.matrix_rank.html">torch.linalg.matrix_rank</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.linalg.cholesky.html">torch.linalg.cholesky</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.linalg.qr.html">torch.linalg.qr</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.linalg.lu.html">torch.linalg.lu</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.linalg.lu_factor.html">torch.linalg.lu_factor</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.linalg.eig.html">torch.linalg.eig</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.linalg.eigvals.html">torch.linalg.eigvals</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.linalg.eigh.html">torch.linalg.eigh</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.linalg.eigvalsh.html">torch.linalg.eigvalsh</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.linalg.svd.html">torch.linalg.svd</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.linalg.svdvals.html">torch.linalg.svdvals</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.linalg.solve.html">torch.linalg.solve</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.linalg.solve_triangular.html">torch.linalg.solve_triangular</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.linalg.lu_solve.html">torch.linalg.lu_solve</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.linalg.lstsq.html">torch.linalg.lstsq</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.linalg.inv.html">torch.linalg.inv</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.linalg.pinv.html">torch.linalg.pinv</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.linalg.matrix_exp.html">torch.linalg.matrix_exp</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.linalg.matrix_power.html">torch.linalg.matrix_power</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.linalg.cross.html">torch.linalg.cross</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.linalg.matmul.html">torch.linalg.matmul</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.linalg.vecdot.html">torch.linalg.vecdot</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.linalg.multi_dot.html">torch.linalg.multi_dot</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.linalg.householder_product.html">torch.linalg.householder_product</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.linalg.tensorinv.html">torch.linalg.tensorinv</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.linalg.tensorsolve.html">torch.linalg.tensorsolve</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.linalg.vander.html">torch.linalg.vander</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.linalg.cholesky_ex.html">torch.linalg.cholesky_ex</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.linalg.inv_ex.html">torch.linalg.inv_ex</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.linalg.solve_ex.html">torch.linalg.solve_ex</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.linalg.lu_factor_ex.html">torch.linalg.lu_factor_ex</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.linalg.ldl_factor.html">torch.linalg.ldl_factor</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.linalg.ldl_factor_ex.html">torch.linalg.ldl_factor_ex</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.linalg.ldl_solve.html">torch.linalg.ldl_solve</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="monitor.html">torch.monitor</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="signal.html">torch.signal</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.signal.windows.bartlett.html">torch.signal.windows.bartlett</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.signal.windows.blackman.html">torch.signal.windows.blackman</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.signal.windows.cosine.html">torch.signal.windows.cosine</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.signal.windows.exponential.html">torch.signal.windows.exponential</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.signal.windows.gaussian.html">torch.signal.windows.gaussian</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.signal.windows.general_cosine.html">torch.signal.windows.general_cosine</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.signal.windows.general_hamming.html">torch.signal.windows.general_hamming</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.signal.windows.hamming.html">torch.signal.windows.hamming</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.signal.windows.hann.html">torch.signal.windows.hann</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.signal.windows.kaiser.html">torch.signal.windows.kaiser</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.signal.windows.nuttall.html">torch.signal.windows.nuttall</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="special.html">torch.special</a></li>
<li class="toctree-l1"><a class="reference internal" href="torch.overrides.html">torch.overrides</a></li>
<li class="toctree-l1"><a class="reference internal" href="package.html">torch.package</a></li>
<li class="toctree-l1"><a class="reference internal" href="profiler.html">torch.profiler</a></li>
<li class="toctree-l1"><a class="reference internal" href="nn.init.html">torch.nn.init</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="nn.attention.html">torch.nn.attention</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.attention.sdpa_kernel.html">torch.nn.attention.sdpa_kernel</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.attention.SDPBackend.html">SDPBackend</a></li>
<li class="toctree-l2"><a class="reference internal" href="nn.attention.flex_attention.html">torch.nn.attention.flex_attention</a></li>
<li class="toctree-l2"><a class="reference internal" href="nn.attention.bias.html">torch.nn.attention.bias</a></li>

<li class="toctree-l2"><a class="reference internal" href="nn.attention.experimental.html">torch.nn.attention.experimental</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="onnx.html">torch.onnx</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="onnx_dynamo.html">TorchDynamo-based ONNX Exporter</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="onnx_dynamo_memory_usage.html">Understanding TorchDynamo-based ONNX Exporter Memory Usage</a></li>


</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="onnx_ops.html">torch.onnx.ops</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx_verification.html">torch.onnx.verification</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx_dynamo_onnxruntime_backend.html">ONNX Backend for TorchDynamo</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="onnx_torchscript.html">TorchScript-based ONNX Exporter</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.onnx.JitScalarType.html">JitScalarType</a></li>
</ul>
</details></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="optim.html">torch.optim</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.Optimizer.add_param_group.html">torch.optim.Optimizer.add_param_group</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.Optimizer.load_state_dict.html">torch.optim.Optimizer.load_state_dict</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.Optimizer.register_load_state_dict_pre_hook.html">torch.optim.Optimizer.register_load_state_dict_pre_hook</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.Optimizer.register_load_state_dict_post_hook.html">torch.optim.Optimizer.register_load_state_dict_post_hook</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.Optimizer.state_dict.html">torch.optim.Optimizer.state_dict</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.Optimizer.register_state_dict_pre_hook.html">torch.optim.Optimizer.register_state_dict_pre_hook</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.Optimizer.register_state_dict_post_hook.html">torch.optim.Optimizer.register_state_dict_post_hook</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.Optimizer.step.html">torch.optim.Optimizer.step</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.Optimizer.register_step_pre_hook.html">torch.optim.Optimizer.register_step_pre_hook</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.Optimizer.register_step_post_hook.html">torch.optim.Optimizer.register_step_post_hook</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.Optimizer.zero_grad.html">torch.optim.Optimizer.zero_grad</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.Adadelta.html">Adadelta</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.Adafactor.html">Adafactor</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.Adagrad.html">Adagrad</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.Adam.html">Adam</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.AdamW.html">AdamW</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.SparseAdam.html">SparseAdam</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.Adamax.html">Adamax</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.ASGD.html">ASGD</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.LBFGS.html">LBFGS</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.NAdam.html">NAdam</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.RAdam.html">RAdam</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.RMSprop.html">RMSprop</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.Rprop.html">Rprop</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.SGD.html">SGD</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.lr_scheduler.LRScheduler.html">LRScheduler</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.lr_scheduler.LambdaLR.html">LambdaLR</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.lr_scheduler.MultiplicativeLR.html">MultiplicativeLR</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.lr_scheduler.StepLR.html">StepLR</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.lr_scheduler.MultiStepLR.html">MultiStepLR</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.lr_scheduler.ConstantLR.html">ConstantLR</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.lr_scheduler.LinearLR.html">LinearLR</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.lr_scheduler.ExponentialLR.html">ExponentialLR</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.lr_scheduler.PolynomialLR.html">PolynomialLR</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.lr_scheduler.CosineAnnealingLR.html">CosineAnnealingLR</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.lr_scheduler.ChainedScheduler.html">ChainedScheduler</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.lr_scheduler.SequentialLR.html">SequentialLR</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.lr_scheduler.ReduceLROnPlateau.html">ReduceLROnPlateau</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.lr_scheduler.CyclicLR.html">CyclicLR</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.lr_scheduler.OneCycleLR.html">OneCycleLR</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.lr_scheduler.CosineAnnealingWarmRestarts.html">CosineAnnealingWarmRestarts</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.swa_utils.AveragedModel.html">AveragedModel</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.swa_utils.SWALR.html">SWALR</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="complex_numbers.html">Complex Numbers</a></li>
<li class="toctree-l1"><a class="reference internal" href="ddp_comm_hooks.html">DDP Communication Hooks</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="quantization.html">Quantization</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="quantization-support.html">Quantization API Reference</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.quantize.html">quantize</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.quantize_dynamic.html">quantize_dynamic</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.quantize_qat.html">quantize_qat</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.prepare.html">prepare</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.prepare_qat.html">prepare_qat</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.convert.html">convert</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.fuse_modules.fuse_modules.html">fuse_modules</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.QuantStub.html">QuantStub</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.DeQuantStub.html">DeQuantStub</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.QuantWrapper.html">QuantWrapper</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.add_quant_dequant.html">add_quant_dequant</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.swap_module.html">swap_module</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.propagate_qconfig_.html">propagate_qconfig</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.default_eval_fn.html">default_eval_fn</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.quantize_fx.prepare_fx.html">prepare_fx</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.quantize_fx.prepare_qat_fx.html">prepare_qat_fx</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.quantize_fx.convert_fx.html">convert_fx</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.quantize_fx.fuse_fx.html">fuse_fx</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.qconfig_mapping.QConfigMapping.html">QConfigMapping</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.qconfig_mapping.get_default_qconfig_mapping.html">get_default_qconfig_mapping</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.qconfig_mapping.get_default_qat_qconfig_mapping.html">get_default_qat_qconfig_mapping</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.backend_config.BackendConfig.html">BackendConfig</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.backend_config.BackendPatternConfig.html">BackendPatternConfig</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.backend_config.DTypeConfig.html">DTypeConfig</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.backend_config.DTypeWithConstraints.html">DTypeWithConstraints</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.backend_config.ObservationType.html">ObservationType</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.fx.custom_config.FuseCustomConfig.html">FuseCustomConfig</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.fx.custom_config.PrepareCustomConfig.html">PrepareCustomConfig</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.fx.custom_config.ConvertCustomConfig.html">ConvertCustomConfig</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.fx.custom_config.StandaloneModuleConfigEntry.html">StandaloneModuleConfigEntry</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.pt2e.export_utils.model_is_exported.html">model_is_exported</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.generate_numeric_debug_handle.html">generate_numeric_debug_handle</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.CUSTOM_KEY.html">CUSTOM_KEY</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.NUMERIC_DEBUG_HANDLE_KEY.html">NUMERIC_DEBUG_HANDLE_KEY</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.prepare_for_propagation_comparison.html">prepare_for_propagation_comparison</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.extract_results_from_loggers.html">extract_results_from_loggers</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.compare_results.html">compare_results</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.quantize_per_tensor.html">torch.quantize_per_tensor</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.quantize_per_channel.html">torch.quantize_per_channel</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.dequantize.html">torch.dequantize</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.Tensor.view.html">torch.Tensor.view</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.Tensor.as_strided.html">torch.Tensor.as_strided</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.Tensor.expand.html">torch.Tensor.expand</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.Tensor.flatten.html">torch.Tensor.flatten</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.Tensor.select.html">torch.Tensor.select</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.Tensor.ne.html">torch.Tensor.ne</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.Tensor.eq.html">torch.Tensor.eq</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.Tensor.ge.html">torch.Tensor.ge</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.Tensor.le.html">torch.Tensor.le</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.Tensor.gt.html">torch.Tensor.gt</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.Tensor.lt.html">torch.Tensor.lt</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.Tensor.copy_.html">torch.Tensor.copy_</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.Tensor.clone.html">torch.Tensor.clone</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.Tensor.dequantize.html">torch.Tensor.dequantize</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.Tensor.equal.html">torch.Tensor.equal</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.Tensor.int_repr.html">torch.Tensor.int_repr</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.Tensor.max.html">torch.Tensor.max</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.Tensor.mean.html">torch.Tensor.mean</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.Tensor.min.html">torch.Tensor.min</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.Tensor.q_scale.html">torch.Tensor.q_scale</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.Tensor.q_zero_point.html">torch.Tensor.q_zero_point</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.Tensor.q_per_channel_scales.html">torch.Tensor.q_per_channel_scales</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.Tensor.q_per_channel_zero_points.html">torch.Tensor.q_per_channel_zero_points</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.Tensor.q_per_channel_axis.html">torch.Tensor.q_per_channel_axis</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.Tensor.resize_.html">torch.Tensor.resize_</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.Tensor.sort.html">torch.Tensor.sort</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.Tensor.topk.html">torch.Tensor.topk</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.observer.ObserverBase.html">ObserverBase</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.observer.MinMaxObserver.html">MinMaxObserver</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.observer.MovingAverageMinMaxObserver.html">MovingAverageMinMaxObserver</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.observer.PerChannelMinMaxObserver.html">PerChannelMinMaxObserver</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver.html">MovingAveragePerChannelMinMaxObserver</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.observer.HistogramObserver.html">HistogramObserver</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.observer.PlaceholderObserver.html">PlaceholderObserver</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.observer.RecordingObserver.html">RecordingObserver</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.observer.NoopObserver.html">NoopObserver</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.observer.get_observer_state_dict.html">get_observer_state_dict</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.observer.load_observer_state_dict.html">load_observer_state_dict</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.observer.default_observer.html">default_observer</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.observer.default_placeholder_observer.html">default_placeholder_observer</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.observer.default_debug_observer.html">default_debug_observer</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.observer.default_weight_observer.html">default_weight_observer</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.observer.default_histogram_observer.html">default_histogram_observer</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.observer.default_per_channel_weight_observer.html">default_per_channel_weight_observer</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.observer.default_dynamic_quant_observer.html">default_dynamic_quant_observer</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.observer.default_float_qparams_observer.html">default_float_qparams_observer</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.fake_quantize.FakeQuantizeBase.html">FakeQuantizeBase</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.fake_quantize.FakeQuantize.html">FakeQuantize</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.fake_quantize.FixedQParamsFakeQuantize.html">FixedQParamsFakeQuantize</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize.html">FusedMovingAvgObsFakeQuantize</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.fake_quantize.default_fake_quant.html">default_fake_quant</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.fake_quantize.default_weight_fake_quant.html">default_weight_fake_quant</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.fake_quantize.default_per_channel_weight_fake_quant.html">default_per_channel_weight_fake_quant</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.fake_quantize.default_histogram_fake_quant.html">default_histogram_fake_quant</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.fake_quantize.default_fused_act_fake_quant.html">default_fused_act_fake_quant</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.fake_quantize.default_fused_wt_fake_quant.html">default_fused_wt_fake_quant</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.fake_quantize.default_fused_per_channel_wt_fake_quant.html">default_fused_per_channel_wt_fake_quant</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.fake_quantize.disable_fake_quant.html">disable_fake_quant</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.fake_quantize.enable_fake_quant.html">enable_fake_quant</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.fake_quantize.disable_observer.html">disable_observer</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.fake_quantize.enable_observer.html">enable_observer</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.qconfig.QConfig.html">QConfig</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.qconfig.default_qconfig.html">default_qconfig</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.qconfig.default_debug_qconfig.html">default_debug_qconfig</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.qconfig.default_per_channel_qconfig.html">default_per_channel_qconfig</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.qconfig.default_dynamic_qconfig.html">default_dynamic_qconfig</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.qconfig.float16_dynamic_qconfig.html">float16_dynamic_qconfig</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.qconfig.float16_static_qconfig.html">float16_static_qconfig</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.qconfig.per_channel_dynamic_qconfig.html">per_channel_dynamic_qconfig</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.qconfig.float_qparams_weight_only_qconfig.html">float_qparams_weight_only_qconfig</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.qconfig.default_qat_qconfig.html">default_qat_qconfig</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.qconfig.default_weight_only_qconfig.html">default_weight_only_qconfig</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.qconfig.default_activation_only_qconfig.html">default_activation_only_qconfig</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.qconfig.default_qat_qconfig_v2.html">default_qat_qconfig_v2</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.intrinsic.ConvReLU1d.html">ConvReLU1d</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.intrinsic.ConvReLU2d.html">ConvReLU2d</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.intrinsic.ConvReLU3d.html">ConvReLU3d</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.intrinsic.LinearReLU.html">LinearReLU</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.intrinsic.ConvBn1d.html">ConvBn1d</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.intrinsic.ConvBn2d.html">ConvBn2d</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.intrinsic.ConvBn3d.html">ConvBn3d</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.intrinsic.ConvBnReLU1d.html">ConvBnReLU1d</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.intrinsic.ConvBnReLU2d.html">ConvBnReLU2d</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.intrinsic.ConvBnReLU3d.html">ConvBnReLU3d</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.intrinsic.BNReLU2d.html">BNReLU2d</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.intrinsic.BNReLU3d.html">BNReLU3d</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.intrinsic.qat.LinearReLU.html">LinearReLU</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.intrinsic.qat.ConvBn1d.html">ConvBn1d</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.intrinsic.qat.ConvBnReLU1d.html">ConvBnReLU1d</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.intrinsic.qat.ConvBn2d.html">ConvBn2d</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.intrinsic.qat.ConvBnReLU2d.html">ConvBnReLU2d</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.intrinsic.qat.ConvReLU2d.html">ConvReLU2d</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.intrinsic.qat.ConvBn3d.html">ConvBn3d</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.intrinsic.qat.ConvBnReLU3d.html">ConvBnReLU3d</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.intrinsic.qat.ConvReLU3d.html">ConvReLU3d</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.intrinsic.qat.update_bn_stats.html">update_bn_stats</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.intrinsic.qat.freeze_bn_stats.html">freeze_bn_stats</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.intrinsic.quantized.BNReLU2d.html">BNReLU2d</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.intrinsic.quantized.BNReLU3d.html">BNReLU3d</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.intrinsic.quantized.ConvReLU1d.html">ConvReLU1d</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.intrinsic.quantized.ConvReLU2d.html">ConvReLU2d</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.intrinsic.quantized.ConvReLU3d.html">ConvReLU3d</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.intrinsic.quantized.LinearReLU.html">LinearReLU</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.intrinsic.quantized.dynamic.LinearReLU.html">LinearReLU</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.qat.Conv2d.html">Conv2d</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.qat.Conv3d.html">Conv3d</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.qat.Linear.html">Linear</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.qat.dynamic.Linear.html">Linear</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.quantized.ReLU6.html">ReLU6</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.quantized.Hardswish.html">Hardswish</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.quantized.ELU.html">ELU</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.quantized.LeakyReLU.html">LeakyReLU</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.quantized.Sigmoid.html">Sigmoid</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.quantized.BatchNorm2d.html">BatchNorm2d</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.quantized.BatchNorm3d.html">BatchNorm3d</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.quantized.Conv1d.html">Conv1d</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.quantized.Conv2d.html">Conv2d</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.quantized.Conv3d.html">Conv3d</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.quantized.ConvTranspose1d.html">ConvTranspose1d</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.quantized.ConvTranspose2d.html">ConvTranspose2d</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.quantized.ConvTranspose3d.html">ConvTranspose3d</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.quantized.Embedding.html">Embedding</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.quantized.EmbeddingBag.html">EmbeddingBag</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.quantized.FloatFunctional.html">FloatFunctional</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.quantized.FXFloatFunctional.html">FXFloatFunctional</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.quantized.QFunctional.html">QFunctional</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.quantized.Linear.html">Linear</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.quantized.LayerNorm.html">LayerNorm</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.quantized.GroupNorm.html">GroupNorm</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.quantized.InstanceNorm1d.html">InstanceNorm1d</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.quantized.InstanceNorm2d.html">InstanceNorm2d</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.quantized.InstanceNorm3d.html">InstanceNorm3d</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.quantized.functional.avg_pool2d.html">avg_pool2d</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.quantized.functional.avg_pool3d.html">avg_pool3d</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.quantized.functional.adaptive_avg_pool2d.html">adaptive_avg_pool2d</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.quantized.functional.adaptive_avg_pool3d.html">adaptive_avg_pool3d</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.quantized.functional.conv1d.html">conv1d</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.quantized.functional.conv2d.html">conv2d</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.quantized.functional.conv3d.html">conv3d</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.quantized.functional.interpolate.html">interpolate</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.quantized.functional.linear.html">linear</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.quantized.functional.max_pool1d.html">max_pool1d</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.quantized.functional.max_pool2d.html">max_pool2d</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.quantized.functional.celu.html">celu</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.quantized.functional.leaky_relu.html">leaky_relu</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.quantized.functional.hardtanh.html">hardtanh</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.quantized.functional.hardswish.html">hardswish</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.quantized.functional.threshold.html">threshold</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.quantized.functional.elu.html">elu</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.quantized.functional.hardsigmoid.html">hardsigmoid</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.quantized.functional.clamp.html">clamp</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.quantized.functional.upsample.html">upsample</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.quantized.functional.upsample_bilinear.html">upsample_bilinear</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.quantized.functional.upsample_nearest.html">upsample_nearest</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.quantizable.LSTM.html">LSTM</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.quantizable.MultiheadAttention.html">MultiheadAttention</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.quantized.dynamic.Linear.html">Linear</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.quantized.dynamic.LSTM.html">LSTM</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.quantized.dynamic.GRU.html">GRU</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.quantized.dynamic.RNNCell.html">RNNCell</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.quantized.dynamic.LSTMCell.html">LSTMCell</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.quantized.dynamic.GRUCell.html">GRUCell</a></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="quantization-backend-configuration.html">Quantization Backend Configuration</a></li>
<li class="toctree-l2"><a class="reference internal" href="quantization-accuracy-debugging.html">Quantization Accuracy Debugging</a></li>

</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="rpc.html">Distributed RPC Framework</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="rpc/rref.html">Remote Reference Protocol</a></li>
<li class="toctree-l2"><a class="reference internal" href="rpc/distributed_autograd.html">Distributed Autograd Design</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="random.html">torch.random</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="masked.html">torch.masked</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.abs.html">torch.abs</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.absolute.html">torch.absolute</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.acos.html">torch.acos</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.arccos.html">torch.arccos</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.acosh.html">torch.acosh</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.arccosh.html">torch.arccosh</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.angle.html">torch.angle</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.asin.html">torch.asin</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.arcsin.html">torch.arcsin</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.asinh.html">torch.asinh</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.arcsinh.html">torch.arcsinh</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.atan.html">torch.atan</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.arctan.html">torch.arctan</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.atanh.html">torch.atanh</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.arctanh.html">torch.arctanh</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.bitwise_not.html">torch.bitwise_not</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.ceil.html">torch.ceil</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.clamp.html">torch.clamp</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.clip.html">torch.clip</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.conj_physical.html">torch.conj_physical</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cos.html">torch.cos</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cosh.html">torch.cosh</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.deg2rad.html">torch.deg2rad</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.digamma.html">torch.digamma</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.erf.html">torch.erf</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.erfc.html">torch.erfc</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.erfinv.html">torch.erfinv</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.exp.html">torch.exp</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.exp2.html">torch.exp2</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.expm1.html">torch.expm1</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fix.html">torch.fix</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.floor.html">torch.floor</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.frac.html">torch.frac</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.lgamma.html">torch.lgamma</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.log.html">torch.log</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.log10.html">torch.log10</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.log1p.html">torch.log1p</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.log2.html">torch.log2</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.logit.html">torch.logit</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.i0.html">torch.i0</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.isnan.html">torch.isnan</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nan_to_num.html">torch.nan_to_num</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.neg.html">torch.neg</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.negative.html">torch.negative</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.positive.html">torch.positive</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.pow.html">torch.pow</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.rad2deg.html">torch.rad2deg</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.reciprocal.html">torch.reciprocal</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.round.html">torch.round</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.rsqrt.html">torch.rsqrt</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.sigmoid.html">torch.sigmoid</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.sign.html">torch.sign</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.sgn.html">torch.sgn</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.signbit.html">torch.signbit</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.sin.html">torch.sin</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.sinc.html">torch.sinc</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.sinh.html">torch.sinh</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.sqrt.html">torch.sqrt</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.square.html">torch.square</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.tan.html">torch.tan</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.tanh.html">torch.tanh</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.trunc.html">torch.trunc</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.angle.html">torch.angle</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.positive.html">torch.positive</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.signbit.html">torch.signbit</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.isnan.html">torch.isnan</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.add.html">torch.add</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.atan2.html">torch.atan2</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.arctan2.html">torch.arctan2</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.bitwise_and.html">torch.bitwise_and</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.bitwise_or.html">torch.bitwise_or</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.bitwise_xor.html">torch.bitwise_xor</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.bitwise_left_shift.html">torch.bitwise_left_shift</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.bitwise_right_shift.html">torch.bitwise_right_shift</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.div.html">torch.div</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.divide.html">torch.divide</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.floor_divide.html">torch.floor_divide</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fmod.html">torch.fmod</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.logaddexp.html">torch.logaddexp</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.logaddexp2.html">torch.logaddexp2</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mul.html">torch.mul</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.multiply.html">torch.multiply</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nextafter.html">torch.nextafter</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.remainder.html">torch.remainder</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.sub.html">torch.sub</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.subtract.html">torch.subtract</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.true_divide.html">torch.true_divide</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.eq.html">torch.eq</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.ne.html">torch.ne</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.le.html">torch.le</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.ge.html">torch.ge</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.greater.html">torch.greater</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.greater_equal.html">torch.greater_equal</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.gt.html">torch.gt</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.less_equal.html">torch.less_equal</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.lt.html">torch.lt</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.less.html">torch.less</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.maximum.html">torch.maximum</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.minimum.html">torch.minimum</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fmax.html">torch.fmax</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fmin.html">torch.fmin</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.not_equal.html">torch.not_equal</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.logaddexp.html">torch.logaddexp</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.logaddexp2.html">torch.logaddexp2</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.equal.html">torch.equal</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fmin.html">torch.fmin</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.minimum.html">torch.minimum</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fmax.html">torch.fmax</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.sum.html">torch.sum</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mean.html">torch.mean</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.amin.html">torch.amin</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.amax.html">torch.amax</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.argmin.html">torch.argmin</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.argmax.html">torch.argmax</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.prod.html">torch.prod</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.all.html">torch.all</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.norm.html">torch.norm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.var.html">torch.var</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.std.html">torch.std</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.atleast_1d.html">torch.atleast_1d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.broadcast_tensors.html">torch.broadcast_tensors</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.broadcast_to.html">torch.broadcast_to</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cat.html">torch.cat</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.chunk.html">torch.chunk</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.column_stack.html">torch.column_stack</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.dsplit.html">torch.dsplit</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.flatten.html">torch.flatten</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.hsplit.html">torch.hsplit</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.hstack.html">torch.hstack</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.kron.html">torch.kron</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.meshgrid.html">torch.meshgrid</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.narrow.html">torch.narrow</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.unfold.html">torch.nn.functional.unfold</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.ravel.html">torch.ravel</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.select.html">torch.select</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.split.html">torch.split</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.stack.html">torch.stack</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.t.html">torch.t</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.transpose.html">torch.transpose</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.vsplit.html">torch.vsplit</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.vstack.html">torch.vstack</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.expand.html">torch.Tensor.expand</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.expand_as.html">torch.Tensor.expand_as</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.reshape.html">torch.Tensor.reshape</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.reshape_as.html">torch.Tensor.reshape_as</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.unfold.html">torch.Tensor.unfold</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.view.html">torch.Tensor.view</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="nested.html">torch.nested</a></li>
<li class="toctree-l1"><a class="reference internal" href="size.html">torch.Size</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="sparse.html">torch.sparse</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.is_sparse.html">torch.Tensor.is_sparse</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.is_sparse_csr.html">torch.Tensor.is_sparse_csr</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.dense_dim.html">torch.Tensor.dense_dim</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.sparse_dim.html">torch.Tensor.sparse_dim</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.sparse_mask.html">torch.Tensor.sparse_mask</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.to_sparse.html">torch.Tensor.to_sparse</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.to_sparse_coo.html">torch.Tensor.to_sparse_coo</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.to_sparse_csr.html">torch.Tensor.to_sparse_csr</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.to_sparse_csc.html">torch.Tensor.to_sparse_csc</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.to_sparse_bsr.html">torch.Tensor.to_sparse_bsr</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.to_sparse_bsc.html">torch.Tensor.to_sparse_bsc</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.to_dense.html">torch.Tensor.to_dense</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.values.html">torch.Tensor.values</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.coalesce.html">torch.Tensor.coalesce</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.sparse_resize_.html">torch.Tensor.sparse_resize_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.sparse_resize_and_clear_.html">torch.Tensor.sparse_resize_and_clear_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.is_coalesced.html">torch.Tensor.is_coalesced</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.indices.html">torch.Tensor.indices</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.crow_indices.html">torch.Tensor.crow_indices</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.col_indices.html">torch.Tensor.col_indices</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.row_indices.html">torch.Tensor.row_indices</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.ccol_indices.html">torch.Tensor.ccol_indices</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.sparse_coo_tensor.html">torch.sparse_coo_tensor</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.sparse_csr_tensor.html">torch.sparse_csr_tensor</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.sparse_csc_tensor.html">torch.sparse_csc_tensor</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.sparse_bsr_tensor.html">torch.sparse_bsr_tensor</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.sparse_bsc_tensor.html">torch.sparse_bsc_tensor</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.sparse_compressed_tensor.html">torch.sparse_compressed_tensor</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.sparse.sum.html">torch.sparse.sum</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.sparse.addmm.html">torch.sparse.addmm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.sparse.sampled_addmm.html">torch.sparse.sampled_addmm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.sparse.mm.html">torch.sparse.mm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.sspaddmm.html">torch.sspaddmm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.hspmm.html">torch.hspmm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.smm.html">torch.smm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.sparse.softmax.html">torch.sparse.softmax</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.sparse.spsolve.html">torch.sparse.spsolve</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.sparse.log_softmax.html">torch.sparse.log_softmax</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.sparse.spdiags.html">torch.sparse.spdiags</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.sparse.check_sparse_tensor_invariants.html">check_sparse_tensor_invariants</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.sparse.as_sparse_gradcheck.html">torch.sparse.as_sparse_gradcheck</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="storage.html">torch.Storage</a></li>
<li class="toctree-l1"><a class="reference internal" href="testing.html">torch.testing</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="utils.html">torch.utils</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.utils.rename_privateuse1_backend.html">torch.utils.rename_privateuse1_backend</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.utils.generate_methods_for_privateuse1_backend.html">torch.utils.generate_methods_for_privateuse1_backend</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.utils.get_cpp_backtrace.html">torch.utils.get_cpp_backtrace</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.utils.set_module.html">torch.utils.set_module</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.utils.swap_tensors.html">torch.utils.swap_tensors</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="benchmark_utils.html">torch.utils.benchmark</a></li>
<li class="toctree-l1"><a class="reference internal" href="bottleneck.html">torch.utils.bottleneck</a></li>
<li class="toctree-l1"><a class="reference internal" href="checkpoint.html">torch.utils.checkpoint</a></li>
<li class="toctree-l1"><a class="reference internal" href="cpp_extension.html">torch.utils.cpp_extension</a></li>
<li class="toctree-l1"><a class="reference internal" href="data.html">torch.utils.data</a></li>
<li class="toctree-l1"><a class="reference internal" href="deterministic.html">torch.utils.deterministic</a></li>
<li class="toctree-l1"><a class="reference internal" href="jit_utils.html">torch.utils.jit</a></li>
<li class="toctree-l1"><a class="reference internal" href="dlpack.html">torch.utils.dlpack</a></li>
<li class="toctree-l1"><a class="reference internal" href="mobile_optimizer.html">torch.utils.mobile_optimizer</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_zoo.html">torch.utils.model_zoo</a></li>
<li class="toctree-l1"><a class="reference internal" href="tensorboard.html">torch.utils.tensorboard</a></li>
<li class="toctree-l1"><a class="reference internal" href="module_tracker.html">torch.utils.module_tracker</a></li>
<li class="toctree-l1"><a class="reference internal" href="type_info.html">Type Info</a></li>
<li class="toctree-l1"><a class="reference internal" href="named_tensor.html">Named Tensors</a></li>
<li class="toctree-l1"><a class="reference internal" href="name_inference.html">Named Tensors operator coverage</a></li>
<li class="toctree-l1"><a class="reference internal" href="config_mod.html">torch.__config__</a></li>
<li class="toctree-l1"><a class="reference internal" href="future_mod.html">torch.__future__</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="logging.html">torch._logging</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._logging.set_logs.html">torch._logging.set_logs</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="torch_environment_variables.html">Torch Environment Variables</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="threading_environment_variables.html">Threading Environment Variables</a></li>
<li class="toctree-l2"><a class="reference internal" href="cuda_environment_variables.html">CUDA Environment Variables</a></li>
<li class="toctree-l2"><a class="reference internal" href="mps_environment_variables.html">MPS Environment Variables</a></li>
<li class="toctree-l2"><a class="reference internal" href="debugging_environment_variables.html">Debugging Environment Variables</a></li>
<li class="toctree-l2"><a class="reference internal" href="miscellaneous_environment_variables.html">Miscellaneous Environment Variables</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="logging.html">torch._logging</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="generated/torch._logging.set_logs.html">torch._logging.set_logs</a></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="torch_nccl_environment_variables.html">PYTORCH ProcessGroupNCCL Environment Variables</a></li>
</ul>
</details></li>
</ul>
</div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        
          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item">



<nav aria-label="Breadcrumb" class="d-print-none">
  <ul class="bd-breadcrumbs">
    
    <li class="breadcrumb-item breadcrumb-home">
      <a href="index.html" class="nav-link" aria-label="Home">
        <i class="fa-solid fa-home"></i>
      </a>
    </li>
    
    <li class="breadcrumb-item"><a href="pytorch-api.html" class="nav-link">Python API</a></li>
    
    <li class="breadcrumb-item active" aria-current="page">torch</li>
  </ul>
</nav>
</div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">
<div class="rating">
    Rate this Page
    <div class="stars">
        
        <span class="star" data-behavior="tutorial-rating" data-count="1" data-value="1">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="2" data-value="2">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="3" data-value="3">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="4" data-value="4">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="5" data-value="5">★</span>
        
    </div>
</div>
</div>
      
    </div>
  
</div>
</div>
              
              
  
<div id="searchbox"></div>
  <article class="bd-article">
    
    
  <section id="module-torch">
<span id="torch"></span><h1>torch<a class="headerlink" href="#module-torch" title="Permalink to this heading">#</a></h1>
<p>The torch package contains data structures for multi-dimensional
tensors and defines mathematical operations over these tensors.
Additionally, it provides many utilities for efficient serialization of
Tensors and arbitrary types, and other useful utilities.</p>
<p>It has a CUDA counterpart, that enables you to run your tensor computations
on an NVIDIA GPU with compute capability &gt;= 3.0.</p>
<section id="tensors">
<h2>Tensors<a class="headerlink" href="#tensors" title="Permalink to this heading">#</a></h2>
<div class="pst-scrollable-table-container"><table class="autosummary longtable table autosummary">
<tbody>
<tr class="row-odd"><td><p><p id="torch.is_tensor"/><a class="reference internal" href="generated/torch.is_tensor.html#torch.is_tensor" title="torch.is_tensor"><code class="xref py py-obj docutils literal notranslate"><span class="pre">is_tensor</span></code></a></p></td>
<td><p>Returns True if <cite>obj</cite> is a PyTorch tensor.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.is_storage"/><a class="reference internal" href="generated/torch.is_storage.html#torch.is_storage" title="torch.is_storage"><code class="xref py py-obj docutils literal notranslate"><span class="pre">is_storage</span></code></a></p></td>
<td><p>Returns True if <cite>obj</cite> is a PyTorch storage object.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.is_complex"/><a class="reference internal" href="generated/torch.is_complex.html#torch.is_complex" title="torch.is_complex"><code class="xref py py-obj docutils literal notranslate"><span class="pre">is_complex</span></code></a></p></td>
<td><p>Returns True if the data type of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> is a complex data type i.e., one of <code class="docutils literal notranslate"><span class="pre">torch.complex64</span></code>, and <code class="docutils literal notranslate"><span class="pre">torch.complex128</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.is_conj"/><a class="reference internal" href="generated/torch.is_conj.html#torch.is_conj" title="torch.is_conj"><code class="xref py py-obj docutils literal notranslate"><span class="pre">is_conj</span></code></a></p></td>
<td><p>Returns True if the <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> is a conjugated tensor, i.e. its conjugate bit is set to <cite>True</cite>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.is_floating_point"/><a class="reference internal" href="generated/torch.is_floating_point.html#torch.is_floating_point" title="torch.is_floating_point"><code class="xref py py-obj docutils literal notranslate"><span class="pre">is_floating_point</span></code></a></p></td>
<td><p>Returns True if the data type of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> is a floating point data type i.e., one of <code class="docutils literal notranslate"><span class="pre">torch.float64</span></code>, <code class="docutils literal notranslate"><span class="pre">torch.float32</span></code>, <code class="docutils literal notranslate"><span class="pre">torch.float16</span></code>, and <code class="docutils literal notranslate"><span class="pre">torch.bfloat16</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.is_nonzero"/><a class="reference internal" href="generated/torch.is_nonzero.html#torch.is_nonzero" title="torch.is_nonzero"><code class="xref py py-obj docutils literal notranslate"><span class="pre">is_nonzero</span></code></a></p></td>
<td><p>Returns True if the <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> is a single element tensor which is not equal to zero after type conversions.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.set_default_dtype"/><a class="reference internal" href="generated/torch.set_default_dtype.html#torch.set_default_dtype" title="torch.set_default_dtype"><code class="xref py py-obj docutils literal notranslate"><span class="pre">set_default_dtype</span></code></a></p></td>
<td><p>Sets the default floating point dtype to <code class="xref py py-attr docutils literal notranslate"><span class="pre">d</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.get_default_dtype"/><a class="reference internal" href="generated/torch.get_default_dtype.html#torch.get_default_dtype" title="torch.get_default_dtype"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_default_dtype</span></code></a></p></td>
<td><p>Get the current default floating point <a class="reference internal" href="tensor_attributes.html#torch.dtype" title="torch.dtype"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.dtype</span></code></a>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.set_default_device"/><a class="reference internal" href="generated/torch.set_default_device.html#torch.set_default_device" title="torch.set_default_device"><code class="xref py py-obj docutils literal notranslate"><span class="pre">set_default_device</span></code></a></p></td>
<td><p>Sets the default <code class="docutils literal notranslate"><span class="pre">torch.Tensor</span></code> to be allocated on <code class="docutils literal notranslate"><span class="pre">device</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.get_default_device"/><a class="reference internal" href="generated/torch.get_default_device.html#torch.get_default_device" title="torch.get_default_device"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_default_device</span></code></a></p></td>
<td><p>Gets the default <code class="docutils literal notranslate"><span class="pre">torch.Tensor</span></code> to be allocated on <code class="docutils literal notranslate"><span class="pre">device</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.set_default_tensor_type"/><a class="reference internal" href="generated/torch.set_default_tensor_type.html#torch.set_default_tensor_type" title="torch.set_default_tensor_type"><code class="xref py py-obj docutils literal notranslate"><span class="pre">set_default_tensor_type</span></code></a></p></td>
<td><p></p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.numel"/><a class="reference internal" href="generated/torch.numel.html#torch.numel" title="torch.numel"><code class="xref py py-obj docutils literal notranslate"><span class="pre">numel</span></code></a></p></td>
<td><p>Returns the total number of elements in the <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> tensor.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.set_printoptions"/><a class="reference internal" href="generated/torch.set_printoptions.html#torch.set_printoptions" title="torch.set_printoptions"><code class="xref py py-obj docutils literal notranslate"><span class="pre">set_printoptions</span></code></a></p></td>
<td><p>Set options for printing.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.set_flush_denormal"/><a class="reference internal" href="generated/torch.set_flush_denormal.html#torch.set_flush_denormal" title="torch.set_flush_denormal"><code class="xref py py-obj docutils literal notranslate"><span class="pre">set_flush_denormal</span></code></a></p></td>
<td><p>Disables denormal floating numbers on CPU.</p></td>
</tr>
</tbody>
</table>
</div>
<section id="creation-ops">
<span id="tensor-creation-ops"></span><h3>Creation Ops<a class="headerlink" href="#creation-ops" title="Permalink to this heading">#</a></h3>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Random sampling creation ops are listed under <a class="reference internal" href="#random-sampling"><span class="std std-ref">Random sampling</span></a> and
include:
<a class="reference internal" href="generated/torch.rand.html#torch.rand" title="torch.rand"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.rand()</span></code></a>
<a class="reference internal" href="generated/torch.rand_like.html#torch.rand_like" title="torch.rand_like"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.rand_like()</span></code></a>
<a class="reference internal" href="generated/torch.randn.html#torch.randn" title="torch.randn"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.randn()</span></code></a>
<a class="reference internal" href="generated/torch.randn_like.html#torch.randn_like" title="torch.randn_like"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.randn_like()</span></code></a>
<a class="reference internal" href="generated/torch.randint.html#torch.randint" title="torch.randint"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.randint()</span></code></a>
<a class="reference internal" href="generated/torch.randint_like.html#torch.randint_like" title="torch.randint_like"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.randint_like()</span></code></a>
<a class="reference internal" href="generated/torch.randperm.html#torch.randperm" title="torch.randperm"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.randperm()</span></code></a>
You may also use <a class="reference internal" href="generated/torch.empty.html#torch.empty" title="torch.empty"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.empty()</span></code></a> with the <a class="reference internal" href="#inplace-random-sampling"><span class="std std-ref">In-place random sampling</span></a>
methods to create <a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Tensor</span></code></a> s with values sampled from a broader
range of distributions.</p>
</div>
<div class="pst-scrollable-table-container"><table class="autosummary longtable table autosummary">
<tbody>
<tr class="row-odd"><td><p><p id="torch.tensor"/><a class="reference internal" href="generated/torch.tensor.html#torch.tensor" title="torch.tensor"><code class="xref py py-obj docutils literal notranslate"><span class="pre">tensor</span></code></a></p></td>
<td><p>Constructs a tensor with no autograd history (also known as a &quot;leaf tensor&quot;, see <a class="reference internal" href="notes/autograd.html"><span class="doc">Autograd mechanics</span></a>) by copying <code class="xref py py-attr docutils literal notranslate"><span class="pre">data</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.sparse_coo_tensor"/><a class="reference internal" href="generated/torch.sparse_coo_tensor.html#torch.sparse_coo_tensor" title="torch.sparse_coo_tensor"><code class="xref py py-obj docutils literal notranslate"><span class="pre">sparse_coo_tensor</span></code></a></p></td>
<td><p>Constructs a <a class="reference internal" href="sparse.html#sparse-coo-docs"><span class="std std-ref">sparse tensor in COO(rdinate) format</span></a> with specified values at the given <code class="xref py py-attr docutils literal notranslate"><span class="pre">indices</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.sparse_csr_tensor"/><a class="reference internal" href="generated/torch.sparse_csr_tensor.html#torch.sparse_csr_tensor" title="torch.sparse_csr_tensor"><code class="xref py py-obj docutils literal notranslate"><span class="pre">sparse_csr_tensor</span></code></a></p></td>
<td><p>Constructs a <a class="reference internal" href="sparse.html#sparse-csr-docs"><span class="std std-ref">sparse tensor in CSR (Compressed Sparse Row)</span></a> with specified values at the given <code class="xref py py-attr docutils literal notranslate"><span class="pre">crow_indices</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">col_indices</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.sparse_csc_tensor"/><a class="reference internal" href="generated/torch.sparse_csc_tensor.html#torch.sparse_csc_tensor" title="torch.sparse_csc_tensor"><code class="xref py py-obj docutils literal notranslate"><span class="pre">sparse_csc_tensor</span></code></a></p></td>
<td><p>Constructs a <a class="reference internal" href="sparse.html#sparse-csc-docs"><span class="std std-ref">sparse tensor in CSC (Compressed Sparse Column)</span></a> with specified values at the given <code class="xref py py-attr docutils literal notranslate"><span class="pre">ccol_indices</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">row_indices</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.sparse_bsr_tensor"/><a class="reference internal" href="generated/torch.sparse_bsr_tensor.html#torch.sparse_bsr_tensor" title="torch.sparse_bsr_tensor"><code class="xref py py-obj docutils literal notranslate"><span class="pre">sparse_bsr_tensor</span></code></a></p></td>
<td><p>Constructs a <a class="reference internal" href="sparse.html#sparse-bsr-docs"><span class="std std-ref">sparse tensor in BSR (Block Compressed Sparse Row))</span></a> with specified 2-dimensional blocks at the given <code class="xref py py-attr docutils literal notranslate"><span class="pre">crow_indices</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">col_indices</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.sparse_bsc_tensor"/><a class="reference internal" href="generated/torch.sparse_bsc_tensor.html#torch.sparse_bsc_tensor" title="torch.sparse_bsc_tensor"><code class="xref py py-obj docutils literal notranslate"><span class="pre">sparse_bsc_tensor</span></code></a></p></td>
<td><p>Constructs a <a class="reference internal" href="sparse.html#sparse-bsc-docs"><span class="std std-ref">sparse tensor in BSC (Block Compressed Sparse Column))</span></a> with specified 2-dimensional blocks at the given <code class="xref py py-attr docutils literal notranslate"><span class="pre">ccol_indices</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">row_indices</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.asarray"/><a class="reference internal" href="generated/torch.asarray.html#torch.asarray" title="torch.asarray"><code class="xref py py-obj docutils literal notranslate"><span class="pre">asarray</span></code></a></p></td>
<td><p>Converts <code class="xref py py-attr docutils literal notranslate"><span class="pre">obj</span></code> to a tensor.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.as_tensor"/><a class="reference internal" href="generated/torch.as_tensor.html#torch.as_tensor" title="torch.as_tensor"><code class="xref py py-obj docutils literal notranslate"><span class="pre">as_tensor</span></code></a></p></td>
<td><p>Converts <code class="xref py py-attr docutils literal notranslate"><span class="pre">data</span></code> into a tensor, sharing data and preserving autograd history if possible.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.as_strided"/><a class="reference internal" href="generated/torch.as_strided.html#torch.as_strided" title="torch.as_strided"><code class="xref py py-obj docutils literal notranslate"><span class="pre">as_strided</span></code></a></p></td>
<td><p>Create a view of an existing <cite>torch.Tensor</cite> <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> with specified <code class="xref py py-attr docutils literal notranslate"><span class="pre">size</span></code>, <code class="xref py py-attr docutils literal notranslate"><span class="pre">stride</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">storage_offset</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.from_file"/><a class="reference internal" href="generated/torch.from_file.html#torch.from_file" title="torch.from_file"><code class="xref py py-obj docutils literal notranslate"><span class="pre">from_file</span></code></a></p></td>
<td><p>Creates a CPU tensor with a storage backed by a memory-mapped file.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.from_numpy"/><a class="reference internal" href="generated/torch.from_numpy.html#torch.from_numpy" title="torch.from_numpy"><code class="xref py py-obj docutils literal notranslate"><span class="pre">from_numpy</span></code></a></p></td>
<td><p>Creates a <a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></a> from a <a class="reference external" href="https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v2.2)"><code class="xref py py-class docutils literal notranslate"><span class="pre">numpy.ndarray</span></code></a>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.from_dlpack"/><a class="reference internal" href="generated/torch.from_dlpack.html#torch.from_dlpack" title="torch.from_dlpack"><code class="xref py py-obj docutils literal notranslate"><span class="pre">from_dlpack</span></code></a></p></td>
<td><p>Converts a tensor from an external library into a <code class="docutils literal notranslate"><span class="pre">torch.Tensor</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.frombuffer"/><a class="reference internal" href="generated/torch.frombuffer.html#torch.frombuffer" title="torch.frombuffer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">frombuffer</span></code></a></p></td>
<td><p>Creates a 1-dimensional <a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></a> from an object that implements the Python buffer protocol.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.zeros"/><a class="reference internal" href="generated/torch.zeros.html#torch.zeros" title="torch.zeros"><code class="xref py py-obj docutils literal notranslate"><span class="pre">zeros</span></code></a></p></td>
<td><p>Returns a tensor filled with the scalar value <cite>0</cite>, with the shape defined by the variable argument <code class="xref py py-attr docutils literal notranslate"><span class="pre">size</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.zeros_like"/><a class="reference internal" href="generated/torch.zeros_like.html#torch.zeros_like" title="torch.zeros_like"><code class="xref py py-obj docutils literal notranslate"><span class="pre">zeros_like</span></code></a></p></td>
<td><p>Returns a tensor filled with the scalar value <cite>0</cite>, with the same size as <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.ones"/><a class="reference internal" href="generated/torch.ones.html#torch.ones" title="torch.ones"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ones</span></code></a></p></td>
<td><p>Returns a tensor filled with the scalar value <cite>1</cite>, with the shape defined by the variable argument <code class="xref py py-attr docutils literal notranslate"><span class="pre">size</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.ones_like"/><a class="reference internal" href="generated/torch.ones_like.html#torch.ones_like" title="torch.ones_like"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ones_like</span></code></a></p></td>
<td><p>Returns a tensor filled with the scalar value <cite>1</cite>, with the same size as <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.arange"/><a class="reference internal" href="generated/torch.arange.html#torch.arange" title="torch.arange"><code class="xref py py-obj docutils literal notranslate"><span class="pre">arange</span></code></a></p></td>
<td><p>Returns a 1-D tensor of size <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo fence="true">⌈</mo><mfrac><mrow><mtext>end</mtext><mo>−</mo><mtext>start</mtext></mrow><mtext>step</mtext></mfrac><mo fence="true">⌉</mo></mrow><annotation encoding="application/x-tex">\left\lceil \frac{\text{end} - \text{start}}{\text{step}} \right\rceil</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.8em;vertical-align:-0.65em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;"><span class="delimsizing size2">⌈</span></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8801em;"><span style="top:-2.655em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">step</span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">end</span></span><span class="mbin mtight">−</span><span class="mord text mtight"><span class="mord mtight">start</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.4811em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mclose delimcenter" style="top:0em;"><span class="delimsizing size2">⌉</span></span></span></span></span></span></span> with values from the interval <code class="docutils literal notranslate"><span class="pre">[start,</span> <span class="pre">end)</span></code> taken with common difference <code class="xref py py-attr docutils literal notranslate"><span class="pre">step</span></code> beginning from <cite>start</cite>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.range"/><a class="reference internal" href="generated/torch.range.html#torch.range" title="torch.range"><code class="xref py py-obj docutils literal notranslate"><span class="pre">range</span></code></a></p></td>
<td><p>Returns a 1-D tensor of size <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mo fence="true">⌊</mo><mfrac><mrow><mtext>end</mtext><mo>−</mo><mtext>start</mtext></mrow><mtext>step</mtext></mfrac><mo fence="true">⌋</mo></mrow><mo>+</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">\left\lfloor \frac{\text{end} - \text{start}}{\text{step}} \right\rfloor + 1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.8em;vertical-align:-0.65em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;"><span class="delimsizing size2">⌊</span></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8801em;"><span style="top:-2.655em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">step</span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">end</span></span><span class="mbin mtight">−</span><span class="mord text mtight"><span class="mord mtight">start</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.4811em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mclose delimcenter" style="top:0em;"><span class="delimsizing size2">⌋</span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">1</span></span></span></span></span> with values from <code class="xref py py-attr docutils literal notranslate"><span class="pre">start</span></code> to <code class="xref py py-attr docutils literal notranslate"><span class="pre">end</span></code> with step <code class="xref py py-attr docutils literal notranslate"><span class="pre">step</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.linspace"/><a class="reference internal" href="generated/torch.linspace.html#torch.linspace" title="torch.linspace"><code class="xref py py-obj docutils literal notranslate"><span class="pre">linspace</span></code></a></p></td>
<td><p>Creates a one-dimensional tensor of size <code class="xref py py-attr docutils literal notranslate"><span class="pre">steps</span></code> whose values are evenly spaced from <code class="xref py py-attr docutils literal notranslate"><span class="pre">start</span></code> to <code class="xref py py-attr docutils literal notranslate"><span class="pre">end</span></code>, inclusive.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.logspace"/><a class="reference internal" href="generated/torch.logspace.html#torch.logspace" title="torch.logspace"><code class="xref py py-obj docutils literal notranslate"><span class="pre">logspace</span></code></a></p></td>
<td><p>Creates a one-dimensional tensor of size <code class="xref py py-attr docutils literal notranslate"><span class="pre">steps</span></code> whose values are evenly spaced from <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mtext>base</mtext><mtext>start</mtext></msup></mrow><annotation encoding="application/x-tex">{{\text{{base}}}}^{{\text{{start}}}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8779em;"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord text"><span class="mord"><span class="mord">base</span></span></span></span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8779em;"><span style="top:-3.1473em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight"><span class="mord mtight">start</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span> to <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mtext>base</mtext><mtext>end</mtext></msup></mrow><annotation encoding="application/x-tex">{{\text{{base}}}}^{{\text{{end}}}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9334em;"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord text"><span class="mord"><span class="mord">base</span></span></span></span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.9334em;"><span style="top:-3.1473em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight"><span class="mord mtight">end</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span>, inclusive, on a logarithmic scale with base <code class="xref py py-attr docutils literal notranslate"><span class="pre">base</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.eye"/><a class="reference internal" href="generated/torch.eye.html#torch.eye" title="torch.eye"><code class="xref py py-obj docutils literal notranslate"><span class="pre">eye</span></code></a></p></td>
<td><p>Returns a 2-D tensor with ones on the diagonal and zeros elsewhere.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.empty"/><a class="reference internal" href="generated/torch.empty.html#torch.empty" title="torch.empty"><code class="xref py py-obj docutils literal notranslate"><span class="pre">empty</span></code></a></p></td>
<td><p>Returns a tensor filled with uninitialized data.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.empty_like"/><a class="reference internal" href="generated/torch.empty_like.html#torch.empty_like" title="torch.empty_like"><code class="xref py py-obj docutils literal notranslate"><span class="pre">empty_like</span></code></a></p></td>
<td><p>Returns an uninitialized tensor with the same size as <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.empty_strided"/><a class="reference internal" href="generated/torch.empty_strided.html#torch.empty_strided" title="torch.empty_strided"><code class="xref py py-obj docutils literal notranslate"><span class="pre">empty_strided</span></code></a></p></td>
<td><p>Creates a tensor with the specified <code class="xref py py-attr docutils literal notranslate"><span class="pre">size</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">stride</span></code> and filled with undefined data.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.full"/><a class="reference internal" href="generated/torch.full.html#torch.full" title="torch.full"><code class="xref py py-obj docutils literal notranslate"><span class="pre">full</span></code></a></p></td>
<td><p>Creates a tensor of size <code class="xref py py-attr docutils literal notranslate"><span class="pre">size</span></code> filled with <code class="xref py py-attr docutils literal notranslate"><span class="pre">fill_value</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.full_like"/><a class="reference internal" href="generated/torch.full_like.html#torch.full_like" title="torch.full_like"><code class="xref py py-obj docutils literal notranslate"><span class="pre">full_like</span></code></a></p></td>
<td><p>Returns a tensor with the same size as <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> filled with <code class="xref py py-attr docutils literal notranslate"><span class="pre">fill_value</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.quantize_per_tensor"/><a class="reference internal" href="generated/torch.quantize_per_tensor.html#torch.quantize_per_tensor" title="torch.quantize_per_tensor"><code class="xref py py-obj docutils literal notranslate"><span class="pre">quantize_per_tensor</span></code></a></p></td>
<td><p>Converts a float tensor to a quantized tensor with given scale and zero point.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.quantize_per_channel"/><a class="reference internal" href="generated/torch.quantize_per_channel.html#torch.quantize_per_channel" title="torch.quantize_per_channel"><code class="xref py py-obj docutils literal notranslate"><span class="pre">quantize_per_channel</span></code></a></p></td>
<td><p>Converts a float tensor to a per-channel quantized tensor with given scales and zero points.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.dequantize"/><a class="reference internal" href="generated/torch.dequantize.html#torch.dequantize" title="torch.dequantize"><code class="xref py py-obj docutils literal notranslate"><span class="pre">dequantize</span></code></a></p></td>
<td><p>Returns an fp32 Tensor by dequantizing a quantized Tensor</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.complex"/><a class="reference internal" href="generated/torch.complex.html#torch.complex" title="torch.complex"><code class="xref py py-obj docutils literal notranslate"><span class="pre">complex</span></code></a></p></td>
<td><p>Constructs a complex tensor with its real part equal to <a class="reference internal" href="generated/torch.real.html#torch.real" title="torch.real"><code class="xref py py-attr docutils literal notranslate"><span class="pre">real</span></code></a> and its imaginary part equal to <a class="reference internal" href="generated/torch.imag.html#torch.imag" title="torch.imag"><code class="xref py py-attr docutils literal notranslate"><span class="pre">imag</span></code></a>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.polar"/><a class="reference internal" href="generated/torch.polar.html#torch.polar" title="torch.polar"><code class="xref py py-obj docutils literal notranslate"><span class="pre">polar</span></code></a></p></td>
<td><p>Constructs a complex tensor whose elements are Cartesian coordinates corresponding to the polar coordinates with absolute value <a class="reference internal" href="generated/torch.abs.html#torch.abs" title="torch.abs"><code class="xref py py-attr docutils literal notranslate"><span class="pre">abs</span></code></a> and angle <a class="reference internal" href="generated/torch.angle.html#torch.angle" title="torch.angle"><code class="xref py py-attr docutils literal notranslate"><span class="pre">angle</span></code></a>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.heaviside"/><a class="reference internal" href="generated/torch.heaviside.html#torch.heaviside" title="torch.heaviside"><code class="xref py py-obj docutils literal notranslate"><span class="pre">heaviside</span></code></a></p></td>
<td><p>Computes the Heaviside step function for each element in <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="indexing-slicing-joining-mutating-ops">
<span id="indexing-slicing-joining"></span><h3>Indexing, Slicing, Joining, Mutating Ops<a class="headerlink" href="#indexing-slicing-joining-mutating-ops" title="Permalink to this heading">#</a></h3>
<div class="pst-scrollable-table-container"><table class="autosummary longtable table autosummary">
<tbody>
<tr class="row-odd"><td><p><p id="torch.adjoint"/><a class="reference internal" href="generated/torch.adjoint.html#torch.adjoint" title="torch.adjoint"><code class="xref py py-obj docutils literal notranslate"><span class="pre">adjoint</span></code></a></p></td>
<td><p>Returns a view of the tensor conjugated and with the last two dimensions transposed.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.argwhere"/><a class="reference internal" href="generated/torch.argwhere.html#torch.argwhere" title="torch.argwhere"><code class="xref py py-obj docutils literal notranslate"><span class="pre">argwhere</span></code></a></p></td>
<td><p>Returns a tensor containing the indices of all non-zero elements of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.cat"/><a class="reference internal" href="generated/torch.cat.html#torch.cat" title="torch.cat"><code class="xref py py-obj docutils literal notranslate"><span class="pre">cat</span></code></a></p></td>
<td><p>Concatenates the given sequence of tensors in <code class="xref py py-attr docutils literal notranslate"><span class="pre">tensors</span></code> in the given dimension.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.concat"/><a class="reference internal" href="generated/torch.concat.html#torch.concat" title="torch.concat"><code class="xref py py-obj docutils literal notranslate"><span class="pre">concat</span></code></a></p></td>
<td><p>Alias of <a class="reference internal" href="generated/torch.cat.html#torch.cat" title="torch.cat"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.cat()</span></code></a>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.concatenate"/><a class="reference internal" href="generated/torch.concatenate.html#torch.concatenate" title="torch.concatenate"><code class="xref py py-obj docutils literal notranslate"><span class="pre">concatenate</span></code></a></p></td>
<td><p>Alias of <a class="reference internal" href="generated/torch.cat.html#torch.cat" title="torch.cat"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.cat()</span></code></a>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.conj"/><a class="reference internal" href="generated/torch.conj.html#torch.conj" title="torch.conj"><code class="xref py py-obj docutils literal notranslate"><span class="pre">conj</span></code></a></p></td>
<td><p>Returns a view of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> with a flipped conjugate bit.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.chunk"/><a class="reference internal" href="generated/torch.chunk.html#torch.chunk" title="torch.chunk"><code class="xref py py-obj docutils literal notranslate"><span class="pre">chunk</span></code></a></p></td>
<td><p>Attempts to split a tensor into the specified number of chunks.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.dsplit"/><a class="reference internal" href="generated/torch.dsplit.html#torch.dsplit" title="torch.dsplit"><code class="xref py py-obj docutils literal notranslate"><span class="pre">dsplit</span></code></a></p></td>
<td><p>Splits <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>, a tensor with three or more dimensions, into multiple tensors depthwise according to <code class="xref py py-attr docutils literal notranslate"><span class="pre">indices_or_sections</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.column_stack"/><a class="reference internal" href="generated/torch.column_stack.html#torch.column_stack" title="torch.column_stack"><code class="xref py py-obj docutils literal notranslate"><span class="pre">column_stack</span></code></a></p></td>
<td><p>Creates a new tensor by horizontally stacking the tensors in <code class="xref py py-attr docutils literal notranslate"><span class="pre">tensors</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.dstack"/><a class="reference internal" href="generated/torch.dstack.html#torch.dstack" title="torch.dstack"><code class="xref py py-obj docutils literal notranslate"><span class="pre">dstack</span></code></a></p></td>
<td><p>Stack tensors in sequence depthwise (along third axis).</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.gather"/><a class="reference internal" href="generated/torch.gather.html#torch.gather" title="torch.gather"><code class="xref py py-obj docutils literal notranslate"><span class="pre">gather</span></code></a></p></td>
<td><p>Gathers values along an axis specified by <cite>dim</cite>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.hsplit"/><a class="reference internal" href="generated/torch.hsplit.html#torch.hsplit" title="torch.hsplit"><code class="xref py py-obj docutils literal notranslate"><span class="pre">hsplit</span></code></a></p></td>
<td><p>Splits <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>, a tensor with one or more dimensions, into multiple tensors horizontally according to <code class="xref py py-attr docutils literal notranslate"><span class="pre">indices_or_sections</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.hstack"/><a class="reference internal" href="generated/torch.hstack.html#torch.hstack" title="torch.hstack"><code class="xref py py-obj docutils literal notranslate"><span class="pre">hstack</span></code></a></p></td>
<td><p>Stack tensors in sequence horizontally (column wise).</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.index_add"/><a class="reference internal" href="generated/torch.index_add.html#torch.index_add" title="torch.index_add"><code class="xref py py-obj docutils literal notranslate"><span class="pre">index_add</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.Tensor.index_add_.html#torch.Tensor.index_add_" title="torch.Tensor.index_add_"><code class="xref py py-meth docutils literal notranslate"><span class="pre">index_add_()</span></code></a> for function description.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.index_copy"/><a class="reference internal" href="generated/torch.index_copy.html#torch.index_copy" title="torch.index_copy"><code class="xref py py-obj docutils literal notranslate"><span class="pre">index_copy</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.Tensor.index_add_.html#torch.Tensor.index_add_" title="torch.Tensor.index_add_"><code class="xref py py-meth docutils literal notranslate"><span class="pre">index_add_()</span></code></a> for function description.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.index_reduce"/><a class="reference internal" href="generated/torch.index_reduce.html#torch.index_reduce" title="torch.index_reduce"><code class="xref py py-obj docutils literal notranslate"><span class="pre">index_reduce</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.Tensor.index_reduce_.html#torch.Tensor.index_reduce_" title="torch.Tensor.index_reduce_"><code class="xref py py-meth docutils literal notranslate"><span class="pre">index_reduce_()</span></code></a> for function description.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.index_select"/><a class="reference internal" href="generated/torch.index_select.html#torch.index_select" title="torch.index_select"><code class="xref py py-obj docutils literal notranslate"><span class="pre">index_select</span></code></a></p></td>
<td><p>Returns a new tensor which indexes the <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> tensor along dimension <code class="xref py py-attr docutils literal notranslate"><span class="pre">dim</span></code> using the entries in <code class="xref py py-attr docutils literal notranslate"><span class="pre">index</span></code> which is a <cite>LongTensor</cite>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.masked_select"/><a class="reference internal" href="generated/torch.masked_select.html#torch.masked_select" title="torch.masked_select"><code class="xref py py-obj docutils literal notranslate"><span class="pre">masked_select</span></code></a></p></td>
<td><p>Returns a new 1-D tensor which indexes the <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> tensor according to the boolean mask <code class="xref py py-attr docutils literal notranslate"><span class="pre">mask</span></code> which is a <cite>BoolTensor</cite>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.movedim"/><a class="reference internal" href="generated/torch.movedim.html#torch.movedim" title="torch.movedim"><code class="xref py py-obj docutils literal notranslate"><span class="pre">movedim</span></code></a></p></td>
<td><p>Moves the dimension(s) of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> at the position(s) in <code class="xref py py-attr docutils literal notranslate"><span class="pre">source</span></code> to the position(s) in <code class="xref py py-attr docutils literal notranslate"><span class="pre">destination</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.moveaxis"/><a class="reference internal" href="generated/torch.moveaxis.html#torch.moveaxis" title="torch.moveaxis"><code class="xref py py-obj docutils literal notranslate"><span class="pre">moveaxis</span></code></a></p></td>
<td><p>Alias for <a class="reference internal" href="generated/torch.movedim.html#torch.movedim" title="torch.movedim"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.movedim()</span></code></a>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.narrow"/><a class="reference internal" href="generated/torch.narrow.html#torch.narrow" title="torch.narrow"><code class="xref py py-obj docutils literal notranslate"><span class="pre">narrow</span></code></a></p></td>
<td><p>Returns a new tensor that is a narrowed version of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> tensor.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.narrow_copy"/><a class="reference internal" href="generated/torch.narrow_copy.html#torch.narrow_copy" title="torch.narrow_copy"><code class="xref py py-obj docutils literal notranslate"><span class="pre">narrow_copy</span></code></a></p></td>
<td><p>Same as <a class="reference internal" href="generated/torch.Tensor.narrow.html#torch.Tensor.narrow" title="torch.Tensor.narrow"><code class="xref py py-meth docutils literal notranslate"><span class="pre">Tensor.narrow()</span></code></a> except this returns a copy rather than shared storage.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.nonzero"/><a class="reference internal" href="generated/torch.nonzero.html#torch.nonzero" title="torch.nonzero"><code class="xref py py-obj docutils literal notranslate"><span class="pre">nonzero</span></code></a></p></td>
<td><p></p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.permute"/><a class="reference internal" href="generated/torch.permute.html#torch.permute" title="torch.permute"><code class="xref py py-obj docutils literal notranslate"><span class="pre">permute</span></code></a></p></td>
<td><p>Returns a view of the original tensor <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> with its dimensions permuted.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.reshape"/><a class="reference internal" href="generated/torch.reshape.html#torch.reshape" title="torch.reshape"><code class="xref py py-obj docutils literal notranslate"><span class="pre">reshape</span></code></a></p></td>
<td><p>Returns a tensor with the same data and number of elements as <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>, but with the specified shape.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.row_stack"/><a class="reference internal" href="generated/torch.row_stack.html#torch.row_stack" title="torch.row_stack"><code class="xref py py-obj docutils literal notranslate"><span class="pre">row_stack</span></code></a></p></td>
<td><p>Alias of <a class="reference internal" href="generated/torch.vstack.html#torch.vstack" title="torch.vstack"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.vstack()</span></code></a>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.select"/><a class="reference internal" href="generated/torch.select.html#torch.select" title="torch.select"><code class="xref py py-obj docutils literal notranslate"><span class="pre">select</span></code></a></p></td>
<td><p>Slices the <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> tensor along the selected dimension at the given index.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.scatter"/><a class="reference internal" href="generated/torch.scatter.html#torch.scatter" title="torch.scatter"><code class="xref py py-obj docutils literal notranslate"><span class="pre">scatter</span></code></a></p></td>
<td><p>Out-of-place version of <a class="reference internal" href="generated/torch.Tensor.scatter_.html#torch.Tensor.scatter_" title="torch.Tensor.scatter_"><code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.Tensor.scatter_()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.diagonal_scatter"/><a class="reference internal" href="generated/torch.diagonal_scatter.html#torch.diagonal_scatter" title="torch.diagonal_scatter"><code class="xref py py-obj docutils literal notranslate"><span class="pre">diagonal_scatter</span></code></a></p></td>
<td><p>Embeds the values of the <code class="xref py py-attr docutils literal notranslate"><span class="pre">src</span></code> tensor into <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> along the diagonal elements of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>, with respect to <code class="xref py py-attr docutils literal notranslate"><span class="pre">dim1</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">dim2</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.select_scatter"/><a class="reference internal" href="generated/torch.select_scatter.html#torch.select_scatter" title="torch.select_scatter"><code class="xref py py-obj docutils literal notranslate"><span class="pre">select_scatter</span></code></a></p></td>
<td><p>Embeds the values of the <code class="xref py py-attr docutils literal notranslate"><span class="pre">src</span></code> tensor into <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> at the given index.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.slice_scatter"/><a class="reference internal" href="generated/torch.slice_scatter.html#torch.slice_scatter" title="torch.slice_scatter"><code class="xref py py-obj docutils literal notranslate"><span class="pre">slice_scatter</span></code></a></p></td>
<td><p>Embeds the values of the <code class="xref py py-attr docutils literal notranslate"><span class="pre">src</span></code> tensor into <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> at the given dimension.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.scatter_add"/><a class="reference internal" href="generated/torch.scatter_add.html#torch.scatter_add" title="torch.scatter_add"><code class="xref py py-obj docutils literal notranslate"><span class="pre">scatter_add</span></code></a></p></td>
<td><p>Out-of-place version of <a class="reference internal" href="generated/torch.Tensor.scatter_add_.html#torch.Tensor.scatter_add_" title="torch.Tensor.scatter_add_"><code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.Tensor.scatter_add_()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.scatter_reduce"/><a class="reference internal" href="generated/torch.scatter_reduce.html#torch.scatter_reduce" title="torch.scatter_reduce"><code class="xref py py-obj docutils literal notranslate"><span class="pre">scatter_reduce</span></code></a></p></td>
<td><p>Out-of-place version of <a class="reference internal" href="generated/torch.Tensor.scatter_reduce_.html#torch.Tensor.scatter_reduce_" title="torch.Tensor.scatter_reduce_"><code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.Tensor.scatter_reduce_()</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.split"/><a class="reference internal" href="generated/torch.split.html#torch.split" title="torch.split"><code class="xref py py-obj docutils literal notranslate"><span class="pre">split</span></code></a></p></td>
<td><p>Splits the tensor into chunks.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.squeeze"/><a class="reference internal" href="generated/torch.squeeze.html#torch.squeeze" title="torch.squeeze"><code class="xref py py-obj docutils literal notranslate"><span class="pre">squeeze</span></code></a></p></td>
<td><p>Returns a tensor with all specified dimensions of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> of size <cite>1</cite> removed.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.stack"/><a class="reference internal" href="generated/torch.stack.html#torch.stack" title="torch.stack"><code class="xref py py-obj docutils literal notranslate"><span class="pre">stack</span></code></a></p></td>
<td><p>Concatenates a sequence of tensors along a new dimension.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.swapaxes"/><a class="reference internal" href="generated/torch.swapaxes.html#torch.swapaxes" title="torch.swapaxes"><code class="xref py py-obj docutils literal notranslate"><span class="pre">swapaxes</span></code></a></p></td>
<td><p>Alias for <a class="reference internal" href="generated/torch.transpose.html#torch.transpose" title="torch.transpose"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.transpose()</span></code></a>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.swapdims"/><a class="reference internal" href="generated/torch.swapdims.html#torch.swapdims" title="torch.swapdims"><code class="xref py py-obj docutils literal notranslate"><span class="pre">swapdims</span></code></a></p></td>
<td><p>Alias for <a class="reference internal" href="generated/torch.transpose.html#torch.transpose" title="torch.transpose"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.transpose()</span></code></a>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.t"/><a class="reference internal" href="generated/torch.t.html#torch.t" title="torch.t"><code class="xref py py-obj docutils literal notranslate"><span class="pre">t</span></code></a></p></td>
<td><p>Expects <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> to be &lt;= 2-D tensor and transposes dimensions 0 and 1.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.take"/><a class="reference internal" href="generated/torch.take.html#torch.take" title="torch.take"><code class="xref py py-obj docutils literal notranslate"><span class="pre">take</span></code></a></p></td>
<td><p>Returns a new tensor with the elements of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> at the given indices.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.take_along_dim"/><a class="reference internal" href="generated/torch.take_along_dim.html#torch.take_along_dim" title="torch.take_along_dim"><code class="xref py py-obj docutils literal notranslate"><span class="pre">take_along_dim</span></code></a></p></td>
<td><p>Selects values from <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> at the 1-dimensional indices from <code class="xref py py-attr docutils literal notranslate"><span class="pre">indices</span></code> along the given <code class="xref py py-attr docutils literal notranslate"><span class="pre">dim</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.tensor_split"/><a class="reference internal" href="generated/torch.tensor_split.html#torch.tensor_split" title="torch.tensor_split"><code class="xref py py-obj docutils literal notranslate"><span class="pre">tensor_split</span></code></a></p></td>
<td><p>Splits a tensor into multiple sub-tensors, all of which are views of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>, along dimension <code class="xref py py-attr docutils literal notranslate"><span class="pre">dim</span></code> according to the indices or number of sections specified by <code class="xref py py-attr docutils literal notranslate"><span class="pre">indices_or_sections</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.tile"/><a class="reference internal" href="generated/torch.tile.html#torch.tile" title="torch.tile"><code class="xref py py-obj docutils literal notranslate"><span class="pre">tile</span></code></a></p></td>
<td><p>Constructs a tensor by repeating the elements of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.transpose"/><a class="reference internal" href="generated/torch.transpose.html#torch.transpose" title="torch.transpose"><code class="xref py py-obj docutils literal notranslate"><span class="pre">transpose</span></code></a></p></td>
<td><p>Returns a tensor that is a transposed version of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.unbind"/><a class="reference internal" href="generated/torch.unbind.html#torch.unbind" title="torch.unbind"><code class="xref py py-obj docutils literal notranslate"><span class="pre">unbind</span></code></a></p></td>
<td><p>Removes a tensor dimension.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.unravel_index"/><a class="reference internal" href="generated/torch.unravel_index.html#torch.unravel_index" title="torch.unravel_index"><code class="xref py py-obj docutils literal notranslate"><span class="pre">unravel_index</span></code></a></p></td>
<td><p>Converts a tensor of flat indices into a tuple of coordinate tensors that index into an arbitrary tensor of the specified shape.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.unsqueeze"/><a class="reference internal" href="generated/torch.unsqueeze.html#torch.unsqueeze" title="torch.unsqueeze"><code class="xref py py-obj docutils literal notranslate"><span class="pre">unsqueeze</span></code></a></p></td>
<td><p>Returns a new tensor with a dimension of size one inserted at the specified position.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.vsplit"/><a class="reference internal" href="generated/torch.vsplit.html#torch.vsplit" title="torch.vsplit"><code class="xref py py-obj docutils literal notranslate"><span class="pre">vsplit</span></code></a></p></td>
<td><p>Splits <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>, a tensor with two or more dimensions, into multiple tensors vertically according to <code class="xref py py-attr docutils literal notranslate"><span class="pre">indices_or_sections</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.vstack"/><a class="reference internal" href="generated/torch.vstack.html#torch.vstack" title="torch.vstack"><code class="xref py py-obj docutils literal notranslate"><span class="pre">vstack</span></code></a></p></td>
<td><p>Stack tensors in sequence vertically (row wise).</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.where"/><a class="reference internal" href="generated/torch.where.html#torch.where" title="torch.where"><code class="xref py py-obj docutils literal notranslate"><span class="pre">where</span></code></a></p></td>
<td><p>Return a tensor of elements selected from either <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> or <code class="xref py py-attr docutils literal notranslate"><span class="pre">other</span></code>, depending on <code class="xref py py-attr docutils literal notranslate"><span class="pre">condition</span></code>.</p></td>
</tr>
</tbody>
</table>
</div>
</section>
</section>
<section id="accelerators">
<span id="id1"></span><h2>Accelerators<a class="headerlink" href="#accelerators" title="Permalink to this heading">#</a></h2>
<p>Within the PyTorch repo, we define an “Accelerator” as a <a class="reference internal" href="tensor_attributes.html#torch.device" title="torch.device"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.device</span></code></a> that is being used
alongside a CPU to speed up computation. These device use an asynchronous execution scheme,
using <a class="reference internal" href="generated/torch.Stream.html#torch.Stream" title="torch.Stream"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Stream</span></code></a> and <a class="reference internal" href="generated/torch.mtia.Event.html#torch.mtia.Event" title="torch.Event"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Event</span></code></a> as their main way to perform synchronization.
We also assume that only one such accelerator can be available at once on a given host. This allows
us to use the current accelerator as the default device for relevant concepts such as pinned memory,
Stream device_type, FSDP, etc.</p>
<p>As of today, accelerator devices are (in no particular order) <a class="reference internal" href="cuda.html"><span class="doc">“CUDA”</span></a>, <a class="reference internal" href="mtia.html"><span class="doc">“MTIA”</span></a>,
<a class="reference internal" href="xpu.html"><span class="doc">“XPU”</span></a>, <a class="reference internal" href="mps.html"><span class="doc">“MPS”</span></a>, “HPU”, and PrivateUse1 (many device not in the PyTorch repo itself).</p>
<p>Many tools in the PyTorch Ecosystem use fork to create subprocesses (for example dataloading
or intra-op parallelism), it is thus important to delay as much as possible any
operation that would prevent further forks. This is especially important here as most accelerator’s initialization has such effect.
In practice, you should keep in mind that checking <a class="reference internal" href="generated/torch.accelerator.current_accelerator.html#torch.accelerator.current_accelerator" title="torch.accelerator.current_accelerator"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.accelerator.current_accelerator()</span></code></a>
is a compile-time check by default, it is thus always fork-safe.
On the contrary, passing the <code class="docutils literal notranslate"><span class="pre">check_available=True</span></code> flag to this function or calling
<a class="reference internal" href="generated/torch.accelerator.is_available.html#torch.accelerator.is_available" title="torch.accelerator.is_available"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.accelerator.is_available()</span></code></a> will usually prevent later fork.</p>
<p>Some backends provide an experimental opt-in option to make the runtime availability
check fork-safe. When using the CUDA device <code class="docutils literal notranslate"><span class="pre">PYTORCH_NVML_BASED_CUDA_CHECK=1</span></code> can be
used for example.</p>
<div class="pst-scrollable-table-container"><table class="autosummary longtable table autosummary">
<tbody>
<tr class="row-odd"><td><p><p id="torch.Stream"/><a class="reference internal" href="generated/torch.Stream.html#torch.Stream" title="torch.Stream"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Stream</span></code></a></p></td>
<td><p>An in-order queue of executing the respective tasks asynchronously in first in first out (FIFO) order.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.mtia.Event"/><a class="reference internal" href="generated/torch.mtia.Event.html#torch.mtia.Event" title="torch.Event"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Event</span></code></a></p></td>
<td><p>Query and record Stream status to identify or control dependencies across Stream and measure timing.</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="generators">
<span id="id2"></span><h2>Generators<a class="headerlink" href="#generators" title="Permalink to this heading">#</a></h2>
<div class="pst-scrollable-table-container"><table class="autosummary longtable table autosummary">
<tbody>
<tr class="row-odd"><td><p><p id="torch.Generator"/><a class="reference internal" href="generated/torch.Generator.html#torch.Generator" title="torch.Generator"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Generator</span></code></a></p></td>
<td><p>Creates and returns a generator object that manages the state of the algorithm which produces pseudo random numbers.</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="random-sampling">
<span id="id3"></span><h2>Random sampling<a class="headerlink" href="#random-sampling" title="Permalink to this heading">#</a></h2>
<div class="pst-scrollable-table-container"><table class="autosummary longtable table autosummary">
<tbody>
<tr class="row-odd"><td><p><p id="torch.seed"/><a class="reference internal" href="generated/torch.seed.html#torch.seed" title="torch.seed"><code class="xref py py-obj docutils literal notranslate"><span class="pre">seed</span></code></a></p></td>
<td><p>Sets the seed for generating random numbers to a non-deterministic random number on all devices.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.manual_seed"/><a class="reference internal" href="generated/torch.manual_seed.html#torch.manual_seed" title="torch.manual_seed"><code class="xref py py-obj docutils literal notranslate"><span class="pre">manual_seed</span></code></a></p></td>
<td><p>Sets the seed for generating random numbers on all devices.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.initial_seed"/><a class="reference internal" href="generated/torch.initial_seed.html#torch.initial_seed" title="torch.initial_seed"><code class="xref py py-obj docutils literal notranslate"><span class="pre">initial_seed</span></code></a></p></td>
<td><p>Returns the initial seed for generating random numbers as a Python <cite>long</cite>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.get_rng_state"/><a class="reference internal" href="generated/torch.get_rng_state.html#torch.get_rng_state" title="torch.get_rng_state"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_rng_state</span></code></a></p></td>
<td><p>Returns the random number generator state as a <cite>torch.ByteTensor</cite>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.set_rng_state"/><a class="reference internal" href="generated/torch.set_rng_state.html#torch.set_rng_state" title="torch.set_rng_state"><code class="xref py py-obj docutils literal notranslate"><span class="pre">set_rng_state</span></code></a></p></td>
<td><p>Sets the random number generator state.</p></td>
</tr>
</tbody>
</table>
</div>
<dl class="py attribute">
<dt class="sig sig-object py" id="torch.torch.default_generator">
<span class="sig-prename descclassname"><span class="pre">torch.</span></span><span class="sig-name descname"><span class="pre">default_generator</span></span><em class="property"><span class="w"> </span><span class="pre">Returns</span> <span class="pre">the</span> <span class="pre">default</span> <span class="pre">CPU</span> <span class="pre">torch.Generator</span></em><a class="headerlink" href="#torch.torch.default_generator" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<div class="pst-scrollable-table-container"><table class="autosummary longtable table autosummary">
<tbody>
<tr class="row-odd"><td><p><p id="torch.bernoulli"/><a class="reference internal" href="generated/torch.bernoulli.html#torch.bernoulli" title="torch.bernoulli"><code class="xref py py-obj docutils literal notranslate"><span class="pre">bernoulli</span></code></a></p></td>
<td><p>Draws binary random numbers (0 or 1) from a Bernoulli distribution.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.multinomial"/><a class="reference internal" href="generated/torch.multinomial.html#torch.multinomial" title="torch.multinomial"><code class="xref py py-obj docutils literal notranslate"><span class="pre">multinomial</span></code></a></p></td>
<td><p>Returns a tensor where each row contains <code class="xref py py-attr docutils literal notranslate"><span class="pre">num_samples</span></code> indices sampled from the multinomial (a stricter definition would be multivariate, refer to <a class="reference internal" href="distributions.html#torch.distributions.multinomial.Multinomial" title="torch.distributions.multinomial.Multinomial"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.distributions.multinomial.Multinomial</span></code></a> for more details) probability distribution located in the corresponding row of tensor <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.normal"/><a class="reference internal" href="generated/torch.normal.html#torch.normal" title="torch.normal"><code class="xref py py-obj docutils literal notranslate"><span class="pre">normal</span></code></a></p></td>
<td><p>Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.poisson"/><a class="reference internal" href="generated/torch.poisson.html#torch.poisson" title="torch.poisson"><code class="xref py py-obj docutils literal notranslate"><span class="pre">poisson</span></code></a></p></td>
<td><p>Returns a tensor of the same size as <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> with each element sampled from a Poisson distribution with rate parameter given by the corresponding element in <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> i.e.,</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.rand"/><a class="reference internal" href="generated/torch.rand.html#torch.rand" title="torch.rand"><code class="xref py py-obj docutils literal notranslate"><span class="pre">rand</span></code></a></p></td>
<td><p>Returns a tensor filled with random numbers from a uniform distribution on the interval <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">[</mo><mn>0</mn><mo separator="true">,</mo><mn>1</mn><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">[0, 1)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">[</span><span class="mord">0</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord">1</span><span class="mclose">)</span></span></span></span></span></p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.rand_like"/><a class="reference internal" href="generated/torch.rand_like.html#torch.rand_like" title="torch.rand_like"><code class="xref py py-obj docutils literal notranslate"><span class="pre">rand_like</span></code></a></p></td>
<td><p>Returns a tensor with the same size as <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> that is filled with random numbers from a uniform distribution on the interval <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">[</mo><mn>0</mn><mo separator="true">,</mo><mn>1</mn><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">[0, 1)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">[</span><span class="mord">0</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord">1</span><span class="mclose">)</span></span></span></span></span>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.randint"/><a class="reference internal" href="generated/torch.randint.html#torch.randint" title="torch.randint"><code class="xref py py-obj docutils literal notranslate"><span class="pre">randint</span></code></a></p></td>
<td><p>Returns a tensor filled with random integers generated uniformly between <code class="xref py py-attr docutils literal notranslate"><span class="pre">low</span></code> (inclusive) and <code class="xref py py-attr docutils literal notranslate"><span class="pre">high</span></code> (exclusive).</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.randint_like"/><a class="reference internal" href="generated/torch.randint_like.html#torch.randint_like" title="torch.randint_like"><code class="xref py py-obj docutils literal notranslate"><span class="pre">randint_like</span></code></a></p></td>
<td><p>Returns a tensor with the same shape as Tensor <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> filled with random integers generated uniformly between <code class="xref py py-attr docutils literal notranslate"><span class="pre">low</span></code> (inclusive) and <code class="xref py py-attr docutils literal notranslate"><span class="pre">high</span></code> (exclusive).</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.randn"/><a class="reference internal" href="generated/torch.randn.html#torch.randn" title="torch.randn"><code class="xref py py-obj docutils literal notranslate"><span class="pre">randn</span></code></a></p></td>
<td><p>Returns a tensor filled with random numbers from a normal distribution with mean <cite>0</cite> and variance <cite>1</cite> (also called the standard normal distribution).</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.randn_like"/><a class="reference internal" href="generated/torch.randn_like.html#torch.randn_like" title="torch.randn_like"><code class="xref py py-obj docutils literal notranslate"><span class="pre">randn_like</span></code></a></p></td>
<td><p>Returns a tensor with the same size as <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> that is filled with random numbers from a normal distribution with mean 0 and variance 1.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.randperm"/><a class="reference internal" href="generated/torch.randperm.html#torch.randperm" title="torch.randperm"><code class="xref py py-obj docutils literal notranslate"><span class="pre">randperm</span></code></a></p></td>
<td><p>Returns a random permutation of integers from <code class="docutils literal notranslate"><span class="pre">0</span></code> to <code class="docutils literal notranslate"><span class="pre">n</span> <span class="pre">-</span> <span class="pre">1</span></code>.</p></td>
</tr>
</tbody>
</table>
</div>
<section id="in-place-random-sampling">
<span id="inplace-random-sampling"></span><h3>In-place random sampling<a class="headerlink" href="#in-place-random-sampling" title="Permalink to this heading">#</a></h3>
<p>There are a few more in-place random sampling functions defined on Tensors as well. Click through to refer to their documentation:</p>
<ul class="simple">
<li><p><a class="reference internal" href="generated/torch.Tensor.bernoulli_.html#torch.Tensor.bernoulli_" title="torch.Tensor.bernoulli_"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.Tensor.bernoulli_()</span></code></a> - in-place version of <a class="reference internal" href="generated/torch.bernoulli.html#torch.bernoulli" title="torch.bernoulli"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.bernoulli()</span></code></a></p></li>
<li><p><a class="reference internal" href="generated/torch.Tensor.cauchy_.html#torch.Tensor.cauchy_" title="torch.Tensor.cauchy_"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.Tensor.cauchy_()</span></code></a> - numbers drawn from the Cauchy distribution</p></li>
<li><p><a class="reference internal" href="generated/torch.Tensor.exponential_.html#torch.Tensor.exponential_" title="torch.Tensor.exponential_"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.Tensor.exponential_()</span></code></a> - numbers drawn from the exponential distribution</p></li>
<li><p><a class="reference internal" href="generated/torch.Tensor.geometric_.html#torch.Tensor.geometric_" title="torch.Tensor.geometric_"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.Tensor.geometric_()</span></code></a> - elements drawn from the geometric distribution</p></li>
<li><p><a class="reference internal" href="generated/torch.Tensor.log_normal_.html#torch.Tensor.log_normal_" title="torch.Tensor.log_normal_"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.Tensor.log_normal_()</span></code></a> - samples from the log-normal distribution</p></li>
<li><p><a class="reference internal" href="generated/torch.Tensor.normal_.html#torch.Tensor.normal_" title="torch.Tensor.normal_"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.Tensor.normal_()</span></code></a> - in-place version of <a class="reference internal" href="generated/torch.normal.html#torch.normal" title="torch.normal"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.normal()</span></code></a></p></li>
<li><p><a class="reference internal" href="generated/torch.Tensor.random_.html#torch.Tensor.random_" title="torch.Tensor.random_"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.Tensor.random_()</span></code></a> - numbers sampled from the discrete uniform distribution</p></li>
<li><p><a class="reference internal" href="generated/torch.Tensor.uniform_.html#torch.Tensor.uniform_" title="torch.Tensor.uniform_"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.Tensor.uniform_()</span></code></a> - numbers sampled from the continuous uniform distribution</p></li>
</ul>
</section>
<section id="quasi-random-sampling">
<h3>Quasi-random sampling<a class="headerlink" href="#quasi-random-sampling" title="Permalink to this heading">#</a></h3>
<div class="pst-scrollable-table-container"><table class="autosummary longtable table autosummary">
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.quasirandom.SobolEngine.html#torch.quasirandom.SobolEngine" title="torch.quasirandom.SobolEngine"><code class="xref py py-obj docutils literal notranslate"><span class="pre">quasirandom.SobolEngine</span></code></a></p></td>
<td><p>The <a class="reference internal" href="generated/torch.quasirandom.SobolEngine.html#torch.quasirandom.SobolEngine" title="torch.quasirandom.SobolEngine"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.quasirandom.SobolEngine</span></code></a> is an engine for generating (scrambled) Sobol sequences.</p></td>
</tr>
</tbody>
</table>
</div>
</section>
</section>
<section id="serialization">
<h2>Serialization<a class="headerlink" href="#serialization" title="Permalink to this heading">#</a></h2>
<div class="pst-scrollable-table-container"><table class="autosummary longtable table autosummary">
<tbody>
<tr class="row-odd"><td><p><p id="torch.save"/><a class="reference internal" href="generated/torch.save.html#torch.save" title="torch.save"><code class="xref py py-obj docutils literal notranslate"><span class="pre">save</span></code></a></p></td>
<td><p>Saves an object to a disk file.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.load"/><a class="reference internal" href="generated/torch.load.html#torch.load" title="torch.load"><code class="xref py py-obj docutils literal notranslate"><span class="pre">load</span></code></a></p></td>
<td><p>Loads an object saved with <a class="reference internal" href="generated/torch.save.html#torch.save" title="torch.save"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.save()</span></code></a> from a file.</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="parallelism">
<h2>Parallelism<a class="headerlink" href="#parallelism" title="Permalink to this heading">#</a></h2>
<div class="pst-scrollable-table-container"><table class="autosummary longtable table autosummary">
<tbody>
<tr class="row-odd"><td><p><p id="torch.get_num_threads"/><a class="reference internal" href="generated/torch.get_num_threads.html#torch.get_num_threads" title="torch.get_num_threads"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_num_threads</span></code></a></p></td>
<td><p>Returns the number of threads used for parallelizing CPU operations</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.set_num_threads"/><a class="reference internal" href="generated/torch.set_num_threads.html#torch.set_num_threads" title="torch.set_num_threads"><code class="xref py py-obj docutils literal notranslate"><span class="pre">set_num_threads</span></code></a></p></td>
<td><p>Sets the number of threads used for intraop parallelism on CPU.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.get_num_interop_threads"/><a class="reference internal" href="generated/torch.get_num_interop_threads.html#torch.get_num_interop_threads" title="torch.get_num_interop_threads"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_num_interop_threads</span></code></a></p></td>
<td><p>Returns the number of threads used for inter-op parallelism on CPU (e.g.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.set_num_interop_threads"/><a class="reference internal" href="generated/torch.set_num_interop_threads.html#torch.set_num_interop_threads" title="torch.set_num_interop_threads"><code class="xref py py-obj docutils literal notranslate"><span class="pre">set_num_interop_threads</span></code></a></p></td>
<td><p>Sets the number of threads used for interop parallelism (e.g.</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="locally-disabling-gradient-computation">
<span id="torch-rst-local-disable-grad"></span><h2>Locally disabling gradient computation<a class="headerlink" href="#locally-disabling-gradient-computation" title="Permalink to this heading">#</a></h2>
<p>The context managers <a class="reference internal" href="generated/torch.no_grad.html#torch.no_grad" title="torch.no_grad"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.no_grad()</span></code></a>, <a class="reference internal" href="generated/torch.enable_grad.html#torch.enable_grad" title="torch.enable_grad"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.enable_grad()</span></code></a>, and
<code class="xref py py-func docutils literal notranslate"><span class="pre">torch.set_grad_enabled()</span></code> are helpful for locally disabling and enabling
gradient computation. See <a class="reference internal" href="autograd.html#locally-disable-grad"><span class="std std-ref">Locally disabling gradient computation</span></a> for more details on
their usage.  These context managers are thread local, so they won’t
work if you send work to another thread using the <code class="docutils literal notranslate"><span class="pre">threading</span></code> module, etc.</p>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
<span class="gp">... </span>    <span class="n">y</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="mi">2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span><span class="o">.</span><span class="n">requires_grad</span>
<span class="go">False</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">is_train</span> <span class="o">=</span> <span class="kc">False</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">set_grad_enabled</span><span class="p">(</span><span class="n">is_train</span><span class="p">):</span>
<span class="gp">... </span>    <span class="n">y</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="mi">2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span><span class="o">.</span><span class="n">requires_grad</span>
<span class="go">False</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">set_grad_enabled</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>  <span class="c1"># this can also be used as a function</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="mi">2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span><span class="o">.</span><span class="n">requires_grad</span>
<span class="go">True</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">set_grad_enabled</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="mi">2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span><span class="o">.</span><span class="n">requires_grad</span>
<span class="go">False</span>
</pre></div>
</div>
<div class="pst-scrollable-table-container"><table class="autosummary longtable table autosummary">
<tbody>
<tr class="row-odd"><td><p><p id="torch.no_grad"/><a class="reference internal" href="generated/torch.no_grad.html#torch.no_grad" title="torch.no_grad"><code class="xref py py-obj docutils literal notranslate"><span class="pre">no_grad</span></code></a></p></td>
<td><p>Context-manager that disables gradient calculation.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.enable_grad"/><a class="reference internal" href="generated/torch.enable_grad.html#torch.enable_grad" title="torch.enable_grad"><code class="xref py py-obj docutils literal notranslate"><span class="pre">enable_grad</span></code></a></p></td>
<td><p>Context-manager that enables gradient calculation.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.autograd.grad_mode.set_grad_enabled.html#torch.autograd.grad_mode.set_grad_enabled" title="torch.autograd.grad_mode.set_grad_enabled"><code class="xref py py-obj docutils literal notranslate"><span class="pre">autograd.grad_mode.set_grad_enabled</span></code></a></p></td>
<td><p>Context-manager that sets gradient calculation on or off.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.is_grad_enabled"/><a class="reference internal" href="generated/torch.is_grad_enabled.html#torch.is_grad_enabled" title="torch.is_grad_enabled"><code class="xref py py-obj docutils literal notranslate"><span class="pre">is_grad_enabled</span></code></a></p></td>
<td><p>Returns True if grad mode is currently enabled.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.autograd.grad_mode.inference_mode.html#torch.autograd.grad_mode.inference_mode" title="torch.autograd.grad_mode.inference_mode"><code class="xref py py-obj docutils literal notranslate"><span class="pre">autograd.grad_mode.inference_mode</span></code></a></p></td>
<td><p>Context-manager that enables or disables inference mode.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.is_inference_mode_enabled"/><a class="reference internal" href="generated/torch.is_inference_mode_enabled.html#torch.is_inference_mode_enabled" title="torch.is_inference_mode_enabled"><code class="xref py py-obj docutils literal notranslate"><span class="pre">is_inference_mode_enabled</span></code></a></p></td>
<td><p>Returns True if inference mode is currently enabled.</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="math-operations">
<h2>Math operations<a class="headerlink" href="#math-operations" title="Permalink to this heading">#</a></h2>
<section id="constants">
<h3>Constants<a class="headerlink" href="#constants" title="Permalink to this heading">#</a></h3>
<div class="pst-scrollable-table-container"><table class="table">
<tbody>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">inf</span></code></p></td>
<td><p>A floating-point positive infinity. Alias for <code class="xref py py-attr docutils literal notranslate"><span class="pre">math.inf</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">nan</span></code></p></td>
<td><p>A floating-point “not a number” value. This value is not a legal number. Alias for <code class="xref py py-attr docutils literal notranslate"><span class="pre">math.nan</span></code>.</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="pointwise-ops">
<h3>Pointwise Ops<a class="headerlink" href="#pointwise-ops" title="Permalink to this heading">#</a></h3>
<div class="pst-scrollable-table-container"><table class="autosummary longtable table autosummary">
<tbody>
<tr class="row-odd"><td><p><p id="torch.abs"/><a class="reference internal" href="generated/torch.abs.html#torch.abs" title="torch.abs"><code class="xref py py-obj docutils literal notranslate"><span class="pre">abs</span></code></a></p></td>
<td><p>Computes the absolute value of each element in <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.absolute"/><a class="reference internal" href="generated/torch.absolute.html#torch.absolute" title="torch.absolute"><code class="xref py py-obj docutils literal notranslate"><span class="pre">absolute</span></code></a></p></td>
<td><p>Alias for <a class="reference internal" href="generated/torch.abs.html#torch.abs" title="torch.abs"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.abs()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.acos"/><a class="reference internal" href="generated/torch.acos.html#torch.acos" title="torch.acos"><code class="xref py py-obj docutils literal notranslate"><span class="pre">acos</span></code></a></p></td>
<td><p>Computes the inverse cosine of each element in <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.arccos"/><a class="reference internal" href="generated/torch.arccos.html#torch.arccos" title="torch.arccos"><code class="xref py py-obj docutils literal notranslate"><span class="pre">arccos</span></code></a></p></td>
<td><p>Alias for <a class="reference internal" href="generated/torch.acos.html#torch.acos" title="torch.acos"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.acos()</span></code></a>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.acosh"/><a class="reference internal" href="generated/torch.acosh.html#torch.acosh" title="torch.acosh"><code class="xref py py-obj docutils literal notranslate"><span class="pre">acosh</span></code></a></p></td>
<td><p>Returns a new tensor with the inverse hyperbolic cosine of the elements of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.arccosh"/><a class="reference internal" href="generated/torch.arccosh.html#torch.arccosh" title="torch.arccosh"><code class="xref py py-obj docutils literal notranslate"><span class="pre">arccosh</span></code></a></p></td>
<td><p>Alias for <a class="reference internal" href="generated/torch.acosh.html#torch.acosh" title="torch.acosh"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.acosh()</span></code></a>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.add"/><a class="reference internal" href="generated/torch.add.html#torch.add" title="torch.add"><code class="xref py py-obj docutils literal notranslate"><span class="pre">add</span></code></a></p></td>
<td><p>Adds <code class="xref py py-attr docutils literal notranslate"><span class="pre">other</span></code>, scaled by <code class="xref py py-attr docutils literal notranslate"><span class="pre">alpha</span></code>, to <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.addcdiv"/><a class="reference internal" href="generated/torch.addcdiv.html#torch.addcdiv" title="torch.addcdiv"><code class="xref py py-obj docutils literal notranslate"><span class="pre">addcdiv</span></code></a></p></td>
<td><p>Performs the element-wise division of <code class="xref py py-attr docutils literal notranslate"><span class="pre">tensor1</span></code> by <code class="xref py py-attr docutils literal notranslate"><span class="pre">tensor2</span></code>, multiplies the result by the scalar <code class="xref py py-attr docutils literal notranslate"><span class="pre">value</span></code> and adds it to <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.addcmul"/><a class="reference internal" href="generated/torch.addcmul.html#torch.addcmul" title="torch.addcmul"><code class="xref py py-obj docutils literal notranslate"><span class="pre">addcmul</span></code></a></p></td>
<td><p>Performs the element-wise multiplication of <code class="xref py py-attr docutils literal notranslate"><span class="pre">tensor1</span></code> by <code class="xref py py-attr docutils literal notranslate"><span class="pre">tensor2</span></code>, multiplies the result by the scalar <code class="xref py py-attr docutils literal notranslate"><span class="pre">value</span></code> and adds it to <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.angle"/><a class="reference internal" href="generated/torch.angle.html#torch.angle" title="torch.angle"><code class="xref py py-obj docutils literal notranslate"><span class="pre">angle</span></code></a></p></td>
<td><p>Computes the element-wise angle (in radians) of the given <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> tensor.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.asin"/><a class="reference internal" href="generated/torch.asin.html#torch.asin" title="torch.asin"><code class="xref py py-obj docutils literal notranslate"><span class="pre">asin</span></code></a></p></td>
<td><p>Returns a new tensor with the arcsine of the elements of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.arcsin"/><a class="reference internal" href="generated/torch.arcsin.html#torch.arcsin" title="torch.arcsin"><code class="xref py py-obj docutils literal notranslate"><span class="pre">arcsin</span></code></a></p></td>
<td><p>Alias for <a class="reference internal" href="generated/torch.asin.html#torch.asin" title="torch.asin"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.asin()</span></code></a>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.asinh"/><a class="reference internal" href="generated/torch.asinh.html#torch.asinh" title="torch.asinh"><code class="xref py py-obj docutils literal notranslate"><span class="pre">asinh</span></code></a></p></td>
<td><p>Returns a new tensor with the inverse hyperbolic sine of the elements of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.arcsinh"/><a class="reference internal" href="generated/torch.arcsinh.html#torch.arcsinh" title="torch.arcsinh"><code class="xref py py-obj docutils literal notranslate"><span class="pre">arcsinh</span></code></a></p></td>
<td><p>Alias for <a class="reference internal" href="generated/torch.asinh.html#torch.asinh" title="torch.asinh"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.asinh()</span></code></a>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.atan"/><a class="reference internal" href="generated/torch.atan.html#torch.atan" title="torch.atan"><code class="xref py py-obj docutils literal notranslate"><span class="pre">atan</span></code></a></p></td>
<td><p>Returns a new tensor with the arctangent of the elements of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.arctan"/><a class="reference internal" href="generated/torch.arctan.html#torch.arctan" title="torch.arctan"><code class="xref py py-obj docutils literal notranslate"><span class="pre">arctan</span></code></a></p></td>
<td><p>Alias for <a class="reference internal" href="generated/torch.atan.html#torch.atan" title="torch.atan"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.atan()</span></code></a>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.atanh"/><a class="reference internal" href="generated/torch.atanh.html#torch.atanh" title="torch.atanh"><code class="xref py py-obj docutils literal notranslate"><span class="pre">atanh</span></code></a></p></td>
<td><p>Returns a new tensor with the inverse hyperbolic tangent of the elements of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.arctanh"/><a class="reference internal" href="generated/torch.arctanh.html#torch.arctanh" title="torch.arctanh"><code class="xref py py-obj docutils literal notranslate"><span class="pre">arctanh</span></code></a></p></td>
<td><p>Alias for <a class="reference internal" href="generated/torch.atanh.html#torch.atanh" title="torch.atanh"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.atanh()</span></code></a>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.atan2"/><a class="reference internal" href="generated/torch.atan2.html#torch.atan2" title="torch.atan2"><code class="xref py py-obj docutils literal notranslate"><span class="pre">atan2</span></code></a></p></td>
<td><p>Element-wise arctangent of <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mtext>input</mtext><mi>i</mi></msub><mi mathvariant="normal">/</mi><msub><mtext>other</mtext><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">\text{input}_{i} / \text{other}_{i}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord text"><span class="mord">input</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2175em;"><span style="top:-2.4559em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2441em;"><span></span></span></span></span></span></span><span class="mord">/</span><span class="mord"><span class="mord text"><span class="mord">other</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></span> with consideration of the quadrant.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.arctan2"/><a class="reference internal" href="generated/torch.arctan2.html#torch.arctan2" title="torch.arctan2"><code class="xref py py-obj docutils literal notranslate"><span class="pre">arctan2</span></code></a></p></td>
<td><p>Alias for <a class="reference internal" href="generated/torch.atan2.html#torch.atan2" title="torch.atan2"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.atan2()</span></code></a>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.bitwise_not"/><a class="reference internal" href="generated/torch.bitwise_not.html#torch.bitwise_not" title="torch.bitwise_not"><code class="xref py py-obj docutils literal notranslate"><span class="pre">bitwise_not</span></code></a></p></td>
<td><p>Computes the bitwise NOT of the given input tensor.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.bitwise_and"/><a class="reference internal" href="generated/torch.bitwise_and.html#torch.bitwise_and" title="torch.bitwise_and"><code class="xref py py-obj docutils literal notranslate"><span class="pre">bitwise_and</span></code></a></p></td>
<td><p>Computes the bitwise AND of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">other</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.bitwise_or"/><a class="reference internal" href="generated/torch.bitwise_or.html#torch.bitwise_or" title="torch.bitwise_or"><code class="xref py py-obj docutils literal notranslate"><span class="pre">bitwise_or</span></code></a></p></td>
<td><p>Computes the bitwise OR of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">other</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.bitwise_xor"/><a class="reference internal" href="generated/torch.bitwise_xor.html#torch.bitwise_xor" title="torch.bitwise_xor"><code class="xref py py-obj docutils literal notranslate"><span class="pre">bitwise_xor</span></code></a></p></td>
<td><p>Computes the bitwise XOR of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">other</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.bitwise_left_shift"/><a class="reference internal" href="generated/torch.bitwise_left_shift.html#torch.bitwise_left_shift" title="torch.bitwise_left_shift"><code class="xref py py-obj docutils literal notranslate"><span class="pre">bitwise_left_shift</span></code></a></p></td>
<td><p>Computes the left arithmetic shift of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> by <code class="xref py py-attr docutils literal notranslate"><span class="pre">other</span></code> bits.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.bitwise_right_shift"/><a class="reference internal" href="generated/torch.bitwise_right_shift.html#torch.bitwise_right_shift" title="torch.bitwise_right_shift"><code class="xref py py-obj docutils literal notranslate"><span class="pre">bitwise_right_shift</span></code></a></p></td>
<td><p>Computes the right arithmetic shift of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> by <code class="xref py py-attr docutils literal notranslate"><span class="pre">other</span></code> bits.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.ceil"/><a class="reference internal" href="generated/torch.ceil.html#torch.ceil" title="torch.ceil"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ceil</span></code></a></p></td>
<td><p>Returns a new tensor with the ceil of the elements of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>, the smallest integer greater than or equal to each element.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.clamp"/><a class="reference internal" href="generated/torch.clamp.html#torch.clamp" title="torch.clamp"><code class="xref py py-obj docutils literal notranslate"><span class="pre">clamp</span></code></a></p></td>
<td><p>Clamps all elements in <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> into the range <cite>[</cite> <a class="reference internal" href="generated/torch.min.html#torch.min" title="torch.min"><code class="xref py py-attr docutils literal notranslate"><span class="pre">min</span></code></a>, <a class="reference internal" href="generated/torch.max.html#torch.max" title="torch.max"><code class="xref py py-attr docutils literal notranslate"><span class="pre">max</span></code></a> <cite>]</cite>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.clip"/><a class="reference internal" href="generated/torch.clip.html#torch.clip" title="torch.clip"><code class="xref py py-obj docutils literal notranslate"><span class="pre">clip</span></code></a></p></td>
<td><p>Alias for <a class="reference internal" href="generated/torch.clamp.html#torch.clamp" title="torch.clamp"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.clamp()</span></code></a>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.conj_physical"/><a class="reference internal" href="generated/torch.conj_physical.html#torch.conj_physical" title="torch.conj_physical"><code class="xref py py-obj docutils literal notranslate"><span class="pre">conj_physical</span></code></a></p></td>
<td><p>Computes the element-wise conjugate of the given <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> tensor.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.copysign"/><a class="reference internal" href="generated/torch.copysign.html#torch.copysign" title="torch.copysign"><code class="xref py py-obj docutils literal notranslate"><span class="pre">copysign</span></code></a></p></td>
<td><p>Create a new floating-point tensor with the magnitude of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> and the sign of <code class="xref py py-attr docutils literal notranslate"><span class="pre">other</span></code>, elementwise.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.cos"/><a class="reference internal" href="generated/torch.cos.html#torch.cos" title="torch.cos"><code class="xref py py-obj docutils literal notranslate"><span class="pre">cos</span></code></a></p></td>
<td><p>Returns a new tensor with the cosine  of the elements of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.cosh"/><a class="reference internal" href="generated/torch.cosh.html#torch.cosh" title="torch.cosh"><code class="xref py py-obj docutils literal notranslate"><span class="pre">cosh</span></code></a></p></td>
<td><p>Returns a new tensor with the hyperbolic cosine  of the elements of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.deg2rad"/><a class="reference internal" href="generated/torch.deg2rad.html#torch.deg2rad" title="torch.deg2rad"><code class="xref py py-obj docutils literal notranslate"><span class="pre">deg2rad</span></code></a></p></td>
<td><p>Returns a new tensor with each of the elements of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> converted from angles in degrees to radians.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.div"/><a class="reference internal" href="generated/torch.div.html#torch.div" title="torch.div"><code class="xref py py-obj docutils literal notranslate"><span class="pre">div</span></code></a></p></td>
<td><p>Divides each element of the input <code class="docutils literal notranslate"><span class="pre">input</span></code> by the corresponding element of <code class="xref py py-attr docutils literal notranslate"><span class="pre">other</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.divide"/><a class="reference internal" href="generated/torch.divide.html#torch.divide" title="torch.divide"><code class="xref py py-obj docutils literal notranslate"><span class="pre">divide</span></code></a></p></td>
<td><p>Alias for <a class="reference internal" href="generated/torch.div.html#torch.div" title="torch.div"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.div()</span></code></a>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.digamma"/><a class="reference internal" href="generated/torch.digamma.html#torch.digamma" title="torch.digamma"><code class="xref py py-obj docutils literal notranslate"><span class="pre">digamma</span></code></a></p></td>
<td><p>Alias for <a class="reference internal" href="special.html#torch.special.digamma" title="torch.special.digamma"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.special.digamma()</span></code></a>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.erf"/><a class="reference internal" href="generated/torch.erf.html#torch.erf" title="torch.erf"><code class="xref py py-obj docutils literal notranslate"><span class="pre">erf</span></code></a></p></td>
<td><p>Alias for <a class="reference internal" href="special.html#torch.special.erf" title="torch.special.erf"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.special.erf()</span></code></a>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.erfc"/><a class="reference internal" href="generated/torch.erfc.html#torch.erfc" title="torch.erfc"><code class="xref py py-obj docutils literal notranslate"><span class="pre">erfc</span></code></a></p></td>
<td><p>Alias for <a class="reference internal" href="special.html#torch.special.erfc" title="torch.special.erfc"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.special.erfc()</span></code></a>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.erfinv"/><a class="reference internal" href="generated/torch.erfinv.html#torch.erfinv" title="torch.erfinv"><code class="xref py py-obj docutils literal notranslate"><span class="pre">erfinv</span></code></a></p></td>
<td><p>Alias for <a class="reference internal" href="special.html#torch.special.erfinv" title="torch.special.erfinv"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.special.erfinv()</span></code></a>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.exp"/><a class="reference internal" href="generated/torch.exp.html#torch.exp" title="torch.exp"><code class="xref py py-obj docutils literal notranslate"><span class="pre">exp</span></code></a></p></td>
<td><p>Returns a new tensor with the exponential of the elements of the input tensor <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.exp2"/><a class="reference internal" href="generated/torch.exp2.html#torch.exp2" title="torch.exp2"><code class="xref py py-obj docutils literal notranslate"><span class="pre">exp2</span></code></a></p></td>
<td><p>Alias for <a class="reference internal" href="special.html#torch.special.exp2" title="torch.special.exp2"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.special.exp2()</span></code></a>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.expm1"/><a class="reference internal" href="generated/torch.expm1.html#torch.expm1" title="torch.expm1"><code class="xref py py-obj docutils literal notranslate"><span class="pre">expm1</span></code></a></p></td>
<td><p>Alias for <a class="reference internal" href="special.html#torch.special.expm1" title="torch.special.expm1"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.special.expm1()</span></code></a>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.fake_quantize_per_channel_affine"/><a class="reference internal" href="generated/torch.fake_quantize_per_channel_affine.html#torch.fake_quantize_per_channel_affine" title="torch.fake_quantize_per_channel_affine"><code class="xref py py-obj docutils literal notranslate"><span class="pre">fake_quantize_per_channel_affine</span></code></a></p></td>
<td><p>Returns a new tensor with the data in <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> fake quantized per channel using <code class="xref py py-attr docutils literal notranslate"><span class="pre">scale</span></code>, <code class="xref py py-attr docutils literal notranslate"><span class="pre">zero_point</span></code>, <code class="xref py py-attr docutils literal notranslate"><span class="pre">quant_min</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">quant_max</span></code>, across the channel specified by <code class="xref py py-attr docutils literal notranslate"><span class="pre">axis</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.fake_quantize_per_tensor_affine"/><a class="reference internal" href="generated/torch.fake_quantize_per_tensor_affine.html#torch.fake_quantize_per_tensor_affine" title="torch.fake_quantize_per_tensor_affine"><code class="xref py py-obj docutils literal notranslate"><span class="pre">fake_quantize_per_tensor_affine</span></code></a></p></td>
<td><p>Returns a new tensor with the data in <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> fake quantized using <code class="xref py py-attr docutils literal notranslate"><span class="pre">scale</span></code>, <code class="xref py py-attr docutils literal notranslate"><span class="pre">zero_point</span></code>, <code class="xref py py-attr docutils literal notranslate"><span class="pre">quant_min</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">quant_max</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.fix"/><a class="reference internal" href="generated/torch.fix.html#torch.fix" title="torch.fix"><code class="xref py py-obj docutils literal notranslate"><span class="pre">fix</span></code></a></p></td>
<td><p>Alias for <a class="reference internal" href="generated/torch.trunc.html#torch.trunc" title="torch.trunc"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.trunc()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.float_power"/><a class="reference internal" href="generated/torch.float_power.html#torch.float_power" title="torch.float_power"><code class="xref py py-obj docutils literal notranslate"><span class="pre">float_power</span></code></a></p></td>
<td><p>Raises <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> to the power of <code class="xref py py-attr docutils literal notranslate"><span class="pre">exponent</span></code>, elementwise, in double precision.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.floor"/><a class="reference internal" href="generated/torch.floor.html#torch.floor" title="torch.floor"><code class="xref py py-obj docutils literal notranslate"><span class="pre">floor</span></code></a></p></td>
<td><p>Returns a new tensor with the floor of the elements of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>, the largest integer less than or equal to each element.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.floor_divide"/><a class="reference internal" href="generated/torch.floor_divide.html#torch.floor_divide" title="torch.floor_divide"><code class="xref py py-obj docutils literal notranslate"><span class="pre">floor_divide</span></code></a></p></td>
<td><p></p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.fmod"/><a class="reference internal" href="generated/torch.fmod.html#torch.fmod" title="torch.fmod"><code class="xref py py-obj docutils literal notranslate"><span class="pre">fmod</span></code></a></p></td>
<td><p>Applies C++'s <a class="reference external" href="https://en.cppreference.com/w/cpp/numeric/math/fmod">std::fmod</a> entrywise.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.frac"/><a class="reference internal" href="generated/torch.frac.html#torch.frac" title="torch.frac"><code class="xref py py-obj docutils literal notranslate"><span class="pre">frac</span></code></a></p></td>
<td><p>Computes the fractional portion of each element in <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.frexp"/><a class="reference internal" href="generated/torch.frexp.html#torch.frexp" title="torch.frexp"><code class="xref py py-obj docutils literal notranslate"><span class="pre">frexp</span></code></a></p></td>
<td><p>Decomposes <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> into mantissa and exponent tensors such that <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>input</mtext><mo>=</mo><mtext>mantissa</mtext><mo>×</mo><msup><mn>2</mn><mtext>exponent</mtext></msup></mrow><annotation encoding="application/x-tex">\text{input} = \text{mantissa} \times 2^{\text{exponent}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8623em;vertical-align:-0.1944em;"></span><span class="mord text"><span class="mord">input</span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.7512em;vertical-align:-0.0833em;"></span><span class="mord text"><span class="mord">mantissa</span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.7936em;"></span><span class="mord"><span class="mord">2</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7936em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">exponent</span></span></span></span></span></span></span></span></span></span></span></span></span></span>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.gradient"/><a class="reference internal" href="generated/torch.gradient.html#torch.gradient" title="torch.gradient"><code class="xref py py-obj docutils literal notranslate"><span class="pre">gradient</span></code></a></p></td>
<td><p>Estimates the gradient of a function <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>g</mi><mo>:</mo><msup><mi mathvariant="double-struck">R</mi><mi>n</mi></msup><mo>→</mo><mi mathvariant="double-struck">R</mi></mrow><annotation encoding="application/x-tex">g : \mathbb{R}^n \rightarrow \mathbb{R}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">g</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">:</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6889em;"></span><span class="mord"><span class="mord mathbb">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6644em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">→</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6889em;"></span><span class="mord mathbb">R</span></span></span></span></span> in one or more dimensions using the <a class="reference external" href="https://www.ams.org/journals/mcom/1988-51-184/S0025-5718-1988-0935077-0/S0025-5718-1988-0935077-0.pdf">second-order accurate central differences method</a> and either first or second order estimates at the boundaries.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.imag"/><a class="reference internal" href="generated/torch.imag.html#torch.imag" title="torch.imag"><code class="xref py py-obj docutils literal notranslate"><span class="pre">imag</span></code></a></p></td>
<td><p>Returns a new tensor containing imaginary values of the <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.ldexp"/><a class="reference internal" href="generated/torch.ldexp.html#torch.ldexp" title="torch.ldexp"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ldexp</span></code></a></p></td>
<td><p>Multiplies <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> by 2 ** <code class="xref py py-attr docutils literal notranslate"><span class="pre">other</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.lerp"/><a class="reference internal" href="generated/torch.lerp.html#torch.lerp" title="torch.lerp"><code class="xref py py-obj docutils literal notranslate"><span class="pre">lerp</span></code></a></p></td>
<td><p>Does a linear interpolation of two tensors <code class="xref py py-attr docutils literal notranslate"><span class="pre">start</span></code> (given by <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>) and <code class="xref py py-attr docutils literal notranslate"><span class="pre">end</span></code> based on a scalar or tensor <code class="xref py py-attr docutils literal notranslate"><span class="pre">weight</span></code> and returns the resulting <code class="xref py py-attr docutils literal notranslate"><span class="pre">out</span></code> tensor.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.lgamma"/><a class="reference internal" href="generated/torch.lgamma.html#torch.lgamma" title="torch.lgamma"><code class="xref py py-obj docutils literal notranslate"><span class="pre">lgamma</span></code></a></p></td>
<td><p>Computes the natural logarithm of the absolute value of the gamma function on <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.log"/><a class="reference internal" href="generated/torch.log.html#torch.log" title="torch.log"><code class="xref py py-obj docutils literal notranslate"><span class="pre">log</span></code></a></p></td>
<td><p>Returns a new tensor with the natural logarithm of the elements of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.log10"/><a class="reference internal" href="generated/torch.log10.html#torch.log10" title="torch.log10"><code class="xref py py-obj docutils literal notranslate"><span class="pre">log10</span></code></a></p></td>
<td><p>Returns a new tensor with the logarithm to the base 10 of the elements of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.log1p"/><a class="reference internal" href="generated/torch.log1p.html#torch.log1p" title="torch.log1p"><code class="xref py py-obj docutils literal notranslate"><span class="pre">log1p</span></code></a></p></td>
<td><p>Returns a new tensor with the natural logarithm of (1 + <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>).</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.log2"/><a class="reference internal" href="generated/torch.log2.html#torch.log2" title="torch.log2"><code class="xref py py-obj docutils literal notranslate"><span class="pre">log2</span></code></a></p></td>
<td><p>Returns a new tensor with the logarithm to the base 2 of the elements of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.logaddexp"/><a class="reference internal" href="generated/torch.logaddexp.html#torch.logaddexp" title="torch.logaddexp"><code class="xref py py-obj docutils literal notranslate"><span class="pre">logaddexp</span></code></a></p></td>
<td><p>Logarithm of the sum of exponentiations of the inputs.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.logaddexp2"/><a class="reference internal" href="generated/torch.logaddexp2.html#torch.logaddexp2" title="torch.logaddexp2"><code class="xref py py-obj docutils literal notranslate"><span class="pre">logaddexp2</span></code></a></p></td>
<td><p>Logarithm of the sum of exponentiations of the inputs in base-2.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.logical_and"/><a class="reference internal" href="generated/torch.logical_and.html#torch.logical_and" title="torch.logical_and"><code class="xref py py-obj docutils literal notranslate"><span class="pre">logical_and</span></code></a></p></td>
<td><p>Computes the element-wise logical AND of the given input tensors.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.logical_not"/><a class="reference internal" href="generated/torch.logical_not.html#torch.logical_not" title="torch.logical_not"><code class="xref py py-obj docutils literal notranslate"><span class="pre">logical_not</span></code></a></p></td>
<td><p>Computes the element-wise logical NOT of the given input tensor.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.logical_or"/><a class="reference internal" href="generated/torch.logical_or.html#torch.logical_or" title="torch.logical_or"><code class="xref py py-obj docutils literal notranslate"><span class="pre">logical_or</span></code></a></p></td>
<td><p>Computes the element-wise logical OR of the given input tensors.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.logical_xor"/><a class="reference internal" href="generated/torch.logical_xor.html#torch.logical_xor" title="torch.logical_xor"><code class="xref py py-obj docutils literal notranslate"><span class="pre">logical_xor</span></code></a></p></td>
<td><p>Computes the element-wise logical XOR of the given input tensors.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.logit"/><a class="reference internal" href="generated/torch.logit.html#torch.logit" title="torch.logit"><code class="xref py py-obj docutils literal notranslate"><span class="pre">logit</span></code></a></p></td>
<td><p>Alias for <a class="reference internal" href="special.html#torch.special.logit" title="torch.special.logit"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.special.logit()</span></code></a>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.hypot"/><a class="reference internal" href="generated/torch.hypot.html#torch.hypot" title="torch.hypot"><code class="xref py py-obj docutils literal notranslate"><span class="pre">hypot</span></code></a></p></td>
<td><p>Given the legs of a right triangle, return its hypotenuse.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.i0"/><a class="reference internal" href="generated/torch.i0.html#torch.i0" title="torch.i0"><code class="xref py py-obj docutils literal notranslate"><span class="pre">i0</span></code></a></p></td>
<td><p>Alias for <a class="reference internal" href="special.html#torch.special.i0" title="torch.special.i0"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.special.i0()</span></code></a>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.igamma"/><a class="reference internal" href="generated/torch.igamma.html#torch.igamma" title="torch.igamma"><code class="xref py py-obj docutils literal notranslate"><span class="pre">igamma</span></code></a></p></td>
<td><p>Alias for <a class="reference internal" href="special.html#torch.special.gammainc" title="torch.special.gammainc"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.special.gammainc()</span></code></a>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.igammac"/><a class="reference internal" href="generated/torch.igammac.html#torch.igammac" title="torch.igammac"><code class="xref py py-obj docutils literal notranslate"><span class="pre">igammac</span></code></a></p></td>
<td><p>Alias for <a class="reference internal" href="special.html#torch.special.gammaincc" title="torch.special.gammaincc"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.special.gammaincc()</span></code></a>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.mul"/><a class="reference internal" href="generated/torch.mul.html#torch.mul" title="torch.mul"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mul</span></code></a></p></td>
<td><p>Multiplies <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> by <code class="xref py py-attr docutils literal notranslate"><span class="pre">other</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.multiply"/><a class="reference internal" href="generated/torch.multiply.html#torch.multiply" title="torch.multiply"><code class="xref py py-obj docutils literal notranslate"><span class="pre">multiply</span></code></a></p></td>
<td><p>Alias for <a class="reference internal" href="generated/torch.mul.html#torch.mul" title="torch.mul"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.mul()</span></code></a>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.mvlgamma"/><a class="reference internal" href="generated/torch.mvlgamma.html#torch.mvlgamma" title="torch.mvlgamma"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mvlgamma</span></code></a></p></td>
<td><p>Alias for <a class="reference internal" href="special.html#torch.special.multigammaln" title="torch.special.multigammaln"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.special.multigammaln()</span></code></a>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.nan_to_num"/><a class="reference internal" href="generated/torch.nan_to_num.html#torch.nan_to_num" title="torch.nan_to_num"><code class="xref py py-obj docutils literal notranslate"><span class="pre">nan_to_num</span></code></a></p></td>
<td><p>Replaces <code class="docutils literal notranslate"><span class="pre">NaN</span></code>, positive infinity, and negative infinity values in <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> with the values specified by <code class="xref py py-attr docutils literal notranslate"><span class="pre">nan</span></code>, <code class="xref py py-attr docutils literal notranslate"><span class="pre">posinf</span></code>, and <code class="xref py py-attr docutils literal notranslate"><span class="pre">neginf</span></code>, respectively.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.neg"/><a class="reference internal" href="generated/torch.neg.html#torch.neg" title="torch.neg"><code class="xref py py-obj docutils literal notranslate"><span class="pre">neg</span></code></a></p></td>
<td><p>Returns a new tensor with the negative of the elements of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.negative"/><a class="reference internal" href="generated/torch.negative.html#torch.negative" title="torch.negative"><code class="xref py py-obj docutils literal notranslate"><span class="pre">negative</span></code></a></p></td>
<td><p>Alias for <a class="reference internal" href="generated/torch.neg.html#torch.neg" title="torch.neg"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.neg()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.nextafter"/><a class="reference internal" href="generated/torch.nextafter.html#torch.nextafter" title="torch.nextafter"><code class="xref py py-obj docutils literal notranslate"><span class="pre">nextafter</span></code></a></p></td>
<td><p>Return the next floating-point value after <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> towards <code class="xref py py-attr docutils literal notranslate"><span class="pre">other</span></code>, elementwise.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.polygamma"/><a class="reference internal" href="generated/torch.polygamma.html#torch.polygamma" title="torch.polygamma"><code class="xref py py-obj docutils literal notranslate"><span class="pre">polygamma</span></code></a></p></td>
<td><p>Alias for <a class="reference internal" href="special.html#torch.special.polygamma" title="torch.special.polygamma"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.special.polygamma()</span></code></a>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.positive"/><a class="reference internal" href="generated/torch.positive.html#torch.positive" title="torch.positive"><code class="xref py py-obj docutils literal notranslate"><span class="pre">positive</span></code></a></p></td>
<td><p>Returns <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.pow"/><a class="reference internal" href="generated/torch.pow.html#torch.pow" title="torch.pow"><code class="xref py py-obj docutils literal notranslate"><span class="pre">pow</span></code></a></p></td>
<td><p>Takes the power of each element in <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> with <code class="xref py py-attr docutils literal notranslate"><span class="pre">exponent</span></code> and returns a tensor with the result.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.quantized_batch_norm"/><a class="reference internal" href="generated/torch.quantized_batch_norm.html#torch.quantized_batch_norm" title="torch.quantized_batch_norm"><code class="xref py py-obj docutils literal notranslate"><span class="pre">quantized_batch_norm</span></code></a></p></td>
<td><p>Applies batch normalization on a 4D (NCHW) quantized tensor.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.quantized_max_pool1d"/><a class="reference internal" href="generated/torch.quantized_max_pool1d.html#torch.quantized_max_pool1d" title="torch.quantized_max_pool1d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">quantized_max_pool1d</span></code></a></p></td>
<td><p>Applies a 1D max pooling over an input quantized tensor composed of several input planes.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.quantized_max_pool2d"/><a class="reference internal" href="generated/torch.quantized_max_pool2d.html#torch.quantized_max_pool2d" title="torch.quantized_max_pool2d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">quantized_max_pool2d</span></code></a></p></td>
<td><p>Applies a 2D max pooling over an input quantized tensor composed of several input planes.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.rad2deg"/><a class="reference internal" href="generated/torch.rad2deg.html#torch.rad2deg" title="torch.rad2deg"><code class="xref py py-obj docutils literal notranslate"><span class="pre">rad2deg</span></code></a></p></td>
<td><p>Returns a new tensor with each of the elements of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> converted from angles in radians to degrees.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.real"/><a class="reference internal" href="generated/torch.real.html#torch.real" title="torch.real"><code class="xref py py-obj docutils literal notranslate"><span class="pre">real</span></code></a></p></td>
<td><p>Returns a new tensor containing real values of the <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.reciprocal"/><a class="reference internal" href="generated/torch.reciprocal.html#torch.reciprocal" title="torch.reciprocal"><code class="xref py py-obj docutils literal notranslate"><span class="pre">reciprocal</span></code></a></p></td>
<td><p>Returns a new tensor with the reciprocal of the elements of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.remainder"/><a class="reference internal" href="generated/torch.remainder.html#torch.remainder" title="torch.remainder"><code class="xref py py-obj docutils literal notranslate"><span class="pre">remainder</span></code></a></p></td>
<td><p>Computes <a class="reference external" href="https://docs.python.org/3/reference/expressions.html#binary-arithmetic-operations">Python's modulus operation</a> entrywise.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.round"/><a class="reference internal" href="generated/torch.round.html#torch.round" title="torch.round"><code class="xref py py-obj docutils literal notranslate"><span class="pre">round</span></code></a></p></td>
<td><p>Rounds elements of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> to the nearest integer.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.rsqrt"/><a class="reference internal" href="generated/torch.rsqrt.html#torch.rsqrt" title="torch.rsqrt"><code class="xref py py-obj docutils literal notranslate"><span class="pre">rsqrt</span></code></a></p></td>
<td><p>Returns a new tensor with the reciprocal of the square-root of each of the elements of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.sigmoid"/><a class="reference internal" href="generated/torch.sigmoid.html#torch.sigmoid" title="torch.sigmoid"><code class="xref py py-obj docutils literal notranslate"><span class="pre">sigmoid</span></code></a></p></td>
<td><p>Alias for <a class="reference internal" href="special.html#torch.special.expit" title="torch.special.expit"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.special.expit()</span></code></a>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.sign"/><a class="reference internal" href="generated/torch.sign.html#torch.sign" title="torch.sign"><code class="xref py py-obj docutils literal notranslate"><span class="pre">sign</span></code></a></p></td>
<td><p>Returns a new tensor with the signs of the elements of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.sgn"/><a class="reference internal" href="generated/torch.sgn.html#torch.sgn" title="torch.sgn"><code class="xref py py-obj docutils literal notranslate"><span class="pre">sgn</span></code></a></p></td>
<td><p>This function is an extension of torch.sign() to complex tensors.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.signbit"/><a class="reference internal" href="generated/torch.signbit.html#torch.signbit" title="torch.signbit"><code class="xref py py-obj docutils literal notranslate"><span class="pre">signbit</span></code></a></p></td>
<td><p>Tests if each element of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> has its sign bit set or not.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.sin"/><a class="reference internal" href="generated/torch.sin.html#torch.sin" title="torch.sin"><code class="xref py py-obj docutils literal notranslate"><span class="pre">sin</span></code></a></p></td>
<td><p>Returns a new tensor with the sine of the elements of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.sinc"/><a class="reference internal" href="generated/torch.sinc.html#torch.sinc" title="torch.sinc"><code class="xref py py-obj docutils literal notranslate"><span class="pre">sinc</span></code></a></p></td>
<td><p>Alias for <a class="reference internal" href="special.html#torch.special.sinc" title="torch.special.sinc"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.special.sinc()</span></code></a>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.sinh"/><a class="reference internal" href="generated/torch.sinh.html#torch.sinh" title="torch.sinh"><code class="xref py py-obj docutils literal notranslate"><span class="pre">sinh</span></code></a></p></td>
<td><p>Returns a new tensor with the hyperbolic sine of the elements of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.softmax"/><a class="reference internal" href="generated/torch.softmax.html#torch.softmax" title="torch.softmax"><code class="xref py py-obj docutils literal notranslate"><span class="pre">softmax</span></code></a></p></td>
<td><p>Alias for <a class="reference internal" href="generated/torch.nn.functional.softmax.html#torch.nn.functional.softmax" title="torch.nn.functional.softmax"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.nn.functional.softmax()</span></code></a>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.sqrt"/><a class="reference internal" href="generated/torch.sqrt.html#torch.sqrt" title="torch.sqrt"><code class="xref py py-obj docutils literal notranslate"><span class="pre">sqrt</span></code></a></p></td>
<td><p>Returns a new tensor with the square-root of the elements of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.square"/><a class="reference internal" href="generated/torch.square.html#torch.square" title="torch.square"><code class="xref py py-obj docutils literal notranslate"><span class="pre">square</span></code></a></p></td>
<td><p>Returns a new tensor with the square of the elements of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.sub"/><a class="reference internal" href="generated/torch.sub.html#torch.sub" title="torch.sub"><code class="xref py py-obj docutils literal notranslate"><span class="pre">sub</span></code></a></p></td>
<td><p>Subtracts <code class="xref py py-attr docutils literal notranslate"><span class="pre">other</span></code>, scaled by <code class="xref py py-attr docutils literal notranslate"><span class="pre">alpha</span></code>, from <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.subtract"/><a class="reference internal" href="generated/torch.subtract.html#torch.subtract" title="torch.subtract"><code class="xref py py-obj docutils literal notranslate"><span class="pre">subtract</span></code></a></p></td>
<td><p>Alias for <a class="reference internal" href="generated/torch.sub.html#torch.sub" title="torch.sub"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.sub()</span></code></a>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.tan"/><a class="reference internal" href="generated/torch.tan.html#torch.tan" title="torch.tan"><code class="xref py py-obj docutils literal notranslate"><span class="pre">tan</span></code></a></p></td>
<td><p>Returns a new tensor with the tangent of the elements of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.tanh"/><a class="reference internal" href="generated/torch.tanh.html#torch.tanh" title="torch.tanh"><code class="xref py py-obj docutils literal notranslate"><span class="pre">tanh</span></code></a></p></td>
<td><p>Returns a new tensor with the hyperbolic tangent of the elements of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.true_divide"/><a class="reference internal" href="generated/torch.true_divide.html#torch.true_divide" title="torch.true_divide"><code class="xref py py-obj docutils literal notranslate"><span class="pre">true_divide</span></code></a></p></td>
<td><p>Alias for <a class="reference internal" href="generated/torch.div.html#torch.div" title="torch.div"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.div()</span></code></a> with <code class="docutils literal notranslate"><span class="pre">rounding_mode=None</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.trunc"/><a class="reference internal" href="generated/torch.trunc.html#torch.trunc" title="torch.trunc"><code class="xref py py-obj docutils literal notranslate"><span class="pre">trunc</span></code></a></p></td>
<td><p>Returns a new tensor with the truncated integer values of the elements of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.xlogy"/><a class="reference internal" href="generated/torch.xlogy.html#torch.xlogy" title="torch.xlogy"><code class="xref py py-obj docutils literal notranslate"><span class="pre">xlogy</span></code></a></p></td>
<td><p>Alias for <a class="reference internal" href="special.html#torch.special.xlogy" title="torch.special.xlogy"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.special.xlogy()</span></code></a>.</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="reduction-ops">
<h3>Reduction Ops<a class="headerlink" href="#reduction-ops" title="Permalink to this heading">#</a></h3>
<div class="pst-scrollable-table-container"><table class="autosummary longtable table autosummary">
<tbody>
<tr class="row-odd"><td><p><p id="torch.argmax"/><a class="reference internal" href="generated/torch.argmax.html#torch.argmax" title="torch.argmax"><code class="xref py py-obj docutils literal notranslate"><span class="pre">argmax</span></code></a></p></td>
<td><p>Returns the indices of the maximum value of all elements in the <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> tensor.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.argmin"/><a class="reference internal" href="generated/torch.argmin.html#torch.argmin" title="torch.argmin"><code class="xref py py-obj docutils literal notranslate"><span class="pre">argmin</span></code></a></p></td>
<td><p>Returns the indices of the minimum value(s) of the flattened tensor or along a dimension</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.amax"/><a class="reference internal" href="generated/torch.amax.html#torch.amax" title="torch.amax"><code class="xref py py-obj docutils literal notranslate"><span class="pre">amax</span></code></a></p></td>
<td><p>Returns the maximum value of each slice of the <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> tensor in the given dimension(s) <code class="xref py py-attr docutils literal notranslate"><span class="pre">dim</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.amin"/><a class="reference internal" href="generated/torch.amin.html#torch.amin" title="torch.amin"><code class="xref py py-obj docutils literal notranslate"><span class="pre">amin</span></code></a></p></td>
<td><p>Returns the minimum value of each slice of the <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> tensor in the given dimension(s) <code class="xref py py-attr docutils literal notranslate"><span class="pre">dim</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.aminmax"/><a class="reference internal" href="generated/torch.aminmax.html#torch.aminmax" title="torch.aminmax"><code class="xref py py-obj docutils literal notranslate"><span class="pre">aminmax</span></code></a></p></td>
<td><p>Computes the minimum and maximum values of the <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> tensor.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.all"/><a class="reference internal" href="generated/torch.all.html#torch.all" title="torch.all"><code class="xref py py-obj docutils literal notranslate"><span class="pre">all</span></code></a></p></td>
<td><p>Tests if all elements in <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> evaluate to <cite>True</cite>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.any"/><a class="reference internal" href="generated/torch.any.html#torch.any" title="torch.any"><code class="xref py py-obj docutils literal notranslate"><span class="pre">any</span></code></a></p></td>
<td><p>Tests if any element in <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> evaluates to <cite>True</cite>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.max"/><a class="reference internal" href="generated/torch.max.html#torch.max" title="torch.max"><code class="xref py py-obj docutils literal notranslate"><span class="pre">max</span></code></a></p></td>
<td><p>Returns the maximum value of all elements in the <code class="docutils literal notranslate"><span class="pre">input</span></code> tensor.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.min"/><a class="reference internal" href="generated/torch.min.html#torch.min" title="torch.min"><code class="xref py py-obj docutils literal notranslate"><span class="pre">min</span></code></a></p></td>
<td><p>Returns the minimum value of all elements in the <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> tensor.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.dist"/><a class="reference internal" href="generated/torch.dist.html#torch.dist" title="torch.dist"><code class="xref py py-obj docutils literal notranslate"><span class="pre">dist</span></code></a></p></td>
<td><p>Returns the p-norm of (<code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> - <code class="xref py py-attr docutils literal notranslate"><span class="pre">other</span></code>)</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.logsumexp"/><a class="reference internal" href="generated/torch.logsumexp.html#torch.logsumexp" title="torch.logsumexp"><code class="xref py py-obj docutils literal notranslate"><span class="pre">logsumexp</span></code></a></p></td>
<td><p>Returns the log of summed exponentials of each row of the <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> tensor in the given dimension <code class="xref py py-attr docutils literal notranslate"><span class="pre">dim</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.mean"/><a class="reference internal" href="generated/torch.mean.html#torch.mean" title="torch.mean"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mean</span></code></a></p></td>
<td><p></p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.nanmean"/><a class="reference internal" href="generated/torch.nanmean.html#torch.nanmean" title="torch.nanmean"><code class="xref py py-obj docutils literal notranslate"><span class="pre">nanmean</span></code></a></p></td>
<td><p>Computes the mean of all <cite>non-NaN</cite> elements along the specified dimensions.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.median"/><a class="reference internal" href="generated/torch.median.html#torch.median" title="torch.median"><code class="xref py py-obj docutils literal notranslate"><span class="pre">median</span></code></a></p></td>
<td><p>Returns the median of the values in <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.nanmedian"/><a class="reference internal" href="generated/torch.nanmedian.html#torch.nanmedian" title="torch.nanmedian"><code class="xref py py-obj docutils literal notranslate"><span class="pre">nanmedian</span></code></a></p></td>
<td><p>Returns the median of the values in <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>, ignoring <code class="docutils literal notranslate"><span class="pre">NaN</span></code> values.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.mode"/><a class="reference internal" href="generated/torch.mode.html#torch.mode" title="torch.mode"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mode</span></code></a></p></td>
<td><p>Returns a namedtuple <code class="docutils literal notranslate"><span class="pre">(values,</span> <span class="pre">indices)</span></code> where <code class="docutils literal notranslate"><span class="pre">values</span></code> is the mode value of each row of the <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> tensor in the given dimension <code class="xref py py-attr docutils literal notranslate"><span class="pre">dim</span></code>, i.e. a value which appears most often in that row, and <code class="docutils literal notranslate"><span class="pre">indices</span></code> is the index location of each mode value found.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.norm"/><a class="reference internal" href="generated/torch.norm.html#torch.norm" title="torch.norm"><code class="xref py py-obj docutils literal notranslate"><span class="pre">norm</span></code></a></p></td>
<td><p>Returns the matrix norm or vector norm of a given tensor.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.nansum"/><a class="reference internal" href="generated/torch.nansum.html#torch.nansum" title="torch.nansum"><code class="xref py py-obj docutils literal notranslate"><span class="pre">nansum</span></code></a></p></td>
<td><p>Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.prod"/><a class="reference internal" href="generated/torch.prod.html#torch.prod" title="torch.prod"><code class="xref py py-obj docutils literal notranslate"><span class="pre">prod</span></code></a></p></td>
<td><p>Returns the product of all elements in the <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> tensor.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.quantile"/><a class="reference internal" href="generated/torch.quantile.html#torch.quantile" title="torch.quantile"><code class="xref py py-obj docutils literal notranslate"><span class="pre">quantile</span></code></a></p></td>
<td><p>Computes the q-th quantiles of each row of the <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> tensor along the dimension <code class="xref py py-attr docutils literal notranslate"><span class="pre">dim</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.nanquantile"/><a class="reference internal" href="generated/torch.nanquantile.html#torch.nanquantile" title="torch.nanquantile"><code class="xref py py-obj docutils literal notranslate"><span class="pre">nanquantile</span></code></a></p></td>
<td><p>This is a variant of <a class="reference internal" href="generated/torch.quantile.html#torch.quantile" title="torch.quantile"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.quantile()</span></code></a> that &quot;ignores&quot; <code class="docutils literal notranslate"><span class="pre">NaN</span></code> values, computing the quantiles <code class="xref py py-attr docutils literal notranslate"><span class="pre">q</span></code> as if <code class="docutils literal notranslate"><span class="pre">NaN</span></code> values in <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> did not exist.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.std"/><a class="reference internal" href="generated/torch.std.html#torch.std" title="torch.std"><code class="xref py py-obj docutils literal notranslate"><span class="pre">std</span></code></a></p></td>
<td><p>Calculates the standard deviation over the dimensions specified by <code class="xref py py-attr docutils literal notranslate"><span class="pre">dim</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.std_mean"/><a class="reference internal" href="generated/torch.std_mean.html#torch.std_mean" title="torch.std_mean"><code class="xref py py-obj docutils literal notranslate"><span class="pre">std_mean</span></code></a></p></td>
<td><p>Calculates the standard deviation and mean over the dimensions specified by <code class="xref py py-attr docutils literal notranslate"><span class="pre">dim</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.sum"/><a class="reference internal" href="generated/torch.sum.html#torch.sum" title="torch.sum"><code class="xref py py-obj docutils literal notranslate"><span class="pre">sum</span></code></a></p></td>
<td><p>Returns the sum of all elements in the <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> tensor.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.unique"/><a class="reference internal" href="generated/torch.unique.html#torch.unique" title="torch.unique"><code class="xref py py-obj docutils literal notranslate"><span class="pre">unique</span></code></a></p></td>
<td><p>Returns the unique elements of the input tensor.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.unique_consecutive"/><a class="reference internal" href="generated/torch.unique_consecutive.html#torch.unique_consecutive" title="torch.unique_consecutive"><code class="xref py py-obj docutils literal notranslate"><span class="pre">unique_consecutive</span></code></a></p></td>
<td><p>Eliminates all but the first element from every consecutive group of equivalent elements.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.var"/><a class="reference internal" href="generated/torch.var.html#torch.var" title="torch.var"><code class="xref py py-obj docutils literal notranslate"><span class="pre">var</span></code></a></p></td>
<td><p>Calculates the variance over the dimensions specified by <code class="xref py py-attr docutils literal notranslate"><span class="pre">dim</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.var_mean"/><a class="reference internal" href="generated/torch.var_mean.html#torch.var_mean" title="torch.var_mean"><code class="xref py py-obj docutils literal notranslate"><span class="pre">var_mean</span></code></a></p></td>
<td><p>Calculates the variance and mean over the dimensions specified by <code class="xref py py-attr docutils literal notranslate"><span class="pre">dim</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.count_nonzero"/><a class="reference internal" href="generated/torch.count_nonzero.html#torch.count_nonzero" title="torch.count_nonzero"><code class="xref py py-obj docutils literal notranslate"><span class="pre">count_nonzero</span></code></a></p></td>
<td><p>Counts the number of non-zero values in the tensor <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> along the given <code class="xref py py-attr docutils literal notranslate"><span class="pre">dim</span></code>.</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="comparison-ops">
<h3>Comparison Ops<a class="headerlink" href="#comparison-ops" title="Permalink to this heading">#</a></h3>
<div class="pst-scrollable-table-container"><table class="autosummary longtable table autosummary">
<tbody>
<tr class="row-odd"><td><p><p id="torch.allclose"/><a class="reference internal" href="generated/torch.allclose.html#torch.allclose" title="torch.allclose"><code class="xref py py-obj docutils literal notranslate"><span class="pre">allclose</span></code></a></p></td>
<td><p>This function checks if <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">other</span></code> satisfy the condition:</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.argsort"/><a class="reference internal" href="generated/torch.argsort.html#torch.argsort" title="torch.argsort"><code class="xref py py-obj docutils literal notranslate"><span class="pre">argsort</span></code></a></p></td>
<td><p>Returns the indices that sort a tensor along a given dimension in ascending order by value.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.eq"/><a class="reference internal" href="generated/torch.eq.html#torch.eq" title="torch.eq"><code class="xref py py-obj docutils literal notranslate"><span class="pre">eq</span></code></a></p></td>
<td><p>Computes element-wise equality</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.equal"/><a class="reference internal" href="generated/torch.equal.html#torch.equal" title="torch.equal"><code class="xref py py-obj docutils literal notranslate"><span class="pre">equal</span></code></a></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">True</span></code> if two tensors have the same size and elements, <code class="docutils literal notranslate"><span class="pre">False</span></code> otherwise.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.ge"/><a class="reference internal" href="generated/torch.ge.html#torch.ge" title="torch.ge"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ge</span></code></a></p></td>
<td><p>Computes <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>input</mtext><mo>≥</mo><mtext>other</mtext></mrow><annotation encoding="application/x-tex">\text{input} \geq \text{other}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8623em;vertical-align:-0.1944em;"></span><span class="mord text"><span class="mord">input</span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">≥</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord text"><span class="mord">other</span></span></span></span></span></span> element-wise.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.greater_equal"/><a class="reference internal" href="generated/torch.greater_equal.html#torch.greater_equal" title="torch.greater_equal"><code class="xref py py-obj docutils literal notranslate"><span class="pre">greater_equal</span></code></a></p></td>
<td><p>Alias for <a class="reference internal" href="generated/torch.ge.html#torch.ge" title="torch.ge"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.ge()</span></code></a>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.gt"/><a class="reference internal" href="generated/torch.gt.html#torch.gt" title="torch.gt"><code class="xref py py-obj docutils literal notranslate"><span class="pre">gt</span></code></a></p></td>
<td><p>Computes <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>input</mtext><mo>&gt;</mo><mtext>other</mtext></mrow><annotation encoding="application/x-tex">\text{input} &gt; \text{other}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8623em;vertical-align:-0.1944em;"></span><span class="mord text"><span class="mord">input</span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">&gt;</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord text"><span class="mord">other</span></span></span></span></span></span> element-wise.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.greater"/><a class="reference internal" href="generated/torch.greater.html#torch.greater" title="torch.greater"><code class="xref py py-obj docutils literal notranslate"><span class="pre">greater</span></code></a></p></td>
<td><p>Alias for <a class="reference internal" href="generated/torch.gt.html#torch.gt" title="torch.gt"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.gt()</span></code></a>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.isclose"/><a class="reference internal" href="generated/torch.isclose.html#torch.isclose" title="torch.isclose"><code class="xref py py-obj docutils literal notranslate"><span class="pre">isclose</span></code></a></p></td>
<td><p>Returns a new tensor with boolean elements representing if each element of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> is &quot;close&quot; to the corresponding element of <code class="xref py py-attr docutils literal notranslate"><span class="pre">other</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.isfinite"/><a class="reference internal" href="generated/torch.isfinite.html#torch.isfinite" title="torch.isfinite"><code class="xref py py-obj docutils literal notranslate"><span class="pre">isfinite</span></code></a></p></td>
<td><p>Returns a new tensor with boolean elements representing if each element is <cite>finite</cite> or not.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.isin"/><a class="reference internal" href="generated/torch.isin.html#torch.isin" title="torch.isin"><code class="xref py py-obj docutils literal notranslate"><span class="pre">isin</span></code></a></p></td>
<td><p>Tests if each element of <code class="xref py py-attr docutils literal notranslate"><span class="pre">elements</span></code> is in <code class="xref py py-attr docutils literal notranslate"><span class="pre">test_elements</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.isinf"/><a class="reference internal" href="generated/torch.isinf.html#torch.isinf" title="torch.isinf"><code class="xref py py-obj docutils literal notranslate"><span class="pre">isinf</span></code></a></p></td>
<td><p>Tests if each element of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> is infinite (positive or negative infinity) or not.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.isposinf"/><a class="reference internal" href="generated/torch.isposinf.html#torch.isposinf" title="torch.isposinf"><code class="xref py py-obj docutils literal notranslate"><span class="pre">isposinf</span></code></a></p></td>
<td><p>Tests if each element of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> is positive infinity or not.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.isneginf"/><a class="reference internal" href="generated/torch.isneginf.html#torch.isneginf" title="torch.isneginf"><code class="xref py py-obj docutils literal notranslate"><span class="pre">isneginf</span></code></a></p></td>
<td><p>Tests if each element of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> is negative infinity or not.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.isnan"/><a class="reference internal" href="generated/torch.isnan.html#torch.isnan" title="torch.isnan"><code class="xref py py-obj docutils literal notranslate"><span class="pre">isnan</span></code></a></p></td>
<td><p>Returns a new tensor with boolean elements representing if each element of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> is NaN or not.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.isreal"/><a class="reference internal" href="generated/torch.isreal.html#torch.isreal" title="torch.isreal"><code class="xref py py-obj docutils literal notranslate"><span class="pre">isreal</span></code></a></p></td>
<td><p>Returns a new tensor with boolean elements representing if each element of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> is real-valued or not.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.kthvalue"/><a class="reference internal" href="generated/torch.kthvalue.html#torch.kthvalue" title="torch.kthvalue"><code class="xref py py-obj docutils literal notranslate"><span class="pre">kthvalue</span></code></a></p></td>
<td><p>Returns a namedtuple <code class="docutils literal notranslate"><span class="pre">(values,</span> <span class="pre">indices)</span></code> where <code class="docutils literal notranslate"><span class="pre">values</span></code> is the <code class="xref py py-attr docutils literal notranslate"><span class="pre">k</span></code> th smallest element of each row of the <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> tensor in the given dimension <code class="xref py py-attr docutils literal notranslate"><span class="pre">dim</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.le"/><a class="reference internal" href="generated/torch.le.html#torch.le" title="torch.le"><code class="xref py py-obj docutils literal notranslate"><span class="pre">le</span></code></a></p></td>
<td><p>Computes <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>input</mtext><mo>≤</mo><mtext>other</mtext></mrow><annotation encoding="application/x-tex">\text{input} \leq \text{other}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8623em;vertical-align:-0.1944em;"></span><span class="mord text"><span class="mord">input</span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">≤</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord text"><span class="mord">other</span></span></span></span></span></span> element-wise.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.less_equal"/><a class="reference internal" href="generated/torch.less_equal.html#torch.less_equal" title="torch.less_equal"><code class="xref py py-obj docutils literal notranslate"><span class="pre">less_equal</span></code></a></p></td>
<td><p>Alias for <a class="reference internal" href="generated/torch.le.html#torch.le" title="torch.le"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.le()</span></code></a>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.lt"/><a class="reference internal" href="generated/torch.lt.html#torch.lt" title="torch.lt"><code class="xref py py-obj docutils literal notranslate"><span class="pre">lt</span></code></a></p></td>
<td><p>Computes <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>input</mtext><mo>&lt;</mo><mtext>other</mtext></mrow><annotation encoding="application/x-tex">\text{input} &lt; \text{other}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8623em;vertical-align:-0.1944em;"></span><span class="mord text"><span class="mord">input</span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">&lt;</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord text"><span class="mord">other</span></span></span></span></span></span> element-wise.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.less"/><a class="reference internal" href="generated/torch.less.html#torch.less" title="torch.less"><code class="xref py py-obj docutils literal notranslate"><span class="pre">less</span></code></a></p></td>
<td><p>Alias for <a class="reference internal" href="generated/torch.lt.html#torch.lt" title="torch.lt"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.lt()</span></code></a>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.maximum"/><a class="reference internal" href="generated/torch.maximum.html#torch.maximum" title="torch.maximum"><code class="xref py py-obj docutils literal notranslate"><span class="pre">maximum</span></code></a></p></td>
<td><p>Computes the element-wise maximum of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">other</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.minimum"/><a class="reference internal" href="generated/torch.minimum.html#torch.minimum" title="torch.minimum"><code class="xref py py-obj docutils literal notranslate"><span class="pre">minimum</span></code></a></p></td>
<td><p>Computes the element-wise minimum of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">other</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.fmax"/><a class="reference internal" href="generated/torch.fmax.html#torch.fmax" title="torch.fmax"><code class="xref py py-obj docutils literal notranslate"><span class="pre">fmax</span></code></a></p></td>
<td><p>Computes the element-wise maximum of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">other</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.fmin"/><a class="reference internal" href="generated/torch.fmin.html#torch.fmin" title="torch.fmin"><code class="xref py py-obj docutils literal notranslate"><span class="pre">fmin</span></code></a></p></td>
<td><p>Computes the element-wise minimum of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">other</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.ne"/><a class="reference internal" href="generated/torch.ne.html#torch.ne" title="torch.ne"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ne</span></code></a></p></td>
<td><p>Computes <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>input</mtext><mo mathvariant="normal">≠</mo><mtext>other</mtext></mrow><annotation encoding="application/x-tex">\text{input} \neq \text{other}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord text"><span class="mord">input</span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel"><span class="mrel"><span class="mord vbox"><span class="thinbox"><span class="rlap"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="inner"><span class="mord"><span class="mrel"></span></span></span><span class="fix"></span></span></span></span></span><span class="mrel">=</span></span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord text"><span class="mord">other</span></span></span></span></span></span> element-wise.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.not_equal"/><a class="reference internal" href="generated/torch.not_equal.html#torch.not_equal" title="torch.not_equal"><code class="xref py py-obj docutils literal notranslate"><span class="pre">not_equal</span></code></a></p></td>
<td><p>Alias for <a class="reference internal" href="generated/torch.ne.html#torch.ne" title="torch.ne"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.ne()</span></code></a>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.sort"/><a class="reference internal" href="generated/torch.sort.html#torch.sort" title="torch.sort"><code class="xref py py-obj docutils literal notranslate"><span class="pre">sort</span></code></a></p></td>
<td><p>Sorts the elements of the <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> tensor along a given dimension in ascending order by value.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.topk"/><a class="reference internal" href="generated/torch.topk.html#torch.topk" title="torch.topk"><code class="xref py py-obj docutils literal notranslate"><span class="pre">topk</span></code></a></p></td>
<td><p>Returns the <code class="xref py py-attr docutils literal notranslate"><span class="pre">k</span></code> largest elements of the given <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> tensor along a given dimension.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.msort"/><a class="reference internal" href="generated/torch.msort.html#torch.msort" title="torch.msort"><code class="xref py py-obj docutils literal notranslate"><span class="pre">msort</span></code></a></p></td>
<td><p>Sorts the elements of the <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> tensor along its first dimension in ascending order by value.</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="spectral-ops">
<h3>Spectral Ops<a class="headerlink" href="#spectral-ops" title="Permalink to this heading">#</a></h3>
<div class="pst-scrollable-table-container"><table class="autosummary longtable table autosummary">
<tbody>
<tr class="row-odd"><td><p><p id="torch.stft"/><a class="reference internal" href="generated/torch.stft.html#torch.stft" title="torch.stft"><code class="xref py py-obj docutils literal notranslate"><span class="pre">stft</span></code></a></p></td>
<td><p>Short-time Fourier transform (STFT).</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.istft"/><a class="reference internal" href="generated/torch.istft.html#torch.istft" title="torch.istft"><code class="xref py py-obj docutils literal notranslate"><span class="pre">istft</span></code></a></p></td>
<td><p>Inverse short time Fourier Transform.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.bartlett_window"/><a class="reference internal" href="generated/torch.bartlett_window.html#torch.bartlett_window" title="torch.bartlett_window"><code class="xref py py-obj docutils literal notranslate"><span class="pre">bartlett_window</span></code></a></p></td>
<td><p>Bartlett window function.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.blackman_window"/><a class="reference internal" href="generated/torch.blackman_window.html#torch.blackman_window" title="torch.blackman_window"><code class="xref py py-obj docutils literal notranslate"><span class="pre">blackman_window</span></code></a></p></td>
<td><p>Blackman window function.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.hamming_window"/><a class="reference internal" href="generated/torch.hamming_window.html#torch.hamming_window" title="torch.hamming_window"><code class="xref py py-obj docutils literal notranslate"><span class="pre">hamming_window</span></code></a></p></td>
<td><p>Hamming window function.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.hann_window"/><a class="reference internal" href="generated/torch.hann_window.html#torch.hann_window" title="torch.hann_window"><code class="xref py py-obj docutils literal notranslate"><span class="pre">hann_window</span></code></a></p></td>
<td><p>Hann window function.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.kaiser_window"/><a class="reference internal" href="generated/torch.kaiser_window.html#torch.kaiser_window" title="torch.kaiser_window"><code class="xref py py-obj docutils literal notranslate"><span class="pre">kaiser_window</span></code></a></p></td>
<td><p>Computes the Kaiser window with window length <code class="xref py py-attr docutils literal notranslate"><span class="pre">window_length</span></code> and shape parameter <code class="xref py py-attr docutils literal notranslate"><span class="pre">beta</span></code>.</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="other-operations">
<h3>Other Operations<a class="headerlink" href="#other-operations" title="Permalink to this heading">#</a></h3>
<div class="pst-scrollable-table-container"><table class="autosummary longtable table autosummary">
<tbody>
<tr class="row-odd"><td><p><p id="torch.atleast_1d"/><a class="reference internal" href="generated/torch.atleast_1d.html#torch.atleast_1d" title="torch.atleast_1d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">atleast_1d</span></code></a></p></td>
<td><p>Returns a 1-dimensional view of each input tensor with zero dimensions.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.atleast_2d"/><a class="reference internal" href="generated/torch.atleast_2d.html#torch.atleast_2d" title="torch.atleast_2d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">atleast_2d</span></code></a></p></td>
<td><p>Returns a 2-dimensional view of each input tensor with zero dimensions.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.atleast_3d"/><a class="reference internal" href="generated/torch.atleast_3d.html#torch.atleast_3d" title="torch.atleast_3d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">atleast_3d</span></code></a></p></td>
<td><p>Returns a 3-dimensional view of each input tensor with zero dimensions.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.bincount"/><a class="reference internal" href="generated/torch.bincount.html#torch.bincount" title="torch.bincount"><code class="xref py py-obj docutils literal notranslate"><span class="pre">bincount</span></code></a></p></td>
<td><p>Count the frequency of each value in an array of non-negative ints.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.block_diag"/><a class="reference internal" href="generated/torch.block_diag.html#torch.block_diag" title="torch.block_diag"><code class="xref py py-obj docutils literal notranslate"><span class="pre">block_diag</span></code></a></p></td>
<td><p>Create a block diagonal matrix from provided tensors.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.broadcast_tensors"/><a class="reference internal" href="generated/torch.broadcast_tensors.html#torch.broadcast_tensors" title="torch.broadcast_tensors"><code class="xref py py-obj docutils literal notranslate"><span class="pre">broadcast_tensors</span></code></a></p></td>
<td><p>Broadcasts the given tensors according to <a class="reference internal" href="notes/broadcasting.html#broadcasting-semantics"><span class="std std-ref">Broadcasting semantics</span></a>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.broadcast_to"/><a class="reference internal" href="generated/torch.broadcast_to.html#torch.broadcast_to" title="torch.broadcast_to"><code class="xref py py-obj docutils literal notranslate"><span class="pre">broadcast_to</span></code></a></p></td>
<td><p>Broadcasts <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> to the shape <code class="xref py py-attr docutils literal notranslate"><span class="pre">shape</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.broadcast_shapes"/><a class="reference internal" href="generated/torch.broadcast_shapes.html#torch.broadcast_shapes" title="torch.broadcast_shapes"><code class="xref py py-obj docutils literal notranslate"><span class="pre">broadcast_shapes</span></code></a></p></td>
<td><p>Similar to <a class="reference internal" href="generated/torch.broadcast_tensors.html#torch.broadcast_tensors" title="torch.broadcast_tensors"><code class="xref py py-func docutils literal notranslate"><span class="pre">broadcast_tensors()</span></code></a> but for shapes.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.bucketize"/><a class="reference internal" href="generated/torch.bucketize.html#torch.bucketize" title="torch.bucketize"><code class="xref py py-obj docutils literal notranslate"><span class="pre">bucketize</span></code></a></p></td>
<td><p>Returns the indices of the buckets to which each value in the <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> belongs, where the boundaries of the buckets are set by <code class="xref py py-attr docutils literal notranslate"><span class="pre">boundaries</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.cartesian_prod"/><a class="reference internal" href="generated/torch.cartesian_prod.html#torch.cartesian_prod" title="torch.cartesian_prod"><code class="xref py py-obj docutils literal notranslate"><span class="pre">cartesian_prod</span></code></a></p></td>
<td><p>Do cartesian product of the given sequence of tensors.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.cdist"/><a class="reference internal" href="generated/torch.cdist.html#torch.cdist" title="torch.cdist"><code class="xref py py-obj docutils literal notranslate"><span class="pre">cdist</span></code></a></p></td>
<td><p>Computes batched the p-norm distance between each pair of the two collections of row vectors.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.clone"/><a class="reference internal" href="generated/torch.clone.html#torch.clone" title="torch.clone"><code class="xref py py-obj docutils literal notranslate"><span class="pre">clone</span></code></a></p></td>
<td><p>Returns a copy of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.combinations"/><a class="reference internal" href="generated/torch.combinations.html#torch.combinations" title="torch.combinations"><code class="xref py py-obj docutils literal notranslate"><span class="pre">combinations</span></code></a></p></td>
<td><p>Compute combinations of length <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>r</mi></mrow><annotation encoding="application/x-tex">r</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span></span></span></span></span> of the given tensor.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.corrcoef"/><a class="reference internal" href="generated/torch.corrcoef.html#torch.corrcoef" title="torch.corrcoef"><code class="xref py py-obj docutils literal notranslate"><span class="pre">corrcoef</span></code></a></p></td>
<td><p>Estimates the Pearson product-moment correlation coefficient matrix of the variables given by the <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> matrix, where rows are the variables and columns are the observations.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.cov"/><a class="reference internal" href="generated/torch.cov.html#torch.cov" title="torch.cov"><code class="xref py py-obj docutils literal notranslate"><span class="pre">cov</span></code></a></p></td>
<td><p>Estimates the covariance matrix of the variables given by the <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> matrix, where rows are the variables and columns are the observations.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.cross"/><a class="reference internal" href="generated/torch.cross.html#torch.cross" title="torch.cross"><code class="xref py py-obj docutils literal notranslate"><span class="pre">cross</span></code></a></p></td>
<td><p>Returns the cross product of vectors in dimension <code class="xref py py-attr docutils literal notranslate"><span class="pre">dim</span></code> of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">other</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.cummax"/><a class="reference internal" href="generated/torch.cummax.html#torch.cummax" title="torch.cummax"><code class="xref py py-obj docutils literal notranslate"><span class="pre">cummax</span></code></a></p></td>
<td><p>Returns a namedtuple <code class="docutils literal notranslate"><span class="pre">(values,</span> <span class="pre">indices)</span></code> where <code class="docutils literal notranslate"><span class="pre">values</span></code> is the cumulative maximum of elements of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> in the dimension <code class="xref py py-attr docutils literal notranslate"><span class="pre">dim</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.cummin"/><a class="reference internal" href="generated/torch.cummin.html#torch.cummin" title="torch.cummin"><code class="xref py py-obj docutils literal notranslate"><span class="pre">cummin</span></code></a></p></td>
<td><p>Returns a namedtuple <code class="docutils literal notranslate"><span class="pre">(values,</span> <span class="pre">indices)</span></code> where <code class="docutils literal notranslate"><span class="pre">values</span></code> is the cumulative minimum of elements of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> in the dimension <code class="xref py py-attr docutils literal notranslate"><span class="pre">dim</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.cumprod"/><a class="reference internal" href="generated/torch.cumprod.html#torch.cumprod" title="torch.cumprod"><code class="xref py py-obj docutils literal notranslate"><span class="pre">cumprod</span></code></a></p></td>
<td><p>Returns the cumulative product of elements of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> in the dimension <code class="xref py py-attr docutils literal notranslate"><span class="pre">dim</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.cumsum"/><a class="reference internal" href="generated/torch.cumsum.html#torch.cumsum" title="torch.cumsum"><code class="xref py py-obj docutils literal notranslate"><span class="pre">cumsum</span></code></a></p></td>
<td><p>Returns the cumulative sum of elements of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> in the dimension <code class="xref py py-attr docutils literal notranslate"><span class="pre">dim</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.diag"/><a class="reference internal" href="generated/torch.diag.html#torch.diag" title="torch.diag"><code class="xref py py-obj docutils literal notranslate"><span class="pre">diag</span></code></a></p></td>
<td><p><ul class="simple">
<li><p>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> is a vector (1-D tensor), then returns a 2-D square tensor</p></li>
</ul>
</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.diag_embed"/><a class="reference internal" href="generated/torch.diag_embed.html#torch.diag_embed" title="torch.diag_embed"><code class="xref py py-obj docutils literal notranslate"><span class="pre">diag_embed</span></code></a></p></td>
<td><p>Creates a tensor whose diagonals of certain 2D planes (specified by <code class="xref py py-attr docutils literal notranslate"><span class="pre">dim1</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">dim2</span></code>) are filled by <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.diagflat"/><a class="reference internal" href="generated/torch.diagflat.html#torch.diagflat" title="torch.diagflat"><code class="xref py py-obj docutils literal notranslate"><span class="pre">diagflat</span></code></a></p></td>
<td><p><ul class="simple">
<li><p>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> is a vector (1-D tensor), then returns a 2-D square tensor</p></li>
</ul>
</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.diagonal"/><a class="reference internal" href="generated/torch.diagonal.html#torch.diagonal" title="torch.diagonal"><code class="xref py py-obj docutils literal notranslate"><span class="pre">diagonal</span></code></a></p></td>
<td><p>Returns a partial view of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> with the its diagonal elements with respect to <code class="xref py py-attr docutils literal notranslate"><span class="pre">dim1</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">dim2</span></code> appended as a dimension at the end of the shape.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.diff"/><a class="reference internal" href="generated/torch.diff.html#torch.diff" title="torch.diff"><code class="xref py py-obj docutils literal notranslate"><span class="pre">diff</span></code></a></p></td>
<td><p>Computes the n-th forward difference along the given dimension.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.einsum"/><a class="reference internal" href="generated/torch.einsum.html#torch.einsum" title="torch.einsum"><code class="xref py py-obj docutils literal notranslate"><span class="pre">einsum</span></code></a></p></td>
<td><p>Sums the product of the elements of the input <code class="xref py py-attr docutils literal notranslate"><span class="pre">operands</span></code> along dimensions specified using a notation based on the Einstein summation convention.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.flatten"/><a class="reference internal" href="generated/torch.flatten.html#torch.flatten" title="torch.flatten"><code class="xref py py-obj docutils literal notranslate"><span class="pre">flatten</span></code></a></p></td>
<td><p>Flattens <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> by reshaping it into a one-dimensional tensor.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.flip"/><a class="reference internal" href="generated/torch.flip.html#torch.flip" title="torch.flip"><code class="xref py py-obj docutils literal notranslate"><span class="pre">flip</span></code></a></p></td>
<td><p>Reverse the order of an n-D tensor along given axis in dims.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.fliplr"/><a class="reference internal" href="generated/torch.fliplr.html#torch.fliplr" title="torch.fliplr"><code class="xref py py-obj docutils literal notranslate"><span class="pre">fliplr</span></code></a></p></td>
<td><p>Flip tensor in the left/right direction, returning a new tensor.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.flipud"/><a class="reference internal" href="generated/torch.flipud.html#torch.flipud" title="torch.flipud"><code class="xref py py-obj docutils literal notranslate"><span class="pre">flipud</span></code></a></p></td>
<td><p>Flip tensor in the up/down direction, returning a new tensor.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.kron"/><a class="reference internal" href="generated/torch.kron.html#torch.kron" title="torch.kron"><code class="xref py py-obj docutils literal notranslate"><span class="pre">kron</span></code></a></p></td>
<td><p>Computes the Kronecker product, denoted by <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>⊗</mo></mrow><annotation encoding="application/x-tex">\otimes</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6667em;vertical-align:-0.0833em;"></span><span class="mord">⊗</span></span></span></span></span>, of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">other</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.rot90"/><a class="reference internal" href="generated/torch.rot90.html#torch.rot90" title="torch.rot90"><code class="xref py py-obj docutils literal notranslate"><span class="pre">rot90</span></code></a></p></td>
<td><p>Rotate an n-D tensor by 90 degrees in the plane specified by dims axis.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.gcd"/><a class="reference internal" href="generated/torch.gcd.html#torch.gcd" title="torch.gcd"><code class="xref py py-obj docutils literal notranslate"><span class="pre">gcd</span></code></a></p></td>
<td><p>Computes the element-wise greatest common divisor (GCD) of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">other</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.histc"/><a class="reference internal" href="generated/torch.histc.html#torch.histc" title="torch.histc"><code class="xref py py-obj docutils literal notranslate"><span class="pre">histc</span></code></a></p></td>
<td><p>Computes the histogram of a tensor.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.histogram"/><a class="reference internal" href="generated/torch.histogram.html#torch.histogram" title="torch.histogram"><code class="xref py py-obj docutils literal notranslate"><span class="pre">histogram</span></code></a></p></td>
<td><p>Computes a histogram of the values in a tensor.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.histogramdd"/><a class="reference internal" href="generated/torch.histogramdd.html#torch.histogramdd" title="torch.histogramdd"><code class="xref py py-obj docutils literal notranslate"><span class="pre">histogramdd</span></code></a></p></td>
<td><p>Computes a multi-dimensional histogram of the values in a tensor.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.meshgrid"/><a class="reference internal" href="generated/torch.meshgrid.html#torch.meshgrid" title="torch.meshgrid"><code class="xref py py-obj docutils literal notranslate"><span class="pre">meshgrid</span></code></a></p></td>
<td><p>Creates grids of coordinates specified by the 1D inputs in <cite>attr</cite>:tensors.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.lcm"/><a class="reference internal" href="generated/torch.lcm.html#torch.lcm" title="torch.lcm"><code class="xref py py-obj docutils literal notranslate"><span class="pre">lcm</span></code></a></p></td>
<td><p>Computes the element-wise least common multiple (LCM) of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">other</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.logcumsumexp"/><a class="reference internal" href="generated/torch.logcumsumexp.html#torch.logcumsumexp" title="torch.logcumsumexp"><code class="xref py py-obj docutils literal notranslate"><span class="pre">logcumsumexp</span></code></a></p></td>
<td><p>Returns the logarithm of the cumulative summation of the exponentiation of elements of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> in the dimension <code class="xref py py-attr docutils literal notranslate"><span class="pre">dim</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.ravel"/><a class="reference internal" href="generated/torch.ravel.html#torch.ravel" title="torch.ravel"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ravel</span></code></a></p></td>
<td><p>Return a contiguous flattened tensor.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.renorm"/><a class="reference internal" href="generated/torch.renorm.html#torch.renorm" title="torch.renorm"><code class="xref py py-obj docutils literal notranslate"><span class="pre">renorm</span></code></a></p></td>
<td><p>Returns a tensor where each sub-tensor of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> along dimension <code class="xref py py-attr docutils literal notranslate"><span class="pre">dim</span></code> is normalized such that the <cite>p</cite>-norm of the sub-tensor is lower than the value <code class="xref py py-attr docutils literal notranslate"><span class="pre">maxnorm</span></code></p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.repeat_interleave"/><a class="reference internal" href="generated/torch.repeat_interleave.html#torch.repeat_interleave" title="torch.repeat_interleave"><code class="xref py py-obj docutils literal notranslate"><span class="pre">repeat_interleave</span></code></a></p></td>
<td><p>Repeat elements of a tensor.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.roll"/><a class="reference internal" href="generated/torch.roll.html#torch.roll" title="torch.roll"><code class="xref py py-obj docutils literal notranslate"><span class="pre">roll</span></code></a></p></td>
<td><p>Roll the tensor <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> along the given dimension(s).</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.searchsorted"/><a class="reference internal" href="generated/torch.searchsorted.html#torch.searchsorted" title="torch.searchsorted"><code class="xref py py-obj docutils literal notranslate"><span class="pre">searchsorted</span></code></a></p></td>
<td><p>Find the indices from the <em>innermost</em> dimension of <code class="xref py py-attr docutils literal notranslate"><span class="pre">sorted_sequence</span></code> such that, if the corresponding values in <code class="xref py py-attr docutils literal notranslate"><span class="pre">values</span></code> were inserted before the indices, when sorted, the order of the corresponding <em>innermost</em> dimension within <code class="xref py py-attr docutils literal notranslate"><span class="pre">sorted_sequence</span></code> would be preserved.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.tensordot"/><a class="reference internal" href="generated/torch.tensordot.html#torch.tensordot" title="torch.tensordot"><code class="xref py py-obj docutils literal notranslate"><span class="pre">tensordot</span></code></a></p></td>
<td><p>Returns a contraction of a and b over multiple dimensions.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.trace"/><a class="reference internal" href="generated/torch.trace.html#torch.trace" title="torch.trace"><code class="xref py py-obj docutils literal notranslate"><span class="pre">trace</span></code></a></p></td>
<td><p>Returns the sum of the elements of the diagonal of the input 2-D matrix.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.tril"/><a class="reference internal" href="generated/torch.tril.html#torch.tril" title="torch.tril"><code class="xref py py-obj docutils literal notranslate"><span class="pre">tril</span></code></a></p></td>
<td><p>Returns the lower triangular part of the matrix (2-D tensor) or batch of matrices <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>, the other elements of the result tensor <code class="xref py py-attr docutils literal notranslate"><span class="pre">out</span></code> are set to 0.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.tril_indices"/><a class="reference internal" href="generated/torch.tril_indices.html#torch.tril_indices" title="torch.tril_indices"><code class="xref py py-obj docutils literal notranslate"><span class="pre">tril_indices</span></code></a></p></td>
<td><p>Returns the indices of the lower triangular part of a <code class="xref py py-attr docutils literal notranslate"><span class="pre">row</span></code>-by- <code class="xref py py-attr docutils literal notranslate"><span class="pre">col</span></code> matrix in a 2-by-N Tensor, where the first row contains row coordinates of all indices and the second row contains column coordinates.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.triu"/><a class="reference internal" href="generated/torch.triu.html#torch.triu" title="torch.triu"><code class="xref py py-obj docutils literal notranslate"><span class="pre">triu</span></code></a></p></td>
<td><p>Returns the upper triangular part of a matrix (2-D tensor) or batch of matrices <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>, the other elements of the result tensor <code class="xref py py-attr docutils literal notranslate"><span class="pre">out</span></code> are set to 0.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.triu_indices"/><a class="reference internal" href="generated/torch.triu_indices.html#torch.triu_indices" title="torch.triu_indices"><code class="xref py py-obj docutils literal notranslate"><span class="pre">triu_indices</span></code></a></p></td>
<td><p>Returns the indices of the upper triangular part of a <code class="xref py py-attr docutils literal notranslate"><span class="pre">row</span></code> by <code class="xref py py-attr docutils literal notranslate"><span class="pre">col</span></code> matrix in a 2-by-N Tensor, where the first row contains row coordinates of all indices and the second row contains column coordinates.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.unflatten"/><a class="reference internal" href="generated/torch.unflatten.html#torch.unflatten" title="torch.unflatten"><code class="xref py py-obj docutils literal notranslate"><span class="pre">unflatten</span></code></a></p></td>
<td><p>Expands a dimension of the input tensor over multiple dimensions.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.vander"/><a class="reference internal" href="generated/torch.vander.html#torch.vander" title="torch.vander"><code class="xref py py-obj docutils literal notranslate"><span class="pre">vander</span></code></a></p></td>
<td><p>Generates a Vandermonde matrix.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.view_as_real"/><a class="reference internal" href="generated/torch.view_as_real.html#torch.view_as_real" title="torch.view_as_real"><code class="xref py py-obj docutils literal notranslate"><span class="pre">view_as_real</span></code></a></p></td>
<td><p>Returns a view of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> as a real tensor.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.view_as_complex"/><a class="reference internal" href="generated/torch.view_as_complex.html#torch.view_as_complex" title="torch.view_as_complex"><code class="xref py py-obj docutils literal notranslate"><span class="pre">view_as_complex</span></code></a></p></td>
<td><p>Returns a view of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> as a complex tensor.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.resolve_conj"/><a class="reference internal" href="generated/torch.resolve_conj.html#torch.resolve_conj" title="torch.resolve_conj"><code class="xref py py-obj docutils literal notranslate"><span class="pre">resolve_conj</span></code></a></p></td>
<td><p>Returns a new tensor with materialized conjugation if <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>'s conjugate bit is set to <cite>True</cite>, else returns <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.resolve_neg"/><a class="reference internal" href="generated/torch.resolve_neg.html#torch.resolve_neg" title="torch.resolve_neg"><code class="xref py py-obj docutils literal notranslate"><span class="pre">resolve_neg</span></code></a></p></td>
<td><p>Returns a new tensor with materialized negation if <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>'s negative bit is set to <cite>True</cite>, else returns <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="blas-and-lapack-operations">
<h3>BLAS and LAPACK Operations<a class="headerlink" href="#blas-and-lapack-operations" title="Permalink to this heading">#</a></h3>
<div class="pst-scrollable-table-container"><table class="autosummary longtable table autosummary">
<tbody>
<tr class="row-odd"><td><p><p id="torch.addbmm"/><a class="reference internal" href="generated/torch.addbmm.html#torch.addbmm" title="torch.addbmm"><code class="xref py py-obj docutils literal notranslate"><span class="pre">addbmm</span></code></a></p></td>
<td><p>Performs a batch matrix-matrix product of matrices stored in <code class="xref py py-attr docutils literal notranslate"><span class="pre">batch1</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">batch2</span></code>, with a reduced add step (all matrix multiplications get accumulated along the first dimension).</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.addmm"/><a class="reference internal" href="generated/torch.addmm.html#torch.addmm" title="torch.addmm"><code class="xref py py-obj docutils literal notranslate"><span class="pre">addmm</span></code></a></p></td>
<td><p>Performs a matrix multiplication of the matrices <code class="xref py py-attr docutils literal notranslate"><span class="pre">mat1</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">mat2</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.addmv"/><a class="reference internal" href="generated/torch.addmv.html#torch.addmv" title="torch.addmv"><code class="xref py py-obj docutils literal notranslate"><span class="pre">addmv</span></code></a></p></td>
<td><p>Performs a matrix-vector product of the matrix <code class="xref py py-attr docutils literal notranslate"><span class="pre">mat</span></code> and the vector <code class="xref py py-attr docutils literal notranslate"><span class="pre">vec</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.addr"/><a class="reference internal" href="generated/torch.addr.html#torch.addr" title="torch.addr"><code class="xref py py-obj docutils literal notranslate"><span class="pre">addr</span></code></a></p></td>
<td><p>Performs the outer-product of vectors <code class="xref py py-attr docutils literal notranslate"><span class="pre">vec1</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">vec2</span></code> and adds it to the matrix <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.baddbmm"/><a class="reference internal" href="generated/torch.baddbmm.html#torch.baddbmm" title="torch.baddbmm"><code class="xref py py-obj docutils literal notranslate"><span class="pre">baddbmm</span></code></a></p></td>
<td><p>Performs a batch matrix-matrix product of matrices in <code class="xref py py-attr docutils literal notranslate"><span class="pre">batch1</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">batch2</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.bmm"/><a class="reference internal" href="generated/torch.bmm.html#torch.bmm" title="torch.bmm"><code class="xref py py-obj docutils literal notranslate"><span class="pre">bmm</span></code></a></p></td>
<td><p>Performs a batch matrix-matrix product of matrices stored in <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">mat2</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.chain_matmul"/><a class="reference internal" href="generated/torch.chain_matmul.html#torch.chain_matmul" title="torch.chain_matmul"><code class="xref py py-obj docutils literal notranslate"><span class="pre">chain_matmul</span></code></a></p></td>
<td><p>Returns the matrix product of the <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi></mrow><annotation encoding="application/x-tex">N</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span></span></span></span></span> 2-D tensors.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.cholesky"/><a class="reference internal" href="generated/torch.cholesky.html#torch.cholesky" title="torch.cholesky"><code class="xref py py-obj docutils literal notranslate"><span class="pre">cholesky</span></code></a></p></td>
<td><p>Computes the Cholesky decomposition of a symmetric positive-definite matrix <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi></mrow><annotation encoding="application/x-tex">A</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal">A</span></span></span></span></span> or for batches of symmetric positive-definite matrices.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.cholesky_inverse"/><a class="reference internal" href="generated/torch.cholesky_inverse.html#torch.cholesky_inverse" title="torch.cholesky_inverse"><code class="xref py py-obj docutils literal notranslate"><span class="pre">cholesky_inverse</span></code></a></p></td>
<td><p>Computes the inverse of a complex Hermitian or real symmetric positive-definite matrix given its Cholesky decomposition.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.cholesky_solve"/><a class="reference internal" href="generated/torch.cholesky_solve.html#torch.cholesky_solve" title="torch.cholesky_solve"><code class="xref py py-obj docutils literal notranslate"><span class="pre">cholesky_solve</span></code></a></p></td>
<td><p>Computes the solution of a system of linear equations with complex Hermitian or real symmetric positive-definite lhs given its Cholesky decomposition.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.dot"/><a class="reference internal" href="generated/torch.dot.html#torch.dot" title="torch.dot"><code class="xref py py-obj docutils literal notranslate"><span class="pre">dot</span></code></a></p></td>
<td><p>Computes the dot product of two 1D tensors.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.geqrf"/><a class="reference internal" href="generated/torch.geqrf.html#torch.geqrf" title="torch.geqrf"><code class="xref py py-obj docutils literal notranslate"><span class="pre">geqrf</span></code></a></p></td>
<td><p>This is a low-level function for calling LAPACK's geqrf directly.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.ger"/><a class="reference internal" href="generated/torch.ger.html#torch.ger" title="torch.ger"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ger</span></code></a></p></td>
<td><p>Alias of <a class="reference internal" href="generated/torch.outer.html#torch.outer" title="torch.outer"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.outer()</span></code></a>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.inner"/><a class="reference internal" href="generated/torch.inner.html#torch.inner" title="torch.inner"><code class="xref py py-obj docutils literal notranslate"><span class="pre">inner</span></code></a></p></td>
<td><p>Computes the dot product for 1D tensors.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.inverse"/><a class="reference internal" href="generated/torch.inverse.html#torch.inverse" title="torch.inverse"><code class="xref py py-obj docutils literal notranslate"><span class="pre">inverse</span></code></a></p></td>
<td><p>Alias for <a class="reference internal" href="generated/torch.linalg.inv.html#torch.linalg.inv" title="torch.linalg.inv"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.linalg.inv()</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.det"/><a class="reference internal" href="generated/torch.det.html#torch.det" title="torch.det"><code class="xref py py-obj docutils literal notranslate"><span class="pre">det</span></code></a></p></td>
<td><p>Alias for <a class="reference internal" href="generated/torch.linalg.det.html#torch.linalg.det" title="torch.linalg.det"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.linalg.det()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.logdet"/><a class="reference internal" href="generated/torch.logdet.html#torch.logdet" title="torch.logdet"><code class="xref py py-obj docutils literal notranslate"><span class="pre">logdet</span></code></a></p></td>
<td><p>Calculates log determinant of a square matrix or batches of square matrices.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.slogdet"/><a class="reference internal" href="generated/torch.slogdet.html#torch.slogdet" title="torch.slogdet"><code class="xref py py-obj docutils literal notranslate"><span class="pre">slogdet</span></code></a></p></td>
<td><p>Alias for <a class="reference internal" href="generated/torch.linalg.slogdet.html#torch.linalg.slogdet" title="torch.linalg.slogdet"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.linalg.slogdet()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.lu"/><a class="reference internal" href="generated/torch.lu.html#torch.lu" title="torch.lu"><code class="xref py py-obj docutils literal notranslate"><span class="pre">lu</span></code></a></p></td>
<td><p>Computes the LU factorization of a matrix or batches of matrices <code class="xref py py-attr docutils literal notranslate"><span class="pre">A</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.lu_solve"/><a class="reference internal" href="generated/torch.lu_solve.html#torch.lu_solve" title="torch.lu_solve"><code class="xref py py-obj docutils literal notranslate"><span class="pre">lu_solve</span></code></a></p></td>
<td><p>Returns the LU solve of the linear system <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi><mi>x</mi><mo>=</mo><mi>b</mi></mrow><annotation encoding="application/x-tex">Ax = b</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal">A</span><span class="mord mathnormal">x</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal">b</span></span></span></span></span> using the partially pivoted LU factorization of A from <a class="reference internal" href="generated/torch.linalg.lu_factor.html#torch.linalg.lu_factor" title="torch.linalg.lu_factor"><code class="xref py py-func docutils literal notranslate"><span class="pre">lu_factor()</span></code></a>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.lu_unpack"/><a class="reference internal" href="generated/torch.lu_unpack.html#torch.lu_unpack" title="torch.lu_unpack"><code class="xref py py-obj docutils literal notranslate"><span class="pre">lu_unpack</span></code></a></p></td>
<td><p>Unpacks the LU decomposition returned by <a class="reference internal" href="generated/torch.linalg.lu_factor.html#torch.linalg.lu_factor" title="torch.linalg.lu_factor"><code class="xref py py-func docutils literal notranslate"><span class="pre">lu_factor()</span></code></a> into the <cite>P, L, U</cite> matrices.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.matmul"/><a class="reference internal" href="generated/torch.matmul.html#torch.matmul" title="torch.matmul"><code class="xref py py-obj docutils literal notranslate"><span class="pre">matmul</span></code></a></p></td>
<td><p>Matrix product of two tensors.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.matrix_power"/><a class="reference internal" href="generated/torch.matrix_power.html#torch.matrix_power" title="torch.matrix_power"><code class="xref py py-obj docutils literal notranslate"><span class="pre">matrix_power</span></code></a></p></td>
<td><p>Alias for <a class="reference internal" href="generated/torch.linalg.matrix_power.html#torch.linalg.matrix_power" title="torch.linalg.matrix_power"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.linalg.matrix_power()</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.matrix_exp"/><a class="reference internal" href="generated/torch.matrix_exp.html#torch.matrix_exp" title="torch.matrix_exp"><code class="xref py py-obj docutils literal notranslate"><span class="pre">matrix_exp</span></code></a></p></td>
<td><p>Alias for <a class="reference internal" href="generated/torch.linalg.matrix_exp.html#torch.linalg.matrix_exp" title="torch.linalg.matrix_exp"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.linalg.matrix_exp()</span></code></a>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.mm"/><a class="reference internal" href="generated/torch.mm.html#torch.mm" title="torch.mm"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mm</span></code></a></p></td>
<td><p>Performs a matrix multiplication of the matrices <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">mat2</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.mv"/><a class="reference internal" href="generated/torch.mv.html#torch.mv" title="torch.mv"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mv</span></code></a></p></td>
<td><p>Performs a matrix-vector product of the matrix <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> and the vector <code class="xref py py-attr docutils literal notranslate"><span class="pre">vec</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.orgqr"/><a class="reference internal" href="generated/torch.orgqr.html#torch.orgqr" title="torch.orgqr"><code class="xref py py-obj docutils literal notranslate"><span class="pre">orgqr</span></code></a></p></td>
<td><p>Alias for <a class="reference internal" href="generated/torch.linalg.householder_product.html#torch.linalg.householder_product" title="torch.linalg.householder_product"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.linalg.householder_product()</span></code></a>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.ormqr"/><a class="reference internal" href="generated/torch.ormqr.html#torch.ormqr" title="torch.ormqr"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ormqr</span></code></a></p></td>
<td><p>Computes the matrix-matrix multiplication of a product of Householder matrices with a general matrix.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.outer"/><a class="reference internal" href="generated/torch.outer.html#torch.outer" title="torch.outer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">outer</span></code></a></p></td>
<td><p>Outer product of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">vec2</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.pinverse"/><a class="reference internal" href="generated/torch.pinverse.html#torch.pinverse" title="torch.pinverse"><code class="xref py py-obj docutils literal notranslate"><span class="pre">pinverse</span></code></a></p></td>
<td><p>Alias for <a class="reference internal" href="generated/torch.linalg.pinv.html#torch.linalg.pinv" title="torch.linalg.pinv"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.linalg.pinv()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.qr"/><a class="reference internal" href="generated/torch.qr.html#torch.qr" title="torch.qr"><code class="xref py py-obj docutils literal notranslate"><span class="pre">qr</span></code></a></p></td>
<td><p>Computes the QR decomposition of a matrix or a batch of matrices <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>, and returns a namedtuple (Q, R) of tensors such that <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>input</mtext><mo>=</mo><mi>Q</mi><mi>R</mi></mrow><annotation encoding="application/x-tex">\text{input} = Q R</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8623em;vertical-align:-0.1944em;"></span><span class="mord text"><span class="mord">input</span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.8778em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.00773em;">QR</span></span></span></span></span> with <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Q</mi></mrow><annotation encoding="application/x-tex">Q</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8778em;vertical-align:-0.1944em;"></span><span class="mord mathnormal">Q</span></span></span></span></span> being an orthogonal matrix or batch of orthogonal matrices and <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>R</mi></mrow><annotation encoding="application/x-tex">R</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.00773em;">R</span></span></span></span></span> being an upper triangular matrix or batch of upper triangular matrices.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.svd"/><a class="reference internal" href="generated/torch.svd.html#torch.svd" title="torch.svd"><code class="xref py py-obj docutils literal notranslate"><span class="pre">svd</span></code></a></p></td>
<td><p>Computes the singular value decomposition of either a matrix or batch of matrices <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.svd_lowrank"/><a class="reference internal" href="generated/torch.svd_lowrank.html#torch.svd_lowrank" title="torch.svd_lowrank"><code class="xref py py-obj docutils literal notranslate"><span class="pre">svd_lowrank</span></code></a></p></td>
<td><p>Return the singular value decomposition <code class="docutils literal notranslate"><span class="pre">(U,</span> <span class="pre">S,</span> <span class="pre">V)</span></code> of a matrix, batches of matrices, or a sparse matrix <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi></mrow><annotation encoding="application/x-tex">A</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal">A</span></span></span></span></span> such that <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi><mo>≈</mo><mi>U</mi><mi mathvariant="normal">diag</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi>S</mi><mo stretchy="false">)</mo><msup><mi>V</mi><mtext>H</mtext></msup></mrow><annotation encoding="application/x-tex">A \approx U \operatorname{diag}(S) V^{\text{H}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal">A</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">≈</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.0913em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">U</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mop"><span class="mord mathrm" style="margin-right:0.01389em;">diag</span></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.05764em;">S</span><span class="mclose">)</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.22222em;">V</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">H</span></span></span></span></span></span></span></span></span></span></span></span></span></span>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.pca_lowrank"/><a class="reference internal" href="generated/torch.pca_lowrank.html#torch.pca_lowrank" title="torch.pca_lowrank"><code class="xref py py-obj docutils literal notranslate"><span class="pre">pca_lowrank</span></code></a></p></td>
<td><p>Performs linear Principal Component Analysis (PCA) on a low-rank matrix, batches of such matrices, or sparse matrix.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.lobpcg"/><a class="reference internal" href="generated/torch.lobpcg.html#torch.lobpcg" title="torch.lobpcg"><code class="xref py py-obj docutils literal notranslate"><span class="pre">lobpcg</span></code></a></p></td>
<td><p>Find the k largest (or smallest) eigenvalues and the corresponding eigenvectors of a symmetric positive definite generalized eigenvalue problem using matrix-free LOBPCG methods.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.trapz"/><a class="reference internal" href="generated/torch.trapz.html#torch.trapz" title="torch.trapz"><code class="xref py py-obj docutils literal notranslate"><span class="pre">trapz</span></code></a></p></td>
<td><p>Alias for <a class="reference internal" href="generated/torch.trapezoid.html#torch.trapezoid" title="torch.trapezoid"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.trapezoid()</span></code></a>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.trapezoid"/><a class="reference internal" href="generated/torch.trapezoid.html#torch.trapezoid" title="torch.trapezoid"><code class="xref py py-obj docutils literal notranslate"><span class="pre">trapezoid</span></code></a></p></td>
<td><p>Computes the <a class="reference external" href="https://en.wikipedia.org/wiki/Trapezoidal_rule">trapezoidal rule</a> along <code class="xref py py-attr docutils literal notranslate"><span class="pre">dim</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.cumulative_trapezoid"/><a class="reference internal" href="generated/torch.cumulative_trapezoid.html#torch.cumulative_trapezoid" title="torch.cumulative_trapezoid"><code class="xref py py-obj docutils literal notranslate"><span class="pre">cumulative_trapezoid</span></code></a></p></td>
<td><p><p>Cumulatively computes the <a class="reference external" href="https://en.wikipedia.org/wiki/Trapezoidal_rule">trapezoidal rule</a> along <code class="xref py py-attr docutils literal notranslate"><span class="pre">dim</span></code>.</p>
</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.triangular_solve"/><a class="reference internal" href="generated/torch.triangular_solve.html#torch.triangular_solve" title="torch.triangular_solve"><code class="xref py py-obj docutils literal notranslate"><span class="pre">triangular_solve</span></code></a></p></td>
<td><p>Solves a system of equations with a square upper or lower triangular invertible matrix <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi></mrow><annotation encoding="application/x-tex">A</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal">A</span></span></span></span></span> and multiple right-hand sides <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>b</mi></mrow><annotation encoding="application/x-tex">b</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal">b</span></span></span></span></span>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.vdot"/><a class="reference internal" href="generated/torch.vdot.html#torch.vdot" title="torch.vdot"><code class="xref py py-obj docutils literal notranslate"><span class="pre">vdot</span></code></a></p></td>
<td><p>Computes the dot product of two 1D vectors along a dimension.</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="foreach-operations">
<h3>Foreach Operations<a class="headerlink" href="#foreach-operations" title="Permalink to this heading">#</a></h3>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>This API is in beta and subject to future changes.
Forward-mode AD is not supported.</p>
</div>
<div class="pst-scrollable-table-container"><table class="autosummary longtable table autosummary">
<tbody>
<tr class="row-odd"><td><p><p id="torch._foreach_abs"/><a class="reference internal" href="generated/torch._foreach_abs.html#torch._foreach_abs" title="torch._foreach_abs"><code class="xref py py-obj docutils literal notranslate"><span class="pre">_foreach_abs</span></code></a></p></td>
<td><p>Apply <a class="reference internal" href="generated/torch.abs.html#torch.abs" title="torch.abs"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.abs()</span></code></a> to each Tensor of the input list.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch._foreach_abs_"/><a class="reference internal" href="generated/torch._foreach_abs_.html#torch._foreach_abs_" title="torch._foreach_abs_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">_foreach_abs_</span></code></a></p></td>
<td><p>Apply <a class="reference internal" href="generated/torch.abs.html#torch.abs" title="torch.abs"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.abs()</span></code></a> to each Tensor of the input list.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch._foreach_acos"/><a class="reference internal" href="generated/torch._foreach_acos.html#torch._foreach_acos" title="torch._foreach_acos"><code class="xref py py-obj docutils literal notranslate"><span class="pre">_foreach_acos</span></code></a></p></td>
<td><p>Apply <a class="reference internal" href="generated/torch.acos.html#torch.acos" title="torch.acos"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.acos()</span></code></a> to each Tensor of the input list.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch._foreach_acos_"/><a class="reference internal" href="generated/torch._foreach_acos_.html#torch._foreach_acos_" title="torch._foreach_acos_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">_foreach_acos_</span></code></a></p></td>
<td><p>Apply <a class="reference internal" href="generated/torch.acos.html#torch.acos" title="torch.acos"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.acos()</span></code></a> to each Tensor of the input list.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch._foreach_asin"/><a class="reference internal" href="generated/torch._foreach_asin.html#torch._foreach_asin" title="torch._foreach_asin"><code class="xref py py-obj docutils literal notranslate"><span class="pre">_foreach_asin</span></code></a></p></td>
<td><p>Apply <a class="reference internal" href="generated/torch.asin.html#torch.asin" title="torch.asin"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.asin()</span></code></a> to each Tensor of the input list.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch._foreach_asin_"/><a class="reference internal" href="generated/torch._foreach_asin_.html#torch._foreach_asin_" title="torch._foreach_asin_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">_foreach_asin_</span></code></a></p></td>
<td><p>Apply <a class="reference internal" href="generated/torch.asin.html#torch.asin" title="torch.asin"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.asin()</span></code></a> to each Tensor of the input list.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch._foreach_atan"/><a class="reference internal" href="generated/torch._foreach_atan.html#torch._foreach_atan" title="torch._foreach_atan"><code class="xref py py-obj docutils literal notranslate"><span class="pre">_foreach_atan</span></code></a></p></td>
<td><p>Apply <a class="reference internal" href="generated/torch.atan.html#torch.atan" title="torch.atan"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.atan()</span></code></a> to each Tensor of the input list.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch._foreach_atan_"/><a class="reference internal" href="generated/torch._foreach_atan_.html#torch._foreach_atan_" title="torch._foreach_atan_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">_foreach_atan_</span></code></a></p></td>
<td><p>Apply <a class="reference internal" href="generated/torch.atan.html#torch.atan" title="torch.atan"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.atan()</span></code></a> to each Tensor of the input list.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch._foreach_ceil"/><a class="reference internal" href="generated/torch._foreach_ceil.html#torch._foreach_ceil" title="torch._foreach_ceil"><code class="xref py py-obj docutils literal notranslate"><span class="pre">_foreach_ceil</span></code></a></p></td>
<td><p>Apply <a class="reference internal" href="generated/torch.ceil.html#torch.ceil" title="torch.ceil"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.ceil()</span></code></a> to each Tensor of the input list.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch._foreach_ceil_"/><a class="reference internal" href="generated/torch._foreach_ceil_.html#torch._foreach_ceil_" title="torch._foreach_ceil_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">_foreach_ceil_</span></code></a></p></td>
<td><p>Apply <a class="reference internal" href="generated/torch.ceil.html#torch.ceil" title="torch.ceil"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.ceil()</span></code></a> to each Tensor of the input list.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch._foreach_cos"/><a class="reference internal" href="generated/torch._foreach_cos.html#torch._foreach_cos" title="torch._foreach_cos"><code class="xref py py-obj docutils literal notranslate"><span class="pre">_foreach_cos</span></code></a></p></td>
<td><p>Apply <a class="reference internal" href="generated/torch.cos.html#torch.cos" title="torch.cos"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.cos()</span></code></a> to each Tensor of the input list.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch._foreach_cos_"/><a class="reference internal" href="generated/torch._foreach_cos_.html#torch._foreach_cos_" title="torch._foreach_cos_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">_foreach_cos_</span></code></a></p></td>
<td><p>Apply <a class="reference internal" href="generated/torch.cos.html#torch.cos" title="torch.cos"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.cos()</span></code></a> to each Tensor of the input list.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch._foreach_cosh"/><a class="reference internal" href="generated/torch._foreach_cosh.html#torch._foreach_cosh" title="torch._foreach_cosh"><code class="xref py py-obj docutils literal notranslate"><span class="pre">_foreach_cosh</span></code></a></p></td>
<td><p>Apply <a class="reference internal" href="generated/torch.cosh.html#torch.cosh" title="torch.cosh"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.cosh()</span></code></a> to each Tensor of the input list.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch._foreach_cosh_"/><a class="reference internal" href="generated/torch._foreach_cosh_.html#torch._foreach_cosh_" title="torch._foreach_cosh_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">_foreach_cosh_</span></code></a></p></td>
<td><p>Apply <a class="reference internal" href="generated/torch.cosh.html#torch.cosh" title="torch.cosh"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.cosh()</span></code></a> to each Tensor of the input list.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch._foreach_erf"/><a class="reference internal" href="generated/torch._foreach_erf.html#torch._foreach_erf" title="torch._foreach_erf"><code class="xref py py-obj docutils literal notranslate"><span class="pre">_foreach_erf</span></code></a></p></td>
<td><p>Apply <a class="reference internal" href="generated/torch.erf.html#torch.erf" title="torch.erf"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.erf()</span></code></a> to each Tensor of the input list.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch._foreach_erf_"/><a class="reference internal" href="generated/torch._foreach_erf_.html#torch._foreach_erf_" title="torch._foreach_erf_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">_foreach_erf_</span></code></a></p></td>
<td><p>Apply <a class="reference internal" href="generated/torch.erf.html#torch.erf" title="torch.erf"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.erf()</span></code></a> to each Tensor of the input list.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch._foreach_erfc"/><a class="reference internal" href="generated/torch._foreach_erfc.html#torch._foreach_erfc" title="torch._foreach_erfc"><code class="xref py py-obj docutils literal notranslate"><span class="pre">_foreach_erfc</span></code></a></p></td>
<td><p>Apply <a class="reference internal" href="generated/torch.erfc.html#torch.erfc" title="torch.erfc"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.erfc()</span></code></a> to each Tensor of the input list.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch._foreach_erfc_"/><a class="reference internal" href="generated/torch._foreach_erfc_.html#torch._foreach_erfc_" title="torch._foreach_erfc_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">_foreach_erfc_</span></code></a></p></td>
<td><p>Apply <a class="reference internal" href="generated/torch.erfc.html#torch.erfc" title="torch.erfc"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.erfc()</span></code></a> to each Tensor of the input list.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch._foreach_exp"/><a class="reference internal" href="generated/torch._foreach_exp.html#torch._foreach_exp" title="torch._foreach_exp"><code class="xref py py-obj docutils literal notranslate"><span class="pre">_foreach_exp</span></code></a></p></td>
<td><p>Apply <a class="reference internal" href="generated/torch.exp.html#torch.exp" title="torch.exp"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.exp()</span></code></a> to each Tensor of the input list.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch._foreach_exp_"/><a class="reference internal" href="generated/torch._foreach_exp_.html#torch._foreach_exp_" title="torch._foreach_exp_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">_foreach_exp_</span></code></a></p></td>
<td><p>Apply <a class="reference internal" href="generated/torch.exp.html#torch.exp" title="torch.exp"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.exp()</span></code></a> to each Tensor of the input list.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch._foreach_expm1"/><a class="reference internal" href="generated/torch._foreach_expm1.html#torch._foreach_expm1" title="torch._foreach_expm1"><code class="xref py py-obj docutils literal notranslate"><span class="pre">_foreach_expm1</span></code></a></p></td>
<td><p>Apply <a class="reference internal" href="generated/torch.expm1.html#torch.expm1" title="torch.expm1"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.expm1()</span></code></a> to each Tensor of the input list.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch._foreach_expm1_"/><a class="reference internal" href="generated/torch._foreach_expm1_.html#torch._foreach_expm1_" title="torch._foreach_expm1_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">_foreach_expm1_</span></code></a></p></td>
<td><p>Apply <a class="reference internal" href="generated/torch.expm1.html#torch.expm1" title="torch.expm1"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.expm1()</span></code></a> to each Tensor of the input list.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch._foreach_floor"/><a class="reference internal" href="generated/torch._foreach_floor.html#torch._foreach_floor" title="torch._foreach_floor"><code class="xref py py-obj docutils literal notranslate"><span class="pre">_foreach_floor</span></code></a></p></td>
<td><p>Apply <a class="reference internal" href="generated/torch.floor.html#torch.floor" title="torch.floor"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.floor()</span></code></a> to each Tensor of the input list.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch._foreach_floor_"/><a class="reference internal" href="generated/torch._foreach_floor_.html#torch._foreach_floor_" title="torch._foreach_floor_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">_foreach_floor_</span></code></a></p></td>
<td><p>Apply <a class="reference internal" href="generated/torch.floor.html#torch.floor" title="torch.floor"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.floor()</span></code></a> to each Tensor of the input list.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch._foreach_log"/><a class="reference internal" href="generated/torch._foreach_log.html#torch._foreach_log" title="torch._foreach_log"><code class="xref py py-obj docutils literal notranslate"><span class="pre">_foreach_log</span></code></a></p></td>
<td><p>Apply <a class="reference internal" href="generated/torch.log.html#torch.log" title="torch.log"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.log()</span></code></a> to each Tensor of the input list.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch._foreach_log_"/><a class="reference internal" href="generated/torch._foreach_log_.html#torch._foreach_log_" title="torch._foreach_log_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">_foreach_log_</span></code></a></p></td>
<td><p>Apply <a class="reference internal" href="generated/torch.log.html#torch.log" title="torch.log"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.log()</span></code></a> to each Tensor of the input list.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch._foreach_log10"/><a class="reference internal" href="generated/torch._foreach_log10.html#torch._foreach_log10" title="torch._foreach_log10"><code class="xref py py-obj docutils literal notranslate"><span class="pre">_foreach_log10</span></code></a></p></td>
<td><p>Apply <a class="reference internal" href="generated/torch.log10.html#torch.log10" title="torch.log10"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.log10()</span></code></a> to each Tensor of the input list.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch._foreach_log10_"/><a class="reference internal" href="generated/torch._foreach_log10_.html#torch._foreach_log10_" title="torch._foreach_log10_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">_foreach_log10_</span></code></a></p></td>
<td><p>Apply <a class="reference internal" href="generated/torch.log10.html#torch.log10" title="torch.log10"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.log10()</span></code></a> to each Tensor of the input list.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch._foreach_log1p"/><a class="reference internal" href="generated/torch._foreach_log1p.html#torch._foreach_log1p" title="torch._foreach_log1p"><code class="xref py py-obj docutils literal notranslate"><span class="pre">_foreach_log1p</span></code></a></p></td>
<td><p>Apply <a class="reference internal" href="generated/torch.log1p.html#torch.log1p" title="torch.log1p"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.log1p()</span></code></a> to each Tensor of the input list.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch._foreach_log1p_"/><a class="reference internal" href="generated/torch._foreach_log1p_.html#torch._foreach_log1p_" title="torch._foreach_log1p_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">_foreach_log1p_</span></code></a></p></td>
<td><p>Apply <a class="reference internal" href="generated/torch.log1p.html#torch.log1p" title="torch.log1p"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.log1p()</span></code></a> to each Tensor of the input list.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch._foreach_log2"/><a class="reference internal" href="generated/torch._foreach_log2.html#torch._foreach_log2" title="torch._foreach_log2"><code class="xref py py-obj docutils literal notranslate"><span class="pre">_foreach_log2</span></code></a></p></td>
<td><p>Apply <a class="reference internal" href="generated/torch.log2.html#torch.log2" title="torch.log2"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.log2()</span></code></a> to each Tensor of the input list.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch._foreach_log2_"/><a class="reference internal" href="generated/torch._foreach_log2_.html#torch._foreach_log2_" title="torch._foreach_log2_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">_foreach_log2_</span></code></a></p></td>
<td><p>Apply <a class="reference internal" href="generated/torch.log2.html#torch.log2" title="torch.log2"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.log2()</span></code></a> to each Tensor of the input list.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch._foreach_neg"/><a class="reference internal" href="generated/torch._foreach_neg.html#torch._foreach_neg" title="torch._foreach_neg"><code class="xref py py-obj docutils literal notranslate"><span class="pre">_foreach_neg</span></code></a></p></td>
<td><p>Apply <a class="reference internal" href="generated/torch.neg.html#torch.neg" title="torch.neg"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.neg()</span></code></a> to each Tensor of the input list.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch._foreach_neg_"/><a class="reference internal" href="generated/torch._foreach_neg_.html#torch._foreach_neg_" title="torch._foreach_neg_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">_foreach_neg_</span></code></a></p></td>
<td><p>Apply <a class="reference internal" href="generated/torch.neg.html#torch.neg" title="torch.neg"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.neg()</span></code></a> to each Tensor of the input list.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch._foreach_tan"/><a class="reference internal" href="generated/torch._foreach_tan.html#torch._foreach_tan" title="torch._foreach_tan"><code class="xref py py-obj docutils literal notranslate"><span class="pre">_foreach_tan</span></code></a></p></td>
<td><p>Apply <a class="reference internal" href="generated/torch.tan.html#torch.tan" title="torch.tan"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.tan()</span></code></a> to each Tensor of the input list.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch._foreach_tan_"/><a class="reference internal" href="generated/torch._foreach_tan_.html#torch._foreach_tan_" title="torch._foreach_tan_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">_foreach_tan_</span></code></a></p></td>
<td><p>Apply <a class="reference internal" href="generated/torch.tan.html#torch.tan" title="torch.tan"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.tan()</span></code></a> to each Tensor of the input list.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch._foreach_sin"/><a class="reference internal" href="generated/torch._foreach_sin.html#torch._foreach_sin" title="torch._foreach_sin"><code class="xref py py-obj docutils literal notranslate"><span class="pre">_foreach_sin</span></code></a></p></td>
<td><p>Apply <a class="reference internal" href="generated/torch.sin.html#torch.sin" title="torch.sin"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.sin()</span></code></a> to each Tensor of the input list.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch._foreach_sin_"/><a class="reference internal" href="generated/torch._foreach_sin_.html#torch._foreach_sin_" title="torch._foreach_sin_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">_foreach_sin_</span></code></a></p></td>
<td><p>Apply <a class="reference internal" href="generated/torch.sin.html#torch.sin" title="torch.sin"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.sin()</span></code></a> to each Tensor of the input list.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch._foreach_sinh"/><a class="reference internal" href="generated/torch._foreach_sinh.html#torch._foreach_sinh" title="torch._foreach_sinh"><code class="xref py py-obj docutils literal notranslate"><span class="pre">_foreach_sinh</span></code></a></p></td>
<td><p>Apply <a class="reference internal" href="generated/torch.sinh.html#torch.sinh" title="torch.sinh"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.sinh()</span></code></a> to each Tensor of the input list.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch._foreach_sinh_"/><a class="reference internal" href="generated/torch._foreach_sinh_.html#torch._foreach_sinh_" title="torch._foreach_sinh_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">_foreach_sinh_</span></code></a></p></td>
<td><p>Apply <a class="reference internal" href="generated/torch.sinh.html#torch.sinh" title="torch.sinh"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.sinh()</span></code></a> to each Tensor of the input list.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch._foreach_round"/><a class="reference internal" href="generated/torch._foreach_round.html#torch._foreach_round" title="torch._foreach_round"><code class="xref py py-obj docutils literal notranslate"><span class="pre">_foreach_round</span></code></a></p></td>
<td><p>Apply <a class="reference internal" href="generated/torch.round.html#torch.round" title="torch.round"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.round()</span></code></a> to each Tensor of the input list.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch._foreach_round_"/><a class="reference internal" href="generated/torch._foreach_round_.html#torch._foreach_round_" title="torch._foreach_round_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">_foreach_round_</span></code></a></p></td>
<td><p>Apply <a class="reference internal" href="generated/torch.round.html#torch.round" title="torch.round"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.round()</span></code></a> to each Tensor of the input list.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch._foreach_sqrt"/><a class="reference internal" href="generated/torch._foreach_sqrt.html#torch._foreach_sqrt" title="torch._foreach_sqrt"><code class="xref py py-obj docutils literal notranslate"><span class="pre">_foreach_sqrt</span></code></a></p></td>
<td><p>Apply <a class="reference internal" href="generated/torch.sqrt.html#torch.sqrt" title="torch.sqrt"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.sqrt()</span></code></a> to each Tensor of the input list.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch._foreach_sqrt_"/><a class="reference internal" href="generated/torch._foreach_sqrt_.html#torch._foreach_sqrt_" title="torch._foreach_sqrt_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">_foreach_sqrt_</span></code></a></p></td>
<td><p>Apply <a class="reference internal" href="generated/torch.sqrt.html#torch.sqrt" title="torch.sqrt"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.sqrt()</span></code></a> to each Tensor of the input list.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch._foreach_lgamma"/><a class="reference internal" href="generated/torch._foreach_lgamma.html#torch._foreach_lgamma" title="torch._foreach_lgamma"><code class="xref py py-obj docutils literal notranslate"><span class="pre">_foreach_lgamma</span></code></a></p></td>
<td><p>Apply <a class="reference internal" href="generated/torch.lgamma.html#torch.lgamma" title="torch.lgamma"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.lgamma()</span></code></a> to each Tensor of the input list.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch._foreach_lgamma_"/><a class="reference internal" href="generated/torch._foreach_lgamma_.html#torch._foreach_lgamma_" title="torch._foreach_lgamma_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">_foreach_lgamma_</span></code></a></p></td>
<td><p>Apply <a class="reference internal" href="generated/torch.lgamma.html#torch.lgamma" title="torch.lgamma"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.lgamma()</span></code></a> to each Tensor of the input list.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch._foreach_frac"/><a class="reference internal" href="generated/torch._foreach_frac.html#torch._foreach_frac" title="torch._foreach_frac"><code class="xref py py-obj docutils literal notranslate"><span class="pre">_foreach_frac</span></code></a></p></td>
<td><p>Apply <a class="reference internal" href="generated/torch.frac.html#torch.frac" title="torch.frac"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.frac()</span></code></a> to each Tensor of the input list.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch._foreach_frac_"/><a class="reference internal" href="generated/torch._foreach_frac_.html#torch._foreach_frac_" title="torch._foreach_frac_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">_foreach_frac_</span></code></a></p></td>
<td><p>Apply <a class="reference internal" href="generated/torch.frac.html#torch.frac" title="torch.frac"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.frac()</span></code></a> to each Tensor of the input list.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch._foreach_reciprocal"/><a class="reference internal" href="generated/torch._foreach_reciprocal.html#torch._foreach_reciprocal" title="torch._foreach_reciprocal"><code class="xref py py-obj docutils literal notranslate"><span class="pre">_foreach_reciprocal</span></code></a></p></td>
<td><p>Apply <a class="reference internal" href="generated/torch.reciprocal.html#torch.reciprocal" title="torch.reciprocal"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.reciprocal()</span></code></a> to each Tensor of the input list.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch._foreach_reciprocal_"/><a class="reference internal" href="generated/torch._foreach_reciprocal_.html#torch._foreach_reciprocal_" title="torch._foreach_reciprocal_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">_foreach_reciprocal_</span></code></a></p></td>
<td><p>Apply <a class="reference internal" href="generated/torch.reciprocal.html#torch.reciprocal" title="torch.reciprocal"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.reciprocal()</span></code></a> to each Tensor of the input list.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch._foreach_sigmoid"/><a class="reference internal" href="generated/torch._foreach_sigmoid.html#torch._foreach_sigmoid" title="torch._foreach_sigmoid"><code class="xref py py-obj docutils literal notranslate"><span class="pre">_foreach_sigmoid</span></code></a></p></td>
<td><p>Apply <a class="reference internal" href="generated/torch.sigmoid.html#torch.sigmoid" title="torch.sigmoid"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.sigmoid()</span></code></a> to each Tensor of the input list.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch._foreach_sigmoid_"/><a class="reference internal" href="generated/torch._foreach_sigmoid_.html#torch._foreach_sigmoid_" title="torch._foreach_sigmoid_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">_foreach_sigmoid_</span></code></a></p></td>
<td><p>Apply <a class="reference internal" href="generated/torch.sigmoid.html#torch.sigmoid" title="torch.sigmoid"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.sigmoid()</span></code></a> to each Tensor of the input list.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch._foreach_trunc"/><a class="reference internal" href="generated/torch._foreach_trunc.html#torch._foreach_trunc" title="torch._foreach_trunc"><code class="xref py py-obj docutils literal notranslate"><span class="pre">_foreach_trunc</span></code></a></p></td>
<td><p>Apply <a class="reference internal" href="generated/torch.trunc.html#torch.trunc" title="torch.trunc"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.trunc()</span></code></a> to each Tensor of the input list.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch._foreach_trunc_"/><a class="reference internal" href="generated/torch._foreach_trunc_.html#torch._foreach_trunc_" title="torch._foreach_trunc_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">_foreach_trunc_</span></code></a></p></td>
<td><p>Apply <a class="reference internal" href="generated/torch.trunc.html#torch.trunc" title="torch.trunc"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.trunc()</span></code></a> to each Tensor of the input list.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch._foreach_zero_"/><a class="reference internal" href="generated/torch._foreach_zero_.html#torch._foreach_zero_" title="torch._foreach_zero_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">_foreach_zero_</span></code></a></p></td>
<td><p>Apply <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.zero()</span></code> to each Tensor of the input list.</p></td>
</tr>
</tbody>
</table>
</div>
</section>
</section>
<section id="utilities">
<h2>Utilities<a class="headerlink" href="#utilities" title="Permalink to this heading">#</a></h2>
<div class="pst-scrollable-table-container"><table class="autosummary longtable table autosummary">
<tbody>
<tr class="row-odd"><td><p><p id="torch.compiled_with_cxx11_abi"/><a class="reference internal" href="generated/torch.compiled_with_cxx11_abi.html#torch.compiled_with_cxx11_abi" title="torch.compiled_with_cxx11_abi"><code class="xref py py-obj docutils literal notranslate"><span class="pre">compiled_with_cxx11_abi</span></code></a></p></td>
<td><p>Returns whether PyTorch was built with _GLIBCXX_USE_CXX11_ABI=1</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.result_type"/><a class="reference internal" href="generated/torch.result_type.html#torch.result_type" title="torch.result_type"><code class="xref py py-obj docutils literal notranslate"><span class="pre">result_type</span></code></a></p></td>
<td><p>Returns the <a class="reference internal" href="tensor_attributes.html#torch.dtype" title="torch.dtype"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.dtype</span></code></a> that would result from performing an arithmetic operation on the provided input tensors.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.can_cast"/><a class="reference internal" href="generated/torch.can_cast.html#torch.can_cast" title="torch.can_cast"><code class="xref py py-obj docutils literal notranslate"><span class="pre">can_cast</span></code></a></p></td>
<td><p>Determines if a type conversion is allowed under PyTorch casting rules described in the type promotion <a class="reference internal" href="tensor_attributes.html#type-promotion-doc"><span class="std std-ref">documentation</span></a>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.promote_types"/><a class="reference internal" href="generated/torch.promote_types.html#torch.promote_types" title="torch.promote_types"><code class="xref py py-obj docutils literal notranslate"><span class="pre">promote_types</span></code></a></p></td>
<td><p>Returns the <a class="reference internal" href="tensor_attributes.html#torch.dtype" title="torch.dtype"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.dtype</span></code></a> with the smallest size and scalar kind that is not smaller nor of lower kind than either <cite>type1</cite> or <cite>type2</cite>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.use_deterministic_algorithms"/><a class="reference internal" href="generated/torch.use_deterministic_algorithms.html#torch.use_deterministic_algorithms" title="torch.use_deterministic_algorithms"><code class="xref py py-obj docutils literal notranslate"><span class="pre">use_deterministic_algorithms</span></code></a></p></td>
<td><p>Sets whether PyTorch operations must use &quot;deterministic&quot; algorithms.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.are_deterministic_algorithms_enabled"/><a class="reference internal" href="generated/torch.are_deterministic_algorithms_enabled.html#torch.are_deterministic_algorithms_enabled" title="torch.are_deterministic_algorithms_enabled"><code class="xref py py-obj docutils literal notranslate"><span class="pre">are_deterministic_algorithms_enabled</span></code></a></p></td>
<td><p>Returns True if the global deterministic flag is turned on.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.is_deterministic_algorithms_warn_only_enabled"/><a class="reference internal" href="generated/torch.is_deterministic_algorithms_warn_only_enabled.html#torch.is_deterministic_algorithms_warn_only_enabled" title="torch.is_deterministic_algorithms_warn_only_enabled"><code class="xref py py-obj docutils literal notranslate"><span class="pre">is_deterministic_algorithms_warn_only_enabled</span></code></a></p></td>
<td><p>Returns True if the global deterministic flag is set to warn only.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.set_deterministic_debug_mode"/><a class="reference internal" href="generated/torch.set_deterministic_debug_mode.html#torch.set_deterministic_debug_mode" title="torch.set_deterministic_debug_mode"><code class="xref py py-obj docutils literal notranslate"><span class="pre">set_deterministic_debug_mode</span></code></a></p></td>
<td><p>Sets the debug mode for deterministic operations.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.get_deterministic_debug_mode"/><a class="reference internal" href="generated/torch.get_deterministic_debug_mode.html#torch.get_deterministic_debug_mode" title="torch.get_deterministic_debug_mode"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_deterministic_debug_mode</span></code></a></p></td>
<td><p>Returns the current value of the debug mode for deterministic operations.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.set_float32_matmul_precision"/><a class="reference internal" href="generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision" title="torch.set_float32_matmul_precision"><code class="xref py py-obj docutils literal notranslate"><span class="pre">set_float32_matmul_precision</span></code></a></p></td>
<td><p>Sets the internal precision of float32 matrix multiplications.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.get_float32_matmul_precision"/><a class="reference internal" href="generated/torch.get_float32_matmul_precision.html#torch.get_float32_matmul_precision" title="torch.get_float32_matmul_precision"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_float32_matmul_precision</span></code></a></p></td>
<td><p>Returns the current value of float32 matrix multiplication precision.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.set_warn_always"/><a class="reference internal" href="generated/torch.set_warn_always.html#torch.set_warn_always" title="torch.set_warn_always"><code class="xref py py-obj docutils literal notranslate"><span class="pre">set_warn_always</span></code></a></p></td>
<td><p>When this flag is False (default) then some PyTorch warnings may only appear once per process.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.get_device_module"/><a class="reference internal" href="generated/torch.get_device_module.html#torch.get_device_module" title="torch.get_device_module"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_device_module</span></code></a></p></td>
<td><p>Returns the module associated with a given device(e.g., torch.device('cuda'), &quot;mtia:0&quot;, &quot;xpu&quot;, ...).</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.is_warn_always_enabled"/><a class="reference internal" href="generated/torch.is_warn_always_enabled.html#torch.is_warn_always_enabled" title="torch.is_warn_always_enabled"><code class="xref py py-obj docutils literal notranslate"><span class="pre">is_warn_always_enabled</span></code></a></p></td>
<td><p>Returns True if the global warn_always flag is turned on.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.vmap"/><a class="reference internal" href="generated/torch.vmap.html#torch.vmap" title="torch.vmap"><code class="xref py py-obj docutils literal notranslate"><span class="pre">vmap</span></code></a></p></td>
<td><p>vmap is the vectorizing map; <code class="docutils literal notranslate"><span class="pre">vmap(func)</span></code> returns a new function that maps <code class="docutils literal notranslate"><span class="pre">func</span></code> over some dimension of the inputs.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch._assert"/><a class="reference internal" href="generated/torch._assert.html#torch._assert" title="torch._assert"><code class="xref py py-obj docutils literal notranslate"><span class="pre">_assert</span></code></a></p></td>
<td><p>A wrapper around Python's assert which is symbolically traceable.</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="symbolic-numbers">
<h2>Symbolic Numbers<a class="headerlink" href="#symbolic-numbers" title="Permalink to this heading">#</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="torch.SymInt">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torch.</span></span><span class="sig-name descname"><span class="pre">SymInt</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">node</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/__init__.py#L408"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.SymInt" title="Permalink to this definition">#</a></dt>
<dd><p>Like an int (including magic methods), but redirects all operations on the
wrapped node. This is used in particular to symbolically record operations
in the symbolic shape workflow.</p>
<dl class="py method">
<dt class="sig sig-object py" id="torch.SymInt.as_integer_ratio">
<span class="sig-name descname"><span class="pre">as_integer_ratio</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/__init__.py#L590"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.SymInt.as_integer_ratio" title="Permalink to this definition">#</a></dt>
<dd><p>Represent this int as an exact integer ratio</p>
<dl class="field-list simple">
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Tuple" title="(in Python v3.13)"><em>Tuple</em></a>[<a class="reference internal" href="#torch.SymInt" title="torch.SymInt"><em>SymInt</em></a>, <a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)">int</a>]</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="torch.SymFloat">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torch.</span></span><span class="sig-name descname"><span class="pre">SymFloat</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">node</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/__init__.py#L605"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.SymFloat" title="Permalink to this definition">#</a></dt>
<dd><p>Like an float (including magic methods), but redirects all operations on the
wrapped node. This is used in particular to symbolically record operations
in the symbolic shape workflow.</p>
<dl class="py method">
<dt class="sig sig-object py" id="torch.SymFloat.as_integer_ratio">
<span class="sig-name descname"><span class="pre">as_integer_ratio</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/__init__.py#L702"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.SymFloat.as_integer_ratio" title="Permalink to this definition">#</a></dt>
<dd><p>Represent this float as an exact integer ratio</p>
<dl class="field-list simple">
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Tuple" title="(in Python v3.13)"><em>Tuple</em></a>[<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)">int</a>, <a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)">int</a>]</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.SymFloat.conjugate">
<span class="sig-name descname"><span class="pre">conjugate</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/__init__.py#L715"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.SymFloat.conjugate" title="Permalink to this definition">#</a></dt>
<dd><p>Returns the complex conjugate of the float.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference internal" href="#torch.SymFloat" title="torch.SymFloat"><em>SymFloat</em></a></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.SymFloat.hex">
<span class="sig-name descname"><span class="pre">hex</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/__init__.py#L719"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.SymFloat.hex" title="Permalink to this definition">#</a></dt>
<dd><p>Returns the hexadecimal representation of the float.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)">str</a></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.SymFloat.is_integer">
<span class="sig-name descname"><span class="pre">is_integer</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/__init__.py#L698"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.SymFloat.is_integer" title="Permalink to this definition">#</a></dt>
<dd><p>Return True if the float is an integer.</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="torch.SymBool">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torch.</span></span><span class="sig-name descname"><span class="pre">SymBool</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">node</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/__init__.py#L724"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.SymBool" title="Permalink to this definition">#</a></dt>
<dd><p>Like an bool (including magic methods), but redirects all operations on the
wrapped node. This is used in particular to symbolically record operations
in the symbolic shape workflow.</p>
<p>Unlike regular bools, regular boolean operators will force extra guards instead
of symbolically evaluate.  Use the bitwise operators instead to handle this.</p>
</dd></dl>

<div class="pst-scrollable-table-container"><table class="autosummary longtable table autosummary">
<tbody>
<tr class="row-odd"><td><p><p id="torch.sym_float"/><a class="reference internal" href="generated/torch.sym_float.html#torch.sym_float" title="torch.sym_float"><code class="xref py py-obj docutils literal notranslate"><span class="pre">sym_float</span></code></a></p></td>
<td><p>SymInt-aware utility for float casting.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.sym_fresh_size"/><a class="reference internal" href="generated/torch.sym_fresh_size.html#torch.sym_fresh_size" title="torch.sym_fresh_size"><code class="xref py py-obj docutils literal notranslate"><span class="pre">sym_fresh_size</span></code></a></p></td>
<td><p></p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.sym_int"/><a class="reference internal" href="generated/torch.sym_int.html#torch.sym_int" title="torch.sym_int"><code class="xref py py-obj docutils literal notranslate"><span class="pre">sym_int</span></code></a></p></td>
<td><p>SymInt-aware utility for int casting.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.sym_max"/><a class="reference internal" href="generated/torch.sym_max.html#torch.sym_max" title="torch.sym_max"><code class="xref py py-obj docutils literal notranslate"><span class="pre">sym_max</span></code></a></p></td>
<td><p>SymInt-aware utility for max which avoids branching on a &lt; b.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.sym_min"/><a class="reference internal" href="generated/torch.sym_min.html#torch.sym_min" title="torch.sym_min"><code class="xref py py-obj docutils literal notranslate"><span class="pre">sym_min</span></code></a></p></td>
<td><p>SymInt-aware utility for min().</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.sym_not"/><a class="reference internal" href="generated/torch.sym_not.html#torch.sym_not" title="torch.sym_not"><code class="xref py py-obj docutils literal notranslate"><span class="pre">sym_not</span></code></a></p></td>
<td><p>SymInt-aware utility for logical negation.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.sym_ite"/><a class="reference internal" href="generated/torch.sym_ite.html#torch.sym_ite" title="torch.sym_ite"><code class="xref py py-obj docutils literal notranslate"><span class="pre">sym_ite</span></code></a></p></td>
<td><p></p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.sym_sum"/><a class="reference internal" href="generated/torch.sym_sum.html#torch.sym_sum" title="torch.sym_sum"><code class="xref py py-obj docutils literal notranslate"><span class="pre">sym_sum</span></code></a></p></td>
<td><p>N-ary add which is faster to compute for long lists than iterated binary addition.</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="export-path">
<h2>Export Path<a class="headerlink" href="#export-path" title="Permalink to this heading">#</a></h2>
<div class="pst-scrollable-table-container"><table class="autosummary longtable table autosummary">
<tbody>
</tbody>
</table>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>This feature is a prototype and may have compatibility breaking changes in the future.</p>
<p>export
generated/exportdb/index</p>
</div>
</section>
<section id="control-flow">
<h2>Control Flow<a class="headerlink" href="#control-flow" title="Permalink to this heading">#</a></h2>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>This feature is a prototype and may have compatibility breaking changes in the future.</p>
</div>
<div class="pst-scrollable-table-container"><table class="autosummary longtable table autosummary">
<tbody>
<tr class="row-odd"><td><p><p id="torch.cond"/><a class="reference internal" href="generated/torch.cond.html#torch.cond" title="torch.cond"><code class="xref py py-obj docutils literal notranslate"><span class="pre">cond</span></code></a></p></td>
<td><p>Conditionally applies <cite>true_fn</cite> or <cite>false_fn</cite>.</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="optimizations">
<h2>Optimizations<a class="headerlink" href="#optimizations" title="Permalink to this heading">#</a></h2>
<div class="pst-scrollable-table-container"><table class="autosummary longtable table autosummary">
<tbody>
<tr class="row-odd"><td><p><p id="torch.compile"/><a class="reference internal" href="generated/torch.compile.html#torch.compile" title="torch.compile"><code class="xref py py-obj docutils literal notranslate"><span class="pre">compile</span></code></a></p></td>
<td><p>Optimizes given model/function using TorchDynamo and specified backend.</p></td>
</tr>
</tbody>
</table>
</div>
<p><a class="reference external" href="https://pytorch.org/docs/main/torch.compiler.html">torch.compile documentation</a></p>
</section>
<section id="operator-tags">
<h2>Operator Tags<a class="headerlink" href="#operator-tags" title="Permalink to this heading">#</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="torch.Tag">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torch.</span></span><span class="sig-name descname"><span class="pre">Tag</span></span><a class="headerlink" href="#torch.Tag" title="Permalink to this definition">#</a></dt>
<dd><p>Members:</p>
<p>core</p>
<p>data_dependent_output</p>
<p>dynamic_output_shape</p>
<p>flexible_layout</p>
<p>generated</p>
<p>inplace_view</p>
<p>maybe_aliasing_or_mutating</p>
<p>needs_fixed_stride_order</p>
<p>nondeterministic_bitwise</p>
<p>nondeterministic_seeded</p>
<p>pointwise</p>
<p>pt2_compliant_tag</p>
<p>view_copy</p>
<dl class="py property">
<dt class="sig sig-object py" id="torch.Tag.name">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">name</span></span><a class="headerlink" href="#torch.Tag.name" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

</dd></dl>

<span class="target" id="module-torch.contrib"></span><span class="target" id="module-torch.utils.backcompat"></span><span class="target" id="module-torch.utils.hipify"></span><span class="target" id="module-torch.utils.model_dump"></span><span class="target" id="module-torch.utils.viz"></span><span class="target" id="module-torch.functional"></span><span class="target" id="module-torch.quasirandom"></span><span class="target" id="module-torch.return_types"></span><span class="target" id="module-torch.serialization"></span><span class="target" id="module-torch.signal.windows.windows"></span><span class="target" id="module-torch.sparse.semi_structured"></span><span class="target" id="module-torch.storage"></span><span class="target" id="module-torch.torch_version"></span><span class="target" id="module-torch.types"></span><span class="target" id="module-torch.version"></span></section>
</section>


  </article>

              
              
                <footer class="bd-footer-article">
                  <div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item">
<div class="feedback">
  
<div class="rating">
    Rate this Page
    <div class="stars">
        
        <span class="star" data-behavior="tutorial-rating" data-count="1" data-value="1">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="2" data-value="2">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="3" data-value="3">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="4" data-value="4">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="5" data-value="5">★</span>
        
    </div>
</div>

  <div class="feedback-send">
    <button class="feedback-btn"
            onclick="openGitHubIssue()"
            data-bs-title="Create a GitHub Issue"
            data-bs-placement="bottom"
            data-bs-toggle="tooltip">Send Feedback
    </button>
  </div>
</div>


<div class="prev-next-area">
    <a class="left-prev"
       href="pytorch-api.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Python API</p>
      </div>
    </a>
    <a class="right-next"
       href="generated/torch.is_tensor.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">torch.is_tensor</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>

<div class="footer-info">
  <p class="copyright">
    
      
        © Copyright PyTorch Contributors.
      
      <br/>
    
  </p>

  <p class="theme-version">
    Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.15.4.
  </p>
</div></div>
  
</div>
                </footer>
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="pytorch-api.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Python API</p>
      </div>
    </a>
    <a class="right-next"
       href="generated/torch.is_tensor.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">torch.is_tensor</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc">
<div class="sidebar-secondary-items sidebar-secondary__inner">
    
       <div class="sidebar-secondary-item">
<div
    id="pst-page-navigation-heading-2"
    class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> On this page
  </div>
  <nav class="bd-toc-nav page-toc" aria-labelledby="pst-page-navigation-heading-2">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#tensors">Tensors</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#creation-ops">Creation Ops</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#indexing-slicing-joining-mutating-ops">Indexing, Slicing, Joining, Mutating Ops</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#accelerators">Accelerators</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#generators">Generators</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#random-sampling">Random sampling</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.torch.default_generator"><code class="docutils literal notranslate"><span class="pre">torch.default_generator</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#in-place-random-sampling">In-place random sampling</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#quasi-random-sampling">Quasi-random sampling</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#serialization">Serialization</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#parallelism">Parallelism</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#locally-disabling-gradient-computation">Locally disabling gradient computation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#math-operations">Math operations</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#constants">Constants</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#pointwise-ops">Pointwise Ops</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#reduction-ops">Reduction Ops</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#comparison-ops">Comparison Ops</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#spectral-ops">Spectral Ops</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#other-operations">Other Operations</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#blas-and-lapack-operations">BLAS and LAPACK Operations</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#foreach-operations">Foreach Operations</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#utilities">Utilities</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#symbolic-numbers">Symbolic Numbers</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.SymInt"><code class="docutils literal notranslate"><span class="pre">SymInt</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.SymInt.as_integer_ratio"><code class="docutils literal notranslate"><span class="pre">SymInt.as_integer_ratio()</span></code></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.SymFloat"><code class="docutils literal notranslate"><span class="pre">SymFloat</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.SymFloat.as_integer_ratio"><code class="docutils literal notranslate"><span class="pre">SymFloat.as_integer_ratio()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.SymFloat.conjugate"><code class="docutils literal notranslate"><span class="pre">SymFloat.conjugate()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.SymFloat.hex"><code class="docutils literal notranslate"><span class="pre">SymFloat.hex()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.SymFloat.is_integer"><code class="docutils literal notranslate"><span class="pre">SymFloat.is_integer()</span></code></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.SymBool"><code class="docutils literal notranslate"><span class="pre">SymBool</span></code></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#export-path">Export Path</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#control-flow">Control Flow</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#optimizations">Optimizations</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#operator-tags">Operator Tags</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.Tag"><code class="docutils literal notranslate"><span class="pre">Tag</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.Tag.name"><code class="docutils literal notranslate"><span class="pre">Tag.name</span></code></a></li>
</ul>
</li>
</ul>
</li>
</ul>
  </nav></div>
    
       <div class="sidebar-secondary-item">

  
  <div class="tocsection editthispage">
    <a href="https://github.com/pytorch/pytorch/edit/main/docs/cpp/source/torch.rst">
      <i class="fa-solid fa-pencil"></i>
      
      
        
          Edit on GitHub
        
      
    </a>
  </div>
</div>
    
       <div class="sidebar-secondary-item">
    <div class="tocsection sourcelink">
      <a href="_sources/torch.rst.txt">
        <i class="fa-solid fa-file-lines"></i> Show Source
      </a>
    </div>
</div>
    


<div class="sidebar-secondary-item">
  <div class="sidebar-heading">PyTorch Community</div>
  <ul style="list-style-type: none; padding: 0;">
  
   <li><a class="nav-link nav-external" href="https://pytorch.org/docs/stable/community/index.html" style="color: var(--pst-color-text-muted)">PyTorch Governance</a></li>
  
   <li><a class="nav-link nav-external" href="https://pytorch.org/docs/stable/community/design.html" style="color: var(--pst-color-text-muted)">PyTorch Design Philosophy</a></li>
  
   <li><a class="nav-link nav-external" href="https://github.com/pytorch/pytorch/wiki/The-Ultimate-Guide-to-PyTorch-Contributions" style="color: var(--pst-color-text-muted)">The Ultimate Guide to PyTorch Contributions</a></li>
  
  </ul>
</div>


<div class="sidebar-secondary-item">
 <div class="sidebar-heading">Language Bindings</div>
 <ul style="list-style-type: none; padding: 0;">
 
  <li><a class="nav-link nav-external" href="https://pytorch.org/docs/stable/cpp_index.html" style="color: var(--pst-color-text-muted)">C++</a></li>
 
  <li><a class="nav-link nav-external" href="https://pytorch.org/javadoc/" style="color: var(--pst-color-text-muted)">Javadoc</a></li>
 
  <li><a class="nav-link nav-external" href="https://github.com/pytorch/multipy" style="color: var(--pst-color-text-muted)">torch.multiply</a></li>
 
 </ul>
</div>


<div class="sidebar-secondary-item">
  <div class="sidebar-heading">PyTorch Libraries</div>
  <ul style="list-style-type: none; padding: 0; padding-bottom: 80px;">
  
   <li><a class="nav-link nav-external" href="https://pytorch.org/audio/stable/" style="color: var(--pst-color-text-muted)">torchaudio</a></li>
  
   <li><a class="nav-link nav-external" href="https://pytorch.org/ao" style="color: var(--pst-color-text-muted)">torchao</a></li>
  
   <li><a class="nav-link nav-external" href="https://pytorch.org/executorch" style="color: var(--pst-color-text-muted)">ExecuTorch</a></li>
  
   <li><a class="nav-link nav-external" href="https://pytorch.org/torchrec" style="color: var(--pst-color-text-muted)">torchrec</a></li>
  
   <li><a class="nav-link nav-external" href="https://pytorch.org/serve/" style="color: var(--pst-color-text-muted)">torchserve</a></li>
  
   <li><a class="nav-link nav-external" href="https://pytorch.org/data" style="color: var(--pst-color-text-muted)">torchdata</a></li>
  
   <li><a class="nav-link nav-external" href="https://pytorch.org/data" style="color: var(--pst-color-text-muted)">torchvision</a></li>
  
   <li><a class="nav-link nav-external" href="https://pytorch.org/xla" style="color: var(--pst-color-text-muted)">PyTorch on XLA Devices</a></li>
  
  </ul>
</div>

</div>
</div>
              
            
          </div>
          <footer class="bd-footer-content">
            
          </footer>
        
      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

   <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
      <div class="container">
        <div class="row">
          <div class="col-md-4 text-center">
            <h2>Docs</h2>
            <p>Access comprehensive developer documentation for PyTorch</p>
            <a class="with-right-arrow" href="">View Docs</a>
          </div>

          <div class="col-md-4 text-center">
            <h2>Tutorials</h2>
            <p>Get in-depth tutorials for beginners and advanced developers</p>
            <a class="with-right-arrow" href="">View Tutorials</a>
          </div>

          <div class="col-md-4 text-center">
            <h2>Resources</h2>
            <p>Find development resources and get your questions answered</p>
            <a class="with-right-arrow" href="">View Resources</a>
          </div>
        </div>
      </div>
  </div>

  <footer class="site-footer">
      <div class="container footer-container">
        <div class="footer-logo-wrapper">
          <a href="" class="footer-logo"></a>
        </div>

        <div class="footer-links-wrapper">
          <div class="footer-links-col">
            <ul>
              <li class="list-title"><a href="">PyTorch</a></li>
              <li><a href="">Get Started</a></li>
              <li><a href="">Features</a></li>
              <li><a href="">Ecosystem</a></li>
              <li><a href="">Blog</a></li>
              <li><a href="">Contributing</a></li>
            </ul>
          </div>

          <div class="footer-links-col">
            <ul>
              <li class="list-title"><a href="">Resources</a></li>
              <li><a href="">Tutorials</a></li>
              <li><a href="">Docs</a></li>
              <li><a href="" target="_blank">Discuss</a></li>
              <li><a href="" target="_blank">Github Issues</a></li>
              <li><a href="" target="_blank">Brand Guidelines</a></li>
            </ul>
          </div>

          <div class="footer-links-col">
            <ul>
              <li class="list-title">Stay up to date</li>
              <li><a href="" target="_blank">Facebook</a></li>
              <li><a href="" target="_blank">Twitter</a></li>
              <li><a href="" target="_blank">YouTube</a></li>
              <li><a href="" target="_blank">LinkedIn</a></li>
            </ul>
            </div>

          <div class="footer-links-col">
            <ul>
              <li class="list-title">PyTorch Podcasts</li>
              <li><a href="" target="_blank">Spotify</a></li>
              <li><a href="" target="_blank">Apple</a></li>
              <li><a href="" target="_blank">Google</a></li>
              <li><a href="" target="_blank">Amazon</a></li>
            </ul>
           </div>
          </div>

          <div class="privacy-policy">
            <ul>
              <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/terms/" target="_blank">Terms</a></li>
              <li class="privacy-policy-links">|</li>
              <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/privacy-policy/" target="_blank">Privacy</a></li>
            </ul>
          </div>
          <div class="copyright">
          <p>© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation.
            For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see
            <a href="https://www.linuxfoundation.org/policies/">www.linuxfoundation.org/policies/</a>. The PyTorch Foundation supports the PyTorch open source
            project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC,
            please see <a href="https://www.lfprojects.org/policies/">www.lfprojects.org/policies/</a>.</p>
        </div>
       </div>

  <div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="_static/img/pytorch-x.svg">
  </div>
</div>
 </footer>
   
  <footer class="bd-footer"><div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="_static/img/pytorch-x.svg">
  </div>
</div>
<div class="bd-footer__inner bd-page-width">
  
    <div class="footer-items__start">
      
        <div class="footer-item">

  <p class="copyright">
    
      © Copyright PyTorch Contributors.
      <br/>
    
  </p>
</div>
      
        <div class="footer-item">

  <p class="sphinx-version">
    Created using <a href="https://www.sphinx-doc.org/">Sphinx</a> 5.3.0.
    <br/>
  </p>
</div>
      
    </div>
  
  
  
    <div class="footer-items__end">
      
        <div class="footer-item">
<p class="theme-version">
  Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.15.4.
</p></div>
      
    </div>
  
</div>

  </footer>

  </body>
</html>