
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Serialization semantics &#8212; PyTorch main documentation</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  <!--
    this give us a css class that will be invisible only if js is disabled
  -->
  <noscript>
    <style>
      .pst-js-only { display: none !important; }

    </style>
  </noscript>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=949a1ff5" />
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/katex.min.css" />
    <link rel="stylesheet" type="text/css" href="../_static/katex-math.css?v=91adb8b6" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="../_static/css/jit.css?v=8de1ea5d" />
    <link rel="stylesheet" type="text/css" href="../_static/css/custom.css?v=3ccf5357" />
    <link rel="stylesheet" type="text/css" href="../_static/css/custom2.css?v=3539c01c" />
  
  <!-- So that users can add custom icons -->
  <script src="../_static/scripts/fontawesome.js?digest=8878045cc6db502f8baf"></script>
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf" />

    <script src="../_static/documentation_options.js?v=a8da1a53"></script>
    <script src="../_static/doctools.js?v=9bcbadda"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=940804e7"></script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'notes/serialization';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Frequently Asked Questions" href="faq.html" />
    <link rel="prev" title="Reproducibility" href="randomness.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  <meta name="docsearch:version" content="main (2.6.0 )" />
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>

          <li class="main-menu-item">
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Learn
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="">
                  <span class=dropdown-title>Get Started</span>
                  <p>Run PyTorch locally or get started quickly with one of the supported cloud platforms</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Tutorials</span>
                  <p>Whats new in PyTorch tutorials</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Learn the Basics</span>
                  <p>Familiarize yourself with PyTorch concepts and modules</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">PyTorch Recipes</span>
                  <p>Bite-size, ready-to-deploy PyTorch code examples</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Intro to PyTorch - YouTube Series</span>
                  <p>Master PyTorch basics with our engaging YouTube tutorial series</p>
                </a>
              </div>
            </div>
          </li>

          <li>
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Ecosystem
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Tools</span>
                  <p>Learn about the tools and frameworks in the PyTorch Ecosystem</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class=dropdown-title>Community</span>
                  <p>Join the PyTorch developer community to contribute, learn, and get your questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="" target="_blank">
                  <span class=dropdown-title>Forums</span>
                  <p>A place to discuss PyTorch code, issues, install, research</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class=dropdown-title>Developer Resources</span>
                  <p>Find resources and get questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Contributor Awards - 2024</span>
                  <p>Award winners announced at this year's PyTorch Conference</p>
                </a>
              </div>
            </div>
          </li>

          <li>
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Edge
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">About PyTorch Edge</span>
                  <p>Build innovative and privacy-aware AI experiences for edge devices</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">ExecuTorch</span>
                  <p>End-to-end solution for enabling on-device inference capabilities across mobile and edge devices</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">ExecuTorch Docs</span>
                </a>
              </div>
            </div>  
          </li>

          <li class="main-menu-item">
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Docs
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">PyTorch</span>
                  <p>Explore the documentation for comprehensive guidance on how to use PyTorch</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">PyTorch Domains</span>
                  <p>Read the PyTorch Domains documentation to learn more about domain-specific libraries</p>
                </a>
              </div>
            </div>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Blogs & News 
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">PyTorch Blog</span>
                  <p>Catch up on the latest technical news and happenings</p>
                </a>
                 <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Community Blog</span>
                  <p>Stories from the PyTorch ecosystem</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Videos</span>
                  <p>Learn about the latest PyTorch tutorials, new, and more </p>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Community Stories</span>
                  <p>Learn how our community solves real, everyday machine learning problems with PyTorch</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Events</span>
                  <p>Find events, webinars, and podcasts</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Newsletter</span>
                  <p>Stay up-to-date with the latest updates</p>
                </a>
            </div>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                About
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">PyTorch Foundation</span>
                  <p>Learn more about the PyTorch Foundation</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Governing Board</span>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Cloud Credit Program</span>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Technical Advisory Council</span>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Staff</span>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Contact Us</span>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
            <div class="no-dropdown">
              <a href="" data-cta="join">
                Become a Member
              </a>
            </div>
          </li>
          <li>
           <div class="main-menu-item">
             <a href="" class="github-icon">
             </a>
           </div>
          </li>
          <!--- TODO: This block adds the search icon to the nav bar. We will enable it later. 
          <li>
            <div class="main-menu-item">
             <a href="" class="search-icon">
             </a>
            </div>
          </li>
          --->
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>
  </div>
</div>

  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <dialog id="pst-search-dialog">
    
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </dialog>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
<div class="bd-header__inner bd-page-width">
  <button class="pst-navbar-icon sidebar-toggle primary-toggle" aria-label="Site navigation">
    <span class="fa-solid fa-bars"></span>
  </button>
  
  
  <div class=" navbar-header-items__start">
    
      <div class="navbar-item">
<div class="version-switcher__container dropdown pst-js-only">
  <button id="pst-version-switcher-button-2"
    type="button"
    class="version-switcher__button btn btn-sm dropdown-toggle"
    data-bs-toggle="dropdown"
    aria-haspopup="listbox"
    aria-controls="pst-version-switcher-list-2"
    aria-label="Version switcher list"
  >
    Choose version  <!-- this text may get changed later by javascript -->
    <span class="caret"></span>
  </button>
  <div id="pst-version-switcher-list-2"
    class="version-switcher__menu dropdown-menu list-group-flush py-0"
    role="listbox" aria-labelledby="pst-version-switcher-button-2">
    <!-- dropdown will be populated by javascript on page load -->
  </div>
</div></div>
    
      <div class="navbar-item">

  
    
  

<a class="navbar-brand logo" href="../index.html">
  
  
  
  
  
  
    <p class="title logo__title">Home</p>
  
</a></div>
    
  </div>
  
  <div class=" navbar-header-items">
    
    <div class="me-auto navbar-header-items__center">
      
        <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item ">
  <a class="nav-link nav-internal" href="../python-api/index.html">
    Python API
  </a>
</li>


<li class="nav-item current active">
  <a class="nav-link nav-internal" href="index.html">
    Developer Notes
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../community/index.html">
    Community
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../lang-bindings/index.html">
    Language Bindings
  </a>
</li>

  </ul>
</nav></div>
      
    </div>
    
    
    <div class="navbar-header-items__end">
      
        <div class="navbar-item navbar-persistent--container">
          

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button>
        </div>
      
      
        <div class="navbar-item">

<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button></div>
      
        <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://x.com/PyTorch" title="X" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-x-twitter fa-lg" aria-hidden="true"></i>
            <span class="sr-only">X</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/pytorch/pytorch" title="GitHub" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">GitHub</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://pypi.org/project/torch" title="PyPI" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-custom fa-pypi fa-lg" aria-hidden="true"></i>
            <span class="sr-only">PyPI</span></a>
        </li>
</ul></div>
      
    </div>
    
  </div>
  
  
    <div class="navbar-persistent--mobile">

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button>
    </div>
  

  
    <button class="pst-navbar-icon sidebar-toggle secondary-toggle" aria-label="On this page">
      <span class="fa-solid fa-outdent"></span>
    </button>
  
</div>

    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <dialog id="pst-primary-sidebar-modal"></dialog>
      <div id="pst-primary-sidebar" class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
      <div class="sidebar-header-items__center">
        
          
          
            <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item ">
  <a class="nav-link nav-internal" href="../python-api/index.html">
    Python API
  </a>
</li>


<li class="nav-item current active">
  <a class="nav-link nav-internal" href="index.html">
    Developer Notes
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../community/index.html">
    Community
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../lang-bindings/index.html">
    Language Bindings
  </a>
</li>

  </ul>
</nav></div>
          
        
      </div>
    
    
    
      <div class="sidebar-header-items__end">
        
          <div class="navbar-item">

<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button></div>
        
          <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://x.com/PyTorch" title="X" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-x-twitter fa-lg" aria-hidden="true"></i>
            <span class="sr-only">X</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/pytorch/pytorch" title="GitHub" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">GitHub</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://pypi.org/project/torch" title="PyPI" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-custom fa-pypi fa-lg" aria-hidden="true"></i>
            <span class="sr-only">PyPI</span></a>
        </li>
</ul></div>
        
      </div>
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
<nav class="bd-docs-nav bd-links"
     aria-label="Section Navigation">
  <p class="bd-links__title" role="heading" aria-level="1">Section Navigation</p>
  <div class="bd-toc-item navbar-nav"><p aria-level="2" class="caption" role="heading"><span class="caption-text">Core Concepts</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="autograd.html">Autograd mechanics</a></li>
<li class="toctree-l1"><a class="reference internal" href="gradcheck.html">Gradcheck mechanics</a></li>
<li class="toctree-l1"><a class="reference internal" href="modules.html">Modules</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Device and Hardware Support</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="cuda.html">CUDA semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="hip.html">HIP (ROCm) semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="mps.html">MPS backend</a></li>
<li class="toctree-l1"><a class="reference internal" href="get_start_xpu.html">Getting Started on Intel GPU</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Distributed Training and Deployment</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="ddp.html">Distributed Data Parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="fsdp.html">FSDP Notes</a></li>
<li class="toctree-l1"><a class="reference internal" href="large_scale_deployments.html">Features for large-scale deployments</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Optimization and Performance</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="amp_examples.html">Automatic Mixed Precision examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="cpu_threading_torchscript_inference.html">CPU threading and TorchScript inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="multiprocessing.html">Multiprocessing best practices</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Extending PyTorch</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="extending.html">Extending PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="extending.func.html">Extending torch.func with autograd.Function</a></li>
<li class="toctree-l1"><a class="reference internal" href="custom_operators.html">PyTorch Custom Operators Landing Page</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Numerical Considerations</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="broadcasting.html">Broadcasting semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="numerical_accuracy.html">Numerical accuracy</a></li>
<li class="toctree-l1"><a class="reference internal" href="randomness.html">Reproducibility</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Serialization and Storage</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Serialization semantics</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">FAQs</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="faq.html">Frequently Asked Questions</a></li>
<li class="toctree-l1"><a class="reference internal" href="windows.html">Windows FAQ</a></li>
</ul>
</div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
      <div class="sidebar-primary-item">
<div id="ethical-ad-placement"
      class="flat"
      data-ea-publisher="readthedocs"
      data-ea-type="readthedocs-sidebar"
      data-ea-manual="true">
</div></div>
  </div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        
          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item">

<nav aria-label="Breadcrumb" class="d-print-none">
  <ul class="bd-breadcrumbs">
    
    <li class="breadcrumb-item breadcrumb-home">
      <a href="../index.html" class="nav-link" aria-label="Home">
        <i class="fa-solid fa-home"></i>
      </a>
    </li>
    
    <li class="breadcrumb-item"><a href="index.html" class="nav-link">Developer Notes</a></li>
    
    <li class="breadcrumb-item active" aria-current="page"><span class="ellipsis">Serialization semantics</span></li>
  </ul>
</nav>
</div>
      
    </div>
  
  
</div>
</div>
              
              
              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section id="serialization-semantics">
<h1><a class="toc-backref" href="#id2" role="doc-backlink">Serialization semantics</a><a class="headerlink" href="#serialization-semantics" title="Link to this heading">#</a></h1>
<p>This note describes how you can save and load PyTorch tensors and module states
in Python, and how to serialize Python modules so they can be loaded in C++.</p>
<nav class="contents" id="table-of-contents">
<p class="topic-title">Table of Contents</p>
<ul class="simple">
<li><p><a class="reference internal" href="#serialization-semantics" id="id2">Serialization semantics</a></p>
<ul>
<li><p><a class="reference internal" href="#saving-and-loading-tensors" id="id3">Saving and loading tensors</a></p></li>
<li><p><a class="reference internal" href="#saving-and-loading-tensors-preserves-views" id="id4">Saving and loading tensors preserves views</a></p></li>
<li><p><a class="reference internal" href="#saving-and-loading-torch-nn-modules" id="id5">Saving and loading torch.nn.Modules</a></p></li>
<li><p><a class="reference internal" href="#serialized-file-format-for-torch-save" id="id6">Serialized file format for <code class="docutils literal notranslate"><span class="pre">torch.save</span></code></a></p></li>
<li><p><a class="reference internal" href="#torch-load-with-weights-only-true" id="id7"><code class="docutils literal notranslate"><span class="pre">torch.load</span></code> with <code class="docutils literal notranslate"><span class="pre">weights_only=True</span></code></a></p>
<ul>
<li><p><a class="reference internal" href="#troubleshooting-weights-only" id="id8">Troubleshooting <code class="docutils literal notranslate"><span class="pre">weights_only</span></code></a></p>
<ul>
<li><p><a class="reference internal" href="#getting-unsafe-globals" id="id9">Getting unsafe globals</a></p></li>
<li><p><a class="reference internal" href="#environment-variables" id="id10">Environment Variables</a></p></li>
</ul>
</li>
</ul>
</li>
<li><p><a class="reference internal" href="#serializing-torch-nn-modules-and-loading-them-in-c" id="id11">Serializing torch.nn.Modules and loading them in C++</a></p></li>
<li><p><a class="reference internal" href="#saving-and-loading-scriptmodules-across-pytorch-versions" id="id12">Saving and loading ScriptModules across PyTorch versions</a></p>
<ul>
<li><p><a class="reference internal" href="#torch-div-performing-integer-division" id="id13">torch.div performing integer division</a></p></li>
<li><p><a class="reference internal" href="#torch-full-always-inferring-a-float-dtype" id="id14">torch.full always inferring a float dtype</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#utility-functions" id="id15">Utility functions</a></p></li>
<li><p><a class="reference internal" href="#module-torch.utils.serialization" id="id16">Config</a></p></li>
</ul>
</li>
</ul>
</nav>
<section id="saving-and-loading-tensors">
<span id="saving-loading-tensors"></span><h2><a class="toc-backref" href="#id3" role="doc-backlink">Saving and loading tensors</a><a class="headerlink" href="#saving-and-loading-tensors" title="Link to this heading">#</a></h2>
<p><a class="reference internal" href="../python-api/generated/torch.save.html#torch.save" title="torch.save"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.save()</span></code></a> and <a class="reference internal" href="../python-api/generated/torch.load.html#torch.load" title="torch.load"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.load()</span></code></a> let you easily save and load tensors:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">t</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="s1">&#39;tensor.pt&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;tensor.pt&#39;</span><span class="p">)</span>
<span class="go">tensor([1., 2.])</span>
</pre></div>
</div>
<p>By convention, PyTorch files are typically written with a ‘.pt’ or ‘.pth’ extension.</p>
<p><a class="reference internal" href="../python-api/generated/torch.save.html#torch.save" title="torch.save"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.save()</span></code></a> and <a class="reference internal" href="../python-api/generated/torch.load.html#torch.load" title="torch.load"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.load()</span></code></a> use Python’s pickle by default,
so you can also save multiple tensors as part of Python objects like tuples,
lists, and dicts:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">d</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;a&#39;</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">]),</span> <span class="s1">&#39;b&#39;</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">3.</span><span class="p">,</span> <span class="mf">4.</span><span class="p">])}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">d</span><span class="p">,</span> <span class="s1">&#39;tensor_dict.pt&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;tensor_dict.pt&#39;</span><span class="p">)</span>
<span class="go">{&#39;a&#39;: tensor([1., 2.]), &#39;b&#39;: tensor([3., 4.])}</span>
</pre></div>
</div>
<p>Custom data structures that include PyTorch tensors can also be saved if the
data structure is pickle-able.</p>
</section>
<section id="saving-and-loading-tensors-preserves-views">
<span id="preserve-storage-sharing"></span><h2><a class="toc-backref" href="#id4" role="doc-backlink">Saving and loading tensors preserves views</a><a class="headerlink" href="#saving-and-loading-tensors-preserves-views" title="Link to this heading">#</a></h2>
<p>Saving tensors preserves their view relationships:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">numbers</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">evens</span> <span class="o">=</span> <span class="n">numbers</span><span class="p">[</span><span class="mi">1</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">([</span><span class="n">numbers</span><span class="p">,</span> <span class="n">evens</span><span class="p">],</span> <span class="s1">&#39;tensors.pt&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loaded_numbers</span><span class="p">,</span> <span class="n">loaded_evens</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;tensors.pt&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loaded_evens</span> <span class="o">*=</span> <span class="mi">2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loaded_numbers</span>
<span class="go">tensor([ 1,  4,  3,  8,  5, 12,  7, 16,  9])</span>
</pre></div>
</div>
<p>Behind the scenes, these tensors share the same “storage.” See
<a class="reference external" href="https://pytorch.org/docs/main/tensor_view.html">Tensor Views</a> for more
on views and storage.</p>
<p>When PyTorch saves tensors it saves their storage objects and tensor
metadata separately. This is an implementation detail that may change in the
future, but it typically saves space and lets PyTorch easily
reconstruct the view relationships between the loaded tensors. In the above
snippet, for example, only a single storage is written to ‘tensors.pt’.</p>
<p>In some cases, however, saving the current storage objects may be unnecessary
and create prohibitively large files. In the following snippet a storage much
larger than the saved tensor is written to a file:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">large</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">small</span> <span class="o">=</span> <span class="n">large</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">5</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">small</span><span class="p">,</span> <span class="s1">&#39;small.pt&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loaded_small</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;small.pt&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loaded_small</span><span class="o">.</span><span class="n">storage</span><span class="p">()</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="go">999</span>
</pre></div>
</div>
<p>Instead of saving only the five values in the <cite>small</cite> tensor to ‘small.pt,’
the 999 values in the storage it shares with <cite>large</cite> were saved and loaded.</p>
<p>When saving tensors with fewer elements than their storage objects, the size of
the saved file can be reduced by first cloning the tensors. Cloning a tensor
produces a new tensor with a new storage object containing only the values
in the tensor:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">large</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">small</span> <span class="o">=</span> <span class="n">large</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">5</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">small</span><span class="o">.</span><span class="n">clone</span><span class="p">(),</span> <span class="s1">&#39;small.pt&#39;</span><span class="p">)</span>  <span class="c1"># saves a clone of small</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loaded_small</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;small.pt&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loaded_small</span><span class="o">.</span><span class="n">storage</span><span class="p">()</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="go">5</span>
</pre></div>
</div>
<p>Since the cloned tensors are independent of each other, however, they have
none of the view relationships the original tensors did. If both file size and
view relationships are important when saving tensors smaller than their
storage objects, then care must be taken to construct new tensors that minimize
the size of their storage objects but still have the desired view relationships
before saving.</p>
</section>
<section id="saving-and-loading-torch-nn-modules">
<span id="saving-loading-python-modules"></span><h2><a class="toc-backref" href="#id5" role="doc-backlink">Saving and loading torch.nn.Modules</a><a class="headerlink" href="#saving-and-loading-torch-nn-modules" title="Link to this heading">#</a></h2>
<p>See also: <a class="reference external" href="https://pytorch.org/tutorials/beginner/saving_loading_models.html">Tutorial: Saving and loading modules</a></p>
<p>In PyTorch, a module’s state is frequently serialized using a ‘state dict.’
A module’s state dict contains all of its parameters and persistent buffers:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">bn</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">track_running_stats</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">list</span><span class="p">(</span><span class="n">bn</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">())</span>
<span class="go">[(&#39;weight&#39;, Parameter containing: tensor([1., 1., 1.], requires_grad=True)),</span>
<span class="go"> (&#39;bias&#39;, Parameter containing: tensor([0., 0., 0.], requires_grad=True))]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="nb">list</span><span class="p">(</span><span class="n">bn</span><span class="o">.</span><span class="n">named_buffers</span><span class="p">())</span>
<span class="go">[(&#39;running_mean&#39;, tensor([0., 0., 0.])),</span>
<span class="go"> (&#39;running_var&#39;, tensor([1., 1., 1.])),</span>
<span class="go"> (&#39;num_batches_tracked&#39;, tensor(0))]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">bn</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()</span>
<span class="go">OrderedDict([(&#39;weight&#39;, tensor([1., 1., 1.])),</span>
<span class="go">             (&#39;bias&#39;, tensor([0., 0., 0.])),</span>
<span class="go">             (&#39;running_mean&#39;, tensor([0., 0., 0.])),</span>
<span class="go">             (&#39;running_var&#39;, tensor([1., 1., 1.])),</span>
<span class="go">             (&#39;num_batches_tracked&#39;, tensor(0))])</span>
</pre></div>
</div>
<p>Instead of saving a module directly, for compatibility reasons it is recommended
to instead save only its state dict. Python modules even have a function,
<a class="reference internal" href="../python-api/generated/torch.nn.Module.html#torch.nn.Module.load_state_dict" title="torch.nn.Module.load_state_dict"><code class="xref py py-meth docutils literal notranslate"><span class="pre">load_state_dict()</span></code></a>, to restore their states from a state dict:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">bn</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span> <span class="s1">&#39;bn.pt&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">bn_state_dict</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;bn.pt&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">new_bn</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">track_running_stats</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">new_bn</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">bn_state_dict</span><span class="p">)</span>
<span class="go">&lt;All keys matched successfully&gt;</span>
</pre></div>
</div>
<p>Note that the state dict is first loaded from its file with <a class="reference internal" href="../python-api/generated/torch.load.html#torch.load" title="torch.load"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.load()</span></code></a>
and the state then restored with <a class="reference internal" href="../python-api/generated/torch.nn.Module.html#torch.nn.Module.load_state_dict" title="torch.nn.Module.load_state_dict"><code class="xref py py-meth docutils literal notranslate"><span class="pre">load_state_dict()</span></code></a>.</p>
<p>Even custom modules and modules containing other modules have state dicts and
can use this pattern:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># A module with two linear layers</span>
<span class="o">&gt;&gt;&gt;</span> <span class="k">class</span><span class="w"> </span><span class="nc">MyModule</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
      <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">l0</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">l1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

      <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
        <span class="n">out0</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">l0</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
        <span class="n">out0_relu</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">out0</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">l1</span><span class="p">(</span><span class="n">out0_relu</span><span class="p">)</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">m</span> <span class="o">=</span> <span class="n">MyModule</span><span class="p">()</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">m</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()</span>
<span class="n">OrderedDict</span><span class="p">([(</span><span class="s1">&#39;l0.weight&#39;</span><span class="p">,</span> <span class="n">tensor</span><span class="p">([[</span> <span class="mf">0.1400</span><span class="p">,</span> <span class="mf">0.4563</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0271</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.4406</span><span class="p">],</span>
                                   <span class="p">[</span><span class="o">-</span><span class="mf">0.3289</span><span class="p">,</span> <span class="mf">0.2827</span><span class="p">,</span> <span class="mf">0.4588</span><span class="p">,</span> <span class="mf">0.2031</span><span class="p">]])),</span>
             <span class="p">(</span><span class="s1">&#39;l0.bias&#39;</span><span class="p">,</span> <span class="n">tensor</span><span class="p">([</span> <span class="mf">0.0300</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1316</span><span class="p">])),</span>
             <span class="p">(</span><span class="s1">&#39;l1.weight&#39;</span><span class="p">,</span> <span class="n">tensor</span><span class="p">([[</span><span class="mf">0.6533</span><span class="p">,</span> <span class="mf">0.3413</span><span class="p">]])),</span>
             <span class="p">(</span><span class="s1">&#39;l1.bias&#39;</span><span class="p">,</span> <span class="n">tensor</span><span class="p">([</span><span class="o">-</span><span class="mf">0.1112</span><span class="p">]))])</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span> <span class="s1">&#39;mymodule.pt&#39;</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">m_state_dict</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;mymodule.pt&#39;</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">new_m</span> <span class="o">=</span> <span class="n">MyModule</span><span class="p">()</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">new_m</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">m_state_dict</span><span class="p">)</span>
<span class="o">&lt;</span><span class="n">All</span> <span class="n">keys</span> <span class="n">matched</span> <span class="n">successfully</span><span class="o">&gt;</span>
</pre></div>
</div>
</section>
<section id="serialized-file-format-for-torch-save">
<span id="serialized-file-format"></span><h2><a class="toc-backref" href="#id6" role="doc-backlink">Serialized file format for <code class="docutils literal notranslate"><span class="pre">torch.save</span></code></a><a class="headerlink" href="#serialized-file-format-for-torch-save" title="Link to this heading">#</a></h2>
<p>Since PyTorch 1.6.0, <code class="docutils literal notranslate"><span class="pre">torch.save</span></code> defaults to returning an uncompressed ZIP64
archive unless the user sets <code class="docutils literal notranslate"><span class="pre">_use_new_zipfile_serialization=False</span></code>.</p>
<p>In this archive, the files are ordered as such</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>checkpoint.pth
├── data.pkl
├── byteorder  # added in PyTorch 2.1.0
├── data/
│   ├── 0
│   ├── 1
│   ├── 2
│   └── …
└── version
</pre></div>
</div>
<dl class="simple">
<dt>The entries are as follows:</dt><dd><ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">data.pkl</span></code> is the result of pickling the object passed to <code class="docutils literal notranslate"><span class="pre">torch.save</span></code>
excluding <code class="docutils literal notranslate"><span class="pre">torch.Storage</span></code> objects that it contains</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">byteorder</span></code> contains a string with the <code class="docutils literal notranslate"><span class="pre">sys.byteorder</span></code> when saving (“little” or “big”)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">data/</span></code> contains all the storages in the object, where each storage is a separate file</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">version</span></code> contains a version number at save time that can be used at load time</p></li>
</ul>
</dd>
</dl>
<p>When saving, PyTorch will ensure that the local file header of each file is padded
to an offset that is a multiple of 64 bytes, ensuring that the offset of each file
is 64-byte aligned.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Tensors on certain devices such as XLA are serialized as pickled numpy arrays. As
such, their storages are not serialized. In these cases <code class="docutils literal notranslate"><span class="pre">data/</span></code> might not exist
in the checkpoint.</p>
</div>
</section>
<section id="torch-load-with-weights-only-true">
<span id="weights-only"></span><h2><a class="toc-backref" href="#id7" role="doc-backlink"><code class="docutils literal notranslate"><span class="pre">torch.load</span></code> with <code class="docutils literal notranslate"><span class="pre">weights_only=True</span></code></a><a class="headerlink" href="#torch-load-with-weights-only-true" title="Link to this heading">#</a></h2>
<p>Starting in version 2.6, <code class="docutils literal notranslate"><span class="pre">torch.load</span></code> will use <code class="docutils literal notranslate"><span class="pre">weights_only=True</span></code> if the <code class="docutils literal notranslate"><span class="pre">pickle_module</span></code>
argument is not passed.</p>
<p>As discussed in the documentation for <a class="reference internal" href="../python-api/generated/torch.load.html#torch.load" title="torch.load"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.load()</span></code></a>, <code class="docutils literal notranslate"><span class="pre">weights_only=True</span></code> restricts
the unpickler used in <code class="docutils literal notranslate"><span class="pre">torch.load</span></code> to only executing functions/building classes required for
<code class="docutils literal notranslate"><span class="pre">state_dicts</span></code> of plain <code class="docutils literal notranslate"><span class="pre">torch.Tensors</span></code> as well as some other primitive types. Further,
unlike the default <code class="docutils literal notranslate"><span class="pre">Unpickler</span></code> provided by the <code class="docutils literal notranslate"><span class="pre">pickle</span></code> module, the <code class="docutils literal notranslate"><span class="pre">weights_only</span></code> Unpickler
is not allowed to dynamically import anything during unpickling.</p>
<p>As mentioned above, saving a module’s <code class="docutils literal notranslate"><span class="pre">state_dict</span></code> is a best practice when using <code class="docutils literal notranslate"><span class="pre">torch.save</span></code>. If loading an old
checkpoint that contains an <code class="docutils literal notranslate"><span class="pre">nn.Module</span></code>, we recommend <code class="docutils literal notranslate"><span class="pre">weights_only=False</span></code>. When loading a checkpoint that contains
tensor subclasses, there will likely be functions/classes that need to be allowlisted, see below for further details.</p>
<p>If the <code class="docutils literal notranslate"><span class="pre">weights_only</span></code> Unpickler encounters a function or class that is not allowlisted
by default within the pickle file, you should see an actionable error like such</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>_pickle.UnpicklingError: Weights only load failed. This file can still be loaded,
to do so you have two options, do those steps only if you trust the source of the checkpoint.
    1. Re-running `torch.load` with `weights_only` set to `False` will likely succeed,
        but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.
    2. Alternatively, to load with `weights_only=True` please check the recommended
       steps in the following error message.
       WeightsUnpickler error: Unsupported global: GLOBAL {__module__}.{__name__} was not an allowed global by
       default. Please use `torch.serialization.add_safe_globals([{__name__}])` or the
       `torch.serialization.safe_globals([{__name__}])` context manager to allowlist this global
       if you trust this class/function.
</pre></div>
</div>
<p>Please follow the steps in the error message and allowlist the functions or classes only if you trust them.</p>
<p>To get all GLOBALs (functions/classes) in the checkpoint that are not yet allowlisted you can use
<a class="reference internal" href="#torch.serialization.get_unsafe_globals_in_checkpoint" title="torch.serialization.get_unsafe_globals_in_checkpoint"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.serialization.get_unsafe_globals_in_checkpoint()</span></code></a> which will return a list of strings of the form
<code class="docutils literal notranslate"><span class="pre">{__module__}.{__name__}</span></code>. If you trust these functions/classes, you can import them and allowlist them per
the error message either via <a class="reference internal" href="#torch.serialization.add_safe_globals" title="torch.serialization.add_safe_globals"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.serialization.add_safe_globals()</span></code></a> or the context manager
<a class="reference internal" href="#torch.serialization.safe_globals" title="torch.serialization.safe_globals"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.serialization.safe_globals</span></code></a>.</p>
<p>To access the list of user-allowlisted functions/classes you can use <a class="reference internal" href="#torch.serialization.get_safe_globals" title="torch.serialization.get_safe_globals"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.serialization.get_safe_globals()</span></code></a> and
to clear the current list see <a class="reference internal" href="#torch.serialization.clear_safe_globals" title="torch.serialization.clear_safe_globals"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.serialization.clear_safe_globals()</span></code></a>.</p>
<section id="troubleshooting-weights-only">
<h3><a class="toc-backref" href="#id8" role="doc-backlink">Troubleshooting <code class="docutils literal notranslate"><span class="pre">weights_only</span></code></a><a class="headerlink" href="#troubleshooting-weights-only" title="Link to this heading">#</a></h3>
<section id="getting-unsafe-globals">
<h4><a class="toc-backref" href="#id9" role="doc-backlink">Getting unsafe globals</a><a class="headerlink" href="#getting-unsafe-globals" title="Link to this heading">#</a></h4>
<p>A caveat is that <a class="reference internal" href="#torch.serialization.get_unsafe_globals_in_checkpoint" title="torch.serialization.get_unsafe_globals_in_checkpoint"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.serialization.get_unsafe_globals_in_checkpoint()</span></code></a> analyzes the checkpoint statically,
some types might be built dynamically during the unpickling process and hence will not be reported by
<a class="reference internal" href="#torch.serialization.get_unsafe_globals_in_checkpoint" title="torch.serialization.get_unsafe_globals_in_checkpoint"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.serialization.get_unsafe_globals_in_checkpoint()</span></code></a>. One such example is <code class="docutils literal notranslate"><span class="pre">dtypes</span></code> in numpy. In
<code class="docutils literal notranslate"><span class="pre">numpy</span> <span class="pre">&lt;</span> <span class="pre">1.25</span></code> after allowlisting all the functions/classes reported by
<a class="reference internal" href="#torch.serialization.get_unsafe_globals_in_checkpoint" title="torch.serialization.get_unsafe_globals_in_checkpoint"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.serialization.get_unsafe_globals_in_checkpoint()</span></code></a> you might see an error like</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>WeightsUnpickler error: Can only build Tensor, Parameter, OrderedDict or types allowlisted via `add_safe_globals`,
but got &lt;class &#39;numpy.dtype[float32]&#39;&gt;
</pre></div>
</div>
<p>This can be allowlisted via <code class="docutils literal notranslate"><span class="pre">{add_}safe_globals([type(np.dtype(np.float32))])</span></code>.</p>
<p>In <code class="docutils literal notranslate"><span class="pre">numpy</span> <span class="pre">&gt;=1.25</span></code> you would see</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>WeightsUnpickler error: Can only build Tensor, Parameter, OrderedDict or types allowlisted via `add_safe_globals`,
but got &lt;class &#39;numpy.dtypes.Float32DType&#39;&gt;
</pre></div>
</div>
<p>This can be allowlisted via <code class="docutils literal notranslate"><span class="pre">{add_}safe_globals([np.dtypes.Float32DType])</span></code>.</p>
</section>
<section id="environment-variables">
<h4><a class="toc-backref" href="#id10" role="doc-backlink">Environment Variables</a><a class="headerlink" href="#environment-variables" title="Link to this heading">#</a></h4>
<p>There are two environment variables that will influence the behavior of <code class="docutils literal notranslate"><span class="pre">torch.load</span></code>. These can be helpful
if one does not have access to the <code class="docutils literal notranslate"><span class="pre">torch.load</span></code> callsites.</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">TORCH_FORCE_WEIGHTS_ONLY_LOAD=1</span></code> will override all <code class="docutils literal notranslate"><span class="pre">torch.load</span></code> callsites to use <code class="docutils literal notranslate"><span class="pre">weights_only=True</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">TORCH_FORCE_NO_WEIGHTS_ONLY_LOAD=1</span></code> will make <code class="docutils literal notranslate"><span class="pre">torch.load</span></code> callsites use <code class="docutils literal notranslate"><span class="pre">weights_only=False</span></code> <strong>only</strong>
if <code class="docutils literal notranslate"><span class="pre">weights_only</span></code> was not passed as an argument.</p></li>
</ul>
</section>
</section>
</section>
<section id="serializing-torch-nn-modules-and-loading-them-in-c">
<span id="serializing-python-modules"></span><h2><a class="toc-backref" href="#id11" role="doc-backlink">Serializing torch.nn.Modules and loading them in C++</a><a class="headerlink" href="#serializing-torch-nn-modules-and-loading-them-in-c" title="Link to this heading">#</a></h2>
<p>See also: <a class="reference external" href="https://pytorch.org/tutorials/advanced/cpp_export.html">Tutorial: Loading a TorchScript Model in C++</a></p>
<p>ScriptModules can be serialized as a TorchScript program and loaded
using <a class="reference internal" href="../python-api/generated/torch.jit.load.html#torch.jit.load" title="torch.jit.load"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.jit.load()</span></code></a>.
This serialization encodes all the modules’ methods, submodules, parameters,
and attributes, and it allows the serialized program to be loaded in C++
(i.e. without Python).</p>
<p>The distinction between <a class="reference internal" href="../python-api/generated/torch.jit.save.html#torch.jit.save" title="torch.jit.save"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.jit.save()</span></code></a> and <a class="reference internal" href="../python-api/generated/torch.save.html#torch.save" title="torch.save"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.save()</span></code></a> may not
be immediately clear. <a class="reference internal" href="../python-api/generated/torch.save.html#torch.save" title="torch.save"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.save()</span></code></a> saves Python objects with pickle.
This is especially useful for prototyping, researching, and training.
<a class="reference internal" href="../python-api/generated/torch.jit.save.html#torch.jit.save" title="torch.jit.save"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.jit.save()</span></code></a>, on the other hand, serializes ScriptModules to a format
that can be loaded in Python or C++. This is useful when saving and loading C++
modules or for running modules trained in Python with C++, a common practice
when deploying PyTorch models.</p>
<p>To script, serialize and load a module in Python:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">scripted_module</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">script</span><span class="p">(</span><span class="n">MyModule</span><span class="p">())</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">scripted_module</span><span class="p">,</span> <span class="s1">&#39;mymodule.pt&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;mymodule.pt&#39;</span><span class="p">)</span>
<span class="go">RecursiveScriptModule( original_name=MyModule</span>
<span class="go">                      (l0): RecursiveScriptModule(original_name=Linear)</span>
<span class="go">                      (l1): RecursiveScriptModule(original_name=Linear) )</span>
</pre></div>
</div>
<p>Traced modules can also be saved with <a class="reference internal" href="../python-api/generated/torch.jit.save.html#torch.jit.save" title="torch.jit.save"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.jit.save()</span></code></a>, with the caveat
that only the traced code path is serialized. The following example demonstrates
this:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># A module with control flow</span>
<span class="o">&gt;&gt;&gt;</span> <span class="k">class</span><span class="w"> </span><span class="nc">ControlFlowModule</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
      <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">l0</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">l1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

      <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
        <span class="k">if</span> <span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

        <span class="n">out0</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">l0</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
        <span class="n">out0_relu</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">out0</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">l1</span><span class="p">(</span><span class="n">out0_relu</span><span class="p">)</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">traced_module</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">trace</span><span class="p">(</span><span class="n">ControlFlowModule</span><span class="p">(),</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">))</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">traced_module</span><span class="p">,</span> <span class="s1">&#39;controlflowmodule_traced.pt&#39;</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">loaded</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;controlflowmodule_traced.pt&#39;</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">loaded</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">)))</span>
<span class="n">tensor</span><span class="p">([[</span><span class="o">-</span><span class="mf">0.1571</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mf">0.3793</span><span class="p">]],</span> <span class="n">grad_fn</span><span class="o">=&lt;</span><span class="n">AddBackward0</span><span class="o">&gt;</span><span class="p">)</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">scripted_module</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">script</span><span class="p">(</span><span class="n">ControlFlowModule</span><span class="p">(),</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">))</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">scripted_module</span><span class="p">,</span> <span class="s1">&#39;controlflowmodule_scripted.pt&#39;</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">loaded</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;controlflowmodule_scripted.pt&#39;</span><span class="p">)</span>
<span class="o">&gt;&gt;</span> <span class="n">loaded</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="n">tensor</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
<p>The above module has an if statement that is not triggered by the traced inputs,
and so is not part of the traced module and not serialized with it.
The scripted module, however, contains the if statement and is serialized with it.
See the <a class="reference external" href="https://pytorch.org/docs/stable/jit.html">TorchScript documentation</a>
for more on scripting and tracing.</p>
<p>Finally, to load the module in C++:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="p">::</span><span class="n">jit</span><span class="p">::</span><span class="n">script</span><span class="p">::</span><span class="n">Module</span> <span class="n">module</span><span class="p">;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">module</span> <span class="o">=</span> <span class="n">torch</span><span class="p">::</span><span class="n">jit</span><span class="p">::</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;controlflowmodule_scripted.pt&#39;</span><span class="p">);</span>
</pre></div>
</div>
<p>See the <a class="reference external" href="https://pytorch.org/cppdocs/">PyTorch C++ API documentation</a>
for details about how to use PyTorch modules in C++.</p>
</section>
<section id="saving-and-loading-scriptmodules-across-pytorch-versions">
<span id="saving-loading-across-versions"></span><h2><a class="toc-backref" href="#id12" role="doc-backlink">Saving and loading ScriptModules across PyTorch versions</a><a class="headerlink" href="#saving-and-loading-scriptmodules-across-pytorch-versions" title="Link to this heading">#</a></h2>
<p>The PyTorch Team recommends saving and loading modules with the same version of
PyTorch. Older versions of PyTorch may not support newer modules, and newer
versions may have removed or modified older behavior. These changes are
explicitly described in
PyTorch’s <a class="reference external" href="https://github.com/pytorch/pytorch/releases">release notes</a>,
and modules relying on functionality that has changed may need to be updated
to continue working properly. In limited cases, detailed below, PyTorch will
preserve the historic behavior of serialized ScriptModules so they do not require
an update.</p>
<section id="torch-div-performing-integer-division">
<h3><a class="toc-backref" href="#id13" role="doc-backlink">torch.div performing integer division</a><a class="headerlink" href="#torch-div-performing-integer-division" title="Link to this heading">#</a></h3>
<p>In PyTorch 1.5 and earlier <a class="reference internal" href="../python-api/generated/torch.div.html#torch.div" title="torch.div"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.div()</span></code></a> would perform floor division when
given two integer inputs:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># PyTorch 1.5 (and earlier)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">a</span> <span class="o">/</span> <span class="n">b</span>
<span class="n">tensor</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
<p>In PyTorch 1.7, however, <a class="reference internal" href="../python-api/generated/torch.div.html#torch.div" title="torch.div"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.div()</span></code></a> will always perform a true division
of its inputs, just like division in Python 3:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># PyTorch 1.7</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">a</span> <span class="o">/</span> <span class="n">b</span>
<span class="n">tensor</span><span class="p">(</span><span class="mf">1.6667</span><span class="p">)</span>
</pre></div>
</div>
<p>The behavior of <a class="reference internal" href="../python-api/generated/torch.div.html#torch.div" title="torch.div"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.div()</span></code></a> is preserved in serialized ScriptModules.
That is, ScriptModules serialized with versions of PyTorch before 1.6 will continue
to see <a class="reference internal" href="../python-api/generated/torch.div.html#torch.div" title="torch.div"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.div()</span></code></a> perform floor division when given two integer inputs
even when loaded with newer versions of PyTorch. ScriptModules using <a class="reference internal" href="../python-api/generated/torch.div.html#torch.div" title="torch.div"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.div()</span></code></a>
and serialized on PyTorch 1.6 and later cannot be loaded in earlier versions of
PyTorch, however, since those earlier versions do not understand the new behavior.</p>
</section>
<section id="torch-full-always-inferring-a-float-dtype">
<h3><a class="toc-backref" href="#id14" role="doc-backlink">torch.full always inferring a float dtype</a><a class="headerlink" href="#torch-full-always-inferring-a-float-dtype" title="Link to this heading">#</a></h3>
<p>In PyTorch 1.5 and earlier <a class="reference internal" href="../python-api/generated/torch.full.html#torch.full" title="torch.full"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.full()</span></code></a> always returned a float tensor,
regardless of the fill value it’s given:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># PyTorch 1.5 and earlier</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">full</span><span class="p">((</span><span class="mi">3</span><span class="p">,),</span> <span class="mi">1</span><span class="p">)</span>  <span class="c1"># Note the integer fill value...</span>
<span class="n">tensor</span><span class="p">([</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">])</span>     <span class="c1"># ...but float tensor!</span>
</pre></div>
</div>
<p>In PyTorch 1.7, however, <a class="reference internal" href="../python-api/generated/torch.full.html#torch.full" title="torch.full"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.full()</span></code></a> will infer the returned tensor’s
dtype from the fill value:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># PyTorch 1.7</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">full</span><span class="p">((</span><span class="mi">3</span><span class="p">,),</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">full</span><span class="p">((</span><span class="mi">3</span><span class="p">,),</span> <span class="kc">True</span><span class="p">)</span>
<span class="n">tensor</span><span class="p">([</span><span class="kc">True</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="kc">True</span><span class="p">])</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">full</span><span class="p">((</span><span class="mi">3</span><span class="p">,),</span> <span class="mf">1.</span><span class="p">)</span>
<span class="n">tensor</span><span class="p">([</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">])</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">full</span><span class="p">((</span><span class="mi">3</span><span class="p">,),</span> <span class="mi">1</span> <span class="o">+</span> <span class="mi">1</span><span class="n">j</span><span class="p">)</span>
<span class="n">tensor</span><span class="p">([</span><span class="mf">1.</span><span class="o">+</span><span class="mf">1.</span><span class="n">j</span><span class="p">,</span> <span class="mf">1.</span><span class="o">+</span><span class="mf">1.</span><span class="n">j</span><span class="p">,</span> <span class="mf">1.</span><span class="o">+</span><span class="mf">1.</span><span class="n">j</span><span class="p">])</span>
</pre></div>
</div>
<p>The behavior of <a class="reference internal" href="../python-api/generated/torch.full.html#torch.full" title="torch.full"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.full()</span></code></a> is preserved in serialized ScriptModules. That is,
ScriptModules serialized with versions of PyTorch before 1.6 will continue to see
torch.full return float tensors by default, even when given bool or
integer fill values. ScriptModules using <a class="reference internal" href="../python-api/generated/torch.full.html#torch.full" title="torch.full"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.full()</span></code></a> and serialized on PyTorch 1.6
and later cannot be loaded in earlier versions of PyTorch, however, since those
earlier versions do not understand the new behavior.</p>
</section>
</section>
<section id="utility-functions">
<span id="id1"></span><h2><a class="toc-backref" href="#id15" role="doc-backlink">Utility functions</a><a class="headerlink" href="#utility-functions" title="Link to this heading">#</a></h2>
<p>The following utility functions are related to serialization:</p>
<dl class="py function">
<dt class="sig sig-object py" id="torch.serialization.register_package">
<span class="sig-prename descclassname"><span class="pre">torch.serialization.</span></span><span class="sig-name descname"><span class="pre">register_package</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">priority</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tagger</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">deserializer</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torch/serialization.html#register_package"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/serialization.py#L438"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.serialization.register_package" title="Link to this definition">#</a></dt>
<dd><p>Registers callables for tagging and deserializing storage objects with an associated priority.
Tagging associates a device with a storage object at save time while deserializing moves a
storage object to an appropriate device at load time. <code class="xref py py-attr docutils literal notranslate"><span class="pre">tagger</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">deserializer</span></code>
are run in the order given by their <code class="xref py py-attr docutils literal notranslate"><span class="pre">priority</span></code> until a tagger/deserializer returns a
value that is not <cite>None</cite>.</p>
<p>To override the deserialization behavior for a device in the global registry, one can register a
tagger with a higher priority than the existing tagger.</p>
<p>This function can also be used to register a tagger and deserializer for new devices.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>priority</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – Indicates the priority associated with the tagger and deserializer, where a lower
value indicates higher priority.</p></li>
<li><p><strong>tagger</strong> (<a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Callable" title="(in Python v3.13)"><em>Callable</em></a><em>[</em><em>[</em><em>Storage</em><em> | </em><a class="reference internal" href="../python-api/storage.html#torch.TypedStorage" title="torch.storage.TypedStorage"><em>TypedStorage</em></a><em> | </em><a class="reference internal" href="../python-api/storage.html#torch.UntypedStorage" title="torch.storage.UntypedStorage"><em>UntypedStorage</em></a><em>]</em><em>, </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><em>str</em></a><em> | </em><em>None</em><em>]</em>) – Callable that takes in a storage object and returns its tagged device as a string
or None.</p></li>
<li><p><strong>deserializer</strong> (<a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Callable" title="(in Python v3.13)"><em>Callable</em></a><em>[</em><em>[</em><em>Storage</em><em> | </em><a class="reference internal" href="../python-api/storage.html#torch.TypedStorage" title="torch.storage.TypedStorage"><em>TypedStorage</em></a><em> | </em><a class="reference internal" href="../python-api/storage.html#torch.UntypedStorage" title="torch.storage.UntypedStorage"><em>UntypedStorage</em></a><em>, </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><em>str</em></a><em>]</em><em>, </em><em>Storage</em><em> | </em><a class="reference internal" href="../python-api/storage.html#torch.TypedStorage" title="torch.storage.TypedStorage"><em>TypedStorage</em></a><em> | </em><a class="reference internal" href="../python-api/storage.html#torch.UntypedStorage" title="torch.storage.UntypedStorage"><em>UntypedStorage</em></a><em> | </em><em>None</em><em>]</em>) – Callable that takes in storage object and a device string and returns a storage
object on the appropriate device or None.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><cite>None</cite></p>
</dd>
</dl>
<p class="rubric">Example</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">def</span><span class="w"> </span><span class="nf">ipu_tag</span><span class="p">(</span><span class="n">obj</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">if</span> <span class="n">obj</span><span class="o">.</span><span class="n">device</span><span class="o">.</span><span class="n">type</span> <span class="o">==</span> <span class="s1">&#39;ipu&#39;</span><span class="p">:</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="s1">&#39;ipu&#39;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">def</span><span class="w"> </span><span class="nf">ipu_deserialize</span><span class="p">(</span><span class="n">obj</span><span class="p">,</span> <span class="n">location</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">if</span> <span class="n">location</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s1">&#39;ipu&#39;</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">ipu</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">torch</span><span class="p">,</span> <span class="s2">&quot;ipu&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">assert</span> <span class="n">ipu</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">,</span> <span class="s2">&quot;IPU device module is not loaded&quot;</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">assert</span> <span class="n">torch</span><span class="o">.</span><span class="n">ipu</span><span class="o">.</span><span class="n">is_available</span><span class="p">(),</span> <span class="s2">&quot;ipu is not available&quot;</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="n">obj</span><span class="o">.</span><span class="n">ipu</span><span class="p">(</span><span class="n">location</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">serialization</span><span class="o">.</span><span class="n">register_package</span><span class="p">(</span><span class="mi">11</span><span class="p">,</span> <span class="n">ipu_tag</span><span class="p">,</span> <span class="n">ipu_deserialize</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="torch.serialization.get_crc32_options">
<span class="sig-prename descclassname"><span class="pre">torch.serialization.</span></span><span class="sig-name descname"><span class="pre">get_crc32_options</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torch/serialization.html#get_crc32_options"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/serialization.py#L187"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.serialization.get_crc32_options" title="Link to this definition">#</a></dt>
<dd><p>Get whether <a class="reference internal" href="../python-api/generated/torch.save.html#torch.save" title="torch.save"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.save()</span></code></a> computes and writes crc32 for each record.</p>
<p>Defaults to <code class="docutils literal notranslate"><span class="pre">True</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)">bool</a></p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="torch.serialization.set_crc32_options">
<span class="sig-prename descclassname"><span class="pre">torch.serialization.</span></span><span class="sig-name descname"><span class="pre">set_crc32_options</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">compute_crc32</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torch/serialization.html#set_crc32_options"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/serialization.py#L196"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.serialization.set_crc32_options" title="Link to this definition">#</a></dt>
<dd><p>Set whether <a class="reference internal" href="../python-api/generated/torch.save.html#torch.save" title="torch.save"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.save()</span></code></a> computes and writes crc32 for each record.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Setting this to <code class="docutils literal notranslate"><span class="pre">False</span></code> may make unzipping of the <code class="docutils literal notranslate"><span class="pre">torch.save</span></code> output
fail or warn due to corrupted CRC32. However <code class="docutils literal notranslate"><span class="pre">torch.load</span></code> will be
able to load the file.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>compute_crc32</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><em>bool</em></a>) – set crc32 compuation flag</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="torch.serialization.get_default_load_endianness">
<span class="sig-prename descclassname"><span class="pre">torch.serialization.</span></span><span class="sig-name descname"><span class="pre">get_default_load_endianness</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torch/serialization.html#get_default_load_endianness"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/serialization.py#L153"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.serialization.get_default_load_endianness" title="Link to this definition">#</a></dt>
<dd><p>Get fallback byte order for loading files</p>
<p>If byteorder mark is not present in saved checkpoint,
this byte order is used as fallback.
By default, it’s “native” byte order.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>Optional[LoadEndianness]</p>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p>default_load_endian</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="torch.serialization.set_default_load_endianness">
<span class="sig-prename descclassname"><span class="pre">torch.serialization.</span></span><span class="sig-name descname"><span class="pre">set_default_load_endianness</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">endianness</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torch/serialization.html#set_default_load_endianness"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/serialization.py#L167"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.serialization.set_default_load_endianness" title="Link to this definition">#</a></dt>
<dd><p>Set fallback byte order for loading files</p>
<p>If byteorder mark is not present in saved checkpoint,
this byte order is used as fallback.
By default, it’s “native” byte order.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>endianness</strong> – the new fallback byte order</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="torch.serialization.get_default_mmap_options">
<span class="sig-prename descclassname"><span class="pre">torch.serialization.</span></span><span class="sig-name descname"><span class="pre">get_default_mmap_options</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torch/serialization.html#get_default_mmap_options"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/serialization.py#L215"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.serialization.get_default_mmap_options" title="Link to this definition">#</a></dt>
<dd><p>Get default mmap options for <a class="reference internal" href="../python-api/generated/torch.load.html#torch.load" title="torch.load"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.load()</span></code></a> with <code class="docutils literal notranslate"><span class="pre">mmap=True</span></code>.</p>
<p>Defaults to <code class="docutils literal notranslate"><span class="pre">mmap.MAP_PRIVATE</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>int</p>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p>default_mmap_options</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="torch.serialization.set_default_mmap_options">
<span class="sig-prename descclassname"><span class="pre">torch.serialization.</span></span><span class="sig-name descname"><span class="pre">set_default_mmap_options</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">flags</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torch/serialization.html#set_default_mmap_options"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/serialization.py#L228"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.serialization.set_default_mmap_options" title="Link to this definition">#</a></dt>
<dd><p>Context manager or function to set default mmap options for <a class="reference internal" href="../python-api/generated/torch.load.html#torch.load" title="torch.load"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.load()</span></code></a> with <code class="docutils literal notranslate"><span class="pre">mmap=True</span></code> to flags.</p>
<p>For now, only either <code class="docutils literal notranslate"><span class="pre">mmap.MAP_PRIVATE</span></code> or <code class="docutils literal notranslate"><span class="pre">mmap.MAP_SHARED</span></code> are supported.
Please open an issue if you need any other option to be added here.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This feature is currently not supported for Windows.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>flags</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – <code class="docutils literal notranslate"><span class="pre">mmap.MAP_PRIVATE</span></code> or <code class="docutils literal notranslate"><span class="pre">mmap.MAP_SHARED</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="torch.serialization.add_safe_globals">
<span class="sig-prename descclassname"><span class="pre">torch.serialization.</span></span><span class="sig-name descname"><span class="pre">add_safe_globals</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">safe_globals</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torch/serialization.html#add_safe_globals"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/serialization.py#L278"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.serialization.add_safe_globals" title="Link to this definition">#</a></dt>
<dd><p>Marks the given globals as safe for <code class="docutils literal notranslate"><span class="pre">weights_only</span></code> load. For example, functions
added to this list can be called during unpickling, classes could be instantiated
and have state set.</p>
<p>Each item in the list can either be a function/class or a tuple of the form
(function/class, string) where string is the full path of the function/class.</p>
<p>Within the serialized format, each function is identified with its full
path as <code class="docutils literal notranslate"><span class="pre">{__module__}.{__name__}</span></code>. When calling this API, you can provide this
full path that should match the one in the checkpoint otherwise the default
<code class="docutils literal notranslate"><span class="pre">{fn.__module__}.{fn.__name__}</span></code> will be used.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>safe_globals</strong> (<em>List</em><em>[</em><em>Union</em><em>[</em><em>Callable</em><em>, </em><em>Tuple</em><em>[</em><em>Callable</em><em>, </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><em>str</em></a><em>]</em><em>]</em><em>]</em>) – list of globals to mark as safe</p>
</dd>
</dl>
<p class="rubric">Example</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">tempfile</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">class</span><span class="w"> </span><span class="nc">MyTensor</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
<span class="gp">... </span>    <span class="k">pass</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">t</span> <span class="o">=</span> <span class="n">MyTensor</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">with</span> <span class="n">tempfile</span><span class="o">.</span><span class="n">NamedTemporaryFile</span><span class="p">()</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
<span class="gp">... </span>    <span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">f</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
<span class="go"># Running `torch.load(f.name, weights_only=True)` will fail with</span>
<span class="go"># Unsupported global: GLOBAL __main__.MyTensor was not an allowed global by default.</span>
<span class="go"># Check the code and make sure MyTensor is safe to be used when loaded from an arbitrary checkpoint.</span>
<span class="go">...     torch.serialization.add_safe_globals([MyTensor])</span>
<span class="go">...     torch.load(f.name, weights_only=True)</span>
<span class="go"># MyTensor([[-0.5024, -1.8152, -0.5455],</span>
<span class="go">#          [-0.8234,  2.0500, -0.3657]])</span>
</pre></div>
</div>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="torch.serialization.clear_safe_globals">
<span class="sig-prename descclassname"><span class="pre">torch.serialization.</span></span><span class="sig-name descname"><span class="pre">clear_safe_globals</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torch/serialization.html#clear_safe_globals"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/serialization.py#L264"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.serialization.clear_safe_globals" title="Link to this definition">#</a></dt>
<dd><p>Clears the list of globals that are safe for <code class="docutils literal notranslate"><span class="pre">weights_only</span></code> load.</p>
<dl class="field-list simple">
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="torch.serialization.get_safe_globals">
<span class="sig-prename descclassname"><span class="pre">torch.serialization.</span></span><span class="sig-name descname"><span class="pre">get_safe_globals</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torch/serialization.html#get_safe_globals"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/serialization.py#L271"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.serialization.get_safe_globals" title="Link to this definition">#</a></dt>
<dd><p>Returns the list of user-added globals that are safe for <code class="docutils literal notranslate"><span class="pre">weights_only</span></code> load.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.List" title="(in Python v3.13)"><em>List</em></a>[<a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Callable" title="(in Python v3.13)"><em>Callable</em></a> | <a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Tuple" title="(in Python v3.13)"><em>Tuple</em></a>[<a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Callable" title="(in Python v3.13)"><em>Callable</em></a>, <a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)">str</a>]]</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="torch.serialization.get_unsafe_globals_in_checkpoint">
<span class="sig-prename descclassname"><span class="pre">torch.serialization.</span></span><span class="sig-name descname"><span class="pre">get_unsafe_globals_in_checkpoint</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">f</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torch/serialization.html#get_unsafe_globals_in_checkpoint"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/serialization.py#L339"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.serialization.get_unsafe_globals_in_checkpoint" title="Link to this definition">#</a></dt>
<dd><p>Returns a list of strings of functions/classes in a <code class="docutils literal notranslate"><span class="pre">torch.save</span></code> object that are not safe for <code class="docutils literal notranslate"><span class="pre">weights_only</span></code>.</p>
<p>For a given function or class <code class="docutils literal notranslate"><span class="pre">f</span></code>, the corresponding string will be of the form
<code class="docutils literal notranslate"><span class="pre">{f.__module__}.{f.__name__}</span></code>.</p>
<p>This function will return any GLOBALs in the checkpoint that are not in the set marked safe
for <code class="docutils literal notranslate"><span class="pre">weights_only</span></code> (either via <a class="reference internal" href="#torch.serialization.add_safe_globals" title="torch.serialization.add_safe_globals"><code class="xref py py-func docutils literal notranslate"><span class="pre">add_safe_globals()</span></code></a> or <a class="reference internal" href="#torch.serialization.safe_globals" title="torch.serialization.safe_globals"><code class="xref py py-class docutils literal notranslate"><span class="pre">safe_globals</span></code></a> context or
allowlisted by <code class="docutils literal notranslate"><span class="pre">torch</span></code> by default).</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This function will statically disassemble the pickle file in the checkpoint.
The implication is any classes dynamically pushed onto the stack during unpickling
will not be included in the output.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>f</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><em>str</em></a><em> | </em><a class="reference external" href="https://docs.python.org/3/library/os.html#os.PathLike" title="(in Python v3.13)"><em>PathLike</em></a><em> | </em><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.BinaryIO" title="(in Python v3.13)"><em>BinaryIO</em></a><em> | </em><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.IO" title="(in Python v3.13)"><em>IO</em></a><em>[</em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#bytes" title="(in Python v3.13)"><em>bytes</em></a><em>]</em>) – File-like object or string containing the checkpoint object saved via <code class="docutils literal notranslate"><span class="pre">torch.save</span></code></p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A list of strings of pickle GLOBALs in the checkpoint that are not allowlisted for <code class="docutils literal notranslate"><span class="pre">weights_only</span></code>.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.List" title="(in Python v3.13)"><em>List</em></a>[<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)">str</a>]</p>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="torch.serialization.safe_globals">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torch.serialization.</span></span><span class="sig-name descname"><span class="pre">safe_globals</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">safe_globals</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torch/serialization.html#safe_globals"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/serialization.py#L314"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.serialization.safe_globals" title="Link to this definition">#</a></dt>
<dd><p>Context-manager that adds certain globals as safe for <code class="docutils literal notranslate"><span class="pre">weights_only</span></code> load.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>safe_globals</strong> (<a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.List" title="(in Python v3.13)"><em>List</em></a><em>[</em><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Callable" title="(in Python v3.13)"><em>Callable</em></a><em> | </em><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Tuple" title="(in Python v3.13)"><em>Tuple</em></a><em>[</em><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Callable" title="(in Python v3.13)"><em>Callable</em></a><em>, </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><em>str</em></a><em>]</em><em>]</em>) – List of globals for weights_only load.</p>
</dd>
</dl>
<p class="rubric">Example</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">tempfile</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">class</span><span class="w"> </span><span class="nc">MyTensor</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
<span class="gp">... </span>    <span class="k">pass</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">t</span> <span class="o">=</span> <span class="n">MyTensor</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">with</span> <span class="n">tempfile</span><span class="o">.</span><span class="n">NamedTemporaryFile</span><span class="p">()</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
<span class="gp">... </span>    <span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">f</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
<span class="go"># Running `torch.load(f.name, weights_only=True)` will fail with</span>
<span class="go"># Unsupported global: GLOBAL __main__.MyTensor was not an allowed global by default.</span>
<span class="go"># Check the code and make sure MyTensor is safe to be used when loaded from an arbitrary checkpoint.</span>
<span class="go">...     with torch.serialization.safe_globals([MyTensor]):</span>
<span class="go">...         torch.load(f.name, weights_only=True)</span>
<span class="go"># MyTensor([[-0.5024, -1.8152, -0.5455],</span>
<span class="go">#          [-0.8234,  2.0500, -0.3657]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">assert</span> <span class="n">torch</span><span class="o">.</span><span class="n">serialization</span><span class="o">.</span><span class="n">get_safe_globals</span><span class="p">()</span> <span class="o">==</span> <span class="p">[]</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="torch.serialization.skip_data">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torch.serialization.</span></span><span class="sig-name descname"><span class="pre">skip_data</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">materialize_fake_tensors</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torch/serialization.html#skip_data"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/serialization.py#L381"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.serialization.skip_data" title="Link to this definition">#</a></dt>
<dd><p>Context-manager that skips writing storage bytes for <code class="docutils literal notranslate"><span class="pre">torch.save</span></code> calls.</p>
<p>Storages will still be saved, but the space that their bytes would usually be written to
will be empty space. The storage bytes can then be populated in a separate pass.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>The <code class="docutils literal notranslate"><span class="pre">skip_data</span></code> context manager is an early prototype and is subject to change.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>materialize_fake_tensors</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><em>bool</em></a>) – Whether to materialize FakeTensors.</p>
</dd>
</dl>
<p class="rubric">Example</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">tempfile</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">t</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">with</span> <span class="n">tempfile</span><span class="o">.</span><span class="n">NamedTemporaryFile</span><span class="p">()</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
<span class="gp">... </span>    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">serialization</span><span class="o">.</span><span class="n">skip_data</span><span class="p">():</span>
<span class="gp">... </span>        <span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">f</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
<span class="gp">... </span>    <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">f</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">weights_only</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="go">tensor([[0., 0., 0.],</span>
<span class="go">        [0., 0., 0.]])</span>
</pre></div>
</div>
</dd></dl>

</section>
<section id="module-torch.utils.serialization">
<span id="config"></span><span id="serialization-config"></span><h2><a class="toc-backref" href="#id16" role="doc-backlink">Config</a><a class="headerlink" href="#module-torch.utils.serialization" title="Link to this heading">#</a></h2>
<p id="module-torch.utils.serialization.config"><code class="docutils literal notranslate"><span class="pre">torch.utils.serialization.config</span></code> provides a global config that can control the behavior of
<code class="docutils literal notranslate"><span class="pre">torch.save</span></code> and <code class="docutils literal notranslate"><span class="pre">torch.load</span></code>.</p>
<p><code class="docutils literal notranslate"><span class="pre">torch.utils.serialization.config.save</span></code> contains options that control the behavior of <code class="docutils literal notranslate"><span class="pre">torch.save</span></code>.</p>
<blockquote>
<div><ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">compute_crc32</span></code>: whether to compute and write the zip file checksum (Default : <code class="docutils literal notranslate"><span class="pre">True</span></code>).
See <a class="reference internal" href="#torch.serialization.set_crc32_options" title="torch.serialization.set_crc32_options"><code class="xref py py-func docutils literal notranslate"><span class="pre">set_crc32_options()</span></code></a>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">use_pinned_memory_for_d2h</span></code>: for storages that are on an accelerator when passed to <code class="docutils literal notranslate"><span class="pre">torch.save</span></code>, whether to
move storage to pinned memory or pageable memory on CPU within <code class="docutils literal notranslate"><span class="pre">torch.save</span></code>. (Default: <code class="docutils literal notranslate"><span class="pre">False</span></code> (i.e. pageable))</p></li>
</ul>
</div></blockquote>
<p><code class="docutils literal notranslate"><span class="pre">torch.utils.serialization.config.load</span></code> contains options that control the behavior of <code class="docutils literal notranslate"><span class="pre">torch.load</span></code>.</p>
<blockquote>
<div><ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">mmap</span></code>: See the documentation for <code class="docutils literal notranslate"><span class="pre">mmap</span></code> argument in <a class="reference internal" href="../python-api/generated/torch.load.html#torch.load" title="torch.load"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.load()</span></code></a>.
This config will set the behavior of <code class="docutils literal notranslate"><span class="pre">mmap</span></code> for <code class="docutils literal notranslate"><span class="pre">torch.load</span></code> if it is not
already explicitly passed to the <code class="docutils literal notranslate"><span class="pre">torch.load</span></code> call (Default : <code class="docutils literal notranslate"><span class="pre">False</span></code>).</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">endianness</span></code>: See <a class="reference internal" href="#torch.serialization.set_default_load_endianness" title="torch.serialization.set_default_load_endianness"><code class="xref py py-func docutils literal notranslate"><span class="pre">set_default_load_endianness()</span></code></a>.
(Default : <code class="docutils literal notranslate"><span class="pre">torch.serialization.LoadEndianness.NATIVE</span></code>)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">mmap_flags</span></code>: See <a class="reference internal" href="#torch.serialization.set_default_mmap_options" title="torch.serialization.set_default_mmap_options"><code class="xref py py-class docutils literal notranslate"><span class="pre">set_default_mmap_options</span></code></a>.
(Default : <code class="docutils literal notranslate"><span class="pre">MAP_PRIVATE</span></code>)</p></li>
</ul>
</div></blockquote>
</section>
</section>


                </article>
              
              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="randomness.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Reproducibility</p>
      </div>
    </a>
    <a class="right-next"
       href="faq.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Frequently Asked Questions</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <dialog id="pst-secondary-sidebar-modal"></dialog>
                <div id="pst-secondary-sidebar" class="bd-sidebar-secondary bd-toc">
<div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
<div
    id="pst-page-navigation-heading-2"
    class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> On this page
  </div>
  <nav class="bd-toc-nav page-toc" aria-labelledby="pst-page-navigation-heading-2">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#saving-and-loading-tensors">Saving and loading tensors</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#saving-and-loading-tensors-preserves-views">Saving and loading tensors preserves views</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#saving-and-loading-torch-nn-modules">Saving and loading torch.nn.Modules</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#serialized-file-format-for-torch-save">Serialized file format for <code class="docutils literal notranslate"><span class="pre">torch.save</span></code></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#torch-load-with-weights-only-true"><code class="docutils literal notranslate"><span class="pre">torch.load</span></code> with <code class="docutils literal notranslate"><span class="pre">weights_only=True</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#troubleshooting-weights-only">Troubleshooting <code class="docutils literal notranslate"><span class="pre">weights_only</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#getting-unsafe-globals">Getting unsafe globals</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#environment-variables">Environment Variables</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#serializing-torch-nn-modules-and-loading-them-in-c">Serializing torch.nn.Modules and loading them in C++</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#saving-and-loading-scriptmodules-across-pytorch-versions">Saving and loading ScriptModules across PyTorch versions</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch-div-performing-integer-division">torch.div performing integer division</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch-full-always-inferring-a-float-dtype">torch.full always inferring a float dtype</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#utility-functions">Utility functions</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.serialization.register_package"><code class="docutils literal notranslate"><span class="pre">register_package()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.serialization.get_crc32_options"><code class="docutils literal notranslate"><span class="pre">get_crc32_options()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.serialization.set_crc32_options"><code class="docutils literal notranslate"><span class="pre">set_crc32_options()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.serialization.get_default_load_endianness"><code class="docutils literal notranslate"><span class="pre">get_default_load_endianness()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.serialization.set_default_load_endianness"><code class="docutils literal notranslate"><span class="pre">set_default_load_endianness()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.serialization.get_default_mmap_options"><code class="docutils literal notranslate"><span class="pre">get_default_mmap_options()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.serialization.set_default_mmap_options"><code class="docutils literal notranslate"><span class="pre">set_default_mmap_options()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.serialization.add_safe_globals"><code class="docutils literal notranslate"><span class="pre">add_safe_globals()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.serialization.clear_safe_globals"><code class="docutils literal notranslate"><span class="pre">clear_safe_globals()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.serialization.get_safe_globals"><code class="docutils literal notranslate"><span class="pre">get_safe_globals()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.serialization.get_unsafe_globals_in_checkpoint"><code class="docutils literal notranslate"><span class="pre">get_unsafe_globals_in_checkpoint()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.serialization.safe_globals"><code class="docutils literal notranslate"><span class="pre">safe_globals</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.serialization.skip_data"><code class="docutils literal notranslate"><span class="pre">skip_data</span></code></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#module-torch.utils.serialization">Config</a></li>
</ul>
  </nav></div>

  <div class="sidebar-secondary-item">

  
  <div class="tocsection editthispage">
    <a href="https://github.com/pytorch/pytorch/edit/main/docs/source/notes/serialization.rst">
      <i class="fa-solid fa-pencil"></i>
      
      
        
          Edit on GitHub
        
      
    </a>
  </div>
</div>

  <div class="sidebar-secondary-item">
    <div class="tocsection sourcelink">
      <a href="../_sources/notes/serialization.rst.txt">
        <i class="fa-solid fa-file-lines"></i> Show Source
      </a>
    </div>
</div>

</div>
<div class="sidebar-secondary-item">
 <h6>PyTorch Libraries</h6>
 <ul>
 
  <li><a class="nav-link nav-external" href="https://pytorch.org/audio/stable/">torchaudio</a></li>
 
  <li><a class="nav-link nav-external" href="https://pytorch.org/ao">torchao</a></li>
 
  <li><a class="nav-link nav-external" href="https://pytorch.org/executorch">ExecuTorch</a></li>
 
  <li><a class="nav-link nav-external" href="https://pytorch.org/torchrec">torchrec</a></li>
 
  <li><a class="nav-link nav-external" href="https://pytorch.org/serve/">torchserve</a></li>
 
  <li><a class="nav-link nav-external" href="https://pytorch.org/data">torchdata</a></li>
 
  <li><a class="nav-link nav-external" href="https://pytorch.org/data">torchvision</a></li>
 
  <li><a class="nav-link nav-external" href="https://pytorch.org/xla">PyTorch on XLA Devices</a></li>
 
 </ul>
</div>
</div>
</div>
              
            
          </div>
          <footer class="bd-footer-content">
            
          </footer>
        
      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script defer src="../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf"></script>
<script defer src="../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf"></script>


  <footer class="bd-footer">
<div class="bd-footer__inner bd-page-width">
  
    <div class="footer-items__start">
      
        <div class="footer-item">

  <p class="copyright">
    
      © Copyright PyTorch Contributors.
      <br/>
    
  </p>
</div>
      
        <div class="footer-item">

  <p class="sphinx-version">
    Created using <a href="https://www.sphinx-doc.org/">Sphinx</a> 8.1.0.
    <br/>
  </p>
</div>
      
    </div>
  
  
  
    <div class="footer-items__end">
      
        <div class="footer-item">
<p class="theme-version">
  <!-- # L10n: Setting the PST URL as an argument as this does not need to be localized -->
  Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.16.1.
</p></div>
      
    </div>
  
</div>

  </footer>
  </body>
</html>