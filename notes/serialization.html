

<!DOCTYPE html>


<html lang="en" data-content_root="" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.19: https://docutils.sourceforge.io/" />
<meta property="og:title" content="Serialization semantics" />
<meta property="og:type" content="website" />
<meta property="og:url" content="http://pytorch.org/notes/serialization.html" />
<meta property="og:site_name" content="PyTorch" />
<meta property="og:description" content="This note describes how you can save and load PyTorch tensors and module states in Python, and how to serialize Python modules so they can be loaded in C++. Table of Contents: Serialization semanti..." />
<meta property="og:image" content="https://pytorch.org/docs/stable/_static/img/pytorch-logo-dark.svg" />
<meta property="og:image:alt" content="PyTorch" />
<meta name="description" content="This note describes how you can save and load PyTorch tensors and module states in Python, and how to serialize Python modules so they can be loaded in C++. Table of Contents: Serialization semanti..." />

    <title>Serialization semantics &#8212; PyTorch main documentation</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/theme.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/katex.min.css" />
    <link rel="stylesheet" type="text/css" href="../_static/katex-math.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.1e8bd061cd6da7fc9cf755528e8ffc24.min.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/jit.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/custom.css" />
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/sphinx_highlight.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/design-tabs.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'notes/serialization';</script>
    <script>
        DOCUMENTATION_OPTIONS.theme_version = '0.15.4';
        DOCUMENTATION_OPTIONS.theme_switcher_json_url = 'https://pytorch.org/docs/pytorch-versions.json';
        DOCUMENTATION_OPTIONS.theme_switcher_version_match = 'main';
        DOCUMENTATION_OPTIONS.show_version_warning_banner = true;
        </script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Windows FAQ" href="windows.html" />
    <link rel="prev" title="Reproducibility" href="randomness.html" />

  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
<script src="https://code.jquery.com/jquery-3.7.1.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/2.3.1/list.min.js"></script>
<link rel="stylesheet" type="text/css" href="../_static/css/theme.css">
<script type="text/javascript" src="../_static/js/theme.js"></script>
<link rel="stylesheet" href="../_static/webfonts/all.min.css">

<!-- Google Tag Manager (noscript) -->
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-T8XT4PS"
   height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
   <!-- End Google Tag Manager (noscript) -->
   <!-- Google Tag Manager -->
<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
   new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
   j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
   'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
   j.onload = function() {
     window.dispatchEvent(new Event('gtm_loaded'));
     console.log('GTM loaded successfully');
   };
   })(window,document,'script','dataLayer','GTM-T8XT4PS');
</script>
 <!-- End Google Tag Manager -->
 <!-- Facebook Pixel Code -->
<script>
   !function(f,b,e,v,n,t,s)
   {if(f.fbq)return;n=f.fbq=function(){n.callMethod?
   n.callMethod.apply(n,arguments):n.queue.push(arguments)};
   if(!f._fbq)f._fbq=n;n.push=n;n.loaded=!0;n.version='2.0';
   n.queue=[];t=b.createElement(e);t.async=!0;
   t.src=v;s=b.getElementsByTagName(e)[0];
   s.parentNode.insertBefore(t,s)}(window,document,'script',
   'https://connect.facebook.net/en_US/fbevents.js');
   fbq('init', '243028289693773');
   fbq('track', 'PageView');
</script>
<noscript>
   <img height="1" width="1" src="https://www.facebook.com/tr?id=243028289693773&ev=PageView&noscript=1"/>
</noscript>
<!-- End Facebook Pixel Code -->

<script async src="https://cse.google.com/cse.js?cx=e65585f8c3ea1440e"></script>

   <!--
   Search engines should not index the main version of documentation.
   Stable documentation are built without release == 'main'.
   -->
   <meta name="robots" content="noindex">


  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>

  </head>
<body data-feedback-url="https://github.com/pytorch/pytorch" class="pytorch-body">
     <div class="container-fluid header-holder tutorials-header" id="header-holder">
   <div class="header-container-wrapper">
    <div class="header-container">
      <a class="header-logo" href="" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>

          <li class="main-menu-item">
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Learn
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="">
                  <span class=dropdown-title>Get Started</span>
                  <p>Run PyTorch locally or get started quickly with one of the supported cloud platforms</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Tutorials</span>
                  <p>Whats new in PyTorch tutorials</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Learn the Basics</span>
                  <p>Familiarize yourself with PyTorch concepts and modules</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">PyTorch Recipes</span>
                  <p>Bite-size, ready-to-deploy PyTorch code examples</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Intro to PyTorch - YouTube Series</span>
                  <p>Master PyTorch basics with our engaging YouTube tutorial series</p>
                </a>
              </div>
            </div>
          </li>

          <li>
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Ecosystem
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Tools</span>
                  <p>Learn about the tools and frameworks in the PyTorch Ecosystem</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class=dropdown-title>Community</span>
                  <p>Join the PyTorch developer community to contribute, learn, and get your questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="" target="_blank">
                  <span class=dropdown-title>Forums</span>
                  <p>A place to discuss PyTorch code, issues, install, research</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class=dropdown-title>Developer Resources</span>
                  <p>Find resources and get questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Contributor Awards - 2024</span>
                  <p>Award winners announced at this year's PyTorch Conference</p>
                </a>
              </div>
            </div>
          </li>

          <li>
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Edge
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">About PyTorch Edge</span>
                  <p>Build innovative and privacy-aware AI experiences for edge devices</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">ExecuTorch</span>
                  <p>End-to-end solution for enabling on-device inference capabilities across mobile and edge devices</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">ExecuTorch Docs</span>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Docs
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">PyTorch</span>
                  <p>Explore the documentation for comprehensive guidance on how to use PyTorch</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">PyTorch Domains</span>
                  <p>Read the PyTorch Domains documentation to learn more about domain-specific libraries</p>
                </a>
              </div>
            </div>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Blogs & News
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">PyTorch Blog</span>
                  <p>Catch up on the latest technical news and happenings</p>
                </a>
                 <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Community Blog</span>
                  <p>Stories from the PyTorch ecosystem</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Videos</span>
                  <p>Learn about the latest PyTorch tutorials, new, and more </p>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Community Stories</span>
                  <p>Learn how our community solves real, everyday machine learning problems with PyTorch</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Events</span>
                  <p>Find events, webinars, and podcasts</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Newsletter</span>
                  <p>Stay up-to-date with the latest updates</p>
                </a>
            </div>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                About
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">PyTorch Foundation</span>
                  <p>Learn more about the PyTorch Foundation</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Governing Board</span>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Cloud Credit Program</span>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Technical Advisory Council</span>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Staff</span>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Contact Us</span>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
            <div class="no-dropdown">
              <a href="" data-cta="join">
                Become a Member
              </a>
            </div>
          </li>
          <li>
           <div class="main-menu-item">
             <a href="" class="github-icon">
             </a>
           </div>
          </li>
          <!--- TODO: This block adds the search icon to the nav bar. We will enable it later.
          <li>
            <div class="main-menu-item">
             <a href="" class="search-icon">
             </a>
            </div>
          </li>
          --->
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu">
        <i class="fa-solid fa-ellipsis"></i>
      </a>
    </div>
  </div>
 </div>

 <!-- Begin Mobile Menu -->

<div class="mobile-main-menu">
  <div class="container-fluid">
    <div class="header-container-wrapper">
      <div class="mobile-main-menu-header-container">
        <a class="header-logo" href="" aria-label="PyTorch"></a>
        <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu">
          <i class="fa-solid fa-xmark"></i>
        </a>
      </div>
    </div>
  </div>

  <div class="mobile-main-menu-links-container">
    <div class="main-menu">
      <ul>
         <li class="resources-mobile-menu-title">
           <a>Learn</a>
         </li>
         <ul class="resources-mobile-menu-items">
           <li>
             <a href="">Get Started</a>
           </li>
           <li>
             <a href="">Tutorials</a>
           </li>
           <li>
             <a href="">Learn the Basics</a>
           </li>
           <li>
             <a href="">PyTorch Recipes</a>
           </li>
           <li>
             <a href="">Introduction to PyTorch - YouTube Series</a>
           </li>
         </ul>
         <li class="resources-mobile-menu-title">
           <a>Ecosystem</a>
         </li>
         <ul class="resources-mobile-menu-items">
           <li>
             <a href="">Tools</a>
           </li>
           <li>
             <a href="">Community</a>
           </li>
           <li>
             <a href="">Forums</a>
           </li>
           <li>
             <a href="">Developer Resources</a>
           </li>
           <li>
             <a href="">Contributor Awards - 2024</a>
           </li>
         </ul>

         <li class="resources-mobile-menu-title">
           <a>Edge</a>
         </li>

         <ul class="resources-mobile-menu-items">
           <li>
             <a href="">About PyTorch Edge</a>
           </li>

           <li>
             <a href="">ExecuTorch</a>
           </li>
           <li>
             <a href="">ExecuTorch Documentation</a>
           </li>
         </ul>

         <li class="resources-mobile-menu-title">
           <a>Docs</a>
         </li>

         <ul class="resources-mobile-menu-items">
          <li>
            <a href="">PyTorch</a>
          </li>

          <li>
            <a href="">PyTorch Domains</a>
          </li>
        </ul>

        <li class="resources-mobile-menu-title">
          <a>Blog & News</a>
        </li>

         <ul class="resources-mobile-menu-items">
          <li>
            <a href="">PyTorch Blog</a>
          </li>
          <li>
            <a href="">Community Blog</a>
          </li>

          <li>
            <a href="">Videos</a>
          </li>

          <li>
            <a href="">Community Stories</a>
          </li>
          <li>
            <a href="">Events</a>
          </li>
          <li>
             <a href="">Newsletter</a>
           </li>
        </ul>

        <li class="resources-mobile-menu-title">
          <a>About</a>
        </li>

        <ul class="resources-mobile-menu-items">
          <li>
            <a href="">PyTorch Foundation</a>
          </li>
          <li>
            <a href="">Governing Board</a>
          </li>
          <li>
             <a href="">Cloud Credit Program</a>
          </li>
          <li>
             <a href="">Technical Advisory Council</a>
          </li>
          <li>
             <a href="">Staff</a>
          </li>
          <li>
             <a href="">Contact Us</a>
          </li>
        </ul>
      </ul>
    </div>
  </div>
</div>

<!-- End Mobile Menu -->
   
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>

  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
<div class="bd-header__inner bd-page-width">
  <button class="pst-navbar-icon sidebar-toggle primary-toggle" aria-label="Site navigation">
    <span class="fa-solid fa-bars"></span>
  </button>
  
  
  <div class=" navbar-header-items__start">
    
      <div class="navbar-item">
<script>
document.write(`
  <div class="version-switcher__container dropdown">
    <button id="pst-version-switcher-button-2"
      type="button"
      class="version-switcher__button btn btn-sm dropdown-toggle"
      data-bs-toggle="dropdown"
      aria-haspopup="listbox"
      aria-controls="pst-version-switcher-list-2"
      aria-label="Version switcher list"
    >
      Choose version  <!-- this text may get changed later by javascript -->
      <span class="caret"></span>
    </button>
    <div id="pst-version-switcher-list-2"
      class="version-switcher__menu dropdown-menu list-group-flush py-0"
      role="listbox" aria-labelledby="pst-version-switcher-button-2">
      <!-- dropdown will be populated by javascript on page load -->
    </div>
  </div>
`);
</script></div>
    
      <div class="navbar-item">

  
    
  

<a class="navbar-brand logo" href="../index.html">
  
  
  
  
  
  
    <p class="title logo__title">Home</p>
  
</a></div>
    
  </div>
  
  <div class=" navbar-header-items">
    
    <div class="me-auto navbar-header-items__center">
      
        <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item ">
  <a class="nav-link nav-internal" href="../pytorch-api.html">
    Python API
  </a>
</li>


<li class="nav-item current active">
  <a class="nav-link nav-internal" href="../notes.html">
    Developer Notes
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-external" href="https://pytorch.org/tutorials/">
    Tutorials
  </a>
</li>

  </ul>
</nav></div>
      
    </div>
    
    
    <div class="navbar-header-items__end">
      
      
        <div class="navbar-item">
<div class="search-container-wrapper">
  <div id="sphinx-search" class="search-container">
    
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </div>

  <div id="google-search" class="search-container" style="display:none;">
    <div class="gcse-search-wrapper">
      <i class="fa-solid fa-magnifying-glass" aria-hidden="true"></i>
      <div class="gcse-search"></div>
    </div>
  </div>

  <div class="search-toggle-container" data-bs-title="Google Search Off" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <div class="search-toggle-inner">
      <label class="switch">
        <input type="checkbox" id="search-toggle">
        <span class="slider round"></span>
      </label>
    </div>
  </div>
</div>

<script>
  document.addEventListener('DOMContentLoaded', function() {
    // Define the search callback
    const myWebSearchStartingCallback = (gname, query) => {
      if (typeof dataLayer !== 'undefined' && query) {
        dataLayer.push({
          'event': 'google_search',
          'search_term': query,
          'event_category': 'Search',
          'event_label': 'Google Search'
        });
        console.log('GA event sent via callback: google_search - ' + query);
      }
      return '';
    };

    // Set up the GCSE search callbacks
    window.__gcse || (window.__gcse = {});
    window.__gcse.searchCallbacks = {
      web: {
        starting: myWebSearchStartingCallback,
      },
    };
    if (window.location.pathname.includes('/search.html')) {
      document.body.classList.add('search-page');
    }

    // Function to reinitialize Google CSE
    function reinitializeGoogleSearch() {
      if (window.__gcse) {
        // Force Google CSE to reinitialize
        if (window.__gcse.initializationCallback) {
          window.__gcse.initializationCallback();
        }
      }
    }

    // Function to handle search toggle
    function handleSearchToggle(toggle, sphinxSearch, googleSearch) {
      if (!toggle || !sphinxSearch || !googleSearch) return;

      // Check if the URL contains /stable/ or /tutorials/
      const currentUrl = window.location.href;
      const shouldDefaultToGoogle = currentUrl.includes('/stable/') || currentUrl.includes('/tutorials/');

      // Check if there's a saved preference, otherwise use the URL-based default
      const savedPreference = localStorage.getItem('searchPreference');
      if (savedPreference === 'google' || (savedPreference === null && shouldDefaultToGoogle)) {
        toggle.checked = true;
        sphinxSearch.style.display = 'none';
        googleSearch.style.display = 'block';
        // Save the preference if it wasn't already saved
        if (savedPreference === null) {
          localStorage.setItem('searchPreference', 'google');
        }
        // Ensure Google search is properly initialized
        reinitializeGoogleSearch();
      } else {
        toggle.checked = false;
        sphinxSearch.style.display = 'block';
        googleSearch.style.display = 'none';
      }

      // Update tooltip based on initial state
      const tooltipElement = document.querySelector('.search-toggle-container');
      if (tooltipElement) {
        tooltipElement.setAttribute('data-bs-title', toggle.checked ? 'Google Search On' : 'Google Search Off');
        // Reinitialize tooltip if Bootstrap's tooltip is already initialized
        if (bootstrap && bootstrap.Tooltip) {
          const tooltipInstance = bootstrap.Tooltip.getInstance(tooltipElement);
          if (tooltipInstance) tooltipInstance.dispose();
          new bootstrap.Tooltip(tooltipElement);
        }
      }

      // Add a data attribute to track if this toggle has been initialized
      if (toggle.hasAttribute('data-initialized')) {
        return; // Skip adding another event listener if already initialized
      }
      toggle.setAttribute('data-initialized', 'true');

      toggle.addEventListener('change', function() {
        if (this.checked) {
          sphinxSearch.style.display = 'none';
          googleSearch.style.display = 'block';
          localStorage.setItem('searchPreference', 'google');
          // Reinitialize Google search when switching to it
          reinitializeGoogleSearch();
          const tooltipElement = document.querySelector('.search-toggle-container');
          if (tooltipElement) {
            tooltipElement.setAttribute('data-bs-title', 'Google Search On');
            // Refresh tooltip
            if (bootstrap && bootstrap.Tooltip) {
              const tooltipInstance = bootstrap.Tooltip.getInstance(tooltipElement);
              if (tooltipInstance) tooltipInstance.dispose();
              new bootstrap.Tooltip(tooltipElement);
            }
          }
          if (typeof dataLayer !== 'undefined') {
            dataLayer.push({
              'event': 'search_engine_switch',
              'event_category': 'Search',
              'event_label': 'Google'
            });
            console.log('GA event sent via dataLayer: search_engine_switch - Google');
          } else {
            console.log('GA not available: Cannot track Google search switch');
          }
        } else {
          sphinxSearch.style.display = 'block';
          googleSearch.style.display = 'none';
          localStorage.setItem('searchPreference', 'sphinx');
          const tooltipElement = document.querySelector('.search-toggle-container');
          if (tooltipElement) {
            tooltipElement.setAttribute('data-bs-title', 'Google Search Off');
            // Refresh tooltip
            if (bootstrap && bootstrap.Tooltip) {
              const tooltipInstance = bootstrap.Tooltip.getInstance(tooltipElement);
              if (tooltipInstance) tooltipInstance.dispose();
              new bootstrap.Tooltip(tooltipElement);
            }
          }
          if (typeof dataLayer !== 'undefined') {
            dataLayer.push({
              'event': 'search_engine_switch',
              'event_category': 'Search',
              'event_label': 'Sphinx'
            });
            console.log('GA event sent via dataLayer: search_engine_switch - Sphinx');
          } else {
            console.log('GA not available: Cannot track Sphinx search switch');
          }
        }

        // Also update mobile search if it exists
        updateMobileSearch(false); // Pass false to prevent triggering another event
      });
    }


    // Function to update mobile search based on current toggle state
    function updateMobileSearch() {
      const toggle = document.getElementById('search-toggle');
      if (!toggle) return;

      // Find mobile search container
      const mobileSearchContainer = document.querySelector('.sidebar-header-items__end .navbar-item .search-container-wrapper');
      if (mobileSearchContainer) {
        const mobileSphinxSearch = mobileSearchContainer.querySelector('#sphinx-search');
        const mobileGoogleSearch = mobileSearchContainer.querySelector('#google-search');

        if (mobileSphinxSearch && mobileGoogleSearch) {
          if (toggle.checked) {
            mobileSphinxSearch.style.display = 'none';
            mobileGoogleSearch.style.display = 'block';
            // Reinitialize Google search in mobile view
            reinitializeGoogleSearch();
          } else {
            mobileSphinxSearch.style.display = 'block';
            mobileGoogleSearch.style.display = 'none';
          }
        }
      }
    }

    // Initialize desktop search toggle
    const toggle = document.getElementById('search-toggle');
    const sphinxSearch = document.getElementById('sphinx-search');
    const googleSearch = document.getElementById('google-search');
    handleSearchToggle(toggle, sphinxSearch, googleSearch);

    // Set placeholder text for Google search input
    const observer = new MutationObserver(function(mutations, obs) {
      const searchInputs = document.querySelectorAll('.gsc-input input');
      searchInputs.forEach(input => {
      if (input) {
        input.setAttribute('placeholder', 'Search the docs ...');

        if (!input.hasAttribute('data-tracking-added')) {
          input.setAttribute('data-tracking-added', 'true');
        }
      }
      });
    });

    observer.observe(document.body, { childList: true, subtree: true });

    // Watch for mobile menu creation
    const mobileMenuObserver = new MutationObserver(function(mutations) {
      for (const mutation of mutations) {
        const mobileSearchInputs = document.querySelectorAll('.sidebar-header-items__end .navbar-item .search-container-wrapper .gsc-input input');
      mobileSearchInputs.forEach(input => {
        if (input) {
          input.setAttribute('placeholder', 'Search the docs ...');
          input.style.paddingLeft = '36px';
          }
        });
        if (mutation.addedNodes.length) {
          // Check if the mobile search container was added
          const mobileSearchContainer = document.querySelector('.sidebar-header-items__end .navbar-item .search-container-wrapper');
          if (mobileSearchContainer) {
            // Clone the toggle for mobile if needed
            const mobileToggle = mobileSearchContainer.querySelector('#search-toggle');
            if (mobileToggle) {
              // Sync mobile toggle with desktop toggle
              mobileToggle.checked = toggle.checked;

              // Update mobile search display
              updateMobileSearch();

              // Add event listener to mobile toggle
              mobileToggle.addEventListener('change', function() {
                // Sync desktop toggle with mobile toggle
                toggle.checked = this.checked;
                // Trigger change event on desktop toggle to update both
                toggle.dispatchEvent(new Event('change'));
              });
            }
          }
        }
      }
    });

    mobileMenuObserver.observe(document.body, { childList: true, subtree: true });

    // Ensure Google CSE is properly loaded
    if (window.__gcse) {
      window.__gcse.callback = function() {
        // This will run after Google CSE is fully loaded
        if (toggle && toggle.checked) {
          // If Google search is active, make sure it's properly initialized
          reinitializeGoogleSearch();
        }
      };
    } else {
      // If Google CSE hasn't loaded yet, set up a callback
      window.__gcse = {
        callback: function() {
          if (toggle && toggle.checked) {
            reinitializeGoogleSearch();
          }
        }
      };
    }
  });
</script>
</div>
      
        <div class="navbar-item">

<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script></div>
      
        <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://x.com/PyTorch" title="X" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-x-twitter fa-lg" aria-hidden="true"></i>
            <span class="sr-only">X</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/pytorch/pytorch" title="GitHub" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">GitHub</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://discuss.pytorch.org/" title="PyTorch Forum" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-discourse fa-lg" aria-hidden="true"></i>
            <span class="sr-only">PyTorch Forum</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://pypi.org/project/torch/" title="PyPi" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-python fa-lg" aria-hidden="true"></i>
            <span class="sr-only">PyPi</span></a>
        </li>
</ul></div>
      
    </div>
    
  </div>
  
  

  
    <button class="pst-navbar-icon sidebar-toggle secondary-toggle" aria-label="On this page">
      <span class="fa-solid fa-outdent"></span>
    </button>
  
</div>

    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
      <div class="sidebar-header-items__center">
        
          
          
            <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item ">
  <a class="nav-link nav-internal" href="../pytorch-api.html">
    Python API
  </a>
</li>


<li class="nav-item current active">
  <a class="nav-link nav-internal" href="../notes.html">
    Developer Notes
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-external" href="https://pytorch.org/tutorials/">
    Tutorials
  </a>
</li>

  </ul>
</nav></div>
          
        
      </div>
    
    
    
      <div class="sidebar-header-items__end">
        
          <div class="navbar-item">
<div class="search-container-wrapper">
  <div id="sphinx-search" class="search-container">
    
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </div>

  <div id="google-search" class="search-container" style="display:none;">
    <div class="gcse-search-wrapper">
      <i class="fa-solid fa-magnifying-glass" aria-hidden="true"></i>
      <div class="gcse-search"></div>
    </div>
  </div>

  <div class="search-toggle-container" data-bs-title="Google Search Off" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <div class="search-toggle-inner">
      <label class="switch">
        <input type="checkbox" id="search-toggle">
        <span class="slider round"></span>
      </label>
    </div>
  </div>
</div>

<script>
  document.addEventListener('DOMContentLoaded', function() {
    // Define the search callback
    const myWebSearchStartingCallback = (gname, query) => {
      if (typeof dataLayer !== 'undefined' && query) {
        dataLayer.push({
          'event': 'google_search',
          'search_term': query,
          'event_category': 'Search',
          'event_label': 'Google Search'
        });
        console.log('GA event sent via callback: google_search - ' + query);
      }
      return '';
    };

    // Set up the GCSE search callbacks
    window.__gcse || (window.__gcse = {});
    window.__gcse.searchCallbacks = {
      web: {
        starting: myWebSearchStartingCallback,
      },
    };
    if (window.location.pathname.includes('/search.html')) {
      document.body.classList.add('search-page');
    }

    // Function to reinitialize Google CSE
    function reinitializeGoogleSearch() {
      if (window.__gcse) {
        // Force Google CSE to reinitialize
        if (window.__gcse.initializationCallback) {
          window.__gcse.initializationCallback();
        }
      }
    }

    // Function to handle search toggle
    function handleSearchToggle(toggle, sphinxSearch, googleSearch) {
      if (!toggle || !sphinxSearch || !googleSearch) return;

      // Check if the URL contains /stable/ or /tutorials/
      const currentUrl = window.location.href;
      const shouldDefaultToGoogle = currentUrl.includes('/stable/') || currentUrl.includes('/tutorials/');

      // Check if there's a saved preference, otherwise use the URL-based default
      const savedPreference = localStorage.getItem('searchPreference');
      if (savedPreference === 'google' || (savedPreference === null && shouldDefaultToGoogle)) {
        toggle.checked = true;
        sphinxSearch.style.display = 'none';
        googleSearch.style.display = 'block';
        // Save the preference if it wasn't already saved
        if (savedPreference === null) {
          localStorage.setItem('searchPreference', 'google');
        }
        // Ensure Google search is properly initialized
        reinitializeGoogleSearch();
      } else {
        toggle.checked = false;
        sphinxSearch.style.display = 'block';
        googleSearch.style.display = 'none';
      }

      // Update tooltip based on initial state
      const tooltipElement = document.querySelector('.search-toggle-container');
      if (tooltipElement) {
        tooltipElement.setAttribute('data-bs-title', toggle.checked ? 'Google Search On' : 'Google Search Off');
        // Reinitialize tooltip if Bootstrap's tooltip is already initialized
        if (bootstrap && bootstrap.Tooltip) {
          const tooltipInstance = bootstrap.Tooltip.getInstance(tooltipElement);
          if (tooltipInstance) tooltipInstance.dispose();
          new bootstrap.Tooltip(tooltipElement);
        }
      }

      // Add a data attribute to track if this toggle has been initialized
      if (toggle.hasAttribute('data-initialized')) {
        return; // Skip adding another event listener if already initialized
      }
      toggle.setAttribute('data-initialized', 'true');

      toggle.addEventListener('change', function() {
        if (this.checked) {
          sphinxSearch.style.display = 'none';
          googleSearch.style.display = 'block';
          localStorage.setItem('searchPreference', 'google');
          // Reinitialize Google search when switching to it
          reinitializeGoogleSearch();
          const tooltipElement = document.querySelector('.search-toggle-container');
          if (tooltipElement) {
            tooltipElement.setAttribute('data-bs-title', 'Google Search On');
            // Refresh tooltip
            if (bootstrap && bootstrap.Tooltip) {
              const tooltipInstance = bootstrap.Tooltip.getInstance(tooltipElement);
              if (tooltipInstance) tooltipInstance.dispose();
              new bootstrap.Tooltip(tooltipElement);
            }
          }
          if (typeof dataLayer !== 'undefined') {
            dataLayer.push({
              'event': 'search_engine_switch',
              'event_category': 'Search',
              'event_label': 'Google'
            });
            console.log('GA event sent via dataLayer: search_engine_switch - Google');
          } else {
            console.log('GA not available: Cannot track Google search switch');
          }
        } else {
          sphinxSearch.style.display = 'block';
          googleSearch.style.display = 'none';
          localStorage.setItem('searchPreference', 'sphinx');
          const tooltipElement = document.querySelector('.search-toggle-container');
          if (tooltipElement) {
            tooltipElement.setAttribute('data-bs-title', 'Google Search Off');
            // Refresh tooltip
            if (bootstrap && bootstrap.Tooltip) {
              const tooltipInstance = bootstrap.Tooltip.getInstance(tooltipElement);
              if (tooltipInstance) tooltipInstance.dispose();
              new bootstrap.Tooltip(tooltipElement);
            }
          }
          if (typeof dataLayer !== 'undefined') {
            dataLayer.push({
              'event': 'search_engine_switch',
              'event_category': 'Search',
              'event_label': 'Sphinx'
            });
            console.log('GA event sent via dataLayer: search_engine_switch - Sphinx');
          } else {
            console.log('GA not available: Cannot track Sphinx search switch');
          }
        }

        // Also update mobile search if it exists
        updateMobileSearch(false); // Pass false to prevent triggering another event
      });
    }


    // Function to update mobile search based on current toggle state
    function updateMobileSearch() {
      const toggle = document.getElementById('search-toggle');
      if (!toggle) return;

      // Find mobile search container
      const mobileSearchContainer = document.querySelector('.sidebar-header-items__end .navbar-item .search-container-wrapper');
      if (mobileSearchContainer) {
        const mobileSphinxSearch = mobileSearchContainer.querySelector('#sphinx-search');
        const mobileGoogleSearch = mobileSearchContainer.querySelector('#google-search');

        if (mobileSphinxSearch && mobileGoogleSearch) {
          if (toggle.checked) {
            mobileSphinxSearch.style.display = 'none';
            mobileGoogleSearch.style.display = 'block';
            // Reinitialize Google search in mobile view
            reinitializeGoogleSearch();
          } else {
            mobileSphinxSearch.style.display = 'block';
            mobileGoogleSearch.style.display = 'none';
          }
        }
      }
    }

    // Initialize desktop search toggle
    const toggle = document.getElementById('search-toggle');
    const sphinxSearch = document.getElementById('sphinx-search');
    const googleSearch = document.getElementById('google-search');
    handleSearchToggle(toggle, sphinxSearch, googleSearch);

    // Set placeholder text for Google search input
    const observer = new MutationObserver(function(mutations, obs) {
      const searchInputs = document.querySelectorAll('.gsc-input input');
      searchInputs.forEach(input => {
      if (input) {
        input.setAttribute('placeholder', 'Search the docs ...');

        if (!input.hasAttribute('data-tracking-added')) {
          input.setAttribute('data-tracking-added', 'true');
        }
      }
      });
    });

    observer.observe(document.body, { childList: true, subtree: true });

    // Watch for mobile menu creation
    const mobileMenuObserver = new MutationObserver(function(mutations) {
      for (const mutation of mutations) {
        const mobileSearchInputs = document.querySelectorAll('.sidebar-header-items__end .navbar-item .search-container-wrapper .gsc-input input');
      mobileSearchInputs.forEach(input => {
        if (input) {
          input.setAttribute('placeholder', 'Search the docs ...');
          input.style.paddingLeft = '36px';
          }
        });
        if (mutation.addedNodes.length) {
          // Check if the mobile search container was added
          const mobileSearchContainer = document.querySelector('.sidebar-header-items__end .navbar-item .search-container-wrapper');
          if (mobileSearchContainer) {
            // Clone the toggle for mobile if needed
            const mobileToggle = mobileSearchContainer.querySelector('#search-toggle');
            if (mobileToggle) {
              // Sync mobile toggle with desktop toggle
              mobileToggle.checked = toggle.checked;

              // Update mobile search display
              updateMobileSearch();

              // Add event listener to mobile toggle
              mobileToggle.addEventListener('change', function() {
                // Sync desktop toggle with mobile toggle
                toggle.checked = this.checked;
                // Trigger change event on desktop toggle to update both
                toggle.dispatchEvent(new Event('change'));
              });
            }
          }
        }
      }
    });

    mobileMenuObserver.observe(document.body, { childList: true, subtree: true });

    // Ensure Google CSE is properly loaded
    if (window.__gcse) {
      window.__gcse.callback = function() {
        // This will run after Google CSE is fully loaded
        if (toggle && toggle.checked) {
          // If Google search is active, make sure it's properly initialized
          reinitializeGoogleSearch();
        }
      };
    } else {
      // If Google CSE hasn't loaded yet, set up a callback
      window.__gcse = {
        callback: function() {
          if (toggle && toggle.checked) {
            reinitializeGoogleSearch();
          }
        }
      };
    }
  });
</script>
</div>
        
          <div class="navbar-item">

<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script></div>
        
          <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://x.com/PyTorch" title="X" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-x-twitter fa-lg" aria-hidden="true"></i>
            <span class="sr-only">X</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/pytorch/pytorch" title="GitHub" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">GitHub</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://discuss.pytorch.org/" title="PyTorch Forum" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-discourse fa-lg" aria-hidden="true"></i>
            <span class="sr-only">PyTorch Forum</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://pypi.org/project/torch/" title="PyPi" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-python fa-lg" aria-hidden="true"></i>
            <span class="sr-only">PyPi</span></a>
        </li>
</ul></div>
        
      </div>
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
<nav class="bd-docs-nav bd-links"
     aria-label="Section Navigation">
  <p class="bd-links__title" role="heading" aria-level="1">Section Navigation</p>
  <div class="bd-toc-item navbar-nav"><ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="amp_examples.html">Automatic Mixed Precision examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="autograd.html">Autograd mechanics</a></li>
<li class="toctree-l1"><a class="reference internal" href="broadcasting.html">Broadcasting semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="cpu_threading_torchscript_inference.html">CPU threading and TorchScript inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="cuda.html">CUDA semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="custom_operators.html">PyTorch Custom Operators Landing Page</a></li>
<li class="toctree-l1"><a class="reference internal" href="ddp.html">Distributed Data Parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="extending.html">Extending PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="extending.func.html">Extending torch.func with autograd.Function</a></li>
<li class="toctree-l1"><a class="reference internal" href="faq.html">Frequently Asked Questions</a></li>
<li class="toctree-l1"><a class="reference internal" href="fsdp.html">FSDP Notes</a></li>
<li class="toctree-l1"><a class="reference internal" href="get_start_xpu.html">Getting Started on Intel GPU</a></li>
<li class="toctree-l1"><a class="reference internal" href="gradcheck.html">Gradcheck mechanics</a></li>
<li class="toctree-l1"><a class="reference internal" href="hip.html">HIP (ROCm) semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="large_scale_deployments.html">Features for large-scale deployments</a></li>
<li class="toctree-l1"><a class="reference internal" href="libtorch_stable_abi.html">LibTorch Stable ABI</a></li>
<li class="toctree-l1"><a class="reference internal" href="modules.html">Modules</a></li>
<li class="toctree-l1"><a class="reference internal" href="mps.html">MPS backend</a></li>
<li class="toctree-l1"><a class="reference internal" href="multiprocessing.html">Multiprocessing best practices</a></li>
<li class="toctree-l1"><a class="reference internal" href="numerical_accuracy.html">Numerical accuracy</a></li>
<li class="toctree-l1"><a class="reference internal" href="randomness.html">Reproducibility</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Serialization semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="windows.html">Windows FAQ</a></li>
</ul>
</div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        
          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item">



<nav aria-label="Breadcrumb" class="d-print-none">
  <ul class="bd-breadcrumbs">
    
    <li class="breadcrumb-item breadcrumb-home">
      <a href="../index.html" class="nav-link" aria-label="Home">
        <i class="fa-solid fa-home"></i>
      </a>
    </li>
    
    <li class="breadcrumb-item"><a href="../notes.html" class="nav-link">Developer Notes</a></li>
    
    <li class="breadcrumb-item active" aria-current="page">Serializatio...</li>
  </ul>
</nav>
</div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">
<div class="rating">
    Rate this Page
    <div class="stars">
        
        <span class="star" data-behavior="tutorial-rating" data-count="1" data-value="1"></span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="2" data-value="2"></span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="3" data-value="3"></span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="4" data-value="4"></span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="5" data-value="5"></span>
        
    </div>
</div>
</div>
      
    </div>
  
</div>
</div>
              
              
  
<div id="searchbox"></div>
  <article class="bd-article">
    
    
  <section id="serialization-semantics">
<h1><a class="toc-backref" href="#id2" role="doc-backlink">Serialization semantics</a><a class="headerlink" href="#serialization-semantics" title="Permalink to this heading">#</a></h1>
<p>This note describes how you can save and load PyTorch tensors and module states
in Python, and how to serialize Python modules so they can be loaded in C++.</p>
<nav class="contents" id="table-of-contents">
<p class="topic-title">Table of Contents</p>
<ul class="simple">
<li><p><a class="reference internal" href="#serialization-semantics" id="id2">Serialization semantics</a></p>
<ul>
<li><p><a class="reference internal" href="#saving-and-loading-tensors" id="id3">Saving and loading tensors</a></p></li>
<li><p><a class="reference internal" href="#saving-and-loading-tensors-preserves-views" id="id4">Saving and loading tensors preserves views</a></p></li>
<li><p><a class="reference internal" href="#saving-and-loading-torch-nn-modules" id="id5">Saving and loading torch.nn.Modules</a></p></li>
<li><p><a class="reference internal" href="#serialized-file-format-for-torch-save" id="id6">Serialized file format for <code class="docutils literal notranslate"><span class="pre">torch.save</span></code></a></p></li>
<li><p><a class="reference internal" href="#torch-load-with-weights-only-true" id="id7"><code class="docutils literal notranslate"><span class="pre">torch.load</span></code> with <code class="docutils literal notranslate"><span class="pre">weights_only=True</span></code></a></p>
<ul>
<li><p><a class="reference internal" href="#troubleshooting-weights-only" id="id8">Troubleshooting <code class="docutils literal notranslate"><span class="pre">weights_only</span></code></a></p>
<ul>
<li><p><a class="reference internal" href="#getting-unsafe-globals" id="id9">Getting unsafe globals</a></p></li>
<li><p><a class="reference internal" href="#environment-variables" id="id10">Environment Variables</a></p></li>
</ul>
</li>
</ul>
</li>
<li><p><a class="reference internal" href="#serializing-torch-nn-modules-and-loading-them-in-c" id="id11">Serializing torch.nn.Modules and loading them in C++</a></p></li>
<li><p><a class="reference internal" href="#saving-and-loading-scriptmodules-across-pytorch-versions" id="id12">Saving and loading ScriptModules across PyTorch versions</a></p>
<ul>
<li><p><a class="reference internal" href="#torch-div-performing-integer-division" id="id13">torch.div performing integer division</a></p></li>
<li><p><a class="reference internal" href="#torch-full-always-inferring-a-float-dtype" id="id14">torch.full always inferring a float dtype</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#utility-functions" id="id15">Utility functions</a></p></li>
<li><p><a class="reference internal" href="#module-torch.utils.serialization" id="id16">Config</a></p></li>
</ul>
</li>
</ul>
</nav>
<section id="saving-and-loading-tensors">
<span id="saving-loading-tensors"></span><h2><a class="toc-backref" href="#id3" role="doc-backlink">Saving and loading tensors</a><a class="headerlink" href="#saving-and-loading-tensors" title="Permalink to this heading">#</a></h2>
<p><a class="reference internal" href="../generated/torch.save.html#torch.save" title="torch.save"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.save()</span></code></a> and <a class="reference internal" href="../generated/torch.load.html#torch.load" title="torch.load"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.load()</span></code></a> let you easily save and load tensors:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">t</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="s1">&#39;tensor.pt&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;tensor.pt&#39;</span><span class="p">)</span>
<span class="go">tensor([1., 2.])</span>
</pre></div>
</div>
<p>By convention, PyTorch files are typically written with a .pt or .pth extension.</p>
<p><a class="reference internal" href="../generated/torch.save.html#torch.save" title="torch.save"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.save()</span></code></a> and <a class="reference internal" href="../generated/torch.load.html#torch.load" title="torch.load"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.load()</span></code></a> use Pythons pickle by default,
so you can also save multiple tensors as part of Python objects like tuples,
lists, and dicts:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">d</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;a&#39;</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">]),</span> <span class="s1">&#39;b&#39;</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">3.</span><span class="p">,</span> <span class="mf">4.</span><span class="p">])}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">d</span><span class="p">,</span> <span class="s1">&#39;tensor_dict.pt&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;tensor_dict.pt&#39;</span><span class="p">)</span>
<span class="go">{&#39;a&#39;: tensor([1., 2.]), &#39;b&#39;: tensor([3., 4.])}</span>
</pre></div>
</div>
<p>Custom data structures that include PyTorch tensors can also be saved if the
data structure is pickle-able.</p>
</section>
<section id="saving-and-loading-tensors-preserves-views">
<span id="preserve-storage-sharing"></span><h2><a class="toc-backref" href="#id4" role="doc-backlink">Saving and loading tensors preserves views</a><a class="headerlink" href="#saving-and-loading-tensors-preserves-views" title="Permalink to this heading">#</a></h2>
<p>Saving tensors preserves their view relationships:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">numbers</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">evens</span> <span class="o">=</span> <span class="n">numbers</span><span class="p">[</span><span class="mi">1</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">([</span><span class="n">numbers</span><span class="p">,</span> <span class="n">evens</span><span class="p">],</span> <span class="s1">&#39;tensors.pt&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loaded_numbers</span><span class="p">,</span> <span class="n">loaded_evens</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;tensors.pt&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loaded_evens</span> <span class="o">*=</span> <span class="mi">2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loaded_numbers</span>
<span class="go">tensor([ 1,  4,  3,  8,  5, 12,  7, 16,  9])</span>
</pre></div>
</div>
<p>Behind the scenes, these tensors share the same storage. See
<a class="reference external" href="https://pytorch.org/docs/main/tensor_view.html">Tensor Views</a> for more
on views and storage.</p>
<p>When PyTorch saves tensors it saves their storage objects and tensor
metadata separately. This is an implementation detail that may change in the
future, but it typically saves space and lets PyTorch easily
reconstruct the view relationships between the loaded tensors. In the above
snippet, for example, only a single storage is written to tensors.pt.</p>
<p>In some cases, however, saving the current storage objects may be unnecessary
and create prohibitively large files. In the following snippet a storage much
larger than the saved tensor is written to a file:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">large</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">small</span> <span class="o">=</span> <span class="n">large</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">5</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">small</span><span class="p">,</span> <span class="s1">&#39;small.pt&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loaded_small</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;small.pt&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loaded_small</span><span class="o">.</span><span class="n">storage</span><span class="p">()</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="go">999</span>
</pre></div>
</div>
<p>Instead of saving only the five values in the <cite>small</cite> tensor to small.pt,
the 999 values in the storage it shares with <cite>large</cite> were saved and loaded.</p>
<p>When saving tensors with fewer elements than their storage objects, the size of
the saved file can be reduced by first cloning the tensors. Cloning a tensor
produces a new tensor with a new storage object containing only the values
in the tensor:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">large</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">small</span> <span class="o">=</span> <span class="n">large</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">5</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">small</span><span class="o">.</span><span class="n">clone</span><span class="p">(),</span> <span class="s1">&#39;small.pt&#39;</span><span class="p">)</span>  <span class="c1"># saves a clone of small</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loaded_small</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;small.pt&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loaded_small</span><span class="o">.</span><span class="n">storage</span><span class="p">()</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="go">5</span>
</pre></div>
</div>
<p>Since the cloned tensors are independent of each other, however, they have
none of the view relationships the original tensors did. If both file size and
view relationships are important when saving tensors smaller than their
storage objects, then care must be taken to construct new tensors that minimize
the size of their storage objects but still have the desired view relationships
before saving.</p>
</section>
<section id="saving-and-loading-torch-nn-modules">
<span id="saving-loading-python-modules"></span><h2><a class="toc-backref" href="#id5" role="doc-backlink">Saving and loading torch.nn.Modules</a><a class="headerlink" href="#saving-and-loading-torch-nn-modules" title="Permalink to this heading">#</a></h2>
<p>See also: <a class="reference external" href="https://pytorch.org/tutorials/beginner/saving_loading_models.html">Tutorial: Saving and loading modules</a></p>
<p>In PyTorch, a modules state is frequently serialized using a state dict.
A modules state dict contains all of its parameters and persistent buffers:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">bn</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">track_running_stats</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">list</span><span class="p">(</span><span class="n">bn</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">())</span>
<span class="go">[(&#39;weight&#39;, Parameter containing: tensor([1., 1., 1.], requires_grad=True)),</span>
<span class="go"> (&#39;bias&#39;, Parameter containing: tensor([0., 0., 0.], requires_grad=True))]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="nb">list</span><span class="p">(</span><span class="n">bn</span><span class="o">.</span><span class="n">named_buffers</span><span class="p">())</span>
<span class="go">[(&#39;running_mean&#39;, tensor([0., 0., 0.])),</span>
<span class="go"> (&#39;running_var&#39;, tensor([1., 1., 1.])),</span>
<span class="go"> (&#39;num_batches_tracked&#39;, tensor(0))]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">bn</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()</span>
<span class="go">OrderedDict([(&#39;weight&#39;, tensor([1., 1., 1.])),</span>
<span class="go">             (&#39;bias&#39;, tensor([0., 0., 0.])),</span>
<span class="go">             (&#39;running_mean&#39;, tensor([0., 0., 0.])),</span>
<span class="go">             (&#39;running_var&#39;, tensor([1., 1., 1.])),</span>
<span class="go">             (&#39;num_batches_tracked&#39;, tensor(0))])</span>
</pre></div>
</div>
<p>Instead of saving a module directly, for compatibility reasons it is recommended
to instead save only its state dict. Python modules even have a function,
<a class="reference internal" href="../generated/torch.nn.Module.html#torch.nn.Module.load_state_dict" title="torch.nn.Module.load_state_dict"><code class="xref py py-meth docutils literal notranslate"><span class="pre">load_state_dict()</span></code></a>, to restore their states from a state dict:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">bn</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span> <span class="s1">&#39;bn.pt&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">bn_state_dict</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;bn.pt&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">new_bn</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">track_running_stats</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">new_bn</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">bn_state_dict</span><span class="p">)</span>
<span class="go">&lt;All keys matched successfully&gt;</span>
</pre></div>
</div>
<p>Note that the state dict is first loaded from its file with <a class="reference internal" href="../generated/torch.load.html#torch.load" title="torch.load"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.load()</span></code></a>
and the state then restored with <a class="reference internal" href="../generated/torch.nn.Module.html#torch.nn.Module.load_state_dict" title="torch.nn.Module.load_state_dict"><code class="xref py py-meth docutils literal notranslate"><span class="pre">load_state_dict()</span></code></a>.</p>
<p>Even custom modules and modules containing other modules have state dicts and
can use this pattern:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># A module with two linear layers</span>
<span class="o">&gt;&gt;&gt;</span> <span class="k">class</span><span class="w"> </span><span class="nc">MyModule</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
      <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">l0</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">l1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

      <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
        <span class="n">out0</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">l0</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
        <span class="n">out0_relu</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">out0</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">l1</span><span class="p">(</span><span class="n">out0_relu</span><span class="p">)</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">m</span> <span class="o">=</span> <span class="n">MyModule</span><span class="p">()</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">m</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()</span>
<span class="n">OrderedDict</span><span class="p">([(</span><span class="s1">&#39;l0.weight&#39;</span><span class="p">,</span> <span class="n">tensor</span><span class="p">([[</span> <span class="mf">0.1400</span><span class="p">,</span> <span class="mf">0.4563</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0271</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.4406</span><span class="p">],</span>
                                   <span class="p">[</span><span class="o">-</span><span class="mf">0.3289</span><span class="p">,</span> <span class="mf">0.2827</span><span class="p">,</span> <span class="mf">0.4588</span><span class="p">,</span> <span class="mf">0.2031</span><span class="p">]])),</span>
             <span class="p">(</span><span class="s1">&#39;l0.bias&#39;</span><span class="p">,</span> <span class="n">tensor</span><span class="p">([</span> <span class="mf">0.0300</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1316</span><span class="p">])),</span>
             <span class="p">(</span><span class="s1">&#39;l1.weight&#39;</span><span class="p">,</span> <span class="n">tensor</span><span class="p">([[</span><span class="mf">0.6533</span><span class="p">,</span> <span class="mf">0.3413</span><span class="p">]])),</span>
             <span class="p">(</span><span class="s1">&#39;l1.bias&#39;</span><span class="p">,</span> <span class="n">tensor</span><span class="p">([</span><span class="o">-</span><span class="mf">0.1112</span><span class="p">]))])</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span> <span class="s1">&#39;mymodule.pt&#39;</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">m_state_dict</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;mymodule.pt&#39;</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">new_m</span> <span class="o">=</span> <span class="n">MyModule</span><span class="p">()</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">new_m</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">m_state_dict</span><span class="p">)</span>
<span class="o">&lt;</span><span class="n">All</span> <span class="n">keys</span> <span class="n">matched</span> <span class="n">successfully</span><span class="o">&gt;</span>
</pre></div>
</div>
</section>
<section id="serialized-file-format-for-torch-save">
<span id="serialized-file-format"></span><h2><a class="toc-backref" href="#id6" role="doc-backlink">Serialized file format for <code class="docutils literal notranslate"><span class="pre">torch.save</span></code></a><a class="headerlink" href="#serialized-file-format-for-torch-save" title="Permalink to this heading">#</a></h2>
<p>Since PyTorch 1.6.0, <code class="docutils literal notranslate"><span class="pre">torch.save</span></code> defaults to returning an uncompressed ZIP64
archive unless the user sets <code class="docutils literal notranslate"><span class="pre">_use_new_zipfile_serialization=False</span></code>.</p>
<p>In this archive, the files are ordered as such</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>checkpoint.pth
 data.pkl
 byteorder  # added in PyTorch 2.1.0
 data/
    0
    1
    2
    
 version
</pre></div>
</div>
<dl class="simple">
<dt>The entries are as follows:</dt><dd><ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">data.pkl</span></code> is the result of pickling the object passed to <code class="docutils literal notranslate"><span class="pre">torch.save</span></code>
excluding <code class="docutils literal notranslate"><span class="pre">torch.Storage</span></code> objects that it contains</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">byteorder</span></code> contains a string with the <code class="docutils literal notranslate"><span class="pre">sys.byteorder</span></code> when saving (little or big)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">data/</span></code> contains all the storages in the object, where each storage is a separate file</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">version</span></code> contains a version number at save time that can be used at load time</p></li>
</ul>
</dd>
</dl>
<p>When saving, PyTorch will ensure that the local file header of each file is padded
to an offset that is a multiple of 64 bytes, ensuring that the offset of each file
is 64-byte aligned.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Tensors on certain devices such as XLA are serialized as pickled numpy arrays. As
such, their storages are not serialized. In these cases <code class="docutils literal notranslate"><span class="pre">data/</span></code> might not exist
in the checkpoint.</p>
</div>
</section>
<section id="torch-load-with-weights-only-true">
<span id="weights-only"></span><h2><a class="toc-backref" href="#id7" role="doc-backlink"><code class="docutils literal notranslate"><span class="pre">torch.load</span></code> with <code class="docutils literal notranslate"><span class="pre">weights_only=True</span></code></a><a class="headerlink" href="#torch-load-with-weights-only-true" title="Permalink to this heading">#</a></h2>
<p>Starting in version 2.6, <code class="docutils literal notranslate"><span class="pre">torch.load</span></code> will use <code class="docutils literal notranslate"><span class="pre">weights_only=True</span></code> if the <code class="docutils literal notranslate"><span class="pre">pickle_module</span></code>
argument is not passed.</p>
<p>As discussed in the documentation for <a class="reference internal" href="../generated/torch.load.html#torch.load" title="torch.load"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.load()</span></code></a>, <code class="docutils literal notranslate"><span class="pre">weights_only=True</span></code> restricts
the unpickler used in <code class="docutils literal notranslate"><span class="pre">torch.load</span></code> to only executing functions/building classes required for
<code class="docutils literal notranslate"><span class="pre">state_dicts</span></code> of plain <code class="docutils literal notranslate"><span class="pre">torch.Tensors</span></code> as well as some other primitive types. Further,
unlike the default <code class="docutils literal notranslate"><span class="pre">Unpickler</span></code> provided by the <code class="docutils literal notranslate"><span class="pre">pickle</span></code> module, the <code class="docutils literal notranslate"><span class="pre">weights_only</span></code> Unpickler
is not allowed to dynamically import anything during unpickling.</p>
<p>As mentioned above, saving a modules <code class="docutils literal notranslate"><span class="pre">state_dict</span></code> is a best practice when using <code class="docutils literal notranslate"><span class="pre">torch.save</span></code>. If loading an old
checkpoint that contains an <code class="docutils literal notranslate"><span class="pre">nn.Module</span></code>, we recommend <code class="docutils literal notranslate"><span class="pre">weights_only=False</span></code>. When loading a checkpoint that contains
tensor subclasses, there will likely be functions/classes that need to be allowlisted, see below for further details.</p>
<p>If the <code class="docutils literal notranslate"><span class="pre">weights_only</span></code> Unpickler encounters a function or class that is not allowlisted
by default within the pickle file, you should see an actionable error like such</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>_pickle.UnpicklingError: Weights only load failed. This file can still be loaded,
to do so you have two options, do those steps only if you trust the source of the checkpoint.
    1. Re-running `torch.load` with `weights_only` set to `False` will likely succeed,
        but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.
    2. Alternatively, to load with `weights_only=True` please check the recommended
       steps in the following error message.
       WeightsUnpickler error: Unsupported global: GLOBAL {__module__}.{__name__} was not an allowed global by
       default. Please use `torch.serialization.add_safe_globals([{__name__}])` or the
       `torch.serialization.safe_globals([{__name__}])` context manager to allowlist this global
       if you trust this class/function.
</pre></div>
</div>
<p>Please follow the steps in the error message and allowlist the functions or classes only if you trust them.</p>
<p>To get all GLOBALs (functions/classes) in the checkpoint that are not yet allowlisted you can use
<a class="reference internal" href="#torch.serialization.get_unsafe_globals_in_checkpoint" title="torch.serialization.get_unsafe_globals_in_checkpoint"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.serialization.get_unsafe_globals_in_checkpoint()</span></code></a> which will return a list of strings of the form
<code class="docutils literal notranslate"><span class="pre">{__module__}.{__name__}</span></code>. If you trust these functions/classes, you can import them and allowlist them per
the error message either via <a class="reference internal" href="#torch.serialization.add_safe_globals" title="torch.serialization.add_safe_globals"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.serialization.add_safe_globals()</span></code></a> or the context manager
<a class="reference internal" href="#torch.serialization.safe_globals" title="torch.serialization.safe_globals"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.serialization.safe_globals</span></code></a>.</p>
<p>To access the list of user-allowlisted functions/classes you can use <a class="reference internal" href="#torch.serialization.get_safe_globals" title="torch.serialization.get_safe_globals"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.serialization.get_safe_globals()</span></code></a> and
to clear the current list see <a class="reference internal" href="#torch.serialization.clear_safe_globals" title="torch.serialization.clear_safe_globals"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.serialization.clear_safe_globals()</span></code></a>.</p>
<section id="troubleshooting-weights-only">
<h3><a class="toc-backref" href="#id8" role="doc-backlink">Troubleshooting <code class="docutils literal notranslate"><span class="pre">weights_only</span></code></a><a class="headerlink" href="#troubleshooting-weights-only" title="Permalink to this heading">#</a></h3>
<section id="getting-unsafe-globals">
<h4><a class="toc-backref" href="#id9" role="doc-backlink">Getting unsafe globals</a><a class="headerlink" href="#getting-unsafe-globals" title="Permalink to this heading">#</a></h4>
<p>A caveat is that <a class="reference internal" href="#torch.serialization.get_unsafe_globals_in_checkpoint" title="torch.serialization.get_unsafe_globals_in_checkpoint"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.serialization.get_unsafe_globals_in_checkpoint()</span></code></a> analyzes the checkpoint statically,
some types might be built dynamically during the unpickling process and hence will not be reported by
<a class="reference internal" href="#torch.serialization.get_unsafe_globals_in_checkpoint" title="torch.serialization.get_unsafe_globals_in_checkpoint"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.serialization.get_unsafe_globals_in_checkpoint()</span></code></a>. One such example is <code class="docutils literal notranslate"><span class="pre">dtypes</span></code> in numpy. In
<code class="docutils literal notranslate"><span class="pre">numpy</span> <span class="pre">&lt;</span> <span class="pre">1.25</span></code> after allowlisting all the functions/classes reported by
<a class="reference internal" href="#torch.serialization.get_unsafe_globals_in_checkpoint" title="torch.serialization.get_unsafe_globals_in_checkpoint"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.serialization.get_unsafe_globals_in_checkpoint()</span></code></a> you might see an error like</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>WeightsUnpickler error: Can only build Tensor, Parameter, OrderedDict or types allowlisted via `add_safe_globals`,
but got &lt;class &#39;numpy.dtype[float32]&#39;&gt;
</pre></div>
</div>
<p>This can be allowlisted via <code class="docutils literal notranslate"><span class="pre">{add_}safe_globals([type(np.dtype(np.float32))])</span></code>.</p>
<p>In <code class="docutils literal notranslate"><span class="pre">numpy</span> <span class="pre">&gt;=1.25</span></code> you would see</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>WeightsUnpickler error: Can only build Tensor, Parameter, OrderedDict or types allowlisted via `add_safe_globals`,
but got &lt;class &#39;numpy.dtypes.Float32DType&#39;&gt;
</pre></div>
</div>
<p>This can be allowlisted via <code class="docutils literal notranslate"><span class="pre">{add_}safe_globals([np.dtypes.Float32DType])</span></code>.</p>
</section>
<section id="environment-variables">
<h4><a class="toc-backref" href="#id10" role="doc-backlink">Environment Variables</a><a class="headerlink" href="#environment-variables" title="Permalink to this heading">#</a></h4>
<p>There are two environment variables that will influence the behavior of <code class="docutils literal notranslate"><span class="pre">torch.load</span></code>. These can be helpful
if one does not have access to the <code class="docutils literal notranslate"><span class="pre">torch.load</span></code> callsites.</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">TORCH_FORCE_WEIGHTS_ONLY_LOAD=1</span></code> will override all <code class="docutils literal notranslate"><span class="pre">torch.load</span></code> callsites to use <code class="docutils literal notranslate"><span class="pre">weights_only=True</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">TORCH_FORCE_NO_WEIGHTS_ONLY_LOAD=1</span></code> will make <code class="docutils literal notranslate"><span class="pre">torch.load</span></code> callsites use <code class="docutils literal notranslate"><span class="pre">weights_only=False</span></code> <strong>only</strong>
if <code class="docutils literal notranslate"><span class="pre">weights_only</span></code> was not passed as an argument.</p></li>
</ul>
</section>
</section>
</section>
<section id="serializing-torch-nn-modules-and-loading-them-in-c">
<span id="serializing-python-modules"></span><h2><a class="toc-backref" href="#id11" role="doc-backlink">Serializing torch.nn.Modules and loading them in C++</a><a class="headerlink" href="#serializing-torch-nn-modules-and-loading-them-in-c" title="Permalink to this heading">#</a></h2>
<p>See also: <a class="reference external" href="https://pytorch.org/tutorials/advanced/cpp_export.html">Tutorial: Loading a TorchScript Model in C++</a></p>
<p>ScriptModules can be serialized as a TorchScript program and loaded
using <a class="reference internal" href="../generated/torch.jit.load.html#torch.jit.load" title="torch.jit.load"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.jit.load()</span></code></a>.
This serialization encodes all the modules methods, submodules, parameters,
and attributes, and it allows the serialized program to be loaded in C++
(i.e. without Python).</p>
<p>The distinction between <a class="reference internal" href="../generated/torch.jit.save.html#torch.jit.save" title="torch.jit.save"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.jit.save()</span></code></a> and <a class="reference internal" href="../generated/torch.save.html#torch.save" title="torch.save"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.save()</span></code></a> may not
be immediately clear. <a class="reference internal" href="../generated/torch.save.html#torch.save" title="torch.save"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.save()</span></code></a> saves Python objects with pickle.
This is especially useful for prototyping, researching, and training.
<a class="reference internal" href="../generated/torch.jit.save.html#torch.jit.save" title="torch.jit.save"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.jit.save()</span></code></a>, on the other hand, serializes ScriptModules to a format
that can be loaded in Python or C++. This is useful when saving and loading C++
modules or for running modules trained in Python with C++, a common practice
when deploying PyTorch models.</p>
<p>To script, serialize and load a module in Python:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">scripted_module</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">script</span><span class="p">(</span><span class="n">MyModule</span><span class="p">())</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">scripted_module</span><span class="p">,</span> <span class="s1">&#39;mymodule.pt&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;mymodule.pt&#39;</span><span class="p">)</span>
<span class="go">RecursiveScriptModule( original_name=MyModule</span>
<span class="go">                      (l0): RecursiveScriptModule(original_name=Linear)</span>
<span class="go">                      (l1): RecursiveScriptModule(original_name=Linear) )</span>
</pre></div>
</div>
<p>Traced modules can also be saved with <a class="reference internal" href="../generated/torch.jit.save.html#torch.jit.save" title="torch.jit.save"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.jit.save()</span></code></a>, with the caveat
that only the traced code path is serialized. The following example demonstrates
this:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># A module with control flow</span>
<span class="o">&gt;&gt;&gt;</span> <span class="k">class</span><span class="w"> </span><span class="nc">ControlFlowModule</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
      <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">l0</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">l1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

      <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
        <span class="k">if</span> <span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

        <span class="n">out0</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">l0</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
        <span class="n">out0_relu</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">out0</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">l1</span><span class="p">(</span><span class="n">out0_relu</span><span class="p">)</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">traced_module</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">trace</span><span class="p">(</span><span class="n">ControlFlowModule</span><span class="p">(),</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">))</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">traced_module</span><span class="p">,</span> <span class="s1">&#39;controlflowmodule_traced.pt&#39;</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">loaded</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;controlflowmodule_traced.pt&#39;</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">loaded</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">)))</span>
<span class="n">tensor</span><span class="p">([[</span><span class="o">-</span><span class="mf">0.1571</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mf">0.3793</span><span class="p">]],</span> <span class="n">grad_fn</span><span class="o">=&lt;</span><span class="n">AddBackward0</span><span class="o">&gt;</span><span class="p">)</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">scripted_module</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">script</span><span class="p">(</span><span class="n">ControlFlowModule</span><span class="p">(),</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">))</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">scripted_module</span><span class="p">,</span> <span class="s1">&#39;controlflowmodule_scripted.pt&#39;</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">loaded</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;controlflowmodule_scripted.pt&#39;</span><span class="p">)</span>
<span class="o">&gt;&gt;</span> <span class="n">loaded</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="n">tensor</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
<p>The above module has an if statement that is not triggered by the traced inputs,
and so is not part of the traced module and not serialized with it.
The scripted module, however, contains the if statement and is serialized with it.
See the <a class="reference external" href="https://pytorch.org/docs/stable/jit.html">TorchScript documentation</a>
for more on scripting and tracing.</p>
<p>Finally, to load the module in C++:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="p">::</span><span class="n">jit</span><span class="p">::</span><span class="n">script</span><span class="p">::</span><span class="n">Module</span> <span class="n">module</span><span class="p">;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">module</span> <span class="o">=</span> <span class="n">torch</span><span class="p">::</span><span class="n">jit</span><span class="p">::</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;controlflowmodule_scripted.pt&#39;</span><span class="p">);</span>
</pre></div>
</div>
<p>See the <a class="reference external" href="https://pytorch.org/cppdocs/">PyTorch C++ API documentation</a>
for details about how to use PyTorch modules in C++.</p>
</section>
<section id="saving-and-loading-scriptmodules-across-pytorch-versions">
<span id="saving-loading-across-versions"></span><h2><a class="toc-backref" href="#id12" role="doc-backlink">Saving and loading ScriptModules across PyTorch versions</a><a class="headerlink" href="#saving-and-loading-scriptmodules-across-pytorch-versions" title="Permalink to this heading">#</a></h2>
<p>The PyTorch Team recommends saving and loading modules with the same version of
PyTorch. Older versions of PyTorch may not support newer modules, and newer
versions may have removed or modified older behavior. These changes are
explicitly described in
PyTorchs <a class="reference external" href="https://github.com/pytorch/pytorch/releases">release notes</a>,
and modules relying on functionality that has changed may need to be updated
to continue working properly. In limited cases, detailed below, PyTorch will
preserve the historic behavior of serialized ScriptModules so they do not require
an update.</p>
<section id="torch-div-performing-integer-division">
<h3><a class="toc-backref" href="#id13" role="doc-backlink">torch.div performing integer division</a><a class="headerlink" href="#torch-div-performing-integer-division" title="Permalink to this heading">#</a></h3>
<p>In PyTorch 1.5 and earlier <a class="reference internal" href="../generated/torch.div.html#torch.div" title="torch.div"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.div()</span></code></a> would perform floor division when
given two integer inputs:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># PyTorch 1.5 (and earlier)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">a</span> <span class="o">/</span> <span class="n">b</span>
<span class="n">tensor</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
<p>In PyTorch 1.7, however, <a class="reference internal" href="../generated/torch.div.html#torch.div" title="torch.div"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.div()</span></code></a> will always perform a true division
of its inputs, just like division in Python 3:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># PyTorch 1.7</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">a</span> <span class="o">/</span> <span class="n">b</span>
<span class="n">tensor</span><span class="p">(</span><span class="mf">1.6667</span><span class="p">)</span>
</pre></div>
</div>
<p>The behavior of <a class="reference internal" href="../generated/torch.div.html#torch.div" title="torch.div"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.div()</span></code></a> is preserved in serialized ScriptModules.
That is, ScriptModules serialized with versions of PyTorch before 1.6 will continue
to see <a class="reference internal" href="../generated/torch.div.html#torch.div" title="torch.div"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.div()</span></code></a> perform floor division when given two integer inputs
even when loaded with newer versions of PyTorch. ScriptModules using <a class="reference internal" href="../generated/torch.div.html#torch.div" title="torch.div"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.div()</span></code></a>
and serialized on PyTorch 1.6 and later cannot be loaded in earlier versions of
PyTorch, however, since those earlier versions do not understand the new behavior.</p>
</section>
<section id="torch-full-always-inferring-a-float-dtype">
<h3><a class="toc-backref" href="#id14" role="doc-backlink">torch.full always inferring a float dtype</a><a class="headerlink" href="#torch-full-always-inferring-a-float-dtype" title="Permalink to this heading">#</a></h3>
<p>In PyTorch 1.5 and earlier <a class="reference internal" href="../generated/torch.full.html#torch.full" title="torch.full"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.full()</span></code></a> always returned a float tensor,
regardless of the fill value its given:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># PyTorch 1.5 and earlier</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">full</span><span class="p">((</span><span class="mi">3</span><span class="p">,),</span> <span class="mi">1</span><span class="p">)</span>  <span class="c1"># Note the integer fill value...</span>
<span class="n">tensor</span><span class="p">([</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">])</span>     <span class="c1"># ...but float tensor!</span>
</pre></div>
</div>
<p>In PyTorch 1.7, however, <a class="reference internal" href="../generated/torch.full.html#torch.full" title="torch.full"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.full()</span></code></a> will infer the returned tensors
dtype from the fill value:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># PyTorch 1.7</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">full</span><span class="p">((</span><span class="mi">3</span><span class="p">,),</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">full</span><span class="p">((</span><span class="mi">3</span><span class="p">,),</span> <span class="kc">True</span><span class="p">)</span>
<span class="n">tensor</span><span class="p">([</span><span class="kc">True</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="kc">True</span><span class="p">])</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">full</span><span class="p">((</span><span class="mi">3</span><span class="p">,),</span> <span class="mf">1.</span><span class="p">)</span>
<span class="n">tensor</span><span class="p">([</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">])</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">full</span><span class="p">((</span><span class="mi">3</span><span class="p">,),</span> <span class="mi">1</span> <span class="o">+</span> <span class="mi">1</span><span class="n">j</span><span class="p">)</span>
<span class="n">tensor</span><span class="p">([</span><span class="mf">1.</span><span class="o">+</span><span class="mf">1.</span><span class="n">j</span><span class="p">,</span> <span class="mf">1.</span><span class="o">+</span><span class="mf">1.</span><span class="n">j</span><span class="p">,</span> <span class="mf">1.</span><span class="o">+</span><span class="mf">1.</span><span class="n">j</span><span class="p">])</span>
</pre></div>
</div>
<p>The behavior of <a class="reference internal" href="../generated/torch.full.html#torch.full" title="torch.full"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.full()</span></code></a> is preserved in serialized ScriptModules. That is,
ScriptModules serialized with versions of PyTorch before 1.6 will continue to see
torch.full return float tensors by default, even when given bool or
integer fill values. ScriptModules using <a class="reference internal" href="../generated/torch.full.html#torch.full" title="torch.full"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.full()</span></code></a> and serialized on PyTorch 1.6
and later cannot be loaded in earlier versions of PyTorch, however, since those
earlier versions do not understand the new behavior.</p>
</section>
</section>
<section id="utility-functions">
<span id="id1"></span><h2><a class="toc-backref" href="#id15" role="doc-backlink">Utility functions</a><a class="headerlink" href="#utility-functions" title="Permalink to this heading">#</a></h2>
<p>The following utility functions are related to serialization:</p>
<dl class="py function">
<dt class="sig sig-object py" id="torch.serialization.register_package">
<span class="sig-prename descclassname"><span class="pre">torch.serialization.</span></span><span class="sig-name descname"><span class="pre">register_package</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">priority</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tagger</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">deserializer</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/serialization.py#L438"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.serialization.register_package" title="Permalink to this definition">#</a></dt>
<dd><p>Registers callables for tagging and deserializing storage objects with an associated priority.
Tagging associates a device with a storage object at save time while deserializing moves a
storage object to an appropriate device at load time. <code class="xref py py-attr docutils literal notranslate"><span class="pre">tagger</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">deserializer</span></code>
are run in the order given by their <code class="xref py py-attr docutils literal notranslate"><span class="pre">priority</span></code> until a tagger/deserializer returns a
value that is not <cite>None</cite>.</p>
<p>To override the deserialization behavior for a device in the global registry, one can register a
tagger with a higher priority than the existing tagger.</p>
<p>This function can also be used to register a tagger and deserializer for new devices.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>priority</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>)  Indicates the priority associated with the tagger and deserializer, where a lower
value indicates higher priority.</p></li>
<li><p><strong>tagger</strong> (<a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Callable" title="(in Python v3.13)"><em>Callable</em></a><em>[</em><em>[</em><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Union" title="(in Python v3.13)"><em>Union</em></a><em>[</em><em>Storage</em><em>, </em><a class="reference internal" href="../storage.html#torch.TypedStorage" title="torch.storage.TypedStorage"><em>TypedStorage</em></a><em>, </em><a class="reference internal" href="../storage.html#torch.UntypedStorage" title="torch.storage.UntypedStorage"><em>UntypedStorage</em></a><em>]</em><em>]</em><em>, </em><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Optional" title="(in Python v3.13)"><em>Optional</em></a><em>[</em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><em>str</em></a><em>]</em><em>]</em>)  Callable that takes in a storage object and returns its tagged device as a string
or None.</p></li>
<li><p><strong>deserializer</strong> (<a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Callable" title="(in Python v3.13)"><em>Callable</em></a><em>[</em><em>[</em><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Union" title="(in Python v3.13)"><em>Union</em></a><em>[</em><em>Storage</em><em>, </em><a class="reference internal" href="../storage.html#torch.TypedStorage" title="torch.storage.TypedStorage"><em>TypedStorage</em></a><em>, </em><a class="reference internal" href="../storage.html#torch.UntypedStorage" title="torch.storage.UntypedStorage"><em>UntypedStorage</em></a><em>]</em><em>, </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><em>str</em></a><em>]</em><em>, </em><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Optional" title="(in Python v3.13)"><em>Optional</em></a><em>[</em><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Union" title="(in Python v3.13)"><em>Union</em></a><em>[</em><em>Storage</em><em>, </em><a class="reference internal" href="../storage.html#torch.TypedStorage" title="torch.storage.TypedStorage"><em>TypedStorage</em></a><em>, </em><a class="reference internal" href="../storage.html#torch.UntypedStorage" title="torch.storage.UntypedStorage"><em>UntypedStorage</em></a><em>]</em><em>]</em><em>]</em>)  Callable that takes in storage object and a device string and returns a storage
object on the appropriate device or None.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><cite>None</cite></p>
</dd>
</dl>
<p class="rubric">Example</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">def</span><span class="w"> </span><span class="nf">ipu_tag</span><span class="p">(</span><span class="n">obj</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">if</span> <span class="n">obj</span><span class="o">.</span><span class="n">device</span><span class="o">.</span><span class="n">type</span> <span class="o">==</span> <span class="s1">&#39;ipu&#39;</span><span class="p">:</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="s1">&#39;ipu&#39;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">def</span><span class="w"> </span><span class="nf">ipu_deserialize</span><span class="p">(</span><span class="n">obj</span><span class="p">,</span> <span class="n">location</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">if</span> <span class="n">location</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s1">&#39;ipu&#39;</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">ipu</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">torch</span><span class="p">,</span> <span class="s2">&quot;ipu&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">assert</span> <span class="n">ipu</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">,</span> <span class="s2">&quot;IPU device module is not loaded&quot;</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">assert</span> <span class="n">torch</span><span class="o">.</span><span class="n">ipu</span><span class="o">.</span><span class="n">is_available</span><span class="p">(),</span> <span class="s2">&quot;ipu is not available&quot;</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="n">obj</span><span class="o">.</span><span class="n">ipu</span><span class="p">(</span><span class="n">location</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">serialization</span><span class="o">.</span><span class="n">register_package</span><span class="p">(</span><span class="mi">11</span><span class="p">,</span> <span class="n">ipu_tag</span><span class="p">,</span> <span class="n">ipu_deserialize</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="torch.serialization.get_crc32_options">
<span class="sig-prename descclassname"><span class="pre">torch.serialization.</span></span><span class="sig-name descname"><span class="pre">get_crc32_options</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/serialization.py#L187"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.serialization.get_crc32_options" title="Permalink to this definition">#</a></dt>
<dd><p>Get whether <a class="reference internal" href="../generated/torch.save.html#torch.save" title="torch.save"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.save()</span></code></a> computes and writes crc32 for each record.</p>
<p>Defaults to <code class="docutils literal notranslate"><span class="pre">True</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)">bool</a></p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="torch.serialization.set_crc32_options">
<span class="sig-prename descclassname"><span class="pre">torch.serialization.</span></span><span class="sig-name descname"><span class="pre">set_crc32_options</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">compute_crc32</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/serialization.py#L196"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.serialization.set_crc32_options" title="Permalink to this definition">#</a></dt>
<dd><p>Set whether <a class="reference internal" href="../generated/torch.save.html#torch.save" title="torch.save"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.save()</span></code></a> computes and writes crc32 for each record.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Setting this to <code class="docutils literal notranslate"><span class="pre">False</span></code> may make unzipping of the <code class="docutils literal notranslate"><span class="pre">torch.save</span></code> output
fail or warn due to corrupted CRC32. However <code class="docutils literal notranslate"><span class="pre">torch.load</span></code> will be
able to load the file.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>compute_crc32</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><em>bool</em></a>)  set crc32 compuation flag</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="torch.serialization.get_default_load_endianness">
<span class="sig-prename descclassname"><span class="pre">torch.serialization.</span></span><span class="sig-name descname"><span class="pre">get_default_load_endianness</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/serialization.py#L153"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.serialization.get_default_load_endianness" title="Permalink to this definition">#</a></dt>
<dd><p>Get fallback byte order for loading files</p>
<p>If byteorder mark is not present in saved checkpoint,
this byte order is used as fallback.
By default, its native byte order.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>Optional[LoadEndianness]</p>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p>default_load_endian</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="torch.serialization.set_default_load_endianness">
<span class="sig-prename descclassname"><span class="pre">torch.serialization.</span></span><span class="sig-name descname"><span class="pre">set_default_load_endianness</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">endianness</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/serialization.py#L167"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.serialization.set_default_load_endianness" title="Permalink to this definition">#</a></dt>
<dd><p>Set fallback byte order for loading files</p>
<p>If byteorder mark is not present in saved checkpoint,
this byte order is used as fallback.
By default, its native byte order.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>endianness</strong>  the new fallback byte order</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="torch.serialization.get_default_mmap_options">
<span class="sig-prename descclassname"><span class="pre">torch.serialization.</span></span><span class="sig-name descname"><span class="pre">get_default_mmap_options</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/serialization.py#L215"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.serialization.get_default_mmap_options" title="Permalink to this definition">#</a></dt>
<dd><p>Get default mmap options for <a class="reference internal" href="../generated/torch.load.html#torch.load" title="torch.load"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.load()</span></code></a> with <code class="docutils literal notranslate"><span class="pre">mmap=True</span></code>.</p>
<p>Defaults to <code class="docutils literal notranslate"><span class="pre">mmap.MAP_PRIVATE</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>int</p>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p>default_mmap_options</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="torch.serialization.set_default_mmap_options">
<span class="sig-prename descclassname"><span class="pre">torch.serialization.</span></span><span class="sig-name descname"><span class="pre">set_default_mmap_options</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">flags</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/serialization.py#L228"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.serialization.set_default_mmap_options" title="Permalink to this definition">#</a></dt>
<dd><p>Context manager or function to set default mmap options for <a class="reference internal" href="../generated/torch.load.html#torch.load" title="torch.load"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.load()</span></code></a> with <code class="docutils literal notranslate"><span class="pre">mmap=True</span></code> to flags.</p>
<p>For now, only either <code class="docutils literal notranslate"><span class="pre">mmap.MAP_PRIVATE</span></code> or <code class="docutils literal notranslate"><span class="pre">mmap.MAP_SHARED</span></code> are supported.
Please open an issue if you need any other option to be added here.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This feature is currently not supported for Windows.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>flags</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>)  <code class="docutils literal notranslate"><span class="pre">mmap.MAP_PRIVATE</span></code> or <code class="docutils literal notranslate"><span class="pre">mmap.MAP_SHARED</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="torch.serialization.add_safe_globals">
<span class="sig-prename descclassname"><span class="pre">torch.serialization.</span></span><span class="sig-name descname"><span class="pre">add_safe_globals</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">safe_globals</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/serialization.py#L278"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.serialization.add_safe_globals" title="Permalink to this definition">#</a></dt>
<dd><p>Marks the given globals as safe for <code class="docutils literal notranslate"><span class="pre">weights_only</span></code> load. For example, functions
added to this list can be called during unpickling, classes could be instantiated
and have state set.</p>
<p>Each item in the list can either be a function/class or a tuple of the form
(function/class, string) where string is the full path of the function/class.</p>
<p>Within the serialized format, each function is identified with its full
path as <code class="docutils literal notranslate"><span class="pre">{__module__}.{__name__}</span></code>. When calling this API, you can provide this
full path that should match the one in the checkpoint otherwise the default
<code class="docutils literal notranslate"><span class="pre">{fn.__module__}.{fn.__name__}</span></code> will be used.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>safe_globals</strong> (<em>List</em><em>[</em><em>Union</em><em>[</em><em>Callable</em><em>, </em><em>Tuple</em><em>[</em><em>Callable</em><em>, </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><em>str</em></a><em>]</em><em>]</em><em>]</em>)  list of globals to mark as safe</p>
</dd>
</dl>
<p class="rubric">Example</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">tempfile</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">class</span><span class="w"> </span><span class="nc">MyTensor</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
<span class="gp">... </span>    <span class="k">pass</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">t</span> <span class="o">=</span> <span class="n">MyTensor</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">with</span> <span class="n">tempfile</span><span class="o">.</span><span class="n">NamedTemporaryFile</span><span class="p">()</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
<span class="gp">... </span>    <span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">f</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
<span class="go"># Running `torch.load(f.name, weights_only=True)` will fail with</span>
<span class="go"># Unsupported global: GLOBAL __main__.MyTensor was not an allowed global by default.</span>
<span class="go"># Check the code and make sure MyTensor is safe to be used when loaded from an arbitrary checkpoint.</span>
<span class="go">...     torch.serialization.add_safe_globals([MyTensor])</span>
<span class="go">...     torch.load(f.name, weights_only=True)</span>
<span class="go"># MyTensor([[-0.5024, -1.8152, -0.5455],</span>
<span class="go">#          [-0.8234,  2.0500, -0.3657]])</span>
</pre></div>
</div>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="torch.serialization.clear_safe_globals">
<span class="sig-prename descclassname"><span class="pre">torch.serialization.</span></span><span class="sig-name descname"><span class="pre">clear_safe_globals</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/serialization.py#L264"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.serialization.clear_safe_globals" title="Permalink to this definition">#</a></dt>
<dd><p>Clears the list of globals that are safe for <code class="docutils literal notranslate"><span class="pre">weights_only</span></code> load.</p>
<dl class="field-list simple">
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="torch.serialization.get_safe_globals">
<span class="sig-prename descclassname"><span class="pre">torch.serialization.</span></span><span class="sig-name descname"><span class="pre">get_safe_globals</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/serialization.py#L271"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.serialization.get_safe_globals" title="Permalink to this definition">#</a></dt>
<dd><p>Returns the list of user-added globals that are safe for <code class="docutils literal notranslate"><span class="pre">weights_only</span></code> load.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.List" title="(in Python v3.13)"><em>List</em></a>[<a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Union" title="(in Python v3.13)"><em>Union</em></a>[<a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Callable" title="(in Python v3.13)"><em>Callable</em></a>, <a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Tuple" title="(in Python v3.13)"><em>Tuple</em></a>[<a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Callable" title="(in Python v3.13)"><em>Callable</em></a>, <a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)">str</a>]]]</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="torch.serialization.get_unsafe_globals_in_checkpoint">
<span class="sig-prename descclassname"><span class="pre">torch.serialization.</span></span><span class="sig-name descname"><span class="pre">get_unsafe_globals_in_checkpoint</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">f</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/serialization.py#L339"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.serialization.get_unsafe_globals_in_checkpoint" title="Permalink to this definition">#</a></dt>
<dd><p>Returns a list of strings of functions/classes in a <code class="docutils literal notranslate"><span class="pre">torch.save</span></code> object that are not safe for <code class="docutils literal notranslate"><span class="pre">weights_only</span></code>.</p>
<p>For a given function or class <code class="docutils literal notranslate"><span class="pre">f</span></code>, the corresponding string will be of the form
<code class="docutils literal notranslate"><span class="pre">{f.__module__}.{f.__name__}</span></code>.</p>
<p>This function will return any GLOBALs in the checkpoint that are not in the set marked safe
for <code class="docutils literal notranslate"><span class="pre">weights_only</span></code> (either via <a class="reference internal" href="#torch.serialization.add_safe_globals" title="torch.serialization.add_safe_globals"><code class="xref py py-func docutils literal notranslate"><span class="pre">add_safe_globals()</span></code></a> or <a class="reference internal" href="#torch.serialization.safe_globals" title="torch.serialization.safe_globals"><code class="xref py py-class docutils literal notranslate"><span class="pre">safe_globals</span></code></a> context or
allowlisted by <code class="docutils literal notranslate"><span class="pre">torch</span></code> by default).</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This function will statically disassemble the pickle file in the checkpoint.
The implication is any classes dynamically pushed onto the stack during unpickling
will not be included in the output.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>f</strong> (<a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Union" title="(in Python v3.13)"><em>Union</em></a><em>[</em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><em>str</em></a><em>, </em><a class="reference external" href="https://docs.python.org/3/library/os.html#os.PathLike" title="(in Python v3.13)"><em>PathLike</em></a><em>, </em><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.BinaryIO" title="(in Python v3.13)"><em>BinaryIO</em></a><em>, </em><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.IO" title="(in Python v3.13)"><em>IO</em></a><em>[</em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#bytes" title="(in Python v3.13)"><em>bytes</em></a><em>]</em><em>]</em>)  File-like object or string containing the checkpoint object saved via <code class="docutils literal notranslate"><span class="pre">torch.save</span></code></p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A list of strings of pickle GLOBALs in the checkpoint that are not allowlisted for <code class="docutils literal notranslate"><span class="pre">weights_only</span></code>.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.List" title="(in Python v3.13)"><em>List</em></a>[<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)">str</a>]</p>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="torch.serialization.safe_globals">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torch.serialization.</span></span><span class="sig-name descname"><span class="pre">safe_globals</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">safe_globals</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/serialization.py#L314"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.serialization.safe_globals" title="Permalink to this definition">#</a></dt>
<dd><p>Context-manager that adds certain globals as safe for <code class="docutils literal notranslate"><span class="pre">weights_only</span></code> load.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>safe_globals</strong> (<a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.List" title="(in Python v3.13)"><em>List</em></a><em>[</em><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Union" title="(in Python v3.13)"><em>Union</em></a><em>[</em><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Callable" title="(in Python v3.13)"><em>Callable</em></a><em>, </em><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Tuple" title="(in Python v3.13)"><em>Tuple</em></a><em>[</em><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Callable" title="(in Python v3.13)"><em>Callable</em></a><em>, </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><em>str</em></a><em>]</em><em>]</em><em>]</em>)  List of globals for weights_only load.</p>
</dd>
</dl>
<p class="rubric">Example</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">tempfile</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">class</span><span class="w"> </span><span class="nc">MyTensor</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
<span class="gp">... </span>    <span class="k">pass</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">t</span> <span class="o">=</span> <span class="n">MyTensor</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">with</span> <span class="n">tempfile</span><span class="o">.</span><span class="n">NamedTemporaryFile</span><span class="p">()</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
<span class="gp">... </span>    <span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">f</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
<span class="go"># Running `torch.load(f.name, weights_only=True)` will fail with</span>
<span class="go"># Unsupported global: GLOBAL __main__.MyTensor was not an allowed global by default.</span>
<span class="go"># Check the code and make sure MyTensor is safe to be used when loaded from an arbitrary checkpoint.</span>
<span class="go">...     with torch.serialization.safe_globals([MyTensor]):</span>
<span class="go">...         torch.load(f.name, weights_only=True)</span>
<span class="go"># MyTensor([[-0.5024, -1.8152, -0.5455],</span>
<span class="go">#          [-0.8234,  2.0500, -0.3657]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">assert</span> <span class="n">torch</span><span class="o">.</span><span class="n">serialization</span><span class="o">.</span><span class="n">get_safe_globals</span><span class="p">()</span> <span class="o">==</span> <span class="p">[]</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="torch.serialization.skip_data">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torch.serialization.</span></span><span class="sig-name descname"><span class="pre">skip_data</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">materialize_fake_tensors</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/2236df1770800ffea5697b11b0bb0d910b2e59e1/torch/serialization.py#L381"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.serialization.skip_data" title="Permalink to this definition">#</a></dt>
<dd><p>Context-manager that skips writing storage bytes for <code class="docutils literal notranslate"><span class="pre">torch.save</span></code> calls.</p>
<p>Storages will still be saved, but the space that their bytes would usually be written to
will be empty space. The storage bytes can then be populated in a separate pass.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>The <code class="docutils literal notranslate"><span class="pre">skip_data</span></code> context manager is an early prototype and is subject to change.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>materialize_fake_tensors</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><em>bool</em></a>)  Whether to materialize FakeTensors.</p>
</dd>
</dl>
<p class="rubric">Example</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">tempfile</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">t</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">with</span> <span class="n">tempfile</span><span class="o">.</span><span class="n">NamedTemporaryFile</span><span class="p">()</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
<span class="gp">... </span>    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">serialization</span><span class="o">.</span><span class="n">skip_data</span><span class="p">():</span>
<span class="gp">... </span>        <span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">f</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
<span class="gp">... </span>    <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">f</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">weights_only</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="go">tensor([[0., 0., 0.],</span>
<span class="go">        [0., 0., 0.]])</span>
</pre></div>
</div>
</dd></dl>

</section>
<section id="module-torch.utils.serialization">
<span id="config"></span><span id="serialization-config"></span><h2><a class="toc-backref" href="#id16" role="doc-backlink">Config</a><a class="headerlink" href="#module-torch.utils.serialization" title="Permalink to this heading">#</a></h2>
<span class="target" id="module-torch.utils.serialization.config"></span><p><code class="docutils literal notranslate"><span class="pre">torch.utils.serialization.config</span></code> provides a global config that can control the behavior of
<code class="docutils literal notranslate"><span class="pre">torch.save</span></code> and <code class="docutils literal notranslate"><span class="pre">torch.load</span></code>.</p>
<p><code class="docutils literal notranslate"><span class="pre">torch.utils.serialization.config.save</span></code> contains options that control the behavior of <code class="docutils literal notranslate"><span class="pre">torch.save</span></code>.</p>
<blockquote>
<div><ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">compute_crc32</span></code>: whether to compute and write the zip file checksum (Default : <code class="docutils literal notranslate"><span class="pre">True</span></code>).
See <a class="reference internal" href="#torch.serialization.set_crc32_options" title="torch.serialization.set_crc32_options"><code class="xref py py-func docutils literal notranslate"><span class="pre">set_crc32_options()</span></code></a>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">use_pinned_memory_for_d2h</span></code>: for storages that are on an accelerator when passed to <code class="docutils literal notranslate"><span class="pre">torch.save</span></code>, whether to
move storage to pinned memory or pageable memory on CPU within <code class="docutils literal notranslate"><span class="pre">torch.save</span></code>. (Default: <code class="docutils literal notranslate"><span class="pre">False</span></code> (i.e. pageable))</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">storage_alignment</span></code>: alignment of storages in the checkpoint during <code class="docutils literal notranslate"><span class="pre">torch.save</span></code> in bytes. (Default <code class="docutils literal notranslate"><span class="pre">64</span></code>)</p></li>
</ul>
</div></blockquote>
<p><code class="docutils literal notranslate"><span class="pre">torch.utils.serialization.config.load</span></code> contains options that control the behavior of <code class="docutils literal notranslate"><span class="pre">torch.load</span></code>.</p>
<blockquote>
<div><ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">mmap</span></code>: See the documentation for <code class="docutils literal notranslate"><span class="pre">mmap</span></code> argument in <a class="reference internal" href="../generated/torch.load.html#torch.load" title="torch.load"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.load()</span></code></a>.
This config will set the behavior of <code class="docutils literal notranslate"><span class="pre">mmap</span></code> for <code class="docutils literal notranslate"><span class="pre">torch.load</span></code> if it is not
already explicitly passed to the <code class="docutils literal notranslate"><span class="pre">torch.load</span></code> call (Default : <code class="docutils literal notranslate"><span class="pre">False</span></code>).</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">endianness</span></code>: See <a class="reference internal" href="#torch.serialization.set_default_load_endianness" title="torch.serialization.set_default_load_endianness"><code class="xref py py-func docutils literal notranslate"><span class="pre">set_default_load_endianness()</span></code></a>.
(Default : <code class="docutils literal notranslate"><span class="pre">torch.serialization.LoadEndianness.NATIVE</span></code>)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">mmap_flags</span></code>: See <a class="reference internal" href="#torch.serialization.set_default_mmap_options" title="torch.serialization.set_default_mmap_options"><code class="xref py py-class docutils literal notranslate"><span class="pre">set_default_mmap_options</span></code></a>.
(Default : <code class="docutils literal notranslate"><span class="pre">MAP_PRIVATE</span></code>)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">calculate_storage_offsets</span></code>: If this config is set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, offsets for storages will be
calculated rather than read via random reads when using <code class="docutils literal notranslate"><span class="pre">torch.load(mmap=True)</span></code>. This minimizes
random reads, which can be helpful when the file is being loaded over a network. (Default : <code class="docutils literal notranslate"><span class="pre">False</span></code>)</p></li>
</ul>
</div></blockquote>
</section>
</section>


  </article>

              
              
                <footer class="bd-footer-article">
                  <div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item">
<div class="feedback">
  
<div class="rating">
    Rate this Page
    <div class="stars">
        
        <span class="star" data-behavior="tutorial-rating" data-count="1" data-value="1"></span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="2" data-value="2"></span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="3" data-value="3"></span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="4" data-value="4"></span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="5" data-value="5"></span>
        
    </div>
</div>

  <div class="feedback-send">
    <button class="feedback-btn"
            onclick="openGitHubIssue()"
            data-bs-title="Create a GitHub Issue"
            data-bs-placement="bottom"
            data-bs-toggle="tooltip">Send Feedback
    </button>
  </div>
</div>


<div class="prev-next-area">
    <a class="left-prev"
       href="randomness.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Reproducibility</p>
      </div>
    </a>
    <a class="right-next"
       href="windows.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Windows FAQ</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>

<div class="footer-info">
  <p class="copyright">
    
      
         Copyright PyTorch Contributors.
      
      <br/>
    
  </p>

  <p class="theme-version">
    Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.15.4.
  </p>
</div></div>
  
</div>
                </footer>
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="randomness.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Reproducibility</p>
      </div>
    </a>
    <a class="right-next"
       href="windows.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Windows FAQ</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc">
<div class="sidebar-secondary-items sidebar-secondary__inner">
    
       <div class="sidebar-secondary-item">
<div
    id="pst-page-navigation-heading-2"
    class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> On this page
  </div>
  <nav class="bd-toc-nav page-toc" aria-labelledby="pst-page-navigation-heading-2">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#saving-and-loading-tensors">Saving and loading tensors</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#saving-and-loading-tensors-preserves-views">Saving and loading tensors preserves views</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#saving-and-loading-torch-nn-modules">Saving and loading torch.nn.Modules</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#serialized-file-format-for-torch-save">Serialized file format for <code class="docutils literal notranslate"><span class="pre">torch.save</span></code></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#torch-load-with-weights-only-true"><code class="docutils literal notranslate"><span class="pre">torch.load</span></code> with <code class="docutils literal notranslate"><span class="pre">weights_only=True</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#troubleshooting-weights-only">Troubleshooting <code class="docutils literal notranslate"><span class="pre">weights_only</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#getting-unsafe-globals">Getting unsafe globals</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#environment-variables">Environment Variables</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#serializing-torch-nn-modules-and-loading-them-in-c">Serializing torch.nn.Modules and loading them in C++</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#saving-and-loading-scriptmodules-across-pytorch-versions">Saving and loading ScriptModules across PyTorch versions</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch-div-performing-integer-division">torch.div performing integer division</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch-full-always-inferring-a-float-dtype">torch.full always inferring a float dtype</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#utility-functions">Utility functions</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.serialization.register_package"><code class="docutils literal notranslate"><span class="pre">register_package()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.serialization.get_crc32_options"><code class="docutils literal notranslate"><span class="pre">get_crc32_options()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.serialization.set_crc32_options"><code class="docutils literal notranslate"><span class="pre">set_crc32_options()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.serialization.get_default_load_endianness"><code class="docutils literal notranslate"><span class="pre">get_default_load_endianness()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.serialization.set_default_load_endianness"><code class="docutils literal notranslate"><span class="pre">set_default_load_endianness()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.serialization.get_default_mmap_options"><code class="docutils literal notranslate"><span class="pre">get_default_mmap_options()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.serialization.set_default_mmap_options"><code class="docutils literal notranslate"><span class="pre">set_default_mmap_options()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.serialization.add_safe_globals"><code class="docutils literal notranslate"><span class="pre">add_safe_globals()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.serialization.clear_safe_globals"><code class="docutils literal notranslate"><span class="pre">clear_safe_globals()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.serialization.get_safe_globals"><code class="docutils literal notranslate"><span class="pre">get_safe_globals()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.serialization.get_unsafe_globals_in_checkpoint"><code class="docutils literal notranslate"><span class="pre">get_unsafe_globals_in_checkpoint()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.serialization.safe_globals"><code class="docutils literal notranslate"><span class="pre">safe_globals</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.serialization.skip_data"><code class="docutils literal notranslate"><span class="pre">skip_data</span></code></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#module-torch.utils.serialization">Config</a></li>
</ul>
  </nav></div>
    
       <div class="sidebar-secondary-item">

  
  <div class="tocsection editthispage">
    <a href="https://github.com/pytorch/pytorch/edit/main/docs/cpp/source/notes/serialization.rst">
      <i class="fa-solid fa-pencil"></i>
      
      
        
          Edit on GitHub
        
      
    </a>
  </div>
</div>
    
       <div class="sidebar-secondary-item">
    <div class="tocsection sourcelink">
      <a href="../_sources/notes/serialization.rst.txt">
        <i class="fa-solid fa-file-lines"></i> Show Source
      </a>
    </div>
</div>
    


<div class="sidebar-secondary-item">
  <div class="sidebar-heading">PyTorch Community</div>
  <ul style="list-style-type: none; padding: 0;">
  
   <li><a class="nav-link nav-external" href="https://pytorch.org/docs/stable/community/index.html" style="color: var(--pst-color-text-muted)">PyTorch Governance</a></li>
  
   <li><a class="nav-link nav-external" href="https://pytorch.org/docs/stable/community/design.html" style="color: var(--pst-color-text-muted)">PyTorch Design Philosophy</a></li>
  
   <li><a class="nav-link nav-external" href="https://github.com/pytorch/pytorch/wiki/The-Ultimate-Guide-to-PyTorch-Contributions" style="color: var(--pst-color-text-muted)">The Ultimate Guide to PyTorch Contributions</a></li>
  
  </ul>
</div>


<div class="sidebar-secondary-item">
 <div class="sidebar-heading">Language Bindings</div>
 <ul style="list-style-type: none; padding: 0;">
 
  <li><a class="nav-link nav-external" href="https://pytorch.org/docs/stable/cpp_index.html" style="color: var(--pst-color-text-muted)">C++</a></li>
 
  <li><a class="nav-link nav-external" href="https://pytorch.org/javadoc/" style="color: var(--pst-color-text-muted)">Javadoc</a></li>
 
  <li><a class="nav-link nav-external" href="https://github.com/pytorch/multipy" style="color: var(--pst-color-text-muted)">torch.multiply</a></li>
 
 </ul>
</div>


<div class="sidebar-secondary-item">
  <div class="sidebar-heading">PyTorch Libraries</div>
  <ul style="list-style-type: none; padding: 0; padding-bottom: 80px;">
  
   <li><a class="nav-link nav-external" href="https://pytorch.org/audio/stable/" style="color: var(--pst-color-text-muted)">torchaudio</a></li>
  
   <li><a class="nav-link nav-external" href="https://pytorch.org/ao" style="color: var(--pst-color-text-muted)">torchao</a></li>
  
   <li><a class="nav-link nav-external" href="https://pytorch.org/executorch" style="color: var(--pst-color-text-muted)">ExecuTorch</a></li>
  
   <li><a class="nav-link nav-external" href="https://pytorch.org/torchrec" style="color: var(--pst-color-text-muted)">torchrec</a></li>
  
   <li><a class="nav-link nav-external" href="https://pytorch.org/serve/" style="color: var(--pst-color-text-muted)">torchserve</a></li>
  
   <li><a class="nav-link nav-external" href="https://pytorch.org/data" style="color: var(--pst-color-text-muted)">torchdata</a></li>
  
   <li><a class="nav-link nav-external" href="https://pytorch.org/data" style="color: var(--pst-color-text-muted)">torchvision</a></li>
  
   <li><a class="nav-link nav-external" href="https://pytorch.org/xla" style="color: var(--pst-color-text-muted)">PyTorch on XLA Devices</a></li>
  
  </ul>
</div>

</div>
</div>
              
            
          </div>
          <footer class="bd-footer-content">
            
          </footer>
        
      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

   <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
      <div class="container">
        <div class="row">
          <div class="col-md-4 text-center">
            <h2>Docs</h2>
            <p>Access comprehensive developer documentation for PyTorch</p>
            <a class="with-right-arrow" href="">View Docs</a>
          </div>

          <div class="col-md-4 text-center">
            <h2>Tutorials</h2>
            <p>Get in-depth tutorials for beginners and advanced developers</p>
            <a class="with-right-arrow" href="">View Tutorials</a>
          </div>

          <div class="col-md-4 text-center">
            <h2>Resources</h2>
            <p>Find development resources and get your questions answered</p>
            <a class="with-right-arrow" href="">View Resources</a>
          </div>
        </div>
      </div>
  </div>

  <footer class="site-footer">
      <div class="container footer-container">
        <div class="footer-logo-wrapper">
          <a href="" class="footer-logo"></a>
        </div>

        <div class="footer-links-wrapper">
          <div class="footer-links-col">
            <ul>
              <li class="list-title"><a href="">PyTorch</a></li>
              <li><a href="">Get Started</a></li>
              <li><a href="">Features</a></li>
              <li><a href="">Ecosystem</a></li>
              <li><a href="">Blog</a></li>
              <li><a href="">Contributing</a></li>
            </ul>
          </div>

          <div class="footer-links-col">
            <ul>
              <li class="list-title"><a href="">Resources</a></li>
              <li><a href="">Tutorials</a></li>
              <li><a href="">Docs</a></li>
              <li><a href="" target="_blank">Discuss</a></li>
              <li><a href="" target="_blank">Github Issues</a></li>
              <li><a href="" target="_blank">Brand Guidelines</a></li>
            </ul>
          </div>

          <div class="footer-links-col">
            <ul>
              <li class="list-title">Stay up to date</li>
              <li><a href="" target="_blank">Facebook</a></li>
              <li><a href="" target="_blank">Twitter</a></li>
              <li><a href="" target="_blank">YouTube</a></li>
              <li><a href="" target="_blank">LinkedIn</a></li>
            </ul>
            </div>

          <div class="footer-links-col">
            <ul>
              <li class="list-title">PyTorch Podcasts</li>
              <li><a href="" target="_blank">Spotify</a></li>
              <li><a href="" target="_blank">Apple</a></li>
              <li><a href="" target="_blank">Google</a></li>
              <li><a href="" target="_blank">Amazon</a></li>
            </ul>
           </div>
          </div>

          <div class="privacy-policy">
            <ul>
              <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/terms/" target="_blank">Terms</a></li>
              <li class="privacy-policy-links">|</li>
              <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/privacy-policy/" target="_blank">Privacy</a></li>
            </ul>
          </div>
          <div class="copyright">
          <p> Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation.
            For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see
            <a href="https://www.linuxfoundation.org/policies/">www.linuxfoundation.org/policies/</a>. The PyTorch Foundation supports the PyTorch open source
            project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC,
            please see <a href="https://www.lfprojects.org/policies/">www.lfprojects.org/policies/</a>.</p>
        </div>
       </div>

  <div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebooks Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="../_static/img/pytorch-x.svg">
  </div>
</div>
 </footer>
   
  <footer class="bd-footer"><div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebooks Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="../_static/img/pytorch-x.svg">
  </div>
</div>
<div class="bd-footer__inner bd-page-width">
  
    <div class="footer-items__start">
      
        <div class="footer-item">

  <p class="copyright">
    
       Copyright PyTorch Contributors.
      <br/>
    
  </p>
</div>
      
        <div class="footer-item">

  <p class="sphinx-version">
    Created using <a href="https://www.sphinx-doc.org/">Sphinx</a> 5.3.0.
    <br/>
  </p>
</div>
      
    </div>
  
  
  
    <div class="footer-items__end">
      
        <div class="footer-item">
<p class="theme-version">
  Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.15.4.
</p></div>
      
    </div>
  
</div>

  </footer>

  </body>
</html>