
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Extending torch.func with autograd.Function &#8212; PyTorch main documentation</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  <!--
    this give us a css class that will be invisible only if js is disabled
  -->
  <noscript>
    <style>
      .pst-js-only { display: none !important; }

    </style>
  </noscript>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=949a1ff5" />
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/katex.min.css" />
    <link rel="stylesheet" type="text/css" href="../_static/katex-math.css?v=91adb8b6" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="../_static/css/jit.css?v=8de1ea5d" />
    <link rel="stylesheet" type="text/css" href="../_static/css/custom.css?v=3ccf5357" />
    <link rel="stylesheet" type="text/css" href="../_static/css/custom2.css?v=3539c01c" />
  
  <!-- So that users can add custom icons -->
  <script src="../_static/scripts/fontawesome.js?digest=8878045cc6db502f8baf"></script>
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf" />

    <script src="../_static/documentation_options.js?v=a8da1a53"></script>
    <script src="../_static/doctools.js?v=9bcbadda"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=940804e7"></script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'notes/extending.func';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="PyTorch Custom Operators Landing Page" href="custom_operators.html" />
    <link rel="prev" title="Extending PyTorch" href="extending.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  <meta name="docsearch:version" content="main (2.6.0 )" />
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>

          <li class="main-menu-item">
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Learn
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="">
                  <span class=dropdown-title>Get Started</span>
                  <p>Run PyTorch locally or get started quickly with one of the supported cloud platforms</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Tutorials</span>
                  <p>Whats new in PyTorch tutorials</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Learn the Basics</span>
                  <p>Familiarize yourself with PyTorch concepts and modules</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">PyTorch Recipes</span>
                  <p>Bite-size, ready-to-deploy PyTorch code examples</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Intro to PyTorch - YouTube Series</span>
                  <p>Master PyTorch basics with our engaging YouTube tutorial series</p>
                </a>
              </div>
            </div>
          </li>

          <li>
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Ecosystem
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Tools</span>
                  <p>Learn about the tools and frameworks in the PyTorch Ecosystem</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class=dropdown-title>Community</span>
                  <p>Join the PyTorch developer community to contribute, learn, and get your questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="" target="_blank">
                  <span class=dropdown-title>Forums</span>
                  <p>A place to discuss PyTorch code, issues, install, research</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class=dropdown-title>Developer Resources</span>
                  <p>Find resources and get questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Contributor Awards - 2024</span>
                  <p>Award winners announced at this year's PyTorch Conference</p>
                </a>
              </div>
            </div>
          </li>

          <li>
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Edge
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">About PyTorch Edge</span>
                  <p>Build innovative and privacy-aware AI experiences for edge devices</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">ExecuTorch</span>
                  <p>End-to-end solution for enabling on-device inference capabilities across mobile and edge devices</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">ExecuTorch Docs</span>
                </a>
              </div>
            </div>  
          </li>

          <li class="main-menu-item">
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Docs
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">PyTorch</span>
                  <p>Explore the documentation for comprehensive guidance on how to use PyTorch</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">PyTorch Domains</span>
                  <p>Read the PyTorch Domains documentation to learn more about domain-specific libraries</p>
                </a>
              </div>
            </div>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Blogs & News 
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">PyTorch Blog</span>
                  <p>Catch up on the latest technical news and happenings</p>
                </a>
                 <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Community Blog</span>
                  <p>Stories from the PyTorch ecosystem</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Videos</span>
                  <p>Learn about the latest PyTorch tutorials, new, and more </p>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Community Stories</span>
                  <p>Learn how our community solves real, everyday machine learning problems with PyTorch</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Events</span>
                  <p>Find events, webinars, and podcasts</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Newsletter</span>
                  <p>Stay up-to-date with the latest updates</p>
                </a>
            </div>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                About
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">PyTorch Foundation</span>
                  <p>Learn more about the PyTorch Foundation</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Governing Board</span>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Cloud Credit Program</span>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Technical Advisory Council</span>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Staff</span>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Contact Us</span>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
            <div class="no-dropdown">
              <a href="" data-cta="join">
                Become a Member
              </a>
            </div>
          </li>
          <li>
           <div class="main-menu-item">
             <a href="" class="github-icon">
             </a>
           </div>
          </li>
          <!--- TODO: This block adds the search icon to the nav bar. We will enable it later. 
          <li>
            <div class="main-menu-item">
             <a href="" class="search-icon">
             </a>
            </div>
          </li>
          --->
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>
  </div>
</div>

  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <dialog id="pst-search-dialog">
    
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </dialog>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
<div class="bd-header__inner bd-page-width">
  <button class="pst-navbar-icon sidebar-toggle primary-toggle" aria-label="Site navigation">
    <span class="fa-solid fa-bars"></span>
  </button>
  
  
  <div class=" navbar-header-items__start">
    
      <div class="navbar-item">
<div class="version-switcher__container dropdown pst-js-only">
  <button id="pst-version-switcher-button-2"
    type="button"
    class="version-switcher__button btn btn-sm dropdown-toggle"
    data-bs-toggle="dropdown"
    aria-haspopup="listbox"
    aria-controls="pst-version-switcher-list-2"
    aria-label="Version switcher list"
  >
    Choose version  <!-- this text may get changed later by javascript -->
    <span class="caret"></span>
  </button>
  <div id="pst-version-switcher-list-2"
    class="version-switcher__menu dropdown-menu list-group-flush py-0"
    role="listbox" aria-labelledby="pst-version-switcher-button-2">
    <!-- dropdown will be populated by javascript on page load -->
  </div>
</div></div>
    
      <div class="navbar-item">

  
    
  

<a class="navbar-brand logo" href="../index.html">
  
  
  
  
  
  
    <p class="title logo__title">Home</p>
  
</a></div>
    
  </div>
  
  <div class=" navbar-header-items">
    
    <div class="me-auto navbar-header-items__center">
      
        <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item ">
  <a class="nav-link nav-internal" href="../python-api/index.html">
    Python API
  </a>
</li>


<li class="nav-item current active">
  <a class="nav-link nav-internal" href="index.html">
    Developer Notes
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../community/index.html">
    Community
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../lang-bindings/index.html">
    Language Bindings
  </a>
</li>

  </ul>
</nav></div>
      
    </div>
    
    
    <div class="navbar-header-items__end">
      
        <div class="navbar-item navbar-persistent--container">
          

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button>
        </div>
      
      
        <div class="navbar-item">

<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button></div>
      
        <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://x.com/PyTorch" title="X" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-x-twitter fa-lg" aria-hidden="true"></i>
            <span class="sr-only">X</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/pytorch/pytorch" title="GitHub" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">GitHub</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://pypi.org/project/torch" title="PyPI" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-custom fa-pypi fa-lg" aria-hidden="true"></i>
            <span class="sr-only">PyPI</span></a>
        </li>
</ul></div>
      
    </div>
    
  </div>
  
  
    <div class="navbar-persistent--mobile">

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button>
    </div>
  

  
    <button class="pst-navbar-icon sidebar-toggle secondary-toggle" aria-label="On this page">
      <span class="fa-solid fa-outdent"></span>
    </button>
  
</div>

    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <dialog id="pst-primary-sidebar-modal"></dialog>
      <div id="pst-primary-sidebar" class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
      <div class="sidebar-header-items__center">
        
          
          
            <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item ">
  <a class="nav-link nav-internal" href="../python-api/index.html">
    Python API
  </a>
</li>


<li class="nav-item current active">
  <a class="nav-link nav-internal" href="index.html">
    Developer Notes
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../community/index.html">
    Community
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../lang-bindings/index.html">
    Language Bindings
  </a>
</li>

  </ul>
</nav></div>
          
        
      </div>
    
    
    
      <div class="sidebar-header-items__end">
        
          <div class="navbar-item">

<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button></div>
        
          <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://x.com/PyTorch" title="X" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-x-twitter fa-lg" aria-hidden="true"></i>
            <span class="sr-only">X</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/pytorch/pytorch" title="GitHub" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">GitHub</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://pypi.org/project/torch" title="PyPI" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-custom fa-pypi fa-lg" aria-hidden="true"></i>
            <span class="sr-only">PyPI</span></a>
        </li>
</ul></div>
        
      </div>
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
<nav class="bd-docs-nav bd-links"
     aria-label="Section Navigation">
  <p class="bd-links__title" role="heading" aria-level="1">Section Navigation</p>
  <div class="bd-toc-item navbar-nav"><p aria-level="2" class="caption" role="heading"><span class="caption-text">Core Concepts</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="autograd.html">Autograd mechanics</a></li>
<li class="toctree-l1"><a class="reference internal" href="gradcheck.html">Gradcheck mechanics</a></li>
<li class="toctree-l1"><a class="reference internal" href="modules.html">Modules</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Device and Hardware Support</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="cuda.html">CUDA semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="hip.html">HIP (ROCm) semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="mps.html">MPS backend</a></li>
<li class="toctree-l1"><a class="reference internal" href="get_start_xpu.html">Getting Started on Intel GPU</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Distributed Training and Deployment</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="ddp.html">Distributed Data Parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="fsdp.html">FSDP Notes</a></li>
<li class="toctree-l1"><a class="reference internal" href="large_scale_deployments.html">Features for large-scale deployments</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Optimization and Performance</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="amp_examples.html">Automatic Mixed Precision examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="cpu_threading_torchscript_inference.html">CPU threading and TorchScript inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="multiprocessing.html">Multiprocessing best practices</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Extending PyTorch</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="extending.html">Extending PyTorch</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Extending torch.func with autograd.Function</a></li>
<li class="toctree-l1"><a class="reference internal" href="custom_operators.html">PyTorch Custom Operators Landing Page</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Numerical Considerations</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="broadcasting.html">Broadcasting semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="numerical_accuracy.html">Numerical accuracy</a></li>
<li class="toctree-l1"><a class="reference internal" href="randomness.html">Reproducibility</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Serialization and Storage</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="serialization.html">Serialization semantics</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">FAQs</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="faq.html">Frequently Asked Questions</a></li>
<li class="toctree-l1"><a class="reference internal" href="windows.html">Windows FAQ</a></li>
</ul>
</div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
      <div class="sidebar-primary-item">
<div id="ethical-ad-placement"
      class="flat"
      data-ea-publisher="readthedocs"
      data-ea-type="readthedocs-sidebar"
      data-ea-manual="true">
</div></div>
  </div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        
          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item">

<nav aria-label="Breadcrumb" class="d-print-none">
  <ul class="bd-breadcrumbs">
    
    <li class="breadcrumb-item breadcrumb-home">
      <a href="../index.html" class="nav-link" aria-label="Home">
        <i class="fa-solid fa-home"></i>
      </a>
    </li>
    
    <li class="breadcrumb-item"><a href="index.html" class="nav-link">Developer Notes</a></li>
    
    <li class="breadcrumb-item active" aria-current="page"><span class="ellipsis">Extending torch.func with autograd.Function</span></li>
  </ul>
</nav>
</div>
      
    </div>
  
  
</div>
</div>
              
              
              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section id="extending-torch-func-with-autograd-function">
<span id="func-autograd-function"></span><h1>Extending torch.func with autograd.Function<a class="headerlink" href="#extending-torch-func-with-autograd-function" title="Link to this heading">#</a></h1>
<p>So you’d like to use <a class="reference internal" href="../python-api/autograd.html#torch.autograd.Function" title="torch.autograd.Function"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.autograd.Function</span></code></a> with the <a class="reference internal" href="../python-api/func.api.html#module-torch.func" title="torch.func"><code class="xref py py-mod docutils literal notranslate"><span class="pre">torch.func</span></code></a>
transforms like <a class="reference internal" href="../python-api/generated/torch.vmap.html#torch.vmap" title="torch.vmap"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.vmap()</span></code></a>, <a class="reference internal" href="../python-api/generated/torch.func.grad.html#torch.func.grad" title="torch.func.grad"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.func.grad()</span></code></a>, etc.</p>
<p>There are two main use cases:</p>
<ul class="simple">
<li><p>you wish to call code that does not contain PyTorch operations and
have it work with function transforms. That is, the <a class="reference internal" href="../python-api/autograd.html#torch.autograd.Function" title="torch.autograd.Function"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.autograd.Function</span></code></a>’s
forward/backward/etc calls into functions from other systems like C++, CUDA, numpy.</p></li>
<li><p>you wish to specify custom gradient rules, like
JAX’s <a class="reference external" href="https://jax.readthedocs.io/en/latest/notebooks/Custom_derivative_rules_for_Python_code.html">custom_vjp/custom_jvp</a></p></li>
</ul>
<p>PyTorch combines both of these concepts into <a class="reference internal" href="../python-api/autograd.html#torch.autograd.Function" title="torch.autograd.Function"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.autograd.Function</span></code></a>.</p>
<section id="basic-usage">
<h2>Basic Usage<a class="headerlink" href="#basic-usage" title="Link to this heading">#</a></h2>
<p>This guide assumes you are familiar with <a class="reference internal" href="extending.html#extending-autograd"><span class="std std-ref">Extending torch.autograd</span></a>,
which explains how to use <a class="reference internal" href="../python-api/autograd.html#torch.autograd.Function" title="torch.autograd.Function"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.autograd.Function</span></code></a>.</p>
<p><a class="reference internal" href="../python-api/autograd.html#torch.autograd.Function" title="torch.autograd.Function"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.autograd.Function</span></code></a> can either have a <a class="reference internal" href="../python-api/generated/torch.autograd.Function.forward.html#torch.autograd.Function.forward" title="torch.autograd.Function.forward"><code class="xref py py-meth docutils literal notranslate"><span class="pre">forward()</span></code></a> that accepts a ctx object,
or it can have separate <a class="reference internal" href="../python-api/generated/torch.autograd.Function.forward.html#torch.autograd.Function.forward" title="torch.autograd.Function.forward"><code class="xref py py-meth docutils literal notranslate"><span class="pre">forward()</span></code></a> (that does not accept <code class="docutils literal notranslate"><span class="pre">ctx</span></code>) and a <code class="xref py py-meth docutils literal notranslate"><span class="pre">setup_context()</span></code>
staticmethod that modifies the <code class="docutils literal notranslate"><span class="pre">ctx</span></code> object.</p>
<p>Only the latter is supported with function transforms:</p>
<ul class="simple">
<li><p><a class="reference internal" href="../python-api/generated/torch.autograd.Function.forward.html#torch.autograd.Function.forward" title="torch.autograd.Function.forward"><code class="xref py py-meth docutils literal notranslate"><span class="pre">forward()</span></code></a> is the code that performs the operation and it should not accept
a <code class="docutils literal notranslate"><span class="pre">ctx</span></code> object.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">setup_context(ctx,</span> <span class="pre">inputs,</span> <span class="pre">output)</span></code> is the code where you can
call methods on <code class="docutils literal notranslate"><span class="pre">ctx</span></code>. Here is where you should save Tensors for backward
(by calling <code class="docutils literal notranslate"><span class="pre">ctx.save_for_backward(*tensors)</span></code>), or save non-Tensors
(by assigning them to the <code class="docutils literal notranslate"><span class="pre">ctx</span></code> object).</p></li>
</ul>
<p>Because <code class="xref py py-meth docutils literal notranslate"><span class="pre">setup_context()</span></code> accepts only <code class="docutils literal notranslate"><span class="pre">inputs</span></code> and <code class="docutils literal notranslate"><span class="pre">output</span></code>,
the only quantities that can be saved are either objects (such as Tensors) in
the inputs or outputs or quantities (like <code class="docutils literal notranslate"><span class="pre">Tensor.shape</span></code>) derived from them.
If you wish to save a non-input intermediate activation from
<a class="reference internal" href="../python-api/generated/torch.autograd.Function.forward.html#torch.autograd.Function.forward" title="torch.autograd.Function.forward"><code class="xref py py-meth docutils literal notranslate"><span class="pre">Function.forward()</span></code></a> for backward, then you’ll need to return it as an
output from <a class="reference internal" href="../python-api/generated/torch.autograd.Function.forward.html#torch.autograd.Function.forward" title="torch.autograd.Function.forward"><code class="xref py py-meth docutils literal notranslate"><span class="pre">forward()</span></code></a> so that it gets passed to
<code class="xref py py-meth docutils literal notranslate"><span class="pre">setup_context()</span></code>.</p>
<p>Depending on the transform,</p>
<ul class="simple">
<li><p>to support reverse-mode AD (<a class="reference internal" href="../python-api/generated/torch.func.grad.html#torch.func.grad" title="torch.func.grad"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.func.grad()</span></code></a>, <a class="reference internal" href="../python-api/generated/torch.func.vjp.html#torch.func.vjp" title="torch.func.vjp"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.func.vjp()</span></code></a>),
the <a class="reference internal" href="../python-api/autograd.html#torch.autograd.Function" title="torch.autograd.Function"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.autograd.Function</span></code></a> needs a <a class="reference internal" href="../python-api/generated/torch.autograd.Function.backward.html#torch.autograd.Function.backward" title="torch.autograd.Function.backward"><code class="xref py py-meth docutils literal notranslate"><span class="pre">backward()</span></code></a> staticmethod.</p></li>
<li><p>to support <a class="reference internal" href="../python-api/generated/torch.vmap.html#torch.vmap" title="torch.vmap"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.vmap()</span></code></a>, the <a class="reference internal" href="../python-api/autograd.html#torch.autograd.Function" title="torch.autograd.Function"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.autograd.Function</span></code></a> needs a <a class="reference internal" href="../python-api/generated/torch.autograd.Function.vmap.html#torch.autograd.Function.vmap" title="torch.autograd.Function.vmap"><code class="xref py py-meth docutils literal notranslate"><span class="pre">vmap()</span></code></a> staticmethod.</p></li>
<li><p>to support <a class="reference internal" href="../python-api/generated/torch.func.jvp.html#torch.func.jvp" title="torch.func.jvp"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.func.jvp()</span></code></a>, the <a class="reference internal" href="../python-api/autograd.html#torch.autograd.Function" title="torch.autograd.Function"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.autograd.Function</span></code></a> needs a <a class="reference internal" href="../python-api/generated/torch.autograd.Function.jvp.html#torch.autograd.Function.jvp" title="torch.autograd.Function.jvp"><code class="xref py py-meth docutils literal notranslate"><span class="pre">jvp()</span></code></a> staticmethod.</p></li>
<li><p>to support compositions of transforms (like <a class="reference internal" href="../python-api/generated/torch.func.jacrev.html#torch.func.jacrev" title="torch.func.jacrev"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.func.jacrev()</span></code></a>,
<a class="reference internal" href="../python-api/generated/torch.func.jacfwd.html#torch.func.jacfwd" title="torch.func.jacfwd"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.func.jacfwd()</span></code></a>, <a class="reference internal" href="../python-api/generated/torch.func.hessian.html#torch.func.hessian" title="torch.func.hessian"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.func.hessian()</span></code></a>) – you may need multiple
of the above.</p></li>
</ul>
<p>In order for the <a class="reference internal" href="../python-api/autograd.html#torch.autograd.Function" title="torch.autograd.Function"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.autograd.Function</span></code></a> to be arbitrarily composable with function
transforms, we recommend that all other staticmethods other than <a class="reference internal" href="../python-api/generated/torch.autograd.Function.forward.html#torch.autograd.Function.forward" title="torch.autograd.Function.forward"><code class="xref py py-meth docutils literal notranslate"><span class="pre">forward()</span></code></a> and
<code class="xref py py-meth docutils literal notranslate"><span class="pre">setup_context()</span></code> must be transformable: that is, they must consist of only PyTorch
operators or call other <a class="reference internal" href="../python-api/autograd.html#torch.autograd.Function" title="torch.autograd.Function"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.autograd.Function</span></code></a> (that may call into C++/CUDA/etc).</p>
<p>Let’s go over some examples of common use cases.</p>
<section id="example-1-autograd-function-calls-into-another-system">
<h3>Example 1: autograd.Function calls into another system<a class="headerlink" href="#example-1-autograd-function-calls-into-another-system" title="Link to this heading">#</a></h3>
<p>A common case is a <a class="reference internal" href="../python-api/autograd.html#torch.autograd.Function" title="torch.autograd.Function"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.autograd.Function</span></code></a> with both forward() and backward() calling
into another system (like C++, CUDA, numpy, triton).</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

<span class="k">def</span><span class="w"> </span><span class="nf">to_numpy</span><span class="p">(</span><span class="n">tensor</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">tensor</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>

<span class="k">class</span><span class="w"> </span><span class="nc">NumpySort</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">Function</span><span class="p">):</span>
    <span class="c1"># Note that forward does not take ctx</span>
    <span class="nd">@staticmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dim</span><span class="p">):</span>
        <span class="n">device</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">device</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">to_numpy</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">ind</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="n">dim</span><span class="p">)</span>
        <span class="n">ind_inv</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">ind</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="n">dim</span><span class="p">)</span>
        <span class="n">result</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">take_along_axis</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">ind</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="n">dim</span><span class="p">)</span>
        <span class="c1"># Any intermediates to be saved in backward must be returned as</span>
        <span class="c1"># outputs.</span>
        <span class="k">return</span> <span class="p">(</span>
            <span class="c1"># The desired output</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">result</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">),</span>
            <span class="c1"># intermediate to save for backward</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">ind</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">),</span>
            <span class="c1"># intermediate to save for backward</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">ind_inv</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">),</span>
        <span class="p">)</span>

    <span class="c1"># setup_context is responsible for calling methods and/or assigning to</span>
    <span class="c1"># the ctx object. Please do not do additional compute (e.g. add</span>
    <span class="c1"># Tensors together) in setup_context.</span>
    <span class="nd">@staticmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">setup_context</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">output</span><span class="p">):</span>
        <span class="n">x</span><span class="p">,</span> <span class="n">dim</span> <span class="o">=</span> <span class="n">inputs</span>
        <span class="c1"># Note that output is whatever you returned from forward.</span>
        <span class="c1"># If you returned multiple values, then output is a Tuple of multiple values.</span>
        <span class="c1"># If you returned a single Tensor, then output is a Tensor.</span>
        <span class="c1"># If you returned a Tuple with a single Tensor, then output is a</span>
        <span class="c1"># Tuple with a single Tensor.</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">ind</span><span class="p">,</span> <span class="n">ind_inv</span> <span class="o">=</span> <span class="n">output</span>
        <span class="n">ctx</span><span class="o">.</span><span class="n">mark_non_differentiable</span><span class="p">(</span><span class="n">ind</span><span class="p">,</span> <span class="n">ind_inv</span><span class="p">)</span>
        <span class="c1"># Tensors must be saved via ctx.save_for_backward. Please do not</span>
        <span class="c1"># assign them directly onto the ctx object.</span>
        <span class="n">ctx</span><span class="o">.</span><span class="n">save_for_backward</span><span class="p">(</span><span class="n">ind</span><span class="p">,</span> <span class="n">ind_inv</span><span class="p">)</span>
        <span class="c1"># Non-tensors may be saved by assigning them as attributes on the ctx object.</span>
        <span class="n">ctx</span><span class="o">.</span><span class="n">dim</span> <span class="o">=</span> <span class="n">dim</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">backward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">grad_output</span><span class="p">,</span> <span class="n">_0</span><span class="p">,</span> <span class="n">_1</span><span class="p">):</span>
        <span class="c1"># For the autograd.Function to be arbitrarily composable with function</span>
        <span class="c1"># transforms, all staticmethod other than forward and setup_context</span>
        <span class="c1"># must be implemented in a &quot;transformable&quot; way; that is, they must</span>
        <span class="c1"># only consist of PyTorch operations or autograd.Function.</span>
        <span class="c1">#</span>
        <span class="c1"># For example, this allows us to do double backwards and/or compute</span>
        <span class="c1"># second order gradients.</span>
        <span class="c1">#</span>
        <span class="c1"># We&#39;ve written the backward pass of NumpySort in terms of another</span>
        <span class="c1"># autograd.Function, NumpyTake.</span>
        <span class="n">ind</span><span class="p">,</span> <span class="n">ind_inv</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">saved_tensors</span>
        <span class="k">return</span> <span class="n">NumpyTake</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">grad_output</span><span class="p">,</span> <span class="n">ind_inv</span><span class="p">,</span> <span class="n">ind</span><span class="p">,</span> <span class="n">ctx</span><span class="o">.</span><span class="n">dim</span><span class="p">),</span> <span class="kc">None</span>

<span class="k">class</span><span class="w"> </span><span class="nc">NumpyTake</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">Function</span><span class="p">):</span>
    <span class="nd">@staticmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">ind</span><span class="p">,</span> <span class="n">ind_inv</span><span class="p">,</span> <span class="n">dim</span><span class="p">):</span>
        <span class="n">device</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">device</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">to_numpy</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">ind</span> <span class="o">=</span> <span class="n">to_numpy</span><span class="p">(</span><span class="n">ind</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">take_along_axis</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">ind</span><span class="p">,</span> <span class="n">dim</span><span class="p">),</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">setup_context</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">output</span><span class="p">):</span>
        <span class="n">x</span><span class="p">,</span> <span class="n">ind</span><span class="p">,</span> <span class="n">ind_inv</span><span class="p">,</span> <span class="n">dim</span> <span class="o">=</span> <span class="n">inputs</span>
        <span class="n">ctx</span><span class="o">.</span><span class="n">save_for_backward</span><span class="p">(</span><span class="n">ind</span><span class="p">,</span> <span class="n">ind_inv</span><span class="p">)</span>
        <span class="n">ctx</span><span class="o">.</span><span class="n">dim</span> <span class="o">=</span> <span class="n">dim</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">backward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">grad_output</span><span class="p">):</span>
        <span class="n">ind</span><span class="p">,</span> <span class="n">ind_inv</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">saved_tensors</span>
        <span class="n">result</span> <span class="o">=</span> <span class="n">NumpyTake</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">grad_output</span><span class="p">,</span> <span class="n">ind_inv</span><span class="p">,</span> <span class="n">ind</span><span class="p">,</span> <span class="n">ctx</span><span class="o">.</span><span class="n">dim</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">result</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span>
</pre></div>
</div>
<p>Now, to make it easier to use <code class="docutils literal notranslate"><span class="pre">NumpySort</span></code> (to hide away the intermediates we
returned as outputs, as well as allow default args and kwargs), we create a new
function that invokes it:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">numpy_sort</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">):</span>
    <span class="n">result</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">NumpySort</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dim</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">result</span>
</pre></div>
</div>
<p>And here’s a sanity check:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">grad_x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">func</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">numpy_sort</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">())(</span><span class="n">x</span><span class="p">)</span>
<span class="k">assert</span> <span class="n">torch</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">grad_x</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
</pre></div>
</div>
</section>
<section id="example-2-autograd-function-specifies-custom-gradient-rules">
<h3>Example 2: autograd.Function specifies custom gradient rules<a class="headerlink" href="#example-2-autograd-function-specifies-custom-gradient-rules" title="Link to this heading">#</a></h3>
<p>Another common case is an <a class="reference internal" href="../python-api/autograd.html#torch.autograd.Function" title="torch.autograd.Function"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.autograd.Function</span></code></a> that is implemented with PyTorch
operations. PyTorch is able to compute gradients for PyTorch operations automatically,
but perhaps we wish to customize how the gradients are computed. Some reasons why
we may want a custom backward different from the one PyTorch gives us are:</p>
<ul class="simple">
<li><p>improving numeric stability</p></li>
<li><p>changing the performance characteristics of the backward</p></li>
<li><p>changing how edge cases are handled (e.g. nans, inf)</p></li>
<li><p>modifying the gradient (e.g. gradient clipping)</p></li>
</ul>
<p>Here’s an example of an <a class="reference internal" href="../python-api/autograd.html#torch.autograd.Function" title="torch.autograd.Function"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.autograd.Function</span></code></a> for the function <code class="docutils literal notranslate"><span class="pre">y</span> <span class="pre">=</span> <span class="pre">x</span> <span class="pre">**</span> <span class="pre">3</span></code> where we
change the performance characteristics (some computation that would normally happen
during the backward pass, computing dx, happens in the forward pass).</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">MyCube</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">Function</span><span class="p">):</span>
    <span class="nd">@staticmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
        <span class="n">result</span> <span class="o">=</span> <span class="n">x</span> <span class="o">**</span> <span class="mi">3</span>
        <span class="c1"># In regular PyTorch, if we had just run y = x ** 3, then the backward</span>
        <span class="c1"># pass computes dx = 3 * x ** 2. In this autograd.Function, we&#39;ve done</span>
        <span class="c1"># that computation here in the forward pass instead.</span>
        <span class="n">dx</span> <span class="o">=</span> <span class="mi">3</span> <span class="o">*</span> <span class="n">x</span> <span class="o">**</span> <span class="mi">2</span>
        <span class="k">return</span> <span class="n">result</span><span class="p">,</span> <span class="n">dx</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">setup_context</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">output</span><span class="p">):</span>
        <span class="n">x</span><span class="p">,</span> <span class="o">=</span> <span class="n">inputs</span>
        <span class="n">result</span><span class="p">,</span> <span class="n">dx</span> <span class="o">=</span> <span class="n">output</span>
        <span class="n">ctx</span><span class="o">.</span><span class="n">save_for_backward</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dx</span><span class="p">)</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">backward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">grad_output</span><span class="p">,</span> <span class="n">grad_dx</span><span class="p">):</span>
        <span class="n">x</span><span class="p">,</span> <span class="n">dx</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">saved_tensors</span>
        <span class="c1"># In order for the autograd.Function to work with higher-order</span>
        <span class="c1"># gradients, we must add the gradient contribution of `dx`.</span>
        <span class="n">result</span> <span class="o">=</span> <span class="n">grad_output</span> <span class="o">*</span> <span class="n">dx</span> <span class="o">+</span> <span class="n">grad_dx</span> <span class="o">*</span> <span class="mi">6</span> <span class="o">*</span> <span class="n">x</span>
        <span class="k">return</span> <span class="n">result</span>
</pre></div>
</div>
<p>Now, to make it easier to use <code class="docutils literal notranslate"><span class="pre">NumpySort</span></code> (and hide away the intermediates we
returned as outputs) we create a new function that invokes it:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">my_cube</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="n">result</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">MyCube</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">result</span>
</pre></div>
</div>
<p>Here’s a sanity check computing the second-order gradients:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">([])</span>
<span class="n">ggx</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">func</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">func</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">my_cube</span><span class="p">))(</span><span class="n">x</span><span class="p">)</span>
<span class="k">assert</span> <span class="n">torch</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">ggx</span><span class="p">,</span> <span class="mi">6</span> <span class="o">*</span> <span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="limitations-and-gotchas">
<h3>Limitations and gotchas<a class="headerlink" href="#limitations-and-gotchas" title="Link to this heading">#</a></h3>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Please read these limitations of <a class="reference internal" href="../python-api/autograd.html#torch.autograd.Function" title="torch.autograd.Function"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.autograd.Function</span></code></a> with torch.func transforms
carefully. We are not able to catch many of these situations and error out
gracefully so they will lead to undefined behavior.</p>
</div>
<p>Please do not capture Tensors that are being transformed over, have
requires_grad=True, or are dual tensors, into the methods of the
<a class="reference internal" href="../python-api/autograd.html#torch.autograd.Function" title="torch.autograd.Function"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.autograd.Function</span></code></a>. The way to be completely safe is to ensure that the only
Tensors being used inside any method of the <a class="reference internal" href="../python-api/autograd.html#torch.autograd.Function" title="torch.autograd.Function"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.autograd.Function</span></code></a> must be directly
passed as inputs (or via the ctx object) rather than come from outside
the <a class="reference internal" href="../python-api/autograd.html#torch.autograd.Function" title="torch.autograd.Function"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.autograd.Function</span></code></a>.</p>
<p><a class="reference internal" href="../python-api/autograd.html#torch.autograd.Function" title="torch.autograd.Function"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.autograd.Function</span></code></a> does not handle Tensors in pytrees (arbitrary nested
Python data structures that may or may not contain Tensors). For
those Tensors to be tracked by autograd, they must be passed directly as
an argument to <a class="reference internal" href="../python-api/autograd.html#torch.autograd.Function" title="torch.autograd.Function"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.autograd.Function</span></code></a>. This is in contrast to
jax.{custom_vjp, custom_jvp}, which do accept pytrees.</p>
<p>Please only use <a class="reference internal" href="../python-api/generated/torch.autograd.function.FunctionCtx.save_for_backward.html#torch.autograd.function.FunctionCtx.save_for_backward" title="torch.autograd.function.FunctionCtx.save_for_backward"><code class="xref py py-meth docutils literal notranslate"><span class="pre">save_for_backward()</span></code></a> or
<code class="xref py py-meth docutils literal notranslate"><span class="pre">save_for_forward()</span></code> to save Tensors.
Please do not assign Tensors or collections of Tensors directly onto the ctx object -
these Tensors will not get tracked</p>
</section>
</section>
<section id="torch-vmap-support">
<h2><a class="reference internal" href="../python-api/generated/torch.vmap.html#torch.vmap" title="torch.vmap"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.vmap()</span></code></a> Support<a class="headerlink" href="#torch-vmap-support" title="Link to this heading">#</a></h2>
<p>To use an <a class="reference internal" href="../python-api/autograd.html#torch.autograd.Function" title="torch.autograd.Function"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.autograd.Function</span></code></a> with <a class="reference internal" href="../python-api/generated/torch.vmap.html#torch.vmap" title="torch.vmap"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.vmap()</span></code></a>, you must either:</p>
<ul class="simple">
<li><p>provide a <a class="reference internal" href="../python-api/generated/torch.autograd.Function.vmap.html#torch.autograd.Function.vmap" title="torch.autograd.Function.vmap"><code class="xref py py-meth docutils literal notranslate"><span class="pre">vmap()</span></code></a> staticmethod that tells us the behavior of the <a class="reference internal" href="../python-api/autograd.html#torch.autograd.Function" title="torch.autograd.Function"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.autograd.Function</span></code></a>
under <a class="reference internal" href="../python-api/generated/torch.vmap.html#torch.vmap" title="torch.vmap"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.vmap()</span></code></a></p></li>
<li><p>ask us to autogenerate it by setting <code class="docutils literal notranslate"><span class="pre">generate_vmap_rule=True</span></code>.</p></li>
</ul>
<section id="automatically-generate-a-vmap-rule">
<h3>Automatically generate a vmap rule<a class="headerlink" href="#automatically-generate-a-vmap-rule" title="Link to this heading">#</a></h3>
<p>If your <a class="reference internal" href="../python-api/autograd.html#torch.autograd.Function" title="torch.autograd.Function"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.autograd.Function</span></code></a> fulfills the following additional constraints, then we
are able to generate a vmap rule for it. If it doesn’t fulfill the constraints or if you
want custom behavior under vmap, please manually define a vmap staticmethod (see next section).</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>We are not easily able to check for the following constraints and error
out gracefully. Violation of the constraints may lead to undefined
behavior.</p>
</div>
<ul class="simple">
<li><p>The <a class="reference internal" href="../python-api/autograd.html#torch.autograd.Function" title="torch.autograd.Function"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.autograd.Function</span></code></a>’s <a class="reference internal" href="../python-api/generated/torch.autograd.Function.forward.html#torch.autograd.Function.forward" title="torch.autograd.Function.forward"><code class="xref py py-meth docutils literal notranslate"><span class="pre">forward()</span></code></a>, <a class="reference internal" href="../python-api/generated/torch.autograd.Function.backward.html#torch.autograd.Function.backward" title="torch.autograd.Function.backward"><code class="xref py py-meth docutils literal notranslate"><span class="pre">backward()</span></code></a> (if it exists) and <a class="reference internal" href="../python-api/generated/torch.autograd.Function.jvp.html#torch.autograd.Function.jvp" title="torch.autograd.Function.jvp"><code class="xref py py-meth docutils literal notranslate"><span class="pre">jvp()</span></code></a>
(if it exists) staticmethods must be transformable via <a class="reference internal" href="../python-api/generated/torch.vmap.html#torch.vmap" title="torch.vmap"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.vmap()</span></code></a>. That
is, they must consist of only PyTorch operations (as opposed to e.g. NumPy or custom
CUDA kernels).</p></li>
</ul>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">MyCube</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">Function</span><span class="p">):</span>
    <span class="c1"># Set generate_vmap_rule to True to ask PyTorch to automatically generate</span>
    <span class="c1"># a vmap rule.</span>
    <span class="n">generate_vmap_rule</span> <span class="o">=</span> <span class="kc">True</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
        <span class="n">result</span> <span class="o">=</span> <span class="n">x</span> <span class="o">**</span> <span class="mi">3</span>
        <span class="n">dx</span> <span class="o">=</span> <span class="mi">3</span> <span class="o">*</span> <span class="n">x</span> <span class="o">**</span> <span class="mi">2</span>
        <span class="k">return</span> <span class="n">result</span><span class="p">,</span> <span class="n">dx</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">setup_context</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">output</span><span class="p">):</span>
        <span class="n">x</span><span class="p">,</span> <span class="o">=</span> <span class="n">inputs</span>
        <span class="n">result</span><span class="p">,</span> <span class="n">dx</span> <span class="o">=</span> <span class="n">output</span>
        <span class="n">ctx</span><span class="o">.</span><span class="n">save_for_backward</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dx</span><span class="p">)</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">backward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">grad_output</span><span class="p">,</span> <span class="n">grad_dx</span><span class="p">):</span>
        <span class="n">x</span><span class="p">,</span> <span class="n">dx</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">saved_tensors</span>
        <span class="n">result</span> <span class="o">=</span> <span class="n">grad_output</span> <span class="o">*</span> <span class="n">dx</span> <span class="o">+</span> <span class="n">grad_dx</span> <span class="o">*</span> <span class="mi">6</span> <span class="o">*</span> <span class="n">x</span>
        <span class="k">return</span> <span class="n">result</span>

<span class="k">def</span><span class="w"> </span><span class="nf">my_cube</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="n">result</span><span class="p">,</span> <span class="n">dx</span> <span class="o">=</span> <span class="n">MyCube</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">result</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">vmap</span><span class="p">(</span><span class="n">my_cube</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<span class="k">assert</span> <span class="n">torch</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">result</span><span class="p">,</span> <span class="n">x</span> <span class="o">**</span> <span class="mi">3</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="defining-the-vmap-staticmethod">
<h3>Defining the vmap staticmethod<a class="headerlink" href="#defining-the-vmap-staticmethod" title="Link to this heading">#</a></h3>
<p>If your <a class="reference internal" href="../python-api/autograd.html#torch.autograd.Function" title="torch.autograd.Function"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.autograd.Function</span></code></a> calls into another system (like NumPy, C++, CUDA, triton),
then to get it to work with <a class="reference internal" href="../python-api/generated/torch.vmap.html#torch.vmap" title="torch.vmap"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.vmap()</span></code></a> or transforms that use it, you’ll
need to manually define a <a class="reference internal" href="../python-api/generated/torch.autograd.Function.vmap.html#torch.autograd.Function.vmap" title="torch.autograd.Function.vmap"><code class="xref py py-meth docutils literal notranslate"><span class="pre">vmap()</span></code></a> staticmethod.</p>
<p>Depending on what transforms you want to use and your use case, you may not need
to add a <a class="reference internal" href="../python-api/generated/torch.autograd.Function.vmap.html#torch.autograd.Function.vmap" title="torch.autograd.Function.vmap"><code class="xref py py-meth docutils literal notranslate"><span class="pre">vmap()</span></code></a> staticmethod to all of your <a class="reference internal" href="../python-api/autograd.html#torch.autograd.Function" title="torch.autograd.Function"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.autograd.Function</span></code></a>:</p>
<ul class="simple">
<li><p>For example, <a class="reference internal" href="../python-api/generated/torch.func.jacrev.html#torch.func.jacrev" title="torch.func.jacrev"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.func.jacrev()</span></code></a> performs <a class="reference internal" href="../python-api/generated/torch.vmap.html#torch.vmap" title="torch.vmap"><code class="xref py py-func docutils literal notranslate"><span class="pre">vmap()</span></code></a> over the backward pass.
So if you’re only interested in using <a class="reference internal" href="../python-api/generated/torch.func.jacrev.html#torch.func.jacrev" title="torch.func.jacrev"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.func.jacrev()</span></code></a>, only
the <a class="reference internal" href="../python-api/generated/torch.autograd.Function.backward.html#torch.autograd.Function.backward" title="torch.autograd.Function.backward"><code class="xref py py-meth docutils literal notranslate"><span class="pre">backward()</span></code></a> staticmethod needs to be vmappable.</p></li>
</ul>
<p>We do recommend ensuring all of your <a class="reference internal" href="../python-api/autograd.html#torch.autograd.Function" title="torch.autograd.Function"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.autograd.Function</span></code></a> have support for
<a class="reference internal" href="../python-api/generated/torch.vmap.html#torch.vmap" title="torch.vmap"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.vmap()</span></code></a> though, especially if you are writing a third-party library and you want your
<a class="reference internal" href="../python-api/autograd.html#torch.autograd.Function" title="torch.autograd.Function"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.autograd.Function</span></code></a> to work with all combinations of <a class="reference internal" href="../python-api/func.api.html#module-torch.func" title="torch.func"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.func()</span></code></a> transforms.</p>
<p>Conceptually, the vmap staticmethod is responsible for defining how the <a class="reference internal" href="../python-api/generated/torch.autograd.Function.forward.html#torch.autograd.Function.forward" title="torch.autograd.Function.forward"><code class="xref py py-meth docutils literal notranslate"><span class="pre">forward()</span></code></a>
should behave under <a class="reference internal" href="../python-api/generated/torch.vmap.html#torch.vmap" title="torch.vmap"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.vmap()</span></code></a>. That is, it defines how to transform
the <a class="reference internal" href="../python-api/generated/torch.autograd.Function.forward.html#torch.autograd.Function.forward" title="torch.autograd.Function.forward"><code class="xref py py-meth docutils literal notranslate"><span class="pre">forward()</span></code></a> to run over inputs with an additional dimension (the dimension
being vmapped over). This is similar to how <a class="reference internal" href="../python-api/generated/torch.vmap.html#torch.vmap" title="torch.vmap"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.vmap()</span></code></a> is implemented over
PyTorch operations: for each operation, we define a vmap rule (sometimes also
referred to as a “batching rule”).</p>
<p>Here’s how to define the <a class="reference internal" href="../python-api/generated/torch.autograd.Function.vmap.html#torch.autograd.Function.vmap" title="torch.autograd.Function.vmap"><code class="xref py py-meth docutils literal notranslate"><span class="pre">vmap()</span></code></a> staticmethod:</p>
<ul class="simple">
<li><p>the signature is <code class="docutils literal notranslate"><span class="pre">vmap(info,</span> <span class="pre">in_dims:</span> <span class="pre">Tuple[Optional[int]],</span> <span class="pre">*args)</span></code>, where
<code class="docutils literal notranslate"><span class="pre">*args</span></code> is the same as the args to <a class="reference internal" href="../python-api/generated/torch.autograd.Function.forward.html#torch.autograd.Function.forward" title="torch.autograd.Function.forward"><code class="xref py py-meth docutils literal notranslate"><span class="pre">forward()</span></code></a>.</p></li>
<li><p>The vmap staticmethod is responsible for defining how the <a class="reference internal" href="../python-api/generated/torch.autograd.Function.forward.html#torch.autograd.Function.forward" title="torch.autograd.Function.forward"><code class="xref py py-meth docutils literal notranslate"><span class="pre">forward()</span></code></a> should behave
under <a class="reference internal" href="../python-api/generated/torch.vmap.html#torch.vmap" title="torch.vmap"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.vmap()</span></code></a>. That is, given inputs with an additional dimension
(specified by <code class="docutils literal notranslate"><span class="pre">in_dims</span></code>), how do we compute the batched version of <a class="reference internal" href="../python-api/generated/torch.autograd.Function.forward.html#torch.autograd.Function.forward" title="torch.autograd.Function.forward"><code class="xref py py-meth docutils literal notranslate"><span class="pre">forward()</span></code></a>?</p></li>
<li><p>For each arg in <code class="docutils literal notranslate"><span class="pre">args</span></code>, <code class="docutils literal notranslate"><span class="pre">in_dims</span></code> has a corresponding <code class="docutils literal notranslate"><span class="pre">Optional[int]</span></code>.
It is <code class="docutils literal notranslate"><span class="pre">None</span></code> if the arg is not a Tensor or if the arg is not being vmapped over,
otherwise, it is an integer specifying what dimension of the Tensor is being vmapped
over.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">info</span></code> is a collection of additional metadata that may be helpful:
<code class="docutils literal notranslate"><span class="pre">info.batch_size</span></code> specifies the size of the dimension being vmapped over, while
<code class="docutils literal notranslate"><span class="pre">info.randomness</span></code> is the <code class="docutils literal notranslate"><span class="pre">randomness</span></code> option that was passed to <a class="reference internal" href="../python-api/generated/torch.vmap.html#torch.vmap" title="torch.vmap"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.vmap()</span></code></a>.</p></li>
<li><p>The return of the vmap staticmethod is a tuple of <code class="docutils literal notranslate"><span class="pre">(output,</span> <span class="pre">out_dims)</span></code>. Similar
to <code class="docutils literal notranslate"><span class="pre">in_dims</span></code>, <code class="docutils literal notranslate"><span class="pre">out_dims</span></code> should be of the same structure as <code class="docutils literal notranslate"><span class="pre">output</span></code> and contain
one <code class="docutils literal notranslate"><span class="pre">out_dim</span></code> per output that specifies if the output has the vmapped
dimension and what index it is in.</p></li>
</ul>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">to_numpy</span><span class="p">(</span><span class="n">tensor</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">tensor</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>

<span class="k">class</span><span class="w"> </span><span class="nc">NumpySort</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">Function</span><span class="p">):</span>
    <span class="nd">@staticmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dim</span><span class="p">):</span>
        <span class="n">device</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">device</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">to_numpy</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">ind</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="n">dim</span><span class="p">)</span>
        <span class="n">ind_inv</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">ind</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="n">dim</span><span class="p">)</span>
        <span class="n">result</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">take_along_axis</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">ind</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="n">dim</span><span class="p">)</span>
        <span class="k">return</span> <span class="p">(</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">result</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">),</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">ind</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">),</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">ind_inv</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">),</span>
        <span class="p">)</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">setup_context</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">output</span><span class="p">):</span>
        <span class="n">x</span><span class="p">,</span> <span class="n">dim</span> <span class="o">=</span> <span class="n">inputs</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">ind</span><span class="p">,</span> <span class="n">ind_inv</span> <span class="o">=</span> <span class="n">output</span>
        <span class="n">ctx</span><span class="o">.</span><span class="n">mark_non_differentiable</span><span class="p">(</span><span class="n">ind</span><span class="p">,</span> <span class="n">ind_inv</span><span class="p">)</span>
        <span class="n">ctx</span><span class="o">.</span><span class="n">save_for_backward</span><span class="p">(</span><span class="n">ind</span><span class="p">,</span> <span class="n">ind_inv</span><span class="p">)</span>
        <span class="n">ctx</span><span class="o">.</span><span class="n">dim</span> <span class="o">=</span> <span class="n">dim</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">backward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">grad_output</span><span class="p">,</span> <span class="n">_0</span><span class="p">,</span> <span class="n">_1</span><span class="p">):</span>
        <span class="n">ind</span><span class="p">,</span> <span class="n">ind_inv</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">saved_tensors</span>
        <span class="k">return</span> <span class="n">NumpyTake</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">grad_output</span><span class="p">,</span> <span class="n">ind_inv</span><span class="p">,</span> <span class="n">ind</span><span class="p">,</span> <span class="n">ctx</span><span class="o">.</span><span class="n">dim</span><span class="p">),</span> <span class="kc">None</span>

    <span class="c1"># The signature of the vmap staticmethod is:</span>
    <span class="c1"># vmap(info, in_dims: Tuple[Optional[int]], *args)</span>
    <span class="c1"># where *args is the same as the arguments to `forward`.</span>
    <span class="nd">@staticmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">vmap</span><span class="p">(</span><span class="n">info</span><span class="p">,</span> <span class="n">in_dims</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">dim</span><span class="p">):</span>
        <span class="c1"># For every input (x and dim), in_dims stores an Optional[int]</span>
        <span class="c1"># that is:</span>
        <span class="c1"># - None if the input is not being vmapped over or if the input</span>
        <span class="c1">#   is not a Tensor</span>
        <span class="c1"># - an integer if the input is being vmapped over that represents</span>
        <span class="c1">#   the index of the dimension being vmapped over.</span>
        <span class="n">x_bdim</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">in_dims</span>

        <span class="c1"># A &quot;vmap rule&quot; is the logic of how to perform the operation given</span>
        <span class="c1"># inputs with one additional dimension. In NumpySort, x has an</span>
        <span class="c1"># additional dimension (x_bdim). The vmap rule is simply</span>
        <span class="c1"># to call NumpySort again but pass it a different `dim`.</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">movedim</span><span class="p">(</span><span class="n">x_bdim</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
        <span class="c1"># Handle negative dims correctly</span>
        <span class="n">dim</span> <span class="o">=</span> <span class="n">dim</span> <span class="k">if</span> <span class="n">dim</span> <span class="o">&gt;=</span> <span class="mi">0</span> <span class="k">else</span> <span class="n">dim</span> <span class="o">+</span> <span class="n">x</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">-</span> <span class="mi">1</span>
        <span class="n">result</span> <span class="o">=</span> <span class="n">NumpySort</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dim</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>

        <span class="c1"># The vmap rule must return a tuple of two things</span>
        <span class="c1"># 1. the output. Should be the same amount of things</span>
        <span class="c1">#    as returned by the forward().</span>
        <span class="c1"># 2. one Optional[int] for each output specifying if each output</span>
        <span class="c1"># is being vmapped over, and if so, the index of the</span>
        <span class="c1"># dimension being vmapped over.</span>
        <span class="c1">#</span>
        <span class="c1"># NumpySort.forward returns a Tuple of 3 Tensors. Since we moved the</span>
        <span class="c1"># dimension being vmapped over to the front of `x`, that appears at</span>
        <span class="c1"># dimension 0 of all outputs.</span>
        <span class="c1"># The return is (output, out_dims) -- output is a tuple of 3 Tensors</span>
        <span class="c1"># and out_dims is a Tuple of 3 Optional[int]</span>
        <span class="k">return</span> <span class="n">NumpySort</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dim</span> <span class="o">+</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

<span class="k">class</span><span class="w"> </span><span class="nc">NumpyTake</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">Function</span><span class="p">):</span>
    <span class="nd">@staticmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">ind</span><span class="p">,</span> <span class="n">ind_inv</span><span class="p">,</span> <span class="n">dim</span><span class="p">):</span>
        <span class="n">device</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">device</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">to_numpy</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">ind</span> <span class="o">=</span> <span class="n">to_numpy</span><span class="p">(</span><span class="n">ind</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">take_along_axis</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">ind</span><span class="p">,</span> <span class="n">dim</span><span class="p">),</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">setup_context</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">output</span><span class="p">):</span>
        <span class="n">x</span><span class="p">,</span> <span class="n">ind</span><span class="p">,</span> <span class="n">ind_inv</span><span class="p">,</span> <span class="n">dim</span> <span class="o">=</span> <span class="n">inputs</span>
        <span class="n">ctx</span><span class="o">.</span><span class="n">save_for_backward</span><span class="p">(</span><span class="n">ind</span><span class="p">,</span> <span class="n">ind_inv</span><span class="p">)</span>
        <span class="n">ctx</span><span class="o">.</span><span class="n">dim</span> <span class="o">=</span> <span class="n">dim</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">backward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">grad_output</span><span class="p">):</span>
        <span class="n">ind</span><span class="p">,</span> <span class="n">ind_inv</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">saved_tensors</span>
        <span class="n">result</span> <span class="o">=</span> <span class="n">NumpyTake</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">grad_output</span><span class="p">,</span> <span class="n">ind_inv</span><span class="p">,</span> <span class="n">ind</span><span class="p">,</span> <span class="n">ctx</span><span class="o">.</span><span class="n">dim</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">result</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">vmap</span><span class="p">(</span><span class="n">info</span><span class="p">,</span> <span class="n">in_dims</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">ind</span><span class="p">,</span> <span class="n">ind_inv</span><span class="p">,</span> <span class="n">dim</span><span class="p">):</span>
        <span class="n">x_bdim</span><span class="p">,</span> <span class="n">ind_bdim</span><span class="p">,</span> <span class="n">ind_inv_bdim</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">in_dims</span>

        <span class="c1"># The strategy is: expand {x, ind, ind_inv} to all have the dimension</span>
        <span class="c1"># being vmapped over.</span>
        <span class="c1"># Then, call back into NumpyTake(expanded_x, expanded_ind, expanded_ind_inv, new_dim).</span>

        <span class="c1"># Handle negative dims by wrapping them to be positive</span>
        <span class="n">logical_dim</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="k">if</span> <span class="n">x_bdim</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">x_bdim</span> <span class="o">-</span> <span class="mi">1</span>
        <span class="n">dim</span> <span class="o">=</span> <span class="n">dim</span> <span class="k">if</span> <span class="n">dim</span> <span class="o">&gt;=</span> <span class="mi">0</span> <span class="k">else</span> <span class="n">dim</span> <span class="o">+</span> <span class="n">logical_dim</span>

        <span class="k">def</span><span class="w"> </span><span class="nf">maybe_expand_bdim_at_front</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x_bdim</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">x_bdim</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">x</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="n">info</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">*</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">x</span><span class="o">.</span><span class="n">movedim</span><span class="p">(</span><span class="n">x_bdim</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

        <span class="c1"># If the Tensor doesn&#39;t have the dimension being vmapped over,</span>
        <span class="c1"># expand it out. Otherwise, move it to the front of the Tensor</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">maybe_expand_bdim_at_front</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x_bdim</span><span class="p">)</span>
        <span class="n">ind</span> <span class="o">=</span> <span class="n">maybe_expand_bdim_at_front</span><span class="p">(</span><span class="n">ind</span><span class="p">,</span> <span class="n">ind_bdim</span><span class="p">)</span>
        <span class="n">ind_inv</span> <span class="o">=</span> <span class="n">maybe_expand_bdim_at_front</span><span class="p">(</span><span class="n">ind_inv</span><span class="p">,</span> <span class="n">ind_inv_bdim</span><span class="p">)</span>

        <span class="c1"># The return is a tuple (output, out_dims). Since output is a Tensor,</span>
        <span class="c1"># then out_dims is an Optional[int] (instead of being a Tuple).</span>
        <span class="k">return</span> <span class="n">NumpyTake</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">ind</span><span class="p">,</span> <span class="n">ind_inv</span><span class="p">,</span> <span class="n">dim</span> <span class="o">+</span> <span class="mi">1</span><span class="p">),</span> <span class="mi">0</span>

<span class="k">def</span><span class="w"> </span><span class="nf">numpy_sort</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">):</span>
    <span class="n">result</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">NumpySort</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dim</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">result</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">vmap</span><span class="p">(</span><span class="n">numpy_sort</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<span class="k">assert</span> <span class="n">torch</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">result</span><span class="p">,</span> <span class="n">numpy_sort</span><span class="p">(</span><span class="n">result</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The vmap staticmethod should aim to preserve the semantics of the
entire <a class="reference internal" href="../python-api/autograd.html#torch.autograd.Function" title="torch.autograd.Function"><code class="xref py py-class docutils literal notranslate"><span class="pre">Function</span></code></a>. That is, (pseudocode) <code class="docutils literal notranslate"><span class="pre">grad(vmap(MyFunc))</span></code>
should be replaceable with a <code class="docutils literal notranslate"><span class="pre">grad(map(MyFunc))</span></code>.</p>
<p>If your autograd.Function has any custom behavior in the backward pass, please
keep this in mind.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>It is a legitimate use case to write a custom vmap staticmethod for a
<a class="reference internal" href="../python-api/autograd.html#torch.autograd.Function" title="torch.autograd.Function"><code class="xref py py-class docutils literal notranslate"><span class="pre">Function</span></code></a> that PyTorch is able to generate a vmap
rule for via <code class="docutils literal notranslate"><span class="pre">generate_vmap_rule=True</span></code>. You may wish to do this if the
generated vmap rule doesn’t have the semantics you’re looking for.</p>
</div>
</section>
</section>
<section id="torch-func-jvp-support">
<h2><a class="reference internal" href="../python-api/generated/torch.func.jvp.html#torch.func.jvp" title="torch.func.jvp"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.func.jvp()</span></code></a> Support<a class="headerlink" href="#torch-func-jvp-support" title="Link to this heading">#</a></h2>
<p>To support forward-mode AD, a <a class="reference internal" href="../python-api/autograd.html#torch.autograd.Function" title="torch.autograd.Function"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.autograd.Function</span></code></a> must have a <a class="reference internal" href="../python-api/generated/torch.autograd.Function.jvp.html#torch.autograd.Function.jvp" title="torch.autograd.Function.jvp"><code class="xref py py-meth docutils literal notranslate"><span class="pre">jvp()</span></code></a> staticmethod.
Please see <a class="reference internal" href="extending.html#forward-ad-autograd-function"><span class="std std-ref">Forward mode AD</span></a> for details.</p>
</section>
</section>


                </article>
              
              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="extending.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Extending PyTorch</p>
      </div>
    </a>
    <a class="right-next"
       href="custom_operators.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">PyTorch Custom Operators Landing Page</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <dialog id="pst-secondary-sidebar-modal"></dialog>
                <div id="pst-secondary-sidebar" class="bd-sidebar-secondary bd-toc">
<div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
<div
    id="pst-page-navigation-heading-2"
    class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> On this page
  </div>
  <nav class="bd-toc-nav page-toc" aria-labelledby="pst-page-navigation-heading-2">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#basic-usage">Basic Usage</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-1-autograd-function-calls-into-another-system">Example 1: autograd.Function calls into another system</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-2-autograd-function-specifies-custom-gradient-rules">Example 2: autograd.Function specifies custom gradient rules</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#limitations-and-gotchas">Limitations and gotchas</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#torch-vmap-support"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.vmap()</span></code> Support</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#automatically-generate-a-vmap-rule">Automatically generate a vmap rule</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#defining-the-vmap-staticmethod">Defining the vmap staticmethod</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#torch-func-jvp-support"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.func.jvp()</span></code> Support</a></li>
</ul>
  </nav></div>

  <div class="sidebar-secondary-item">

  
  <div class="tocsection editthispage">
    <a href="https://github.com/pytorch/pytorch/edit/main/docs/source/notes/extending.func.rst">
      <i class="fa-solid fa-pencil"></i>
      
      
        
          Edit on GitHub
        
      
    </a>
  </div>
</div>

  <div class="sidebar-secondary-item">
    <div class="tocsection sourcelink">
      <a href="../_sources/notes/extending.func.rst.txt">
        <i class="fa-solid fa-file-lines"></i> Show Source
      </a>
    </div>
</div>

</div>
<div class="sidebar-secondary-item">
 <h6>PyTorch Libraries</h6>
 <ul>
 
  <li><a class="nav-link nav-external" href="https://pytorch.org/audio/stable/">torchaudio</a></li>
 
  <li><a class="nav-link nav-external" href="https://pytorch.org/ao">torchao</a></li>
 
  <li><a class="nav-link nav-external" href="https://pytorch.org/executorch">ExecuTorch</a></li>
 
  <li><a class="nav-link nav-external" href="https://pytorch.org/torchrec">torchrec</a></li>
 
  <li><a class="nav-link nav-external" href="https://pytorch.org/serve/">torchserve</a></li>
 
  <li><a class="nav-link nav-external" href="https://pytorch.org/data">torchdata</a></li>
 
  <li><a class="nav-link nav-external" href="https://pytorch.org/data">torchvision</a></li>
 
  <li><a class="nav-link nav-external" href="https://pytorch.org/xla">PyTorch on XLA Devices</a></li>
 
 </ul>
</div>
</div>
</div>
              
            
          </div>
          <footer class="bd-footer-content">
            
          </footer>
        
      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script defer src="../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf"></script>
<script defer src="../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf"></script>


  <footer class="bd-footer">
<div class="bd-footer__inner bd-page-width">
  
    <div class="footer-items__start">
      
        <div class="footer-item">

  <p class="copyright">
    
      © Copyright PyTorch Contributors.
      <br/>
    
  </p>
</div>
      
        <div class="footer-item">

  <p class="sphinx-version">
    Created using <a href="https://www.sphinx-doc.org/">Sphinx</a> 8.1.0.
    <br/>
  </p>
</div>
      
    </div>
  
  
  
    <div class="footer-items__end">
      
        <div class="footer-item">
<p class="theme-version">
  <!-- # L10n: Setting the PST URL as an argument as this does not need to be localized -->
  Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.16.1.
</p></div>
      
    </div>
  
</div>

  </footer>
  </body>
</html>